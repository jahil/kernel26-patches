diff -urN oldtree/Documentation/dsdt-initrd.txt newtree/Documentation/dsdt-initrd.txt
--- oldtree/Documentation/dsdt-initrd.txt	1969-12-31 19:00:00.000000000 -0500
+++ newtree/Documentation/dsdt-initrd.txt	2006-07-12 19:00:43.000000000 -0400
@@ -0,0 +1,98 @@
+ACPI Custom DSDT read from initramfs
+
+2003 by Markuss Gaugusch < dsdt at gaugusch dot org >
+Special thanks go to Thomas Renninger from SuSE, who updated the patch for
+2.6.0 and later modified it to read inside initramfs
+2004 - 2006 maintained by Eric Piel < eric dot piel at tremplin-utc dot net >
+
+This option is intended for people who would like to hack their DSDT and don't want
+to recompile their kernel after every change. It can also be useful to distros
+which offers pre-compiled kernels and want to allow their users to use a
+modified DSDT. In the Kernel config, enable the initial RAM filesystem support
+(in Device Drivers|Block Devices) and enable ACPI_CUSTOM_DSDT_INITRD at the ACPI
+options (General Setup|ACPI Support|Read custom DSDT from initrd).
+
+A custom DSDT (Differentiated System Description Table) is useful when your
+computer uses ACPI but problems occur due to broken implementation. Typically,
+your computer works but there are some troubles with the hardware detection or
+the power management. You can check that troubles come from errors in the DSDT by
+activating the ACPI debug option and reading the logs. This table is provided
+by the BIOS, therefore it might be a good idea to check for BIOS update on your
+vendor website before going any further. Errors are often caused by vendors
+testing their hardware only with Windows or because there is code which is
+executed only on a specific OS with a specific version and Linux hasn't been
+considered during the development.
+
+Before you run away from customising your DSDT, you should note that already
+corrected tables are available for a fair amount of computers on this web-page:
+http://acpi.sf.net/dsdt . If you are part of the unluckies who cannot find
+their hardware in this database, you can modify your DSDT by yourself. This
+process is less painful than it sounds. Download the Intel ASL 
+compiler/decompiler at http://www.intel.com/technology/IAPC/acpi/downloads.htm .
+As root, you then have to dump your DSDT and decompile it. By using the
+compiler messages as well as the kernel ACPI debug messages and the reference book
+(available at the Intel website and also at http://www.acpi.info), it is quite
+easy to obtain a fully working table.
+
+Once your new DSDT is ready you'll have to add it to an initrd so that the
+kernel can read the table at the very beginning of the boot. As the file has
+to be accessed very early during the boot process the initrd has to be an
+initramfs. The file is contained into the initramfs under the name /DSDT.aml .
+To obtain such an initrd, you might have to modify your mkinitrd script or you
+can add it later to the initrd with the script appended to this document. The
+command will look like:
+initrd-add-dsdt initrd.img my-dsdt.aml
+
+In case you don't use any initrd, the possibilities you have are to either start
+using one (try mkinitrd or yaird), or use the "Include Custom DSDT" configure
+option to directly include your DSDT inside the kernel.
+
+The message "Looking for DSDT in initramfs..." will tell you if the DSDT was
+found or not. If you need to update your DSDT, generate a new initrd and
+perform the steps above. Don't forget that with Lilo, you'll have to re-run it.
+
+
+======================= Here starts initrd-add-dsdt ===============================
+#!/bin/bash
+# Adds a DSDT file to the initrd (if it's an initramfs)
+# first argument is the name of archive
+# second argurment is the name of the file to add
+# The file will be copied as /DSDT.aml
+
+# 20060126: fix "Premature end of file" with some old cpio (Roland Robic)
+# 20060205: this time it should really work
+
+# check the arguments
+if [ $# -ne 2 ]; then
+	program_name=$(basename $0)
+	echo "\
+$program_name: too few arguments
+Usage: $program_name initrd-name.img DSDT-to-add.aml
+Adds a DSDT file to an initrd (in initramfs format)
+
+  initrd-name.img: filename of the initrd in initramfs format
+  DSDT-to-add.aml: filename of the DSDT file to add
+  " 1>&2
+    exit 1
+fi
+
+# we should check it's an initramfs
+
+tempcpio=$(mktemp -d)
+# cleanup on exit, hangup, interrupt, quit, termination
+trap 'rm -rf $tempcpio' 0 1 2 3 15
+
+# extract the archive
+gunzip -c "$1" > "$tempcpio"/initramfs.cpio || exit 1
+
+# copy the DSDT file at the root of the directory so that we can call it "/DSDT.aml"
+cp -f "$2" "$tempcpio"/DSDT.aml
+
+# add the file
+cd "$tempcpio"
+(echo DSDT.aml | cpio --quiet -H newc -o -A -O "$tempcpio"/initramfs.cpio) || exit 1
+cd "$OLDPWD"
+
+# re-compress the archive
+gzip -c "$tempcpio"/initramfs.cpio > "$1"
+
diff -urN oldtree/Documentation/fb/00-INDEX newtree/Documentation/fb/00-INDEX
--- oldtree/Documentation/fb/00-INDEX	2006-07-05 10:06:57.000000000 -0400
+++ newtree/Documentation/fb/00-INDEX	2006-07-12 19:01:04.000000000 -0400
@@ -19,6 +19,8 @@
 	- info on the Matrox frame buffer driver
 pvr2fb.txt
 	- info on the PowerVR 2 frame buffer driver
+splash.txt
+	- info on the Framebuffer Splash
 tgafb.txt
 	- info on the TGA (DECChip 21030) frame buffer driver
 vesafb.txt
diff -urN oldtree/Documentation/fb/splash.txt newtree/Documentation/fb/splash.txt
--- oldtree/Documentation/fb/splash.txt	1969-12-31 19:00:00.000000000 -0500
+++ newtree/Documentation/fb/splash.txt	2006-07-12 19:01:04.000000000 -0400
@@ -0,0 +1,207 @@
+What is it?
+-----------
+
+The framebuffer splash is a kernel feature that allows displaying a background
+picture on selected consoles.
+
+What do I need to get it to work?
+---------------------------------
+
+To get fb splash up-and-running you will have to:
+ 1) get a copy of splashutils [1] or a similar program
+ 2) get some splash themes
+ 3) build the kernel helper program
+ 4) build your kernel with the FB_SPLASH option enabled.
+
+To get fbsplash operational right after fbcon initialization is finished, you
+will have to include a theme and the kernel helper into your initramfs image.
+Please refer to splashutils documentation for instructions on how to do that.
+
+[1] The splashutils package can be downloaded from:
+    http://dev.gentoo.org/~spock/projects/splashutils/
+
+The userspace helper
+--------------------
+
+The userspace splash helper (by default: /sbin/splash_helper) is called by the
+kernel whenever an important event occurs and the kernel needs some kind of
+job to be carried out. Important events include console switches and video
+mode switches (the kernel requests background images and configuration
+parameters for the current console). The splash helper must be accessible at
+all times. If it's not, fbsplash will be switched off automatically.
+
+It's possible to set path to the splash helper by writing it to
+/proc/sys/kernel/fbsplash.
+
+*****************************************************************************
+
+The information below is mostly technical stuff. There's probably no need to
+read it unless you plan to develop a userspace helper.
+
+The splash protocol
+-------------------
+
+The splash protocol defines a communication interface between the kernel and
+the userspace splash helper.
+
+The kernel side is responsible for:
+
+ * rendering console text, using an image as a background (instead of a
+   standard solid color fbcon uses),
+ * accepting commands from the user via ioctls on the fbsplash device,
+ * calling the userspace helper to set things up as soon as the fb subsystem 
+   is initialized.
+
+The userspace helper is responsible for everything else, including parsing
+configuration files, decompressing the image files whenever the kernel needs
+it, and communicating with the kernel if necessary.
+
+The splash protocol specifies how communication is done in both ways:
+kernel->userspace and userspace->helper.
+  
+Kernel -> Userspace
+-------------------
+
+The kernel communicates with the userspace helper by calling it and specifying
+the task to be done in a series of arguments.
+
+The arguments follow the pattern:
+<splash protocol version> <command> <parameters>
+
+All commands defined in splash protocol v2 have the following parameters:
+ virtual console
+ framebuffer number
+ theme
+
+Splash protocol v1 specified an additional 'fbsplash mode' after the
+framebuffer number. Splash protocol v1 is deprecated and should not be used.
+
+Splash protocol v2 specifies the following commands:
+
+getpic
+------
+ The kernel issues this command to request image data. It's up to the 
+ userspace  helper to find a background image appropriate for the specified 
+ theme and the current resolution. The userspace helper should respond by 
+ issuing the FBIOSPLASH_SETPIC ioctl.
+
+init
+----
+ The kernel issues this command after the fbsplash device is created and
+ the fbsplash interface is initialized. Upon receiving 'init', the userspace
+ helper should parse the kernel command line (/proc/cmdline) or otherwise
+ decide whether fbsplash is to be activated.
+
+ To activate fbsplash on the first console the helper should issue the
+ FBIOSPLASH_SETCFG, FBIOSPLASH_SETPIC and FBIOSPLASH_SETSTATE commands,
+ in the above-mentioned order.
+
+ When the userspace helper is called in an early phase of the boot process
+ (right after the initialization of fbcon), no filesystems will be mounted.
+ The helper program should mount sysfs and then create the appropriate
+ framebuffer, fbsplash and tty0 devices (if they don't already exist) to get
+ current display settings and to be able to communicate with the kernel side.
+ It should probably also mount the procfs to be able to parse the kernel
+ command line parameters.
+
+ Note that the console sem is not held when the kernel calls splash_helper
+ with the 'init' command. The splash helper should perform all ioctls with
+ origin set to FB_SPLASH_IO_ORIG_USER.
+
+modechange
+----------
+ The kernel issues this command on a mode change. The helper's response should
+ be similar to the response to the 'init' command. Note that this time the
+ console sem is held and all ioctls must be performed with origin set to
+ FB_SPLASH_IO_ORIG_KERNEL.
+
+
+Userspace -> Kernel
+-------------------
+
+Userspace programs can communicate with fbsplash via ioctls on the fbsplash
+device. These ioctls are to be used by both the userspace helper (called
+only by the kernel) and userspace configuration tools (run by the users).
+
+The splash helper should set the origin field to FB_SPLASH_IO_ORIG_KERNEL
+when doing the appropriate ioctls. All userspace configuration tools should
+use FB_SPLASH_IO_ORIG_USER. Failure to set the appropriate value in the origin
+field when performing ioctls from the kernel helper will most likely result
+in a console deadlock.
+
+FB_SPLASH_IO_ORIG_KERNEL instructs fbsplash not to try to acquire the console
+semaphore. Not surprisingly, FB_SPLASH_IO_ORIG_USER instructs it to acquire
+the console sem.
+
+The framebuffer splash provides the following ioctls (all defined in 
+linux/fb.h):
+
+FBIOSPLASH_SETPIC
+description: loads a background picture for a virtual console
+argument: struct fb_splash_iowrapper*; data: struct fb_image*
+notes: 
+If called for consoles other than the current foreground one, the picture data
+will be ignored.
+
+If the current virtual console is running in a 8-bpp mode, the cmap substruct
+of fb_image has to be filled appropriately: start should be set to 16 (first
+16 colors are reserved for fbcon), len to a value <= 240 and red, green and
+blue should point to valid cmap data. The transp field is ingored. The fields
+dx, dy, bg_color, fg_color in fb_image are ignored as well.
+
+FBIOSPLASH_SETCFG
+description: sets the fbsplash config for a virtual console
+argument: struct fb_splash_iowrapper*; data: struct vc_splash*
+notes: The structure has to be filled with valid data.
+
+FBIOSPLASH_GETCFG
+description: gets the fbsplash config for a virtual console
+argument: struct fb_splash_iowrapper*; data: struct vc_splash*
+
+FBIOSPLASH_SETSTATE
+description: sets the fbsplash state for a virtual console
+argument: struct fb_splash_iowrapper*; data: unsigned int*
+          values: 0 = disabled, 1 = enabled.
+
+FBIOSPLASH_GETSTATE
+description: gets the fbsplash state for a virtual console
+argument: struct fb_splash_iowrapper*; data: unsigned int*
+          values: as in FBIOSPLASH_SETSTATE
+
+Info on used structures:
+
+Definition of struct vc_splash can be found in linux/console_splash.h. It's
+heavily commented. Note that the 'theme' field should point to a string
+no longer than FB_SPLASH_THEME_LEN. When FBIOSPLASH_GETCFG call is
+performed, the theme field should point to a char buffer of length
+FB_SPLASH_THEME_LEN.
+
+Definition of struct fb_splash_iowrapper can be found in linux/fb.h.
+The fields in this struct have the following meaning:
+
+vc: 
+Virtual console number.
+
+origin: 
+Specifies if the ioctl is performed as a response to a kernel request. The
+splash helper should set this field to FB_SPLASH_IO_ORIG_KERNEL, userspace
+programs should set it to FB_SPLASH_IO_ORIG_USER. This field is necessary to
+avoid console semaphore deadlocks.
+
+data: 
+Pointer to a data structure appropriate for the performed ioctl. Type of
+the data struct is specified in the ioctls description.
+
+*****************************************************************************
+
+Credit
+------
+
+Original 'bootsplash' project & implementation by:
+  Volker Poplawski <volker@poplawski.de>, Stefan Reinauer <stepan@suse.de>,
+  Steffen Winterfeldt <snwint@suse.de>, Michael Schroeder <mls@suse.de>,
+  Ken Wimer <wimer@suse.de>.
+
+Fbsplash, splash protocol design, current implementation & docs by:
+  Michal Januszewski <spock@gentoo.org>
+
diff -urN oldtree/Documentation/fb/vesafb.txt newtree/Documentation/fb/vesafb.txt
--- oldtree/Documentation/fb/vesafb.txt	2006-07-05 10:06:57.000000000 -0400
+++ newtree/Documentation/fb/vesafb.txt	2006-07-12 19:01:08.000000000 -0400
@@ -2,16 +2,18 @@
 What is vesafb?
 ===============
 
-This is a generic driver for a graphic framebuffer on intel boxes.
+Vesafb is a generic framebuffer driver for x86 and x86_64 boxes.
 
-The idea is simple:  Turn on graphics mode at boot time with the help
-of the BIOS, and use this as framebuffer device /dev/fb0, like the m68k
-(and other) ports do.
-
-This means we decide at boot time whenever we want to run in text or
-graphics mode.  Switching mode later on (in protected mode) is
-impossible; BIOS calls work in real mode only.  VESA BIOS Extensions
-Version 2.0 are required, because we need a linear frame buffer.
+VESA BIOS Extensions Version 2.0 are required, because we need access to
+a linear frame buffer. VBE 3.0 is required if you want to use modes with a
+higher (than the standard 60 Hz) refresh rate.
+
+The VESA framebuffer driver comes in two flavors - the standard 'vesafb'
+and 'vesafb-tng'. Vesafb-tng is available only on 32-bit x86 due to the
+technology it uses (vm86). Vesafb-tng has more features than vesafb
+(adjusting the refresh rate on VBE 3.0 compliant boards, switching the
+video mode without rebooting, selecting a mode by providing its
+modedb name, and more).
 
 Advantages:
 
@@ -29,26 +31,35 @@
 How to use it?
 ==============
 
-Switching modes is done using the vga=... boot parameter.  Read
-Documentation/svga.txt for details.
-
-You should compile in both vgacon (for text mode) and vesafb (for
-graphics mode). Which of them takes over the console depends on
-whenever the specified mode is text or graphics.
-
-The graphic modes are NOT in the list which you get if you boot with
-vga=ask and hit return. The mode you wish to use is derived from the
-VESA mode number. Here are those VESA mode numbers:
+If you are running a 32-bit x86 system and you decide to use vesafb-tng,
+you can either compile the driver into the kernel or use it as a module.
+The graphics mode you want to use is in both cases specified using the
+standard modedb format.
+
+If your system doesn't support vm86 calls, things get a little more tricky.
+Since on such systems you can't do BIOS calls from protected mode in which
+kernel runs, you have to decide at boot time whenever you want to run in text
+or in graphics mode. Switching mode later on is impossible. Switching modes
+is done using the vga=... boot parameter.  Read Documentation/svga.txt for
+details. Below is a more detailed description of what to do on systems using
+the standard vesafb driver.
+
+You should compile in both vgacon (for text mode) and vesafb (for graphics
+mode). Which of them takes over the console depends on whenever the
+specified mode is text or graphics.
+
+The graphic modes are NOT in the list which you get if you boot with vga=ask
+and hit return. The mode you wish to use is derived from the VESA mode number.
+Here are those VESA mode numbers:
 
     | 640x480  800x600  1024x768 1280x1024
 ----+-------------------------------------
-256 |  0x101    0x103    0x105    0x107   
-32k |  0x110    0x113    0x116    0x119   
-64k |  0x111    0x114    0x117    0x11A   
-16M |  0x112    0x115    0x118    0x11B   
+256 |  0x101    0x103    0x105    0x107
+32k |  0x110    0x113    0x116    0x119
+64k |  0x111    0x114    0x117    0x11A
+16M |  0x112    0x115    0x118    0x11B
 
-The video mode number of the Linux kernel is the VESA mode number plus
-0x200.
+The video mode number of the Linux kernel is the VESA mode number plus 0x200.
  
  Linux_kernel_mode_number = VESA_mode_number + 0x200
 
@@ -56,15 +67,15 @@
 
     | 640x480  800x600  1024x768 1280x1024
 ----+-------------------------------------
-256 |  0x301    0x303    0x305    0x307   
-32k |  0x310    0x313    0x316    0x319   
-64k |  0x311    0x314    0x317    0x31A   
-16M |  0x312    0x315    0x318    0x31B   
-
-To enable one of those modes you have to specify "vga=ask" in the
-lilo.conf file and rerun LILO. Then you can type in the desired
-mode at the "vga=ask" prompt. For example if you like to use 
-1024x768x256 colors you have to say "305" at this prompt.
+256 |  0x301    0x303    0x305    0x307
+32k |  0x310    0x313    0x316    0x319
+64k |  0x311    0x314    0x317    0x31A
+16M |  0x312    0x315    0x318    0x31B
+
+To enable one of those modes you have to specify "vga=ask" in the lilo.conf
+file and rerun LILO. Then you can type in the desired mode at the "vga=ask"
+prompt. For example if you like to use 1024x768x256 colors you have to say
+"305" at this prompt.
 
 If this does not work, this might be because your BIOS does not support
 linear framebuffers or because it does not support this mode at all.
@@ -72,11 +83,12 @@
 Extensions v2.0 are required, 1.2 is NOT sufficient.  You will get a
 "bad mode number" message if something goes wrong.
 
-1. Note: LILO cannot handle hex, for booting directly with 
+1. Note: LILO cannot handle hex, for booting directly with
          "vga=mode-number" you have to transform the numbers to decimal.
 2. Note: Some newer versions of LILO appear to work with those hex values,
          if you set the 0x in front of the numbers.
 
+
 X11
 ===
 
@@ -84,98 +96,164 @@
 another (accelerated) X-Server like XF86_SVGA might or might not work.
 It depends on X-Server and graphics board.
 
-The X-Server must restore the video mode correctly, else you end up
+The X-Server must restore the video mode correctly, or else you end up
 with a broken console (and vesafb cannot do anything about this).
+With vesafb-tng chances are that the console will be restored properly
+even if the X server messes up the video mode.
 
 
 Refresh rates
 =============
 
-There is no way to change the vesafb video mode and/or timings after
-booting linux.  If you are not happy with the 60 Hz refresh rate, you
-have these options:
-
- * configure and load the DOS-Tools for your the graphics board (if
-   available) and boot linux with loadlin.
- * use a native driver (matroxfb/atyfb) instead if vesafb.  If none
+With VBE 3.0 compatible BIOSes and vesafb-tng it is possible to change
+the refresh rate either at boot time (by specifying the @<rr> part of
+the mode name) or later, using the fbset utility.
+
+If you want to use the default BIOS refresh rate while switching modes
+on a running system, set pixclock to 0.
+
+With VBE 2.0 there is no way to change the mode timings after booting
+Linux. If you are not happy with the 60 Hz refresh rate, you have
+the following options:
+
+ * Configure and load the DOS tools for your the graphics board (if
+   available) and boot Linux with loadlin.
+ * Use a native driver (matroxfb/atyfb) instead of vesafb.  If none
    is available, write a new one!
- * VBE 3.0 might work too.  I have neither a gfx board with VBE 3.0
-   support nor the specs, so I have not checked this yet.
+ * Use a BIOS editor to change the default refresh rate (such an
+   editor does exist at least for ATI Radeon BIOSes).
+ * If you're running a non-vm86 and VBE 3.0 compatible system, you can
+   use a kernel patch (vesafb-rrc) to hard-code some mode timings in
+   the kernel and use these while setting the video mode at boot time.
+
+Note that there are some boards (nVidia 59**, 57** and newer models)
+claiming that their Video BIOS is VBE 3.0 compliant, while ignoring the
+CRTC values provided by software such as vesafb-tng. You'll not be able
+to adjust the refresh rate if you're using one of these boards.
 
 
 Configuration
 =============
 
-The VESA BIOS provides protected mode interface for changing
-some parameters.  vesafb can use it for palette changes and
-to pan the display.  It is turned off by default because it
-seems not to work with some BIOS versions, but there are options
-to turn it on.
-
-You can pass options to vesafb using "video=vesafb:option" on
-the kernel command line.  Multiple options should be separated
-by comma, like this: "video=vesafb:ypan,invers"
-
-Accepted options:
-
-invers	no comment...
-
-ypan	enable display panning using the VESA protected mode 
-	interface.  The visible screen is just a window of the
-	video memory, console scrolling is done by changing the
-	start of the window.
-	pro:	* scrolling (fullscreen) is fast, because there is
-		  no need to copy around data.
-		* You'll get scrollback (the Shift-PgUp thing),
-		  the video memory can be used as scrollback buffer
-	kontra: * scrolling only parts of the screen causes some
-		  ugly flicker effects (boot logo flickers for
-		  example).
-
-ywrap	Same as ypan, but assumes your gfx board can wrap-around 
-	the video memory (i.e. starts reading from top if it
-	reaches the end of video memory).  Faster than ypan.
-
-redraw	scroll by redrawing the affected part of the screen, this
-	is the safe (and slow) default.
-
-
-vgapal	Use the standard vga registers for palette changes.
-	This is the default.
-pmipal	Use the protected mode interface for palette changes.
-
-mtrr:n	setup memory type range registers for the vesafb framebuffer
-	where n:
-	      0 - disabled (equivalent to nomtrr) (default)
-	      1 - uncachable
-	      2 - write-back
-	      3 - write-combining
-	      4 - write-through
+The VESA BIOS provides protected mode interface for changing some parameters.
+vesafb can use it for palette changes and to pan the display. It is turned
+off by default because it seems not to work with some BIOS versions, but
+there are options to turn it on.
+
+You can pass options to vesafb using "video=vesafb:option" on the kernel
+command line. Multiple options should be separated by a comma, like this:
+"video=vesafb:ypan,1024x768-32@85"
+
+Note that vesafb-tng still uses the "video=vesafb:option" format of the
+kernel command line video parameter. "video=vesafb-tng:xxx" is incorrect.
+
+Accepted options (both vesafb and vesafb-tng):
+
+ypan    Enable display panning using the VESA protected mode interface
+        The visible screen is just a window of the video memory,
+        console scrolling is done by changing the start of the window.
+        pro:    * scrolling (fullscreen) is fast, because there is
+                  no need to copy around data.
+                * you'll get scrollback (the Shift-PgUp thing),
+                  the video memory can be used as scrollback buffer
+        con:    * scrolling only parts of the screen causes some
+                  ugly flicker effects (boot logo flickers for
+                  example).
+
+ywrap   Same as ypan, but assumes your gfx board can wrap-around the video
+        memory (i.e. starts reading from top if it reaches the end of
+        video memory). Faster than ypan.
+
+redraw  Scroll by redrawing the affected part of the screen, this is the
+        safe (and slow) default.
+
+vgapal  Use the standard VGA registers for palette changes.
+
+pmipal  Use the protected mode interface for palette changes.
+        This is the default is the protected mode interface is available.
+
+mtrr:n  Setup memory type range registers for the vesafb framebuffer
+        where n:
+              0 - disabled (equivalent to nomtrr) (default)
+              1 - uncachable
+              2 - write-back
+              3 - write-combining
+              4 - write-through
 
-	If you see the following in dmesg, choose the type that matches the
-	old one. In this example, use "mtrr:2".
+        If you see the following in dmesg, choose the type that matches
+        the old one. In this example, use "mtrr:2".
 ...
 mtrr: type mismatch for e0000000,8000000 old: write-back new: write-combining
 ...
 
-nomtrr  disable mtrr
+nomtrr  Do not use memory type range registers for vesafb.
 
 vremap:n
         remap 'n' MiB of video RAM. If 0 or not specified, remap memory
-	according to video mode. (2.5.66 patch/idea by Antonino Daplas
-	reversed to give override possibility (allocate more fb memory
-	than the kernel would) to 2.4 by tmb@iki.fi)
+        according to video mode. (2.5.66 patch/idea by Antonino Daplas
+        reversed to give override possibility (allocate more fb memory
+        than the kernel would) to 2.4 by tmb@iki.fi)
 
 vtotal:n
         if the video BIOS of your card incorrectly determines the total
         amount of video RAM, use this option to override the BIOS (in MiB).
 
-Have fun!
+Options accepted only by vesafb-tng:
 
-  Gerd
+<mode>  The mode you want to set, in the standard modedb format. Refer to
+        modedb.txt for a detailed description. If you specify a mode that is
+        not supported by your board's BIOS, vesafb-tng will attempt to set a
+        similar mode. The list of supported modes can be found in
+        /proc/fbx/modes, where x is the framebuffer number (usually 0).
+        When vesafb-tng is compiled as a module, the mode string should be
+        provided as a value of the parameter 'mode'.
+
+vbemode:x
+        Force the use of VBE mode x. The mode will only be set if it's
+        found in the VBE-provided list of supported modes.
+        NOTE: The mode number 'x' should be specified in VESA mode number
+        notation, not the Linux kernel one (eg. 257 instead of 769).
+        HINT: If you use this option because normal <mode> parameter does
+        not work for you and you use a X server, you'll probably want to
+        set the 'nocrtc' option to ensure that the video mode is properly
+        restored after console <-> X switches.
+
+nocrtc  Do not use CRTC timings while setting the video mode. This option
+        makes sence only with VBE 3.0 compliant systems. Use it if you have
+        problems with modes set in the standard way. Note that using this
+		option means that any refresh rate adjustments will be ignored
+		and the refresh rate will stay at your BIOS default (60 Hz).
+
+noedid  Do not try to fetch and use EDID-provided modes.
+
+noblank Disable hardware blanking.
+
+gtf     Force the use of VESA's GTF (Generalized Timing Formula). Specifying
+        this will cause vesafb to skip its internal modedb and EDID-modedb
+        and jump straight to the GTF part of the code (normally used only if
+        everything else failed). This can be useful if you want to get as
+        much as possible from your graphics board but your BIOS doesn't
+        support modes with the refresh rates you require. Note that you may 
+		need to specify the maxhf, maxvf and maxclk parameters if they are not
+        provided by the EDID block.
+
+Additionally, the following parameters may be provided. They all override the
+EDID-provided values and BIOS defaults. Refer to your monitor's specs to get
+the correct values for maxhf, maxvf and maxclk for your hardware.
+
+maxhf:n     Maximum horizontal frequency (in kHz).
+maxvf:n     Maximum vertical frequency (in Hz).
+maxclk:n    Maximum pixel clock (in MHz).
+
+Have fun!
 
 --
+Original document for the vesafb driver by
 Gerd Knorr <kraxel@goldbach.in-berlin.de>
 
-Minor (mostly typo) changes 
-by Nico Schmoigl <schmoigl@rumms.uni-mannheim.de>
+Minor (mostly typo) changes by
+Nico Schmoigl <schmoigl@rumms.uni-mannheim.de>
+
+Extended documentation for vm86, VBE 3.0 and vesafb-tng by
+Michal Januszewski <spock@gentoo.org>
+
diff -urN oldtree/Documentation/realtime-lsm.txt newtree/Documentation/realtime-lsm.txt
--- oldtree/Documentation/realtime-lsm.txt	1969-12-31 19:00:00.000000000 -0500
+++ newtree/Documentation/realtime-lsm.txt	2006-07-12 19:02:04.000000000 -0400
@@ -0,0 +1,39 @@
+
+		    Realtime Linux Security Module
+
+
+This Linux Security Module (LSM) enables realtime capabilities.  It
+was written by Torben Hohn and Jack O'Quin, under the provisions of
+the GPL (see the COPYING file).  We make no warranty concerning the
+safety, security or even stability of your system when using it.  But,
+we will fix problems if you report them.
+
+Once the LSM has been installed and the kernel for which it was built
+is running, the root user can load it and pass parameters as follows:
+
+  # modprobe realtime any=1
+
+  Any program can request realtime privileges.  This allows any local
+  user to crash the system by hogging the CPU in a tight loop or
+  locking down too much memory.  But, it is simple to administer.  :-)
+
+  # modprobe realtime gid=29
+
+  All users belonging to group 29 and programs that are setgid to that
+  group have realtime privileges.  Use any group number you like.  A
+  `gid' of -1 disables group access.
+
+  # modprobe realtime mlock=0
+
+  Grants realtime scheduling privileges without the ability to lock
+  memory using mlock() or mlockall() system calls.  This option can be
+  used in conjunction with any of the other options.
+
+After the module is loaded, its parameters can be changed dynamically
+via sysfs.
+
+  # echo 1  > /sys/module/realtime/parameters/any
+  # echo 29 > /sys/module/realtime/parameters/gid
+  # echo 1  > /sys/module/realtime/parameters/mlock
+
+Jack O'Quin, joq@joq.us
diff -urN oldtree/Documentation/sysctl/kernel.txt newtree/Documentation/sysctl/kernel.txt
--- oldtree/Documentation/sysctl/kernel.txt	2006-07-05 10:06:57.000000000 -0400
+++ newtree/Documentation/sysctl/kernel.txt	2006-07-12 19:00:11.000000000 -0400
@@ -18,6 +18,7 @@
 show up in /proc/sys/kernel:
 - acpi_video_flags
 - acct
+- compute
 - core_pattern
 - core_uses_pid
 - ctrl-alt-del
@@ -25,6 +26,7 @@
 - domainname
 - hostname
 - hotplug
+- interactive
 - java-appletviewer           [ binfmt_java, obsolete ]
 - java-interpreter            [ binfmt_java, obsolete ]
 - l2cr                        [ PPC only ]
@@ -84,6 +86,16 @@
 
 ==============================================================
 
+compute (staircase only):
+
+This flag controls the long timeslice, delayed preemption mode in the
+cpu scheduler suitable for scientific computation applications. It
+leads to large latencies so is unsuitable for normal usage.
+
+Disabled by default.
+
+==============================================================
+
 core_pattern:
 
 core_pattern is used to specify a core dumpfile pattern name.
@@ -161,6 +173,15 @@
 
 ==============================================================
 
+interactive (staircase only):
+
+This flag controls the allocation of dynamic priorities in the cpu
+scheduler. It gives low cpu using tasks high priority for lowest
+latencies. Nice value is still observed but stricter cpu proportions
+are obeyed if this tunable is disabled. Enabled by default.
+
+==============================================================
+
 l2cr: (PPC only)
 
 This flag controls the L2 cache of G3 processor boards. If
diff -urN oldtree/Documentation/sysctl/vm.txt newtree/Documentation/sysctl/vm.txt
--- oldtree/Documentation/sysctl/vm.txt	2006-07-05 10:06:57.000000000 -0400
+++ newtree/Documentation/sysctl/vm.txt	2006-07-12 19:02:29.000000000 -0400
@@ -22,6 +22,8 @@
 - dirty_background_ratio
 - dirty_expire_centisecs
 - dirty_writeback_centisecs
+- hardmaplimit
+- mapped
 - max_map_count
 - min_free_kbytes
 - laptop_mode
@@ -88,6 +90,27 @@
 
 ==============================================================
 
+hardmaplimit:
+
+This flag makes the vm adhere to the mapped value as closely as possible
+except in the most extreme vm stress where doing so would provoke an out
+of memory condition (see mapped below).
+
+Enabled by default.
+
+==============================================================
+
+mapped:
+
+This is the percentage ram that is filled with mapped pages (applications)
+before the vm will start reclaiming mapped pages by moving them to swap.
+It is altered by the relative stress of the vm at the time so is not
+strictly adhered to to prevent provoking out of memory kills.
+
+Set to 66 by default.
+
+==============================================================
+
 max_map_count:
 
 This file contains the maximum number of memory map areas a process
diff -urN oldtree/Makefile newtree/Makefile
--- oldtree/Makefile	2006-07-05 10:06:57.000000000 -0400
+++ newtree/Makefile	2006-07-13 09:08:00.000000000 -0400
@@ -1,8 +1,8 @@
 VERSION = 2
 PATCHLEVEL = 6
 SUBLEVEL = 17
-EXTRAVERSION = -mm6
-NAME=Crazed Snow-Weasel
+EXTRAVERSION = -no4
+NAME=Kickin' With G's On The West Side
 
 # *DOCUMENTATION*
 # To see a list of typical targets execute "make help"
@@ -333,6 +333,7 @@
 export CPP AR NM STRIP OBJCOPY OBJDUMP MAKE AWK GENKSYMS PERL UTS_MACHINE
 export HOSTCXX HOSTCXXFLAGS LDFLAGS_MODULE CHECK CHECKFLAGS
 export RANLIB KLIBCARCH KLIBCARCHDIR
+export NAME
 
 export CPPFLAGS NOSTDINC_FLAGS LINUXINCLUDE OBJCOPYFLAGS LDFLAGS
 export CFLAGS CFLAGS_KERNEL CFLAGS_MODULE
diff -urN oldtree/arch/i386/Kconfig newtree/arch/i386/Kconfig
--- oldtree/arch/i386/Kconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/i386/Kconfig	2006-07-12 19:03:48.000000000 -0400
@@ -399,9 +399,9 @@
 
 config MICROCODE
 	tristate "/dev/cpu/microcode - Intel IA32 CPU microcode support"
+	select FW_LOADER
 	---help---
-	  If you say Y here and also to "/dev file system support" in the
-	  'File systems' section, you will be able to update the microcode on
+	  If you say Y here, you will be able to update the microcode on
 	  Intel processors in the IA32 family, e.g. Pentium Pro, Pentium II,
 	  Pentium III, Pentium 4, Xeon etc.  You will obviously need the
 	  actual microcode binary data itself which is not shipped with the
@@ -497,7 +497,7 @@
 
 choice
 	depends on EXPERIMENTAL && !X86_PAE
-	prompt "Memory split" if EMBEDDED
+	prompt "Memory split"
 	default VMSPLIT_3G
 	help
 	  Select the desired split between kernel and user memory.
@@ -516,13 +516,13 @@
 	  option alone!
 
 	config VMSPLIT_3G
-		bool "3G/1G user/kernel split"
+		bool "Default 896MB lowmem (3G/1G user/kernel split)"
 	config VMSPLIT_3G_OPT
-		bool "3G/1G user/kernel split (for full 1G low memory)"
+		bool "1GB lowmem (3G/1G user/kernel split)"
 	config VMSPLIT_2G
-		bool "2G/2G user/kernel split"
+		bool "2GB lowmem (2G/2G user/kernel split)"
 	config VMSPLIT_1G
-		bool "1G/3G user/kernel split"
+		bool "3GB lowmem (1G/3G user/kernel split)"
 endchoice
 
 config PAGE_OFFSET
diff -urN oldtree/arch/i386/boot/video.S newtree/arch/i386/boot/video.S
--- oldtree/arch/i386/boot/video.S	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/i386/boot/video.S	2006-07-12 19:01:08.000000000 -0400
@@ -165,10 +165,12 @@
 # parameters in the default 80x25 mode -- these are set directly,
 # because some very obscure BIOSes supply insane values.
 mode_params:
+#ifdef CONFIG_FB_VESA_STD
 #ifdef CONFIG_VIDEO_SELECT
 	cmpb	$0, graphic_mode
 	jnz	mopar_gr
 #endif
+#endif
 	movb	$0x03, %ah			# Read cursor position
 	xorb	%bh, %bh
 	int	$0x10
@@ -201,6 +203,7 @@
 	ret
 
 #ifdef CONFIG_VIDEO_SELECT
+#ifdef CONFIG_FB_VESA_STD
 # Fetching of VESA frame buffer parameters
 mopar_gr:
 	leaw	modelist+1024, %di
@@ -283,6 +286,7 @@
 	movw	%es, %fs:(PARAM_VESAPM_SEG)
 	movw	%di, %fs:(PARAM_VESAPM_OFF)
 no_pm:	ret
+#endif
 
 # The video mode menu
 mode_menu:
@@ -497,10 +501,12 @@
 	
 	cmpb	$VIDEO_FIRST_V7>>8, %ah
 	jz	setv7
-	
+
+#ifdef CONFIG_FB_VESA_STD
 	cmpb	$VIDEO_FIRST_VESA>>8, %ah
 	jnc	check_vesa
-	
+#endif	
+
 	orb	%ah, %ah
 	jz	setmenu
 	
@@ -572,6 +578,7 @@
 	movw	-4(%si), %ax			# Fetch mode ID
 	jmp	_m_s
 
+#ifdef CONFIG_FB_VESA_STD
 check_vesa:
 	leaw	modelist+1024, %di
 	subb	$VIDEO_FIRST_VESA>>8, %bh
@@ -605,6 +612,7 @@
 	ret
 
 _setbad:	jmp	setbad          	# Ugly...
+#endif
 
 # Recalculate vertical display end registers -- this fixes various
 # inconsistencies of extended modes on many adapters. Called when
diff -urN oldtree/arch/i386/defconfig newtree/arch/i386/defconfig
--- oldtree/arch/i386/defconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/i386/defconfig	2006-07-12 19:00:51.000000000 -0400
@@ -71,19 +71,6 @@
 # CONFIG_LBD is not set
 
 #
-# IO Schedulers
-#
-CONFIG_IOSCHED_NOOP=y
-# CONFIG_IOSCHED_AS is not set
-# CONFIG_IOSCHED_DEADLINE is not set
-CONFIG_IOSCHED_CFQ=y
-# CONFIG_DEFAULT_AS is not set
-# CONFIG_DEFAULT_DEADLINE is not set
-CONFIG_DEFAULT_CFQ=y
-# CONFIG_DEFAULT_NOOP is not set
-CONFIG_DEFAULT_IOSCHED="cfq"
-
-#
 # Processor type and features
 #
 CONFIG_X86_PC=y
@@ -135,9 +122,6 @@
 CONFIG_X86_TSC=y
 # CONFIG_HPET_TIMER is not set
 # CONFIG_SMP is not set
-CONFIG_PREEMPT_NONE=y
-# CONFIG_PREEMPT_VOLUNTARY is not set
-# CONFIG_PREEMPT is not set
 CONFIG_X86_UP_APIC=y
 CONFIG_X86_UP_IOAPIC=y
 CONFIG_X86_LOCAL_APIC=y
@@ -182,10 +166,6 @@
 # CONFIG_EFI is not set
 CONFIG_REGPARM=y
 # CONFIG_SECCOMP is not set
-CONFIG_HZ_100=y
-# CONFIG_HZ_250 is not set
-# CONFIG_HZ_1000 is not set
-CONFIG_HZ=100
 # CONFIG_KEXEC is not set
 CONFIG_PHYSICAL_START=0x100000
 CONFIG_DOUBLEFAULT=y
diff -urN oldtree/arch/i386/kernel/cpu/Makefile newtree/arch/i386/kernel/cpu/Makefile
--- oldtree/arch/i386/kernel/cpu/Makefile	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/i386/kernel/cpu/Makefile	2006-07-12 19:02:09.000000000 -0400
@@ -4,14 +4,14 @@
 
 obj-y	:=	common.o proc.o
 
-obj-y	+=	amd.o
-obj-y	+=	cyrix.o
-obj-y	+=	centaur.o
-obj-y	+=	transmeta.o
-obj-y	+=	intel.o intel_cacheinfo.o
-obj-y	+=	rise.o
-obj-y	+=	nexgen.o
-obj-y	+=	umc.o
+obj-$(CONFIG_CPU_SUP_AMD)	+=	amd.o
+obj-$(CONFIG_CPU_SUP_CYRIX)	+=	cyrix.o
+obj-$(CONFIG_CPU_SUP_CENTAUR)	+=	centaur.o
+obj-$(CONFIG_CPU_SUP_TRANSMETA)	+=	transmeta.o
+obj-$(CONFIG_CPU_SUP_INTEL)	+=	intel.o intel_cacheinfo.o
+obj-$(CONFIG_CPU_SUP_RISE)	+=	rise.o
+obj-$(CONFIG_CPU_SUP_NEXGEN)	+=	nexgen.o
+obj-$(CONFIG_CPU_SUP_UMC)	+=	umc.o
 
 obj-$(CONFIG_X86_MCE)	+=	mcheck/
 
diff -urN oldtree/arch/i386/kernel/cpu/common.c newtree/arch/i386/kernel/cpu/common.c
--- oldtree/arch/i386/kernel/cpu/common.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/i386/kernel/cpu/common.c	2006-07-12 19:02:09.000000000 -0400
@@ -11,6 +11,7 @@
 #include <asm/msr.h>
 #include <asm/io.h>
 #include <asm/mmu_context.h>
+#include <asm/uaccess.h>
 #include <asm/mtrr.h>
 #include <asm/mce.h>
 #ifdef CONFIG_X86_LOCAL_APIC
@@ -316,7 +317,9 @@
 		}
 	}
 
+#ifdef CONFIG_CPU_SUP_INTEL
 	early_intel_workaround(c);
+#endif
 
 #ifdef CONFIG_X86_HT
 	c->phys_proc_id = (cpuid_ebx(1) >> 24) & 0xff;
@@ -563,15 +566,33 @@
 
 void __init early_cpu_init(void)
 {
+#ifdef CONFIG_CPU_SUP_INTEL
 	intel_cpu_init();
+#endif
+#ifdef CONFIG_CPU_SUP_CYRIX
 	cyrix_init_cpu();
+#endif
+#ifdef CONFIG_CPU_SUP_NSC
 	nsc_init_cpu();
+#endif
+#ifdef CONFIG_CPU_SUP_AMD
 	amd_init_cpu();
+#endif
+#ifdef CONFIG_CPU_SUP_CENTAUR
 	centaur_init_cpu();
+#endif
+#ifdef CONFIG_CPU_SUP_TRANSMETA
 	transmeta_init_cpu();
+#endif
+#ifdef CONFIG_CPU_SUP_RISE
 	rise_init_cpu();
+#endif
+#ifdef CONFIG_CPU_SUP_NEXGEN
 	nexgen_init_cpu();
+#endif
+#ifdef CONFIG_CPU_SUP_UMC
 	umc_init_cpu();
+#endif
 	early_cpu_detect();
 
 #ifdef CONFIG_DEBUG_PAGEALLOC
diff -urN oldtree/arch/i386/kernel/cpu/cpufreq/Kconfig newtree/arch/i386/kernel/cpu/cpufreq/Kconfig
--- oldtree/arch/i386/kernel/cpu/cpufreq/Kconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/i386/kernel/cpu/cpufreq/Kconfig	2006-07-12 19:03:04.000000000 -0400
@@ -2,6 +2,9 @@
 # CPU Frequency scaling
 #
 
+# This file has been patched with Linux PHC: http://linux-phc.sourceforge.net
+# Patch version: linux-phc-0.2.5-kernel-vanilla-2.6.17.patch
+
 menu "CPU Frequency scaling"
 
 source "drivers/cpufreq/Kconfig"
@@ -107,13 +110,36 @@
 config X86_SPEEDSTEP_CENTRINO
 	tristate "Intel Enhanced SpeedStep"
 	select CPU_FREQ_TABLE
-	select X86_SPEEDSTEP_CENTRINO_TABLE if (!X86_SPEEDSTEP_CENTRINO_ACPI)
+	select X86_SPEEDSTEP_CENTRINO_ACPI if (!X86_SPEEDSTEP_CENTRINO_BUILTIN || (!X86_SPEEDSTEP_CENTRINO_BUILTIN_BANIAS && !X86_SPEEDSTEP_CENTRINO_BUILTIN_DOTHAN && !X86_SPEEDSTEP_CENTRINO_BUILTIN_SONOMA ))
 	help
 	  This adds the CPUFreq driver for Enhanced SpeedStep enabled
 	  mobile CPUs.  This means Intel Pentium M (Centrino) CPUs. However,
-	  you also need to say Y to "Use ACPI tables to decode..." below
-	  [which might imply enabling ACPI] if you want to use this driver
-	  on non-Banias CPUs.
+	  you also need to say Y below to at least one of the following options:
+	   - "Use ACPI tables to decode..." [which might imply enabling ACPI]
+	   - "Built-in Tables for ... CPUs"
+
+	  You can also say yes to all of these options. In this configuration the
+	  driver will first try to use ACPI. Then if it fails it will try to use
+	  a built-in table if there is one matching the CPU.
+
+	  For details, take a look at <file:Documentation/cpu-freq/>.
+
+	  If in doubt, say N.
+
+config X86_SPEEDSTEP_CENTRINO_SYSFS
+	bool "Userspace control of CPU frequency/voltage table"
+	depends on X86_SPEEDSTEP_CENTRINO
+	depends on SYSFS
+	depends on (X86_SPEEDSTEP_CENTRINO_BUILTIN && (X86_SPEEDSTEP_CENTRINO_BUILTIN_BANIAS || X86_SPEEDSTEP_CENTRINO_BUILTIN_DOTHAN || X86_SPEEDSTEP_CENTRINO_BUILTIN_SONOMA )) || X86_SPEEDSTEP_CENTRINO_ACPI || X86_SPEEDSTEP_CENTRINO_DEFAULT
+	default y
+	help
+	  Add support for user space control of the CPU frequency/voltage 
+	  operating points table through a sysfs interface.
+
+	  If you say Y here files will be created in 
+	  /sys/devices/system/cpu/cpu*/cpufreq/op_points_table
+	  allowing reading and writing of the current table values as well as 
+	  adding or removing operating points.
 
 	  For details, take a look at <file:Documentation/cpu-freq/>.
 
@@ -126,20 +152,68 @@
 	default y
 	help
 	  Use primarily the information provided in the BIOS ACPI tables
-	  to determine valid CPU frequency and voltage pairings. It is
-	  required for the driver to work on non-Banias CPUs.
+	  to determine valid CPU frequency and voltage pairings.
+	  It is required for the driver to work on CPUs with no built-in
+	  table available
 
 	  If in doubt, say Y.
 
-config X86_SPEEDSTEP_CENTRINO_TABLE
-	bool "Built-in tables for Banias CPUs"
+config X86_SPEEDSTEP_CENTRINO_BUILTIN
+	bool "Built-in tables"
 	depends on X86_SPEEDSTEP_CENTRINO
 	default y
 	help
-	  Use built-in tables for Banias CPUs if ACPI encoding
+	  Use "hard coded" built-in tables if ACPI decoding
 	  is not available.
 
-	  If in doubt, say N.
+	  If you say Y here you must select at least one of the CPU below.
+
+	  If you are not sure of your exact CPU model you can select several CPU
+	  models or all of them. The driver will only use the table that match
+	  the exact CPU name and family/model/stepping numbers.
+	  Selecting all the built-in tables will only add a small size overhead
+	  to the kernel and an insignificant extra time to intialize the driver.
+
+	  If both ACPI and built-in tables support are enabled then built-in
+	  tables will be used only if ACPI table decoding fails.
+
+	  If you want to force usage of built-in tables over ACPI you need to say
+	  Y here and N to X86_SPEEDSTEP_CENTRINO_ACPI.
+
+	  If in doubt, say Y.
+
+config X86_SPEEDSTEP_CENTRINO_BUILTIN_BANIAS
+	bool "Built-in tables for Banias CPUs"
+	depends on X86_SPEEDSTEP_CENTRINO_BUILTIN
+	default y
+	help
+	  Use built-in tables for Banias CPUs if ACPI encoding is not available.
+	  Banias CPUs are the first generation of Pentium-M, with a 1 MB L2 cache
+	  and 400 MHz FSB manufactured on 0.13 micron process.
+
+	  If in doubt, say Y.
+
+config X86_SPEEDSTEP_CENTRINO_BUILTIN_DOTHAN
+	bool "Built-in tables for Dothan CPUs"
+	depends on X86_SPEEDSTEP_CENTRINO_BUILTIN
+	default y
+	help
+	  Use built-in tables for Dothan CPUs if ACPI encoding is not available.
+	  Dothan CPUs are the second generation of Pentium-M, with a 2 MB L2
+	  cache and 400 MHz FSB manufactured on 90 nm process.
+
+	  If in doubt, say Y.
+
+config X86_SPEEDSTEP_CENTRINO_BUILTIN_SONOMA
+	bool "Built-in tables for Sonoma CPUs"
+	depends on X86_SPEEDSTEP_CENTRINO_BUILTIN
+	default y
+	help
+	  Use built-in tables for Sonoma CPUs if ACPI encoding is not available.
+	  Sonoma CPUs are the third generation of Pentium-M, with a 2 MB L2 cache
+	  and 533 MHz FSB manufactured on 90 nm process.
+
+	  If in doubt, say Y.
 
 config X86_SPEEDSTEP_ICH
 	tristate "Intel Speedstep on ICH-M chipsets (ioport interface)"
diff -urN oldtree/arch/i386/kernel/cpu/cpufreq/speedstep-centrino.c newtree/arch/i386/kernel/cpu/cpufreq/speedstep-centrino.c
--- oldtree/arch/i386/kernel/cpu/cpufreq/speedstep-centrino.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/i386/kernel/cpu/cpufreq/speedstep-centrino.c	2006-07-12 19:03:04.000000000 -0400
@@ -2,21 +2,31 @@
  * cpufreq driver for Enhanced SpeedStep, as found in Intel's Pentium
  * M (part of the Centrino chipset).
  *
- * Since the original Pentium M, most new Intel CPUs support Enhanced
- * SpeedStep.
- *
  * Despite the "SpeedStep" in the name, this is almost entirely unlike
  * traditional SpeedStep.
  *
  * Modelled on speedstep.c
  *
  * Copyright (C) 2003 Jeremy Fitzhardinge <jeremy@goop.org>
+ *
+ * WARNING WARNING WARNING
+ *
+ * This driver manipulates the PERF_CTL MSR, which is only somewhat
+ * documented.  While it seems to work on my laptop, it has not been
+ * tested anywhere else, and it may not work for you, do strange
+ * things or simply crash.
+ */
+
+/*
+ * This file has been patched with Linux PHC: http://linux-phc.sourceforge.net
+ * Patch version: linux-phc-0.2.5-kernel-vanilla-2.6.17.patch
  */
 
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/init.h>
 #include <linux/cpufreq.h>
+#include <linux/config.h>
 #include <linux/sched.h>	/* current */
 #include <linux/delay.h>
 #include <linux/compiler.h>
@@ -31,7 +41,7 @@
 #include <asm/cpufeature.h>
 
 #define PFX		"speedstep-centrino: "
-#define MAINTAINER	"cpufreq@lists.linux.org.uk"
+#define MAINTAINER	"Jeremy Fitzhardinge <jeremy@goop.org>"
 
 #define dprintk(msg...) cpufreq_debug_printk(CPUFREQ_DEBUG_DRIVER, "speedstep-centrino", msg)
 
@@ -48,6 +58,7 @@
 	CPU_DOTHAN_A1,
 	CPU_DOTHAN_A2,
 	CPU_DOTHAN_B0,
+	CPU_DOTHAN_C0,
 	CPU_MP4HT_D0,
 	CPU_MP4HT_E0,
 };
@@ -57,6 +68,7 @@
 	[CPU_DOTHAN_A1]	= { 6, 13, 1 },
 	[CPU_DOTHAN_A2]	= { 6, 13, 2 },
 	[CPU_DOTHAN_B0]	= { 6, 13, 6 },
+	[CPU_DOTHAN_C0]	= { 6, 13, 8 },
 	[CPU_MP4HT_D0]	= {15,  3, 4 },
 	[CPU_MP4HT_E0]	= {15,  4, 1 },
 };
@@ -67,8 +79,8 @@
 	const struct cpu_id *cpu_id;
 	const char	*model_name;
 	unsigned	max_freq; /* max clock in kHz */
-
 	struct cpufreq_frequency_table *op_points; /* clock/voltage pairs */
+	unsigned	base_freq; /* base frequency used to convert between clock rates and MSR: FSB/4 in kHz */
 };
 static int centrino_verify_cpu_id(const struct cpuinfo_x86 *c, const struct cpu_id *x);
 
@@ -78,7 +90,9 @@
 
 static struct cpufreq_driver centrino_driver;
 
-#ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_TABLE
+#ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN
+
+#ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_BANIAS
 
 /* Computes the correct form for IA32_PERF_CTL MSR for a particular
    frequency/voltage operating point; frequency in MHz, volts in mV.
@@ -126,7 +140,6 @@
 	{ .frequency = CPUFREQ_TABLE_END }
 };
 
-
 /* Low Voltage Intel Pentium M processor 1.20GHz (Banias) */
 static struct cpufreq_frequency_table banias_1200[] =
 {
@@ -203,13 +216,243 @@
 	.model_name	= "Intel(R) Pentium(R) M processor " name "MHz", \
 	.max_freq	= (max)*1000,	\
 	.op_points	= banias_##max,	\
+	.base_freq = 100000,		\
 }
 #define BANIAS(max)	_BANIAS(&cpu_ids[CPU_BANIAS], max, #max)
 
+#endif /* CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_BANIAS */
+
+#ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_DOTHAN
+/* Dothan processor datasheet 30218903.pdf defines 4 voltages for each
+   frequency (VID#A through VID#D) - this macro allows us to define all
+   of these but we only use the VID#A voltages at compile time - this may
+   need some work if we want to select the voltage profile at runtime. */
+
+#define OP(mhz, mva, mvb, mvc, mvd)					\
+	{								\
+		.frequency = (mhz) * 1000,				\
+		.index = (((mhz)/100) << 8) | ((mva - 700) / 16)       	\
+	}
+
+/* Intel Pentium M processor 733 / 1.10GHz (Dothan) */
+static struct cpufreq_frequency_table dothan_1100[] =
+{
+ 	OP( 600, 700, 700, 700, 700),
+ 	OP( 800, 748, 748, 748, 748),
+ 	OP( 900, 764, 764, 764, 764),
+ 	OP(1000, 812, 812, 812, 812),
+ 	OP(1100, 844, 844, 844, 844),
+ 	{ .frequency = CPUFREQ_TABLE_END }
+};
+
+/* Intel Pentium M processor 710 / 1.40GHz (Dothan) */
+static struct cpufreq_frequency_table dothan_1400[] =
+{
+
+	OP( 600,  988,  988,  988,  988),
+	OP( 800, 1068, 1068, 1068, 1052),
+	OP(1000, 1148, 1148, 1132, 1116),
+	OP(1200, 1228, 1212, 1212, 1180),
+	OP(1400, 1340, 1324, 1308, 1276),
+	{ .frequency = CPUFREQ_TABLE_END }
+};
+
+/* Intel Pentium M processor 715 / 1.50GHz (Dothan) */
+static struct cpufreq_frequency_table dothan_1500[] =
+{
+	OP( 600,  988,  988,  988,  988),
+	OP( 800, 1068, 1068, 1068, 1052),
+	OP(1000, 1148, 1148, 1132, 1116),
+	OP(1200, 1228, 1212, 1212, 1180),
+	OP(1500, 1340, 1324, 1308, 1276),
+	{ .frequency = CPUFREQ_TABLE_END }
+};
+
+/* Intel Pentium M processor 725 / 1.60GHz (Dothan) */
+static struct cpufreq_frequency_table dothan_1600[] =
+{
+	OP( 600,  988,  988,  988,  988),
+	OP( 800, 1068, 1068, 1052, 1052),
+	OP(1000, 1132, 1132, 1116, 1116),
+	OP(1200, 1212, 1196, 1180, 1164),
+	OP(1400, 1276, 1260, 1244, 1228),
+	OP(1600, 1340, 1324, 1308, 1276),
+	{ .frequency = CPUFREQ_TABLE_END }
+};
+
+/* Intel Pentium M processor 735 / 1.70GHz (Dothan) */
+static struct cpufreq_frequency_table dothan_1700[] =
+{
+	OP( 600,  988,  988,  988,  988),
+	OP( 800, 1052, 1052, 1052, 1052),
+	OP(1000, 1116, 1116, 1116, 1100),
+	OP(1200, 1180, 1180, 1164, 1148),
+	OP(1400, 1244, 1244, 1228, 1212),
+	OP(1700, 1340, 1324, 1308, 1276),
+	{ .frequency = CPUFREQ_TABLE_END }
+};
+
+/* Intel Pentium M processor 745 / 1.80GHz (Dothan) */
+static struct cpufreq_frequency_table dothan_1800[] =
+{
+	OP( 600,  988,  988,  988,  988),
+	OP( 800, 1052, 1052, 1052, 1036),
+	OP(1000, 1116, 1100, 1100, 1084),
+	OP(1200, 1164, 1164, 1148, 1132),
+	OP(1400, 1228, 1212, 1212, 1180),
+	OP(1600, 1292, 1276, 1260, 1228),
+	OP(1800, 1340, 1324, 1308, 1276),
+	{ .frequency = CPUFREQ_TABLE_END }
+};
+
+/* Intel Pentium M processor 755 / 2.00GHz (Dothan) */
+static struct cpufreq_frequency_table dothan_2000[] =
+{
+	OP( 600,  988,  988,  988,  988),
+	OP( 800, 1052, 1036, 1036, 1036),
+	OP(1000, 1100, 1084, 1084, 1084),
+	OP(1200, 1148, 1132, 1132, 1116),
+	OP(1400, 1196, 1180, 1180, 1164),
+	OP(1600, 1244, 1228, 1228, 1196),
+	OP(1800, 1292, 1276, 1276, 1244),
+	OP(2000, 1340, 1324, 1308, 1276),
+	{ .frequency = CPUFREQ_TABLE_END }
+};
+
+#undef OP
+
+#define DOTHAN(cpuid, max, name)	\
+{	.cpu_id		= cpuid,	\
+	.model_name	= "Intel(R) Pentium(R) M processor " name "GHz", \
+	.max_freq	= (max)*1000,	\
+	.op_points	= dothan_##max,	\
+	.base_freq = 100000,		\
+}
+
+#endif /* CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_DOTHAN */
+
+
+#ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_SONOMA
+
+/* Intel datasheets 30526202.pdf define voltages only for highest and 
+   lowest frequency modes (HFM and LFM). 
+   For LFM the datasheet gives one typical voltage: LFMVccTyp.
+   For HFM the datasheet gives a min and a max voltage: HFMVccMin and HFMVccMax.
+   The tables below are using HFMVccMax for the highest frequency to be on
+   the safe side. The voltages of the intermediate frequencies are linearly 
+   interpolated from LFMVccTyp and HFMVccMax as it is what I have observed
+   to be used by the ACPI tables of my laptop and of some other's one.
+
+   LFMVccTyp is 988 mv for all models
+   HFMVccMin is 1260 mv for all models
+   HFMVccMax is 1356 mv for models 730, 740, 750 and 760.
+   HFMVccMax is 1372 mv for model 770.
+   HFMVccMax is 1404 mv for model 780.
+
+   As only the first voltage of each row of the tables are used I have put 
+   there the values interpolated  from HFMVccMax rounded to the next higher 16 mV step
+   For reference I have put in the other 3 columns:
+   values interpolated from HFMVccMax rounded to the nearest 1 mv
+   values interpolated from HFMVccMin rounded to the next higher 16 mv step
+   values interpolated from HFMVccMin rounded to the nearest 1 mv
+*/
+
+#define OPEX(mhz, base, mva, mvb, mvc, mvd)			\
+{								\
+	.frequency = (mhz) * 1000,				\
+	.index = (((mhz)/(base)) << 8) | ((mva - 700) / 16)	\
+}
+
+/* Intel Pentium M processor 730 / 1.60 GHz (Sonoma) */
+static struct cpufreq_frequency_table sonoma_1596[] =
+{
+	OPEX( 798, 133,  988,  988,  988,  988),
+	OPEX(1064, 133, 1116, 1111, 1084, 1079),
+	OPEX(1330, 133, 1244, 1233, 1180, 1169),
+	OPEX(1596, 133, 1356, 1356, 1260, 1260),
+	{ .frequency = CPUFREQ_TABLE_END }
+};
+
+/* Intel Pentium M processor 740 / 1.73 GHz (Sonoma) */
+static struct cpufreq_frequency_table sonoma_1729[] =
+{
+	OPEX( 798, 133,  988,  988,  988,  988),
+	OPEX(1064, 133, 1100, 1093, 1068, 1066),
+	OPEX(1330, 133, 1212, 1198, 1148, 1143),
+	OPEX(1729, 133, 1356, 1356, 1260, 1260),
+	{ .frequency = CPUFREQ_TABLE_END }
+};
+
+/* Intel Pentium M processor 750 / 1.86 GHz (Sonoma) */
+static struct cpufreq_frequency_table sonoma_1862[] =
+{
+	OPEX( 798, 133,  988,  988,  988,  988),
+	OPEX(1064, 133, 1084, 1080, 1068, 1056),
+	OPEX(1330, 133, 1180, 1172, 1132, 1124),
+	OPEX(1596, 133, 1276, 1264, 1196, 1192),
+	OPEX(1862, 133, 1356, 1356, 1260, 1260),
+	{ .frequency = CPUFREQ_TABLE_END }
+};
+
+/* Intel Pentium M processor 760 / 2.00 GHz (Sonoma) */
+static struct cpufreq_frequency_table sonoma_1995[] =
+{
+	OPEX( 798, 133, 988, 988, 988, 988),
+	OPEX(1064, 133, 1084, 1070, 1052, 1048),
+	OPEX(1330, 133, 1164, 1152, 1116, 1109),
+	OPEX(1596, 133, 1244, 1233, 1180, 1169),
+	OPEX(1995, 133, 1356, 1356, 1260, 1260),
+	{ .frequency = CPUFREQ_TABLE_END }
+};
+
+/* Intel Pentium M processor 770 / 2.13 GHz (Sonoma) */
+static struct cpufreq_frequency_table sonoma_2128[] =
+{
+	OPEX( 798, 133, 988, 988, 988, 988),
+	OPEX(1064, 133, 1068, 1065, 1052, 1042),
+	OPEX(1330, 133, 1148, 1142, 1100, 1097),
+	OPEX(1596, 133, 1228, 1218, 1164, 1151),
+	OPEX(1862, 133, 1308, 1295, 1212, 1206),
+	OPEX(2128, 133, 1372, 1372, 1260, 1260),
+	{ .frequency = CPUFREQ_TABLE_END }
+};
+
+/* Intel Pentium M processor 780 / 2.26 GHz (Sonoma) */
+static struct cpufreq_frequency_table sonoma_2261[] =
+{
+	OPEX( 798, 133, 988, 988, 988, 988),
+	OPEX(1064, 133, 1068, 1064, 1052, 1037),
+	OPEX(1330, 133, 1148, 1139, 1100, 1087),
+	OPEX(1596, 133, 1228, 1215, 1148, 1136),
+	OPEX(1862, 133, 1292, 1291, 1196, 1186),
+	OPEX(2261, 133, 1404, 1404, 1260, 1260),
+	{ .frequency = CPUFREQ_TABLE_END }
+};
+
+#undef OPEX
+
+#define SONOMA(cpuid, max, base, name)	\
+{	.cpu_id		= cpuid,	\
+	.model_name	= "Intel(R) Pentium(R) M processor " name "GHz", \
+	.max_freq	= (max)*1000,	\
+	.op_points	= sonoma_##max,	\
+	.base_freq	= (base)*1000,	\
+}
+
+#endif /* CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_SONOMA */
+
+
+#ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_YONAH
+// To Do
+#endif /* CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_YONAH */
+
+
 /* CPU models, their operating frequency range, and freq/voltage
    operating points */
 static struct cpu_model models[] =
 {
+#ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_BANIAS
+	/* Builtin tables for Banias CPUs */
 	_BANIAS(&cpu_ids[CPU_BANIAS], 900, " 900"),
 	BANIAS(1000),
 	BANIAS(1100),
@@ -219,18 +462,51 @@
 	BANIAS(1500),
 	BANIAS(1600),
 	BANIAS(1700),
+#endif /* CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_BANIAS */
 
-	/* NULL model_name is a wildcard */
-	{ &cpu_ids[CPU_DOTHAN_A1], NULL, 0, NULL },
-	{ &cpu_ids[CPU_DOTHAN_A2], NULL, 0, NULL },
-	{ &cpu_ids[CPU_DOTHAN_B0], NULL, 0, NULL },
-	{ &cpu_ids[CPU_MP4HT_D0], NULL, 0, NULL },
-	{ &cpu_ids[CPU_MP4HT_E0], NULL, 0, NULL },
+#ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_DOTHAN
+	/* Builtin tables for Dothan B0 CPUs */
+	DOTHAN(&cpu_ids[CPU_DOTHAN_B0], 1100, "1.10"),
+	DOTHAN(&cpu_ids[CPU_DOTHAN_B0], 1400, "1.40"),
+	DOTHAN(&cpu_ids[CPU_DOTHAN_B0], 1500, "1.50"),
+	DOTHAN(&cpu_ids[CPU_DOTHAN_B0], 1600, "1.60"),
+	DOTHAN(&cpu_ids[CPU_DOTHAN_B0], 1700, "1.70"),
+	DOTHAN(&cpu_ids[CPU_DOTHAN_B0], 1800, "1.80"),
+	DOTHAN(&cpu_ids[CPU_DOTHAN_B0], 2000, "2.00"),
+#endif /* CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_DOTHAN */
+
+#ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_SONOMA
+	/* Builtin tables for Dothan C0 CPUs, a.k.a Sonoma */
+	SONOMA(&cpu_ids[CPU_DOTHAN_C0], 1596, 133, "1.60"),
+	SONOMA(&cpu_ids[CPU_DOTHAN_C0], 1729, 133, "1.73"),
+	SONOMA(&cpu_ids[CPU_DOTHAN_C0], 1862, 133, "1.86"),
+	SONOMA(&cpu_ids[CPU_DOTHAN_C0], 1995, 133, "2.00"),
+	SONOMA(&cpu_ids[CPU_DOTHAN_C0], 2128, 133, "2.13"),
+	SONOMA(&cpu_ids[CPU_DOTHAN_C0], 2261, 133, "2.26"),
+#endif /* CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_SONOMA */
+
+#ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_YONAH
+	/* Builtin tables for Yonah CPUs */
+	// To Do
+#endif /* CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN_YONAH */
+
+	/* NULL model_name is a wildcard to catch known CPU IDs for which
+	 * we don't have any builtin table */
+	{ &cpu_ids[CPU_BANIAS], NULL, 0, NULL, 0 },
+	{ &cpu_ids[CPU_DOTHAN_A1], NULL, 0, NULL, 0 },
+	{ &cpu_ids[CPU_DOTHAN_A2], NULL, 0, NULL, 0 },
+	{ &cpu_ids[CPU_DOTHAN_B0], NULL, 0, NULL, 0 },
+	{ &cpu_ids[CPU_DOTHAN_C0], NULL, 0, NULL, 0 },
+	{ &cpu_ids[CPU_MP4HT_D0], NULL, 0, NULL, 0 },
+	{ &cpu_ids[CPU_MP4HT_E0], NULL, 0, NULL, 0 },
 
+	/* End of the table */
 	{ NULL, }
 };
 #undef _BANIAS
 #undef BANIAS
+#undef DOTHAN
+#undef SONOMA
 
 static int centrino_cpu_init_table(struct cpufreq_policy *policy)
 {
@@ -245,7 +521,7 @@
 
 	if (model->cpu_id == NULL) {
 		/* No match at all */
-		dprintk("no support for CPU model \"%s\": "
+		dprintk(KERN_INFO PFX "no support for CPU model \"%s\": "
 		       "send /proc/cpuinfo to " MAINTAINER "\n",
 		       cpu->x86_model_id);
 		return -ENOENT;
@@ -253,10 +529,10 @@
 
 	if (model->op_points == NULL) {
 		/* Matched a non-match */
-		dprintk("no table support for CPU model \"%s\"\n",
+		dprintk(KERN_INFO PFX "no table support for CPU model \"%s\"\n",
 		       cpu->x86_model_id);
 #ifndef CONFIG_X86_SPEEDSTEP_CENTRINO_ACPI
-		dprintk("try compiling with CONFIG_X86_SPEEDSTEP_CENTRINO_ACPI enabled\n");
+		dprintk(KERN_INFO PFX "try compiling with CONFIG_X86_SPEEDSTEP_CENTRINO_ACPI enabled\n");
 #endif
 		return -ENOENT;
 	}
@@ -271,7 +547,7 @@
 
 #else
 static inline int centrino_cpu_init_table(struct cpufreq_policy *policy) { return -ENODEV; }
-#endif /* CONFIG_X86_SPEEDSTEP_CENTRINO_TABLE */
+#endif /* CONFIG_X86_SPEEDSTEP_CENTRINO_BUILTIN */
 
 static int centrino_verify_cpu_id(const struct cpuinfo_x86 *c, const struct cpu_id *x)
 {
@@ -292,6 +568,13 @@
 	 * for centrino, as some DSDTs are buggy.
 	 * Ideally, this can be done using the acpi_data structure.
 	 */
+
+	if ((centrino_model[cpu]) && (centrino_model[cpu]->base_freq != 0))
+	{
+		msr = (msr >> 8) & 0xff;
+		return msr * centrino_model[cpu]->base_freq;
+	}
+
 	if ((centrino_cpu[cpu] == &cpu_ids[CPU_BANIAS]) ||
 	    (centrino_cpu[cpu] == &cpu_ids[CPU_DOTHAN_A1]) ||
 	    (centrino_cpu[cpu] == &cpu_ids[CPU_DOTHAN_B0])) {
@@ -343,39 +626,9 @@
 	return clock_freq;
 }
 
-
 #ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_ACPI
 
-static struct acpi_processor_performance *acpi_perf_data[NR_CPUS];
-
-/*
- * centrino_cpu_early_init_acpi - Do the preregistering with ACPI P-States
- * library
- *
- * Before doing the actual init, we need to do _PSD related setup whenever
- * supported by the BIOS. These are handled by this early_init routine.
- */
-static int centrino_cpu_early_init_acpi(void)
-{
-	unsigned int	i, j;
-	struct acpi_processor_performance	*data;
-
-	for_each_possible_cpu(i) {
-		data = kzalloc(sizeof(struct acpi_processor_performance), 
-				GFP_KERNEL);
-		if (!data) {
-			for_each_possible_cpu(j) {
-				kfree(acpi_perf_data[j]);
-				acpi_perf_data[j] = NULL;
-			}
-			return (-ENOMEM);
-		}
-		acpi_perf_data[i] = data;
-	}
-
-	acpi_processor_preregister_performance(acpi_perf_data);
-	return 0;
-}
+static struct acpi_processor_performance p;
 
 /*
  * centrino_cpu_init_acpi - register with ACPI P-States library
@@ -386,60 +639,50 @@
  */
 static int centrino_cpu_init_acpi(struct cpufreq_policy *policy)
 {
-	unsigned long			cur_freq;
-	int				result = 0, i;
-	unsigned int			cpu = policy->cpu;
-	struct acpi_processor_performance	*p;
-
-	p = acpi_perf_data[cpu];
+	unsigned long  cur_freq;
+	int            i;
+	int            result    = 0;
+	unsigned int   cpu       = policy->cpu;
 
 	/* register with ACPI core */
-	if (acpi_processor_register_performance(p, cpu)) {
-		dprintk(PFX "obtaining ACPI data failed\n");
+	if (acpi_processor_register_performance(&p, cpu)) {
+		dprintk(KERN_INFO PFX "obtaining ACPI data failed\n");
 		return -EIO;
 	}
-	policy->shared_type = p->shared_type;
-	/*
-	 * Will let policy->cpus know about dependency only when software 
-	 * coordination is required.
-	 */
-	if (policy->shared_type == CPUFREQ_SHARED_TYPE_ALL ||
-	    policy->shared_type == CPUFREQ_SHARED_TYPE_ANY)
-		policy->cpus = p->shared_cpu_map;
 
 	/* verify the acpi_data */
-	if (p->state_count <= 1) {
+	if (p.state_count <= 1) {
 		dprintk("No P-States\n");
 		result = -ENODEV;
 		goto err_unreg;
 	}
 
-	if ((p->control_register.space_id != ACPI_ADR_SPACE_FIXED_HARDWARE) ||
-	    (p->status_register.space_id != ACPI_ADR_SPACE_FIXED_HARDWARE)) {
+	if ((p.control_register.space_id != ACPI_ADR_SPACE_FIXED_HARDWARE) ||
+	    (p.status_register.space_id != ACPI_ADR_SPACE_FIXED_HARDWARE)) {
 		dprintk("Invalid control/status registers (%x - %x)\n",
-			p->control_register.space_id, p->status_register.space_id);
+			p.control_register.space_id, p.status_register.space_id);
 		result = -EIO;
 		goto err_unreg;
 	}
 
-	for (i=0; i<p->state_count; i++) {
-		if (p->states[i].control != p->states[i].status) {
+	for (i=0; i<p.state_count; i++) {
+		if (p.states[i].control != p.states[i].status) {
 			dprintk("Different control (%llu) and status values (%llu)\n",
-				p->states[i].control, p->states[i].status);
+				p.states[i].control, p.states[i].status);
 			result = -EINVAL;
 			goto err_unreg;
 		}
 
-		if (!p->states[i].core_frequency) {
+		if (!p.states[i].core_frequency) {
 			dprintk("Zero core frequency for state %u\n", i);
 			result = -EINVAL;
 			goto err_unreg;
 		}
 
-		if (p->states[i].core_frequency > p->states[0].core_frequency) {
+		if (p.states[i].core_frequency > p.states[0].core_frequency) {
 			dprintk("P%u has larger frequency (%llu) than P0 (%llu), skipping\n", i,
-				p->states[i].core_frequency, p->states[0].core_frequency);
-			p->states[i].core_frequency = 0;
+				p.states[i].core_frequency, p.states[0].core_frequency);
+			p.states[i].core_frequency = 0;
 			continue;
 		}
 	}
@@ -451,26 +694,27 @@
 	}
 
 	centrino_model[cpu]->model_name=NULL;
-	centrino_model[cpu]->max_freq = p->states[0].core_frequency * 1000;
+	centrino_model[cpu]->max_freq = p.states[0].core_frequency * 1000;
 	centrino_model[cpu]->op_points =  kmalloc(sizeof(struct cpufreq_frequency_table) *
-					     (p->state_count + 1), GFP_KERNEL);
-        if (!centrino_model[cpu]->op_points) {
-                result = -ENOMEM;
-                goto err_kfree;
-        }
-
-        for (i=0; i<p->state_count; i++) {
-		centrino_model[cpu]->op_points[i].index = p->states[i].control;
-		centrino_model[cpu]->op_points[i].frequency = p->states[i].core_frequency * 1000;
+					     (p.state_count + 1), GFP_KERNEL);
+	if (!centrino_model[cpu]->op_points) {
+		result = -ENOMEM;
+		goto err_kfree;
+	}
+
+	for (i=0; i<p.state_count; i++) {
+		centrino_model[cpu]->op_points[i].index = p.states[i].control;
+		centrino_model[cpu]->op_points[i].frequency = p.states[i].core_frequency * 1000;
 		dprintk("adding state %i with frequency %u and control value %04x\n", 
 			i, centrino_model[cpu]->op_points[i].frequency, centrino_model[cpu]->op_points[i].index);
 	}
-	centrino_model[cpu]->op_points[p->state_count].frequency = CPUFREQ_TABLE_END;
+	centrino_model[cpu]->op_points[p.state_count].frequency = CPUFREQ_TABLE_END;
 
 	cur_freq = get_cur_freq(cpu);
+	centrino_model[cpu]->base_freq = 0;
 
-	for (i=0; i<p->state_count; i++) {
-		if (!p->states[i].core_frequency) {
+	for (i=0; i<p.state_count; i++) {
+		if (!p.states[i].core_frequency) {
 			dprintk("skipping state %u\n", i);
 			centrino_model[cpu]->op_points[i].frequency = CPUFREQ_ENTRY_INVALID;
 			continue;
@@ -479,14 +723,14 @@
 		if (extract_clock(centrino_model[cpu]->op_points[i].index, cpu, 0) !=
 		    (centrino_model[cpu]->op_points[i].frequency)) {
 			dprintk("Invalid encoded frequency (%u vs. %u)\n",
-				extract_clock(centrino_model[cpu]->op_points[i].index, cpu, 0),
-				centrino_model[cpu]->op_points[i].frequency);
+			extract_clock(centrino_model[cpu]->op_points[i].index, cpu, 0),
+			centrino_model[cpu]->op_points[i].frequency);
 			result = -EINVAL;
 			goto err_kfree_all;
 		}
 
 		if (cur_freq == centrino_model[cpu]->op_points[i].frequency)
-			p->state = i;
+			p.state = i;
 	}
 
 	/* notify BIOS that we exist */
@@ -499,15 +743,437 @@
  err_kfree:
 	kfree(centrino_model[cpu]);
  err_unreg:
-	acpi_processor_unregister_performance(p, cpu);
-	dprintk(PFX "invalid ACPI data\n");
+	acpi_processor_unregister_performance(&p, cpu);
+	dprintk(KERN_INFO PFX "invalid ACPI data\n");
 	return (result);
 }
 #else
 static inline int centrino_cpu_init_acpi(struct cpufreq_policy *policy) { return -ENODEV; }
-static inline int centrino_cpu_early_init_acpi(void) { return 0; }
 #endif
 
+static int centrino_target (struct cpufreq_policy *policy,
+			    unsigned int target_freq,
+			    unsigned int relation);
+
+
+#ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_SYSFS
+/************************** sysfs interface for user defined voltage table ************************/
+
+static struct cpufreq_frequency_table **original_table = NULL;
+
+static void check_origial_table (unsigned int cpu)
+{
+	int           i;
+
+	if (!original_table) 
+	{
+		original_table = kmalloc(sizeof(struct cpufreq_frequency_table *)*NR_CPUS, GFP_KERNEL);
+		for (i=0; i < NR_CPUS; i++)
+		{
+			original_table[i] = NULL;
+		}
+	}
+
+	if (!original_table[cpu]) 
+	{
+		/* Count number of frequencies and allocate memory for a copy */
+		for (i=0; centrino_model[cpu]->op_points[i].frequency != CPUFREQ_TABLE_END; i++);
+		/* Allocate memory to store the copy */
+		original_table[cpu] = (struct cpufreq_frequency_table*) kmalloc(sizeof(struct cpufreq_frequency_table)*(i+1), GFP_KERNEL);
+		/* Make copy of frequency/voltage pairs */
+		for (i=0; centrino_model[cpu]->op_points[i].frequency != CPUFREQ_TABLE_END; i++) 
+		{
+			original_table[cpu][i].frequency = centrino_model[cpu]->op_points[i].frequency;
+			original_table[cpu][i].index = centrino_model[cpu]->op_points[i].index;
+		}
+		original_table[cpu][i].frequency = CPUFREQ_TABLE_END;
+	}
+}
+
+
+static ssize_t show_user_voltage (struct cpufreq_policy *policy, char *buf)
+{
+	ssize_t       bytes_written = 0;
+	unsigned int  cpu          = policy->cpu;
+	unsigned int  op_index     = 0;
+	unsigned int  voltage      = 0;
+
+	//dprintk("showing user voltage table in sysfs\n");
+
+	while (centrino_model[cpu]->op_points[op_index].frequency != CPUFREQ_TABLE_END)
+	{
+		//dprintk("getting state %i \n", i);
+		voltage = centrino_model[cpu]->op_points[op_index].index;
+		voltage = 700 + ((voltage & 0xFF) << 4); 
+		//dprintk("writing voltage %i: %u mV \n", i, voltage);
+		bytes_written += snprintf (&buf[bytes_written],PAGE_SIZE, "%u",voltage);
+		op_index++;
+		if (centrino_model[cpu]->op_points[op_index].frequency != CPUFREQ_TABLE_END)
+			bytes_written += snprintf (&buf[bytes_written],PAGE_SIZE, ",");
+		else
+			bytes_written += snprintf (&buf[bytes_written],PAGE_SIZE, "\n");	
+	}
+	buf[PAGE_SIZE-1] = 0;
+	return bytes_written;
+}
+
+static ssize_t 
+store_user_voltage (struct cpufreq_policy *policy, const char *buf, size_t count) 
+{
+	unsigned int  cpu;
+	const char   *curr_buf;
+	unsigned int  curr_freq;
+	unsigned int  op_index;
+	int           isok;
+	char         *next_buf;
+	unsigned int  op_point;
+	ssize_t       retval;
+	unsigned int  voltage;
+
+	if (!policy)
+	    return -ENODEV;
+	cpu = policy->cpu;
+	if (!centrino_model[cpu] || !centrino_model[cpu]->op_points)
+	    return -ENODEV;
+
+	check_origial_table(cpu);
+
+	op_index = 0;
+	curr_buf = buf;
+	next_buf = NULL;
+	isok     = 1;
+	
+	while ((centrino_model[cpu]->op_points[op_index].frequency != CPUFREQ_TABLE_END) 
+		&& (isok))
+	{
+		voltage = simple_strtoul(curr_buf, &next_buf, 10);
+		if ((next_buf != curr_buf) && (next_buf != NULL))
+		{
+			if ((voltage >= 700) && (voltage<=1600))
+			{
+				voltage = ((voltage - 700) >> 4) & 0xFF;
+				op_point = (original_table[cpu])[op_index].index;
+				if (voltage <= (op_point & 0xFF))
+				{
+					//dprintk("setting control value %i to %04x\n", op_index, op_point);
+					op_point = (op_point & 0xFFFFFF00) | voltage;
+					centrino_model[cpu]->op_points[op_index].index = op_point;
+				}
+				else
+				{
+					op_point = (op_point & 0xFFFFFF00) | voltage;
+					dprintk("not setting control value %i to %04x because requested voltage is not lower than the default value\n", op_index, op_point);
+					//isok = 0;
+				}
+			}
+			else
+			{
+				dprintk("voltage value %i is out of bounds: %u mV\n", op_index, voltage);
+				isok = 0;
+			}
+			curr_buf = next_buf;
+			if (*curr_buf==',')
+				curr_buf++;
+			next_buf = NULL;
+		}
+		else
+		{
+			dprintk("failed to parse voltage value %i\n", op_index);
+			isok = 0;
+		}
+		op_index++;
+	}
+
+	if (isok)
+	{
+		retval = count;
+		curr_freq = cpufreq_get(policy->cpu);
+		centrino_target(policy, curr_freq, CPUFREQ_RELATION_L);
+	}
+	else
+	{
+		retval = -EINVAL;
+	}
+
+	return retval;
+}
+
+static struct freq_attr centrino_freq_attr_voltage_table = 
+{
+	.attr = { .name = "voltage_table", .mode = 0644, .owner = THIS_MODULE },
+	.show = show_user_voltage,
+	.store = store_user_voltage,
+};
+
+
+static ssize_t show_user_op_points (struct cpufreq_policy *policy, char *buf)
+{
+	ssize_t       bytes_written = 0;
+	unsigned int  cpu          = policy->cpu;
+	unsigned int  op_index     = 0;
+	unsigned int  voltage      = 0;
+	unsigned int  frequency    = 0;
+
+	//dprintk("showing user voltage table in sysfs\n");
+
+	while ( (centrino_model[cpu]->op_points[op_index].frequency != CPUFREQ_TABLE_END)
+	       &&(bytes_written<PAGE_SIZE-16))
+	{
+		//dprintk("getting state %i \n", i);
+		voltage = centrino_model[cpu]->op_points[op_index].index;
+		voltage = 700 + ((voltage & 0xFF) << 4); 
+		frequency = centrino_model[cpu]->op_points[op_index].frequency;
+		//dprintk("writing voltage %i: %u mV \n", i, voltage);
+		bytes_written += snprintf (&buf[bytes_written],PAGE_SIZE-bytes_written-2, "%u:%u",frequency,voltage);
+		op_index++;
+		if (centrino_model[cpu]->op_points[op_index].frequency != CPUFREQ_TABLE_END)
+			bytes_written += snprintf (&buf[bytes_written],PAGE_SIZE-bytes_written-1, ",");
+		else
+			bytes_written += snprintf (&buf[bytes_written],PAGE_SIZE-bytes_written-1, "\n");	
+	}
+	buf[PAGE_SIZE-1] = 0;
+	return bytes_written;
+}
+
+static ssize_t 
+store_user_op_points (struct cpufreq_policy *policy, const char *buf, size_t count) 
+{
+	unsigned int  cpu;
+	const char   *curr_buf;
+	unsigned int  curr_freq;
+	unsigned int  op_index;
+	unsigned int  op_count;
+	int           isok;
+	char         *next_buf;
+	unsigned int  op_point;
+	ssize_t       retval;
+	unsigned int  voltage;
+	unsigned int  frequency;
+	int           found;
+
+	if (!policy)
+	    return -ENODEV;
+	cpu = policy->cpu;
+	if (!centrino_model[cpu] || !centrino_model[cpu]->op_points)
+	    return -ENODEV;
+
+	check_origial_table(cpu);
+
+	op_count = 0;
+	curr_buf = buf;
+	next_buf = NULL;
+	isok     = 1;
+	
+	while ( (isok) && (curr_buf != NULL) )
+	{
+		op_count++;
+		// Parse frequency
+		frequency = simple_strtoul(curr_buf, &next_buf, 10);
+		if ((next_buf != curr_buf) && (next_buf != NULL))
+		{
+			// Parse separator between frequency and voltage 
+			curr_buf = next_buf;
+			next_buf = NULL;
+			if (*curr_buf==':')
+			{
+				curr_buf++;
+				// Parse voltage
+				voltage = simple_strtoul(curr_buf, &next_buf, 10);
+				if ((next_buf != curr_buf) && (next_buf != NULL))
+				{
+					if ((voltage >= 700) && (voltage<=1600))
+					{
+						voltage = ((voltage - 700) >> 4) & 0xFF;
+						op_index = 0;
+						found = 0;
+						while (centrino_model[cpu]->op_points[op_index].frequency != CPUFREQ_TABLE_END)
+						{
+							if (centrino_model[cpu]->op_points[op_index].frequency == frequency)
+							{
+								found = 1;
+								op_point = (original_table[cpu])[op_index].index;
+								if (voltage <= (op_point & 0xFF))
+								{
+									//dprintk("setting control value %i to %04x\n", op_index, op_point);
+									op_point = (op_point & 0xFFFFFF00) | voltage;
+									centrino_model[cpu]->op_points[op_index].index = op_point;
+								}
+								else
+								{
+									op_point = (op_point & 0xFFFFFF00) | voltage;
+									dprintk("not setting control value %i to %04x because requested voltage is not lower than the default value (%u MHz)\n", op_index, op_point, frequency);
+								}
+							}
+							op_index++;
+						}
+						if (found == 0)
+						{
+							dprintk("operating point # %u not found: %u MHz\n", op_count, frequency);
+							isok = 0;
+						}
+					}
+					else
+					{
+						dprintk("operating point # %u voltage value is out of bounds: %u mV\n", op_count, voltage);
+						isok = 0;
+					}
+					// Parse seprator before next operating point, if any
+					curr_buf = next_buf;
+					next_buf = NULL;
+					if (*curr_buf==',')
+						curr_buf++;
+					else
+						curr_buf = NULL;
+				}
+				else
+				{
+					dprintk("failed to parse operating point # %u voltage\n", op_count);
+					isok = 0;
+				}
+			}
+			else
+			{
+				dprintk("failed to parse operating point # %u\n", op_count);
+				isok = 0;
+			}
+		}
+		else
+		{
+			dprintk("failed to parse operating point # %u frequency\n", op_count);
+			isok = 0;
+		}
+	}
+
+	if (isok)
+	{
+		retval = count;
+		curr_freq = cpufreq_get(policy->cpu);
+		centrino_target(policy, curr_freq, CPUFREQ_RELATION_L);
+	}
+	else
+	{
+		retval = -EINVAL;
+	}
+
+	return retval;
+}
+
+static struct freq_attr centrino_freq_attr_op_points_table = 
+{
+	.attr = { .name = "op_points_table", .mode = 0644, .owner = THIS_MODULE },
+	.show = show_user_op_points,
+	.store = store_user_op_points,
+};
+
+unsigned long rounded_div(unsigned long x, unsigned long y)
+{
+  return (((x*2) / y)+1)/2;
+}
+
+static ssize_t show_FSB_base_freq (struct cpufreq_policy *policy, char *buf)
+{
+	ssize_t       bytes_written = 0;
+	unsigned int  cpu           = policy->cpu;
+	unsigned int  frequency;
+	unsigned int  index;
+	unsigned int  op_index			= 0;
+
+	frequency = centrino_model[cpu]->base_freq;
+	if (frequency!=0)
+	{
+		bytes_written += snprintf (buf, PAGE_SIZE-2, "User defined base FSB frequency:\n%u kHz\n",frequency);
+	}
+
+	bytes_written += snprintf (buf+bytes_written, PAGE_SIZE-bytes_written-2, 
+	                           "Base FSB frequency computed from operating points table:\n");
+
+	check_origial_table(cpu);
+	while ((original_table[cpu][op_index].frequency != CPUFREQ_TABLE_END)
+	       && (bytes_written < PAGE_SIZE-3))
+	{
+		index = original_table[cpu][op_index].index;
+		index = (index >> 8) & 0xFF;
+		if (index > 0)
+		{
+			frequency = rounded_div((original_table[cpu][op_index].frequency), index);
+			bytes_written += snprintf (buf+bytes_written, PAGE_SIZE-bytes_written-2, "%u kHz (%u / %u)\n",
+			                           frequency, original_table[cpu][op_index].frequency, index);
+		}
+		op_index++;
+	}
+
+	buf[PAGE_SIZE-1] = 0;
+	return bytes_written;
+}
+
+static ssize_t 
+store_FSB_base_freq (struct cpufreq_policy *policy, const char *buf, size_t count) 
+{
+	unsigned int  cpu;
+	const char   *curr_buf;
+	unsigned int  curr_freq;
+	unsigned int  frequency;
+	unsigned int  index;
+	char         *next_buf;
+	unsigned int  op_index			= 0;
+	ssize_t       retval;
+
+	if (!policy)
+		return -ENODEV;
+	cpu = policy->cpu;
+	if (!centrino_model[cpu] || !centrino_model[cpu]->op_points)
+		return -ENODEV;
+
+	curr_buf = buf;
+	next_buf = NULL;
+	frequency = simple_strtoul(curr_buf, &next_buf, 10);
+	if ((next_buf != curr_buf) && (next_buf != NULL))
+	{
+		if (centrino_model[cpu]->base_freq != frequency)
+		{
+			centrino_model[cpu]->base_freq = frequency;
+
+			check_origial_table(cpu);
+			while (centrino_model[cpu]->op_points[op_index].frequency != CPUFREQ_TABLE_END)
+			{
+				if (frequency>0)
+				{
+					index = original_table[cpu][op_index].index;
+					index = (index >> 8) & 0xFF;
+					if (index > 0)
+					{
+						centrino_model[cpu]->op_points[op_index].frequency = frequency * index;
+					}
+				}
+				else
+				{
+					centrino_model[cpu]->op_points[op_index].frequency = original_table[cpu][op_index].frequency;
+				}
+				op_index++;
+			}
+		}
+
+		retval = count;
+		curr_freq = cpufreq_get(policy->cpu);
+		centrino_target(policy, curr_freq, CPUFREQ_RELATION_L);
+	}
+	else
+	{
+		retval = -EINVAL;
+	}
+
+	return retval;
+}
+
+static struct freq_attr centrino_freq_attr_FSB_Base_Freq = 
+{
+	.attr = { .name = "FSB_base_frequency", .mode = 0644, .owner = THIS_MODULE },
+	.show = show_FSB_base_freq,
+	.store = store_FSB_base_freq,
+};
+
+#endif /* CONFIG_X86_SPEEDSTEP_CENTRINO_SYSFS */
+
 static int centrino_cpu_init(struct cpufreq_policy *policy)
 {
 	struct cpuinfo_x86 *cpu = &cpu_data[policy->cpu];
@@ -535,7 +1201,7 @@
 			centrino_cpu[policy->cpu] = &cpu_ids[i];
 
 		if (!centrino_cpu[policy->cpu]) {
-			dprintk("found unsupported CPU with "
+			dprintk(KERN_INFO PFX "found unsupported CPU with "
 			"Enhanced SpeedStep: send /proc/cpuinfo to "
 			MAINTAINER "\n");
 			return -ENODEV;
@@ -591,15 +1257,10 @@
 
 #ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_ACPI
 	if (!centrino_model[cpu]->model_name) {
-		static struct acpi_processor_performance *p;
-
-		if (acpi_perf_data[cpu]) {
-			p = acpi_perf_data[cpu];
-			dprintk("unregistering and freeing ACPI data\n");
-			acpi_processor_unregister_performance(p, cpu);
-			kfree(centrino_model[cpu]->op_points);
-			kfree(centrino_model[cpu]);
-		}
+		dprintk("unregistering and freeing ACPI data\n");
+		acpi_processor_unregister_performance(&p, cpu);
+		kfree(centrino_model[cpu]->op_points);
+		kfree(centrino_model[cpu]);
 	}
 #endif
 
@@ -633,132 +1294,72 @@
 			    unsigned int relation)
 {
 	unsigned int    newstate = 0;
-	unsigned int	msr, oldmsr = 0, h = 0, cpu = policy->cpu;
+	unsigned int	msr, oldmsr, h, cpu = policy->cpu;
 	struct cpufreq_freqs	freqs;
-	cpumask_t		online_policy_cpus;
 	cpumask_t		saved_mask;
-	cpumask_t		set_mask;
-	cpumask_t		covered_cpus;
-	int			retval = 0;
-	unsigned int		j, k, first_cpu, tmp;
+	int			retval;
 
-	if (unlikely(centrino_model[cpu] == NULL))
+	if (centrino_model[cpu] == NULL)
 		return -ENODEV;
 
-	if (unlikely(cpufreq_frequency_table_target(policy,
-			centrino_model[cpu]->op_points,
-			target_freq,
-			relation,
-			&newstate))) {
-		return -EINVAL;
-	}
-
-#ifdef CONFIG_HOTPLUG_CPU
-	/* cpufreq holds the hotplug lock, so we are safe from here on */
-	cpus_and(online_policy_cpus, cpu_online_map, policy->cpus);
-#else
-	online_policy_cpus = policy->cpus;
-#endif
-
+	/*
+	 * Support for SMP systems.
+	 * Make sure we are running on the CPU that wants to change frequency
+	 */
 	saved_mask = current->cpus_allowed;
-	first_cpu = 1;
-	cpus_clear(covered_cpus);
-	for_each_cpu_mask(j, online_policy_cpus) {
-		/*
-		 * Support for SMP systems.
-		 * Make sure we are running on CPU that wants to change freq
-		 */
-		cpus_clear(set_mask);
-		if (policy->shared_type == CPUFREQ_SHARED_TYPE_ANY)
-			cpus_or(set_mask, set_mask, online_policy_cpus);
-		else
-			cpu_set(j, set_mask);
-
-		set_cpus_allowed(current, set_mask);
-		if (unlikely(!cpu_isset(smp_processor_id(), set_mask))) {
-			dprintk("couldn't limit to CPUs in this domain\n");
-			retval = -EAGAIN;
-			if (first_cpu) {
-				/* We haven't started the transition yet. */
-				goto migrate_end;
-			}
-			break;
-		}
-
-		msr = centrino_model[cpu]->op_points[newstate].index;
-
-		if (first_cpu) {
-			rdmsr(MSR_IA32_PERF_CTL, oldmsr, h);
-			if (msr == (oldmsr & 0xffff)) {
-				dprintk("no change needed - msr was and needs "
-					"to be %x\n", oldmsr);
-				retval = 0;
-				goto migrate_end;
-			}
-
-			freqs.old = extract_clock(oldmsr, cpu, 0);
-			freqs.new = extract_clock(msr, cpu, 0);
+	set_cpus_allowed(current, policy->cpus);
+	if (!cpu_isset(smp_processor_id(), policy->cpus)) {
+		dprintk("couldn't limit to CPUs in this domain\n");
+		return(-EAGAIN);
+	}
 
-			dprintk("target=%dkHz old=%d new=%d msr=%04x\n",
-				target_freq, freqs.old, freqs.new, msr);
+	if (cpufreq_frequency_table_target(policy, centrino_model[cpu]->op_points, target_freq,
+					   relation, &newstate)) {
+		retval = -EINVAL;
+		goto migrate_end;
+	}
 
-			for_each_cpu_mask(k, online_policy_cpus) {
-				freqs.cpu = k;
-				cpufreq_notify_transition(&freqs,
-					CPUFREQ_PRECHANGE);
-			}
+	msr = centrino_model[cpu]->op_points[newstate].index;
+	rdmsr(MSR_IA32_PERF_CTL, oldmsr, h);
 
-			first_cpu = 0;
-			/* all but 16 LSB are reserved, treat them with care */
-			oldmsr &= ~0xffff;
-			msr &= 0xffff;
-			oldmsr |= msr;
-		}
+	if (msr == (oldmsr & 0xffff)) {
+		retval = 0;
+		dprintk("no change needed - msr was and needs to be %x\n", oldmsr);
+		goto migrate_end;
+	}
 
-		wrmsr(MSR_IA32_PERF_CTL, oldmsr, h);
-		if (policy->shared_type == CPUFREQ_SHARED_TYPE_ANY)
-			break;
+	freqs.cpu = cpu;
+	freqs.old = extract_clock(oldmsr, cpu, 0);
+	freqs.new = extract_clock(msr, cpu, 0);
 
-		cpu_set(j, covered_cpus);
-	}
+	dprintk("target=%dkHz old=%d new=%d msr=%04x\n",
+		target_freq, freqs.old, freqs.new, msr);
 
-	for_each_cpu_mask(k, online_policy_cpus) {
-		freqs.cpu = k;
-		cpufreq_notify_transition(&freqs, CPUFREQ_POSTCHANGE);
-	}
+	cpufreq_notify_transition(&freqs, CPUFREQ_PRECHANGE);
 
-	if (unlikely(retval)) {
-		/*
-		 * We have failed halfway through the frequency change.
-		 * We have sent callbacks to policy->cpus and
-		 * MSRs have already been written on coverd_cpus.
-		 * Best effort undo..
-		 */
+	/* all but 16 LSB are "reserved", so treat them with
+	   care */
+	oldmsr &= ~0xffff;
+	msr &= 0xffff;
+	oldmsr |= msr;
 
-		if (!cpus_empty(covered_cpus)) {
-			for_each_cpu_mask(j, covered_cpus) {
-				set_cpus_allowed(current, cpumask_of_cpu(j));
-				wrmsr(MSR_IA32_PERF_CTL, oldmsr, h);
-			}
-		}
+	wrmsr(MSR_IA32_PERF_CTL, oldmsr, h);
 
-		tmp = freqs.new;
-		freqs.new = freqs.old;
-		freqs.old = tmp;
-		for_each_cpu_mask(j, online_policy_cpus) {
-			freqs.cpu = j;
-			cpufreq_notify_transition(&freqs, CPUFREQ_PRECHANGE);
-			cpufreq_notify_transition(&freqs, CPUFREQ_POSTCHANGE);
-		}
-	}
+	cpufreq_notify_transition(&freqs, CPUFREQ_POSTCHANGE);
 
+	retval = 0;
 migrate_end:
 	set_cpus_allowed(current, saved_mask);
-	return 0;
+	return (retval);
 }
 
 static struct freq_attr* centrino_attr[] = {
 	&cpufreq_freq_attr_scaling_available_freqs,
+#ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_SYSFS
+	&centrino_freq_attr_voltage_table,
+	&centrino_freq_attr_op_points_table,
+	&centrino_freq_attr_FSB_Base_Freq,
+#endif
 	NULL,
 };
 
@@ -796,25 +1397,12 @@
 	if (!cpu_has(cpu, X86_FEATURE_EST))
 		return -ENODEV;
 
-	centrino_cpu_early_init_acpi();
-
 	return cpufreq_register_driver(&centrino_driver);
 }
 
 static void __exit centrino_exit(void)
 {
-#ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_ACPI
-	unsigned int j;
-#endif
-	
 	cpufreq_unregister_driver(&centrino_driver);
-
-#ifdef CONFIG_X86_SPEEDSTEP_CENTRINO_ACPI
-	for_each_possible_cpu(j) {
-		kfree(acpi_perf_data[j]);
-		acpi_perf_data[j] = NULL;
-	}
-#endif
 }
 
 MODULE_AUTHOR ("Jeremy Fitzhardinge <jeremy@goop.org>");
@@ -823,3 +1411,4 @@
 
 late_initcall(centrino_init);
 module_exit(centrino_exit);
+ 
diff -urN oldtree/arch/i386/kernel/cpu/mtrr/Makefile newtree/arch/i386/kernel/cpu/mtrr/Makefile
--- oldtree/arch/i386/kernel/cpu/mtrr/Makefile	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/i386/kernel/cpu/mtrr/Makefile	2006-07-12 19:02:23.000000000 -0400
@@ -1,5 +1,5 @@
 obj-y		:= main.o if.o generic.o state.o
-obj-y		+= amd.o
-obj-y		+= cyrix.o
-obj-y		+= centaur.o
+obj-$(CONFIG_CPU_SUP_AMD)	+= amd.o
+obj-$(CONFIG_CPU_SUP_CYRIX)	+= cyrix.o
+obj-$(CONFIG_CPU_SUP_CENTAUR)	+= centaur.o
 
diff -urN oldtree/arch/i386/kernel/cpu/mtrr/cyrix.c newtree/arch/i386/kernel/cpu/mtrr/cyrix.c
--- oldtree/arch/i386/kernel/cpu/mtrr/cyrix.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/i386/kernel/cpu/mtrr/cyrix.c	2006-07-12 19:02:23.000000000 -0400
@@ -5,7 +5,7 @@
 #include <asm/io.h>
 #include "mtrr.h"
 
-int arr3_protected;
+extern int arr3_protected;
 
 static void
 cyrix_get_arr(unsigned int reg, unsigned long *base,
diff -urN oldtree/arch/i386/kernel/cpu/mtrr/main.c newtree/arch/i386/kernel/cpu/mtrr/main.c
--- oldtree/arch/i386/kernel/cpu/mtrr/main.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/i386/kernel/cpu/mtrr/main.c	2006-07-12 19:02:23.000000000 -0400
@@ -59,7 +59,7 @@
 static void set_mtrr(unsigned int reg, unsigned long base,
 		     unsigned long size, mtrr_type type);
 
-extern int arr3_protected;
+int arr3_protected;
 
 void set_mtrr_ops(struct mtrr_ops * ops)
 {
@@ -544,9 +544,15 @@
 
 static void __init init_ifs(void)
 {
+#ifdef CONFIG_CPU_SUP_AMD
 	amd_init_mtrr();
+#endif
+#ifdef CONFIG_CPU_SUP_CYRIX
 	cyrix_init_mtrr();
+#endif
+#ifdef CONFIG_CPU_SUP_CENTAUR
 	centaur_init_mtrr();
+#endif
 }
 
 /* The suspend/resume methods are only for CPU without MTRR. CPU using generic
@@ -644,6 +650,7 @@
 		}
 	} else {
 		switch (boot_cpu_data.x86_vendor) {
+#ifdef CONFIG_CPU_SUP_AMD
 		case X86_VENDOR_AMD:
 			if (cpu_has_k6_mtrr) {
 				/* Pre-Athlon (K6) AMD CPU MTRRs */
@@ -652,6 +659,8 @@
 				size_and_mask = 0;
 			}
 			break;
+#endif
+#ifdef CONFIG_CPU_SUP_CENTAUR
 		case X86_VENDOR_CENTAUR:
 			if (cpu_has_centaur_mcr) {
 				mtrr_if = mtrr_ops[X86_VENDOR_CENTAUR];
@@ -659,6 +668,8 @@
 				size_and_mask = 0;
 			}
 			break;
+#endif
+#ifdef CONFIG_CPU_SUP_CYRIX
 		case X86_VENDOR_CYRIX:
 			if (cpu_has_cyrix_arr) {
 				mtrr_if = mtrr_ops[X86_VENDOR_CYRIX];
@@ -666,6 +677,7 @@
 				size_and_mask = 0;
 			}
 			break;
+#endif
 		default:
 			break;
 		}
diff -urN oldtree/arch/i386/lib/usercopy.c newtree/arch/i386/lib/usercopy.c
--- oldtree/arch/i386/lib/usercopy.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/i386/lib/usercopy.c	2006-07-12 19:02:21.000000000 -0400
@@ -14,7 +14,7 @@
 
 static inline int __movsl_is_ok(unsigned long a1, unsigned long a2, unsigned long n)
 {
-#ifdef CONFIG_X86_INTEL_USERCOPY
+#if defined(CONFIG_CPU_SUP_INTEL) && defined(CONFIG_X86_INTEL_USERCOPY)
 	if (n >= 64 && ((a1 ^ a2) & movsl_mask.mask))
 		return 0;
 #endif
diff -urN oldtree/arch/i386/mm/init.c newtree/arch/i386/mm/init.c
--- oldtree/arch/i386/mm/init.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/i386/mm/init.c	2006-07-12 19:02:09.000000000 -0400
@@ -185,12 +185,14 @@
 	}
 }
 
+#ifdef CONFIG_CPU_SUP_INTEL
 static inline int page_kills_ppro(unsigned long pagenr)
 {
 	if (pagenr >= 0x70000 && pagenr <= 0x7003F)
 		return 1;
 	return 0;
 }
+#endif
 
 extern int is_available_memory(efi_memory_desc_t *);
 
@@ -278,7 +280,11 @@
 
 void __init add_one_highpage_init(struct page *page, int pfn, int bad_ppro)
 {
-	if (page_is_ram(pfn) && !(bad_ppro && page_kills_ppro(pfn))) {
+	if (page_is_ram(pfn)
+#ifdef CONFIG_CPU_SUP_INTEL
+	    && !(bad_ppro && page_kills_ppro(pfn))
+#endif
+		) {
 		ClearPageReserved(page);
 		free_new_highpage(page);
 	} else
@@ -578,7 +584,11 @@
 		BUG();
 #endif
 	
+#ifdef CONFIG_CPU_SUP_INTEL
 	bad_ppro = ppro_with_ram_bug();
+#else
+	bad_ppro = 0;
+#endif
 
 #ifdef CONFIG_HIGHMEM
 	/* check that fixmap and pkmap do not overlap */
diff -urN oldtree/arch/ia64/configs/tiger_defconfig newtree/arch/ia64/configs/tiger_defconfig
--- oldtree/arch/ia64/configs/tiger_defconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/ia64/configs/tiger_defconfig	2006-07-12 19:00:51.000000000 -0400
@@ -105,10 +105,10 @@
 # CONFIG_IA64_PAGE_SIZE_64KB is not set
 CONFIG_PGTABLE_3=y
 # CONFIG_PGTABLE_4 is not set
-# CONFIG_HZ_100 is not set
-CONFIG_HZ_250=y
+CONFIG_HZ_100=y
+# CONFIG_HZ_250 is not set
 # CONFIG_HZ_1000 is not set
-CONFIG_HZ=250
+CONFIG_HZ=100
 CONFIG_IA64_L1_CACHE_SHIFT=7
 CONFIG_IA64_CYCLONE=y
 CONFIG_IOSAPIC=y
diff -urN oldtree/arch/ia64/configs/zx1_defconfig newtree/arch/ia64/configs/zx1_defconfig
--- oldtree/arch/ia64/configs/zx1_defconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/ia64/configs/zx1_defconfig	2006-07-12 19:00:51.000000000 -0400
@@ -103,10 +103,10 @@
 # CONFIG_IA64_PAGE_SIZE_64KB is not set
 CONFIG_PGTABLE_3=y
 # CONFIG_PGTABLE_4 is not set
-# CONFIG_HZ_100 is not set
-CONFIG_HZ_250=y
+CONFIG_HZ_100=y
+# CONFIG_HZ_250 is not set
 # CONFIG_HZ_1000 is not set
-CONFIG_HZ=250
+CONFIG_HZ=100
 CONFIG_IA64_L1_CACHE_SHIFT=7
 # CONFIG_IA64_CYCLONE is not set
 CONFIG_IOSAPIC=y
diff -urN oldtree/arch/ia64/defconfig newtree/arch/ia64/defconfig
--- oldtree/arch/ia64/defconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/ia64/defconfig	2006-07-12 19:00:51.000000000 -0400
@@ -65,19 +65,6 @@
 #
 
 #
-# IO Schedulers
-#
-CONFIG_IOSCHED_NOOP=y
-CONFIG_IOSCHED_AS=y
-CONFIG_IOSCHED_DEADLINE=y
-CONFIG_IOSCHED_CFQ=y
-CONFIG_DEFAULT_AS=y
-# CONFIG_DEFAULT_DEADLINE is not set
-# CONFIG_DEFAULT_CFQ is not set
-# CONFIG_DEFAULT_NOOP is not set
-CONFIG_DEFAULT_IOSCHED="anticipatory"
-
-#
 # Processor type and features
 #
 CONFIG_IA64=y
@@ -105,10 +92,6 @@
 # CONFIG_IA64_PAGE_SIZE_64KB is not set
 CONFIG_PGTABLE_3=y
 # CONFIG_PGTABLE_4 is not set
-# CONFIG_HZ_100 is not set
-CONFIG_HZ_250=y
-# CONFIG_HZ_1000 is not set
-CONFIG_HZ=250
 CONFIG_IA64_L1_CACHE_SHIFT=7
 CONFIG_IA64_CYCLONE=y
 CONFIG_IOSAPIC=y
@@ -119,7 +102,6 @@
 CONFIG_IA64_NR_NODES=256
 CONFIG_HOTPLUG_CPU=y
 # CONFIG_SCHED_SMT is not set
-# CONFIG_PREEMPT is not set
 CONFIG_SELECT_MEMORY_MODEL=y
 # CONFIG_FLATMEM_MANUAL is not set
 CONFIG_DISCONTIGMEM_MANUAL=y
diff -urN oldtree/arch/x86_64/Kconfig newtree/arch/x86_64/Kconfig
--- oldtree/arch/x86_64/Kconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/x86_64/Kconfig	2006-07-12 19:03:48.000000000 -0400
@@ -159,6 +159,7 @@
 
 config MICROCODE
 	tristate "/dev/cpu/microcode - Intel CPU microcode support"
+       select FW_LOADER
 	---help---
 	  If you say Y here the 'File systems' section, you will be
 	  able to update the microcode on Intel processors. You will
diff -urN oldtree/arch/x86_64/defconfig newtree/arch/x86_64/defconfig
--- oldtree/arch/x86_64/defconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/x86_64/defconfig	2006-07-12 19:00:51.000000000 -0400
@@ -81,19 +81,6 @@
 # CONFIG_LSF is not set
 
 #
-# IO Schedulers
-#
-CONFIG_IOSCHED_NOOP=y
-# CONFIG_IOSCHED_AS is not set
-CONFIG_IOSCHED_DEADLINE=y
-CONFIG_IOSCHED_CFQ=y
-# CONFIG_DEFAULT_AS is not set
-# CONFIG_DEFAULT_DEADLINE is not set
-CONFIG_DEFAULT_CFQ=y
-# CONFIG_DEFAULT_NOOP is not set
-CONFIG_DEFAULT_IOSCHED="cfq"
-
-#
 # Processor type and features
 #
 CONFIG_X86_PC=y
@@ -116,10 +103,6 @@
 CONFIG_SMP=y
 CONFIG_SCHED_SMT=y
 CONFIG_SCHED_MC=y
-# CONFIG_PREEMPT_NONE is not set
-CONFIG_PREEMPT_VOLUNTARY=y
-# CONFIG_PREEMPT is not set
-CONFIG_PREEMPT_BKL=y
 CONFIG_NUMA=y
 CONFIG_K8_NUMA=y
 CONFIG_NODES_SHIFT=6
@@ -154,10 +137,6 @@
 # CONFIG_CRASH_DUMP is not set
 CONFIG_PHYSICAL_START=0x200000
 CONFIG_SECCOMP=y
-# CONFIG_HZ_100 is not set
-CONFIG_HZ_250=y
-# CONFIG_HZ_1000 is not set
-CONFIG_HZ=250
 # CONFIG_REORDER is not set
 CONFIG_K8_NB=y
 CONFIG_GENERIC_HARDIRQS=y
diff -urN oldtree/arch/x86_64/kernel/traps.c newtree/arch/x86_64/kernel/traps.c
--- oldtree/arch/x86_64/kernel/traps.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/x86_64/kernel/traps.c	2006-07-12 19:03:58.000000000 -0400
@@ -551,11 +551,14 @@
 
 void die(const char * str, struct pt_regs * regs, long err)
 {
-	unsigned long flags = oops_begin();
+	unsigned long flags;
 
+	oops_enter();
+	flags = oops_begin();
 	handle_BUG(regs);
 	__die(str, regs, err);
 	oops_end(flags);
+	oops_exit();
 	do_exit(SIGSEGV); 
 }
 
diff -urN oldtree/arch/x86_64/mm/fault.c newtree/arch/x86_64/mm/fault.c
--- oldtree/arch/x86_64/mm/fault.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/arch/x86_64/mm/fault.c	2006-07-12 19:03:58.000000000 -0400
@@ -261,9 +261,11 @@
 static noinline void pgtable_bad(unsigned long address, struct pt_regs *regs,
 				 unsigned long error_code)
 {
-	unsigned long flags = oops_begin();
+	unsigned long flags;
 	struct task_struct *tsk;
 
+	oops_enter();
+	flags = oops_begin();
 	printk(KERN_ALERT "%s: Corrupted page table at address %lx\n",
 	       current->comm, address);
 	dump_pagetable(address);
@@ -273,6 +275,7 @@
 	tsk->thread.error_code = error_code;
 	__die("Bad pagetable", regs, error_code);
 	oops_end(flags);
+	oops_exit();
 	do_exit(SIGKILL);
 }
 
@@ -562,6 +565,7 @@
  * terminate things with extreme prejudice.
  */
 
+	oops_enter();
 	flags = oops_begin();
 
 	if (address < PAGE_SIZE)
@@ -578,6 +582,7 @@
 	/* Executive summary in case the body of the oops scrolled away */
 	printk(KERN_EMERG "CR2: %016lx\n", address);
 	oops_end(flags);
+	oops_exit();
 	do_exit(SIGKILL);
 
 /*
diff -urN oldtree/block/Kconfig.iosched newtree/block/Kconfig.iosched
--- oldtree/block/Kconfig.iosched	2006-07-05 10:06:57.000000000 -0400
+++ newtree/block/Kconfig.iosched	2006-07-12 19:00:51.000000000 -0400
@@ -40,7 +40,7 @@
 
 choice
 	prompt "Default I/O scheduler"
-	default DEFAULT_CFQ
+	default DEFAULT_DEADLINE
 	help
 	  Select the I/O scheduler which will be used by default for all
 	  block devices.
diff -urN oldtree/drivers/Makefile newtree/drivers/Makefile
--- oldtree/drivers/Makefile	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/Makefile	2006-07-12 19:01:05.000000000 -0400
@@ -8,6 +8,9 @@
 obj-$(CONFIG_PCI)		+= pci/
 obj-$(CONFIG_PARISC)		+= parisc/
 obj-$(CONFIG_RAPIDIO)		+= rapidio/
+# char/ comes before serial/ etc so that the VT console is the boot-time
+# default.
+obj-y				+= char/
 obj-y				+= video/
 obj-$(CONFIG_ACPI)		+= acpi/
 # PnP must come after ACPI since it will eventually need to check if acpi
@@ -15,10 +18,6 @@
 obj-$(CONFIG_PNP)		+= pnp/
 obj-$(CONFIG_ARM_AMBA)		+= amba/
 
-# char/ comes before serial/ etc so that the VT console is the boot-time
-# default.
-obj-y				+= char/
-
 obj-$(CONFIG_CONNECTOR)		+= connector/
 
 # i810fb and intelfb depend on char/agp/
diff -urN oldtree/drivers/acpi/Kconfig newtree/drivers/acpi/Kconfig
--- oldtree/drivers/acpi/Kconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/acpi/Kconfig	2006-07-12 19:00:43.000000000 -0400
@@ -290,6 +290,23 @@
 	  Enter the full path name to the file which includes the AmlCode
 	  declaration.
 
+config ACPI_CUSTOM_DSDT_INITRD
+	bool "Read Custom DSDT from initramfs"
+	depends on BLK_DEV_INITRD
+	default y
+	help
+	  The DSDT (Differentiated System Description Table) often needs to be
+	  overridden because of broken BIOS implementations. If this feature is
+	  activated you will be able to provide a customized DSDT by adding it
+	  to your initramfs.  For now you need to use a special mkinitrd tool.
+	  For more details see <file:Documentation/dsdt-initrd.txt> or 
+	  <http://gaugusch.at/kernel.shtml>. If there is no table found, it 
+	  will fallback to the custom DSDT in-kernel (if activated) or to the
+	  DSDT from the BIOS.
+
+	  Even if you do not need a new one at the moment, you may want to use a
+	  better implemented DSDT later. It is safe to say Y here.
+
 config ACPI_BLACKLIST_YEAR
 	int "Disable ACPI for systems before Jan 1st this year" if X86_32
 	default 0
diff -urN oldtree/drivers/acpi/cm_sbs.c newtree/drivers/acpi/cm_sbs.c
--- oldtree/drivers/acpi/cm_sbs.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/acpi/cm_sbs.c	2006-07-12 19:03:25.000000000 -0400
@@ -39,93 +39,78 @@
 static struct proc_dir_entry *acpi_ac_dir;
 static struct proc_dir_entry *acpi_battery_dir;
 
-static struct semaphore cm_sbs_sem;
+static DEFINE_MUTEX(cm_sbs_sem);
 
-static int lock_ac_dir_cnt = 0;
-static int lock_battery_dir_cnt = 0;
+static int lock_ac_dir_cnt;
+static int lock_battery_dir_cnt;
 
 struct proc_dir_entry *acpi_lock_ac_dir(void)
 {
 
-	down(&cm_sbs_sem);
-	if (!acpi_ac_dir) {
+	mutex_lock(&cm_sbs_sem);
+	if (!acpi_ac_dir) 
 		acpi_ac_dir = proc_mkdir(ACPI_AC_CLASS, acpi_root_dir);
-	}
 	if (acpi_ac_dir) {
 		lock_ac_dir_cnt++;
 	} else {
 		ACPI_DEBUG_PRINT((ACPI_DB_ERROR,
 				  "Cannot create %s\n", ACPI_AC_CLASS));
 	}
-	up(&cm_sbs_sem);
+	mutex_unlock(&cm_sbs_sem);
 	return acpi_ac_dir;
 }
-
 EXPORT_SYMBOL(acpi_lock_ac_dir);
 
 void acpi_unlock_ac_dir(struct proc_dir_entry *acpi_ac_dir_param)
 {
 
-	down(&cm_sbs_sem);
-	if (acpi_ac_dir_param) {
+	mutex_lock(&cm_sbs_sem);
+	if (acpi_ac_dir_param) 
 		lock_ac_dir_cnt--;
-	}
 	if (lock_ac_dir_cnt == 0 && acpi_ac_dir_param && acpi_ac_dir) {
 		remove_proc_entry(ACPI_AC_CLASS, acpi_root_dir);
 		acpi_ac_dir = 0;
 	}
-	up(&cm_sbs_sem);
+	mutex_unlock(&cm_sbs_sem);
 }
-
 EXPORT_SYMBOL(acpi_unlock_ac_dir);
 
 struct proc_dir_entry *acpi_lock_battery_dir(void)
 {
 
-	down(&cm_sbs_sem);
-	if (!acpi_battery_dir) {
+	mutex_lock(&cm_sbs_sem);
+	if (!acpi_battery_dir) 
 		acpi_battery_dir =
 		    proc_mkdir(ACPI_BATTERY_CLASS, acpi_root_dir);
-	}
 	if (acpi_battery_dir) {
 		lock_battery_dir_cnt++;
 	} else {
 		ACPI_DEBUG_PRINT((ACPI_DB_ERROR,
 				  "Cannot create %s\n", ACPI_BATTERY_CLASS));
 	}
-	up(&cm_sbs_sem);
+	mutex_unlock(&cm_sbs_sem);
 	return acpi_battery_dir;
 }
-
 EXPORT_SYMBOL(acpi_lock_battery_dir);
 
 void acpi_unlock_battery_dir(struct proc_dir_entry *acpi_battery_dir_param)
 {
 
-	down(&cm_sbs_sem);
-	if (acpi_battery_dir_param) {
+	mutex_lock(&cm_sbs_sem);
+	if (acpi_battery_dir_param) 
 		lock_battery_dir_cnt--;
-	}
 	if (lock_battery_dir_cnt == 0 && acpi_battery_dir_param
 	    && acpi_battery_dir) {
 		remove_proc_entry(ACPI_BATTERY_CLASS, acpi_root_dir);
 		acpi_battery_dir = 0;
 	}
-	up(&cm_sbs_sem);
+	mutex_unlock(&cm_sbs_sem);
 	return;
 }
-
 EXPORT_SYMBOL(acpi_unlock_battery_dir);
 
 static int __init acpi_cm_sbs_init(void)
 {
-
-	if (acpi_disabled)
-		return 0;
-
-	init_MUTEX(&cm_sbs_sem);
-
 	return 0;
 }
-
 subsys_initcall(acpi_cm_sbs_init);
diff -urN oldtree/drivers/acpi/dock.c newtree/drivers/acpi/dock.c
--- oldtree/drivers/acpi/dock.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/acpi/dock.c	2006-07-12 19:04:42.000000000 -0400
@@ -627,6 +627,7 @@
 	INIT_LIST_HEAD(&dock_station->hotplug_devices);
 	spin_lock_init(&dock_station->dd_lock);
 	spin_lock_init(&dock_station->hp_lock);
+        ATOMIC_INIT_NOTIFIER_HEAD(&dock_notifier_list);
 
 	/* Find dependent devices */
 	acpi_walk_namespace(ACPI_TYPE_DEVICE, ACPI_ROOT_OBJECT,
diff -urN oldtree/drivers/acpi/ibm_acpi.c newtree/drivers/acpi/ibm_acpi.c
--- oldtree/drivers/acpi/ibm_acpi.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/acpi/ibm_acpi.c	2006-07-12 19:03:11.000000000 -0400
@@ -1529,6 +1529,7 @@
 {
 	int len = 0;
 	int s;
+	char status_read = 0;
 	u8 lo, hi, status;
 
 	if (gfan_handle) {
@@ -1541,16 +1542,27 @@
 		/* all except 570, 600e/x, 770e, 770x */
 		if (!acpi_ec_read(fan_status_offset, &status))
 			len += sprintf(p + len, "status:\t\tunreadable\n");
-		else
+		else {
 			len += sprintf(p + len, "status:\t\t%s\n",
-				       enabled(status, 7));
+					status ? "enabled" : "disabled");
+			status_read = 1;
 
+		}
 		if (!acpi_ec_read(fan_rpm_offset, &lo) ||
 		    !acpi_ec_read(fan_rpm_offset + 1, &hi))
 			len += sprintf(p + len, "speed:\t\tunreadable\n");
 		else
 			len += sprintf(p + len, "speed:\t\t%d\n",
 				       (hi << 8) + lo);
+		if (status_read) {
+			if (status & 0x40)
+				len += sprintf(p + len, "level:\t\tdisengaged\n");
+			else if (status & 0x80)
+				len += sprintf(p + len, "level:\t\tauto\n");
+			else
+				len += sprintf(p + len, "level:\t\t%d\n", status);
+		}
+
 	}
 
 	if (sfan_handle)
@@ -1559,7 +1571,10 @@
 			       " (<level> is 0-7)\n");
 	if (!gfan_handle)
 		/* all except 570, 600e/x, 770e, 770x */
-		len += sprintf(p + len, "commands:\tenable, disable\n");
+                len += sprintf(p + len, 
+                              "commands:\tenable, disable, level <level>\n"
+                              "         \t(<level> is 0-7, auto "
+                              "or disengaged)\n");
 	if (fans_handle)
 		/* X31, X40 */
 		len += sprintf(p + len, "commands:\tspeed <speed>"
@@ -1580,7 +1595,8 @@
 			/* 570, 770x-JL */
 			if (!acpi_evalf(sfan_handle, NULL, NULL, "vd", level))
 				return -EIO;
-		} else if (!gfan_handle && strlencmp(cmd, "enable") == 0) {
+                } else if (!gfan_handle && ( (strlencmp(cmd, "enable") == 0) || 
+                                (strlencmp(cmd, "level auto") == 0) ) ) {
 			/* all except 570, 600e/x, 770e, 770x */
 			if (!acpi_ec_write(fan_status_offset, 0x80))
 				return -EIO;
@@ -1588,6 +1604,17 @@
 			/* all except 570, 600e/x, 770e, 770x */
 			if (!acpi_ec_write(fan_status_offset, 0x00))
 				return -EIO;
+                } else if (!gfan_handle && 
+				strlencmp(cmd, "level disengaged") == 0) {
+                        /* all except 570, 600e/x, 770e, 770x */
+                        if (!acpi_ec_write(fan_status_offset, 0x40))
+                                return -EIO;
+                } else if (!gfan_handle &&
+                    sscanf(cmd, "level %d", &level) == 1 &&
+                    level >=0 && level <= 7) {
+                        /* all except 570, 600e/x, 770e, 770x */
+                        if (!acpi_ec_write(fan_status_offset, level))
+                                return -EIO;
 		} else if (fans_handle &&
 			   sscanf(cmd, "speed %d", &speed) == 1 &&
 			   speed >= 0 && speed <= 65535) {
diff -urN oldtree/drivers/acpi/osl.c newtree/drivers/acpi/osl.c
--- oldtree/drivers/acpi/osl.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/acpi/osl.c	2006-07-12 19:00:43.000000000 -0400
@@ -70,6 +70,10 @@
 int acpi_specific_hotkey_enabled = TRUE;
 EXPORT_SYMBOL(acpi_specific_hotkey_enabled);
 
+#ifdef CONFIG_ACPI_CUSTOM_DSDT_INITRD
+int acpi_must_unregister_table = FALSE;
+#endif
+
 static unsigned int acpi_irq_irq;
 static acpi_osd_handler acpi_irq_handler;
 static void *acpi_irq_context;
@@ -230,6 +234,67 @@
 	return AE_OK;
 }
 
+#ifdef CONFIG_ACPI_CUSTOM_DSDT_INITRD
+struct acpi_table_header * acpi_find_dsdt_initrd(void)
+{
+	struct file *firmware_file;
+	mm_segment_t oldfs;
+	unsigned long len, len2;
+	struct acpi_table_header *dsdt_buffer, *ret = NULL;
+	struct kstat stat;
+	/* maybe this could be an argument on the cmd line, but let's keep it simple for now */
+	char *ramfs_dsdt_name = "/DSDT.aml";
+
+	printk(KERN_INFO PREFIX "Looking for DSDT in initramfs... ");
+
+	/* 
+	 * Never do this at home, only the user-space is allowed to open a file.
+	 * The clean way would be to use the firmware loader. But this code must be run
+	 * before there is any userspace available. So we need a static/init firmware 
+	 * infrastructure, which doesn't exist yet...
+	 */
+	if (vfs_stat(ramfs_dsdt_name, &stat) < 0) {
+		printk("error, file %s not found.\n", ramfs_dsdt_name);
+		return ret;
+	}
+
+	len = stat.size;
+	/* check especially against empty files */
+	if (len <= 4) {
+		printk("error file is too small, only %lu bytes.\n", len);
+		return ret;
+	}
+
+	firmware_file = filp_open(ramfs_dsdt_name, O_RDONLY, 0);
+	if (IS_ERR(firmware_file)) {
+		printk("error, could not open file %s.\n", ramfs_dsdt_name);
+		return ret;
+	}
+
+	dsdt_buffer = ACPI_ALLOCATE(len);
+	if (!dsdt_buffer) {
+		printk("error when allocating %lu bytes of memory.\n", len);
+		goto err;
+	}
+
+	oldfs = get_fs();
+	set_fs(KERNEL_DS);
+	len2 = vfs_read(firmware_file, (char __user *)dsdt_buffer, len, &firmware_file->f_pos);
+	set_fs(oldfs);
+	if (len2 < len) {
+		printk("error trying to read %lu bytes from %s.\n", len, ramfs_dsdt_name);
+		ACPI_FREE(dsdt_buffer);
+		goto err;
+	}
+
+	printk("successfully read %lu bytes from %s.\n", len, ramfs_dsdt_name);
+	ret = dsdt_buffer;
+err:
+	filp_close(firmware_file, NULL);
+	return ret;
+}
+#endif
+
 acpi_status
 acpi_os_table_override(struct acpi_table_header * existing_table,
 		       struct acpi_table_header ** new_table)
@@ -237,13 +302,20 @@
 	if (!existing_table || !new_table)
 		return AE_BAD_PARAMETER;
 
+	*new_table = NULL;
+
 #ifdef CONFIG_ACPI_CUSTOM_DSDT
 	if (strncmp(existing_table->signature, "DSDT", 4) == 0)
 		*new_table = (struct acpi_table_header *)AmlCode;
-	else
-		*new_table = NULL;
-#else
-	*new_table = NULL;
+#endif
+#ifdef CONFIG_ACPI_CUSTOM_DSDT_INITRD
+	if (strncmp(existing_table->signature, "DSDT", 4) == 0) {
+		struct acpi_table_header* initrd_table = acpi_find_dsdt_initrd();
+		if (initrd_table) {
+			*new_table = initrd_table;
+			acpi_must_unregister_table = TRUE;
+		}
+	}
 #endif
 	return AE_OK;
 }
diff -urN oldtree/drivers/acpi/tables/tbget.c newtree/drivers/acpi/tables/tbget.c
--- oldtree/drivers/acpi/tables/tbget.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/acpi/tables/tbget.c	2006-07-12 19:00:43.000000000 -0400
@@ -278,6 +278,14 @@
 	address.pointer.logical = new_table;
 
 	status = acpi_tb_get_this_table(&address, new_table, table_info);
+
+#ifdef CONFIG_ACPI_CUSTOM_DSDT_INITRD
+	if (acpi_must_unregister_table) {
+		ACPI_FREE(new_table);
+		acpi_must_unregister_table = FALSE;
+	}
+#endif
+
 	if (ACPI_FAILURE(status)) {
 		ACPI_EXCEPTION((AE_INFO, status, "Could not copy ACPI table"));
 		return_ACPI_STATUS(status);
diff -urN oldtree/drivers/char/Kconfig newtree/drivers/char/Kconfig
--- oldtree/drivers/char/Kconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/char/Kconfig	2006-07-12 19:00:36.000000000 -0400
@@ -57,6 +57,20 @@
 
 	  If unsure, say Y.
 
+config NR_TTY_DEVICES
+	int "Maximum tty device number"
+	depends on VT
+	range 12 63
+	default 63
+	---help---
+	  This option is used to change the number of tty devices in /dev.
+	  The default value is 63. The lowest number you can set is 12,
+	  63 is also the upper limit so we don't overrun the serial
+	  consoles.
+
+	  If unsure, say 63.
+
+
 config HW_CONSOLE
 	bool
 	depends on VT && !S390 && !UML
diff -urN oldtree/drivers/net/8390.c newtree/drivers/net/8390.c
--- oldtree/drivers/net/8390.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/net/8390.c	2006-07-12 19:03:43.000000000 -0400
@@ -299,7 +299,7 @@
 	 *	Slow phase with lock held.
 	 */
 	 
-	disable_irq_nosync(dev->irq);
+	disable_irq_nosync_lockdep(dev->irq);
 	
 	spin_lock(&ei_local->page_lock);
 	
@@ -338,7 +338,7 @@
 		netif_stop_queue(dev);
 		outb_p(ENISR_ALL, e8390_base + EN0_IMR);
 		spin_unlock(&ei_local->page_lock);
-		enable_irq(dev->irq);
+		enable_irq_lockdep(dev->irq);
 		ei_local->stat.tx_errors++;
 		return 1;
 	}
@@ -379,7 +379,7 @@
 	outb_p(ENISR_ALL, e8390_base + EN0_IMR);
 	
 	spin_unlock(&ei_local->page_lock);
-	enable_irq(dev->irq);
+	enable_irq_lockdep(dev->irq);
 
 	dev_kfree_skb (skb);
 	ei_local->stat.tx_bytes += send_length;
@@ -505,9 +505,9 @@
 #ifdef CONFIG_NET_POLL_CONTROLLER
 void ei_poll(struct net_device *dev)
 {
-	disable_irq(dev->irq);
+	disable_irq_lockdep(dev->irq);
 	ei_interrupt(dev->irq, dev, NULL);
-	enable_irq(dev->irq);
+	enable_irq_lockdep(dev->irq);
 }
 #endif
 
diff -urN oldtree/drivers/usb/input/Kconfig newtree/drivers/usb/input/Kconfig
--- oldtree/drivers/usb/input/Kconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/usb/input/Kconfig	2006-07-12 19:03:20.000000000 -0400
@@ -24,6 +24,38 @@
 	  To compile this driver as a module, choose M here: the
 	  module will be called usbhid.
 
+config USB_HID_MOUSE_POLLING_INTERVAL
+	int "USB HID Mouse Interrupt Polling Interval"
+	default 10
+	depends on USB_HID
+	help
+	  The "USB HID Mouse Interrupt Polling Interval" is the interval, at
+	  which your USB HID mouse is to be polled at. The interval is
+	  specified in milliseconds.
+
+	  Decreasing the interval will, of course, give you a much more
+	  precise mouse.
+
+	  Generally speaking, a polling interval of 2 ms should be more than
+	  enough for most people, and is great for gaming and other things
+	  that require high precision.
+
+	  An interval lower than the default is not guaranteed work on your
+	  specific piece of hardware. If you want to play it safe, don't
+	  change this value.
+
+	  Now, if you indeed want to feel the joy of a precise mouse, the
+	  following mice are known to work without problems, when the interval
+	  is set to at least 2 ms:
+
+	    * Logitech's MX-family
+	    * Logitech Mouse Man Dual Optical
+	    * Logitech iFeel
+	    * Microsoft Intellimouse Explorer
+	    * Microsoft Intellimouse Optical 1.1
+	
+	  If unsure, keep it at 10 ms.
+
 comment "Input core support is needed for USB HID input layer or HIDBP support"
 	depends on USB_HID && INPUT=n
 
diff -urN oldtree/drivers/usb/input/hid-core.c newtree/drivers/usb/input/hid-core.c
--- oldtree/drivers/usb/input/hid-core.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/usb/input/hid-core.c	2006-07-12 19:03:20.000000000 -0400
@@ -50,7 +50,7 @@
  * Module parameters.
  */
 
-static unsigned int hid_mousepoll_interval;
+static unsigned int hid_mousepoll_interval = CONFIG_USB_HID_MOUSE_POLLING_INTERVAL;
 module_param_named(mousepoll, hid_mousepoll_interval, uint, 0644);
 MODULE_PARM_DESC(mousepoll, "Polling interval of mice");
 
diff -urN oldtree/drivers/usb/storage/Kconfig newtree/drivers/usb/storage/Kconfig
--- oldtree/drivers/usb/storage/Kconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/usb/storage/Kconfig	2006-07-12 19:03:16.000000000 -0400
@@ -135,6 +135,13 @@
 	  this input in any keybinding software. (e.g. gnome's keyboard short-
 	  cuts)
 
+config USB_USBXCHANGE
+	tristate "Adaptec USBXchange and USB2Xchange firmware loader"
+	depends on USB_STORAGE
+	help
+	  Say Y here to include additional code to load the firmware into the
+	  Adaptec USBXchange and USB2Xchange USB --> SCSI converter dongle.
+
 config USB_LIBUSUAL
 	bool "The shared table of common (or usual) storage devices"
 	depends on USB
diff -urN oldtree/drivers/usb/storage/Makefile newtree/drivers/usb/storage/Makefile
--- oldtree/drivers/usb/storage/Makefile	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/usb/storage/Makefile	2006-07-12 19:03:16.000000000 -0400
@@ -24,6 +24,8 @@
 usb-storage-objs :=	scsiglue.o protocol.o transport.o usb.o \
 			initializers.o $(usb-storage-obj-y)
 
+obj-$(CONFIG_USB_USBXCHANGE)			+= usbxchange_fw.o
+
 ifneq ($(CONFIG_USB_LIBUSUAL),)
 	obj-$(CONFIG_USB)	+= libusual.o
 endif
diff -urN oldtree/drivers/usb/storage/initializers.c newtree/drivers/usb/storage/initializers.c
--- oldtree/drivers/usb/storage/initializers.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/usb/storage/initializers.c	2006-07-12 19:03:16.000000000 -0400
@@ -164,3 +164,27 @@
 	return USB_STOR_TRANSPORT_FAILED;
 }
 
+/* Firmware Initialisation for the Adaptec USB2Xchange, needed for
+ * to recognize devices properly. RenÃÂ© Rebe <rene@exactcode.de> */
+int usb2xchange_init(struct us_data *us)
+{
+	int result;
+
+	US_DEBUGP ("usb2xchange_init: initialising after reenumeration.\n");
+
+	result = usb_control_msg(us->pusb_dev, us->send_ctrl_pipe, 
+				 0x5a, 0x40, 0x01,
+				 0, 0,	// buffer,
+				 0,	// length,
+				 300);
+	US_DEBUGP ("usb2xchange_init: reset #1 (%d)\n", result);
+
+	result = usb_control_msg(us->pusb_dev, us->send_ctrl_pipe,
+				 0x5a, 0x40, 0x02,
+				 0, 0,	// buffer,
+				 0,	// length,
+				 300);
+	US_DEBUGP ("usb2xchange_init: reset #2 (%d)\n", result);
+
+	return result;
+}
diff -urN oldtree/drivers/usb/storage/initializers.h newtree/drivers/usb/storage/initializers.h
--- oldtree/drivers/usb/storage/initializers.h	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/usb/storage/initializers.h	2006-07-12 19:03:16.000000000 -0400
@@ -48,3 +48,6 @@
  * flash reader */
 int usb_stor_ucr61s2b_init(struct us_data *us);
 int rio_karma_init(struct us_data *us);
+
+/* Firmware Initialization for the Adaptec USB2Xchange */
+int usb2xchange_init(struct us_data *us);
diff -urN oldtree/drivers/usb/storage/unusual_devs.h newtree/drivers/usb/storage/unusual_devs.h
--- oldtree/drivers/usb/storage/unusual_devs.h	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/usb/storage/unusual_devs.h	2006-07-12 19:03:16.000000000 -0400
@@ -1286,6 +1286,21 @@
 		US_SC_DEVICE, US_PR_DEVICE, NULL,
 		US_FL_FIX_CAPACITY),
 
+/* Adaptec USBXchange and USB2Xchange, after firmware download.
+ * Requires Ez-USB Style firmware loader. RenÃÂ© Rebe <rene@exactcode.de> */
+
+UNUSUAL_DEV(  0x03f3, 0x2001, 0x0000, 0xffff,
+		"Adaptec",
+		"USBXchange",
+		US_SC_SCSI, US_PR_BULK, NULL,
+		0 ),
+
+UNUSUAL_DEV(  0x03f3, 0x2003, 0x0000, 0xffff,
+		"Adaptec",
+		"USB2Xchange",
+		US_SC_SCSI, US_PR_BULK, usb2xchange_init,
+		0 ),
+
 /* Control/Bulk transport for all SubClass values */
 USUAL_DEV(US_SC_RBC, US_PR_CB, USB_US_TYPE_STOR),
 USUAL_DEV(US_SC_8020, US_PR_CB, USB_US_TYPE_STOR),
diff -urN oldtree/drivers/usb/storage/usbxchange_fw.c newtree/drivers/usb/storage/usbxchange_fw.c
--- oldtree/drivers/usb/storage/usbxchange_fw.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/drivers/usb/storage/usbxchange_fw.c	2006-07-12 19:03:16.000000000 -0400
@@ -0,0 +1,215 @@
+/*
+ * Firmware loader for Adaptec USBXchange / USB2Xchange.
+ *
+ * Uploads device firmware into the Adaptec USBXchange and USB2Xchange
+ * USB --> SCSI dongle.
+ *
+ * Current development and maintenance by:
+ *   (c) 2005 RenÃÂ© Rebe <rene@exactcode.de>
+ *
+ * Initial work by:
+ *   (c) 2004 Beier & Dauskardt IT <sda@bdit.de>
+ *
+ * Based on emi26.c:
+ *   (c) 2002 Tapio LaxstrÃÂ¶m <tapio.laxstrom@iptime.fi>
+ *
+ * To use this driver, you need to get the devices firmware from some
+ * windows driver:
+ *   usbxchg_win_v120.exe - for USBXchange
+ *   usb2xchg_win_drv_v200.exe - for USB2Xchange
+ *
+ * Hotplug firmware loader compatible files can be found at:
+ *   http://dl.exactcode.de/adaptec-usbxchange/
+ *
+ * Note:
+ * The USB2Xchange seems to have some internal buffer < 64K. 
+ * Sending 64K requests crashes the device. Possibly it needs a
+ * "max_sectors: 8" setting.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, as published by
+ * the Free Software Foundation, version 2.
+ */
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/usb.h>
+#include <linux/firmware.h>
+
+#include "usbxchange_fw.h"
+
+static int usbxchange_writememory(struct usb_device *dev, int address,
+				  unsigned char *data, int length,
+				  __u8 bRequest);
+static int usbxchange_set_reset(struct usb_device *dev, int cpureg,
+				unsigned char reset_bit);
+static int usbxchange_load_firmware(struct usb_device *dev);
+
+static int usbxchange_probe(struct usb_interface *iface,
+			    const struct usb_device_id *id);
+static void usbxchange_disconnect(struct usb_interface *iface);
+static int __init usbxchange_init(void);
+static void __exit usbxchange_exit(void);
+
+#define usbxchange_VENDOR_ID 0x03f3
+#define usbxchange_PRODUCT_ID 0x2000
+#define usb2xchange_PRODUCT_ID 0x2002
+
+static struct usb_device_id usbxchange_usb_ids[] = {
+	{USB_DEVICE(usbxchange_VENDOR_ID, usbxchange_PRODUCT_ID)},
+	{USB_DEVICE(usbxchange_VENDOR_ID, usb2xchange_PRODUCT_ID)},
+	{}			/* terminating entry */
+};
+
+MODULE_DEVICE_TABLE(usb, usbxchange_usb_ids);
+
+/* thanks to drivers/usb/serial/keyspan_pda.c code */
+static int usbxchange_writememory(struct usb_device *dev, int address,
+                                  unsigned char *data, int length, __u8 request)
+{
+	int result;
+	unsigned char *buffer = kmalloc(length, GFP_KERNEL);
+
+	if (!buffer) {
+		printk(KERN_ERR "usbxchange: kmalloc(%d) failed.\n", length);
+		return -ENOMEM;
+	}
+	memcpy(buffer, data, length);
+	result = usb_control_msg(dev, usb_sndctrlpipe(dev, 0), request, 0x40,
+	                         address, 0, buffer, length, 300);
+	kfree(buffer);
+	return result;
+}
+
+/* thanks to drivers/usb/serial/keyspan_pda.c code */
+static int usbxchange_set_reset(struct usb_device *dev, int cpureg,
+				unsigned char reset_bit)
+{
+	int response;
+	printk(KERN_INFO "%s - %d\n", __FUNCTION__, reset_bit);
+	response =
+	    usbxchange_writememory(dev, cpureg, &reset_bit, 1,
+				   ANCHOR_LOAD_INTERNAL);
+	if (response < 0) {
+		printk(KERN_ERR "usbxchange: set_reset (%d) failed\n",
+		       reset_bit);
+	}
+	return response;
+}
+
+static int usbxchange_load_firmware(struct usb_device *dev)
+{
+	INTEL_HEX_RECORD *record;
+	int err, cpureg;
+
+	const struct firmware *firmware;
+
+	switch (le16_to_cpu(dev->descriptor.idProduct)) {
+	case usbxchange_PRODUCT_ID:
+		err = request_firmware(&firmware, "usbxchange.fw", &dev->dev);
+		cpureg = CPUCS_REG;
+		break;
+	case usb2xchange_PRODUCT_ID:
+		err = request_firmware(&firmware, "usb2xchange.fw", &dev->dev);
+		cpureg = CPUCS_REG_FX2;
+		break;
+	default:
+		printk(KERN_ERR "%s - device not recognized %x\n", __FUNCTION__,
+		       le16_to_cpu(dev->descriptor.idProduct));
+		return 1;
+	}
+
+	if (err != 0) {
+		printk(KERN_ERR "Hotplug firmware request failed.\n");
+		return err;
+	}
+
+	/* Stop CPU */
+	err = usbxchange_set_reset(dev, cpureg, 1);
+	err = usbxchange_set_reset(dev, cpureg, 1);
+	if (err < 0) {
+		printk(KERN_ERR "%s - error stopping dongle CPU: error = %d\n",
+		       __FUNCTION__, err);
+		return err;
+	}
+
+	/* Upload firmware */
+	for (record = (INTEL_HEX_RECORD *)firmware->data;
+	     record->type == 0; record++) {
+
+		err = usbxchange_writememory(dev, le32_to_cpu(record->address),
+					     record->data,
+					     le32_to_cpu(record->length),
+					     ANCHOR_LOAD_INTERNAL);
+		if (err < 0) {
+			printk(KERN_ERR
+			       "%s - error loading firmware: error = %d\n",
+			       __FUNCTION__, err);
+			return err;
+		}
+	}
+
+	/* De-assert reset (let the CPU run) */
+	err = usbxchange_set_reset(dev, cpureg, 1);
+	err = usbxchange_set_reset(dev, cpureg, 0);
+	if (err < 0) {
+		printk(KERN_ERR "%s - error resetting dongle CPU: error = %d\n",
+		       __FUNCTION__, err);
+		return err;
+	}
+
+	return 0;
+}
+
+static int usbxchange_probe(struct usb_interface *iface,
+                            const struct usb_device_id *id)
+{
+	struct usb_device *dev = interface_to_usbdev(iface);
+
+	printk(KERN_INFO "%s start\n", __FUNCTION__);
+
+	usbxchange_load_firmware(dev);
+
+	/* forcing an unload would save some kB of kernel memory ... */
+	return 0;
+}
+
+static void usbxchange_disconnect(struct usb_interface *iface)
+{
+}
+
+static struct usb_driver usbxchange_driver = {
+	.name = "usbxchange_fw",
+	.probe = usbxchange_probe,
+	.disconnect = usbxchange_disconnect,
+	.id_table = usbxchange_usb_ids,
+};
+
+static int __init usbxchange_init(void)
+{
+	usb_register(&usbxchange_driver);
+	return 0;
+}
+
+static void __exit usbxchange_exit(void)
+{
+	usb_deregister(&usbxchange_driver);
+}
+
+module_init(usbxchange_init);
+module_exit(usbxchange_exit);
+
+MODULE_AUTHOR("RenÃÂ© Rebe <rene@exactcode.de>, Sancho Dauskardt <sda@bdit.de>");
+MODULE_DESCRIPTION("Adaptec USBXchange firmware loader.");
+MODULE_LICENSE("GPL");
+
+/* vi:ai:syntax=c:sw=8:ts=8:tw=80
+ */
diff -urN oldtree/drivers/usb/storage/usbxchange_fw.h newtree/drivers/usb/storage/usbxchange_fw.h
--- oldtree/drivers/usb/storage/usbxchange_fw.h	1969-12-31 19:00:00.000000000 -0500
+++ newtree/drivers/usb/storage/usbxchange_fw.h	2006-07-12 19:03:16.000000000 -0400
@@ -0,0 +1,45 @@
+/* 
+ * Firmware loader for Adaptec USBXchange / USB2Xchange.
+ *
+ * Uploads device firmware into the Adaptec USBXchange and USB2Xchange
+ * USB --> SCSI dongle.
+ *
+ * Current development and maintenance by:
+ *   (c) 2005 RenÃÂ© Rebe <rene@exactcode.de>
+ *
+ * Initial work by:
+ *   (c) 2004 Beier & Dauskardt IT <sda@bdit.de>
+ *
+ * Based on emi26.c:
+ *   (c) 2002 Tapio LaxstrÃÂ¶m <tapio.laxstrom@iptime.fi>
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, as published by
+ * the Free Software Foundation, version 2.
+ */
+
+#ifndef _USB_USBXCHANGE_FW_H_INCLUDED
+#define _USB_USBXCHANGE_FW_H_INCLUDED
+
+#define MAX_INTEL_HEX_RECORD_LENGTH 16
+typedef struct _INTEL_HEX_RECORD {
+	__u32 length;
+	__u32 address;
+	__u32 type;
+	__u8 data[MAX_INTEL_HEX_RECORD_LENGTH];
+} INTEL_HEX_RECORD, *PINTEL_HEX_RECORD;
+
+/* Vendor specific request code for Anchor Upload/Download
+   (This one is implemented in the core). */
+#define ANCHOR_LOAD_INTERNAL	0xA0
+
+/* EZ-USB Control and Status Register. Bit 0 controls 8051 reset */
+#define CPUCS_REG		0x7F92	/* original / FX */
+#define CPUCS_REG_FX2		0xE600	/* FX2 */
+
+#endif
diff -urN oldtree/drivers/video/Kconfig newtree/drivers/video/Kconfig
--- oldtree/drivers/video/Kconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/video/Kconfig	2006-07-12 19:01:08.000000000 -0400
@@ -539,8 +539,22 @@
 	  cards. Say Y if you have one of those.
 
 config FB_VESA
-	bool "VESA VGA graphics support"
-	depends on (FB = y) && X86 && !VGA_NOPROBE
+        tristate "VESA VGA graphics support"
+        depends on (FB = y) && (X86 || X86_64)
+        help
+          This is the frame buffer device driver for generic VESA 2.0
+          compliant graphic cards. The older VESA 1.2 cards are not supported.
+          You will get a boot time penguin logo at no additional cost. Please
+          read <file:Documentation/fb/vesafb.txt>. If unsure, say Y.
+
+choice
+        prompt "VESA driver type"
+        depends on FB_VESA
+        default FB_VESA_STD if X86_64
+        default FB_VESA_TNG if X86
+
+config FB_VESA_STD
+        bool "vesafb"
 	select FB_CFB_FILLRECT
 	select FB_CFB_COPYAREA
 	select FB_CFB_IMAGEBLIT
@@ -548,7 +562,43 @@
 	  This is the frame buffer device driver for generic VESA 2.0
 	  compliant graphic cards. The older VESA 1.2 cards are not supported.
 	  You will get a boot time penguin logo at no additional cost. Please
-	  read <file:Documentation/fb/vesafb.txt>. If unsure, say Y.
+	  read <file:Documentation/fb/vesafb.txt>. Choose this driver if you
+	  are experiencing problems with vesafb-tng or if you own a 64-bit system.
+
+	  Note that this driver cannot be compiled as a module.
+
+config FB_VESA_TNG
+	bool "vesafb-tng"
+	depends on !X86_64
+	select FB_MODE_HELPERS
+	select FB_CFB_FILLRECT
+	select FB_CFB_COPYAREA
+	select FB_CFB_IMAGEBLIT
+	help
+	  This is the frame buffer device driver for generic VESA 2.0 
+	  compliant graphic cards. It is capable of taking advantage of 
+	  VBE 3.0 features. With this driver you will be able to adjust
+	  the refresh rate (VBE 3.0 compliant boards only) and change
+	  the graphic mode on-the-fly.
+	  
+	  You will also get a boot time penguin logo at no additional cost. Please
+	  read <file:Documentation/fb/vesafb.txt>.
+
+endchoice
+
+config FB_VESA_DEFAULT_MODE
+	string "VESA default mode"
+	depends on FB_VESA_TNG
+	default "640x480@60"
+	help 
+	  This option is used to determine the default mode vesafb is
+	  supposed to switch to in case no mode is provided as a kernel
+	  command line parameter.
+
+config VIDEO_SELECT
+	bool
+	depends on FB_VESA
+	default y
 
 config FB_IMAC
 	bool "Intel-based Macintosh Framebuffer Support"
@@ -857,7 +907,6 @@
 	select FB_CFB_FILLRECT
 	select FB_CFB_COPYAREA
 	select FB_CFB_IMAGEBLIT
-	select FB_TILEBLITTING
 	select FB_MACMODES if PPC_PMAC
 	---help---
 	  Say Y here if you have a Matrox Millennium, Matrox Millennium II,
@@ -1604,5 +1653,15 @@
 	source "drivers/video/backlight/Kconfig"
 endif
 
-endmenu
+config FB_SPLASH
+	bool "Support for the framebuffer splash"
+	depends on FRAMEBUFFER_CONSOLE=y && !FB_TILEBLITTING
+	default n
+	---help---
+	  This option enables support for the Linux boot-up splash screen and
+	  graphical backgrounds on consoles. Note that you will need userspace
+	  splash utilities in order to take advantage of these features. Refer 
+	  to Documentation/fb/splash.txt for more information.
 
+	  If unsure, say N.
+endmenu
diff -urN oldtree/drivers/video/Makefile newtree/drivers/video/Makefile
--- oldtree/drivers/video/Makefile	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/video/Makefile	2006-07-12 19:01:08.000000000 -0400
@@ -12,6 +12,7 @@
 obj-$(CONFIG_VT)		  += console/
 obj-$(CONFIG_LOGO)		  += logo/
 obj-$(CONFIG_SYSFS)		  += backlight/
+obj-$(CONFIG_FB_SPLASH)           += fbsplash.o cfbsplash.o
 
 obj-$(CONFIG_FB_CFB_FILLRECT)  += cfbfillrect.o
 obj-$(CONFIG_FB_CFB_COPYAREA)  += cfbcopyarea.o
@@ -98,7 +99,11 @@
 obj-$(CONFIG_FB_PNX4008_DUM_RGB)  += pnx4008/
 
 # Platform or fallback drivers go here
-obj-$(CONFIG_FB_VESA)             += vesafb.o
+ifeq ($(CONFIG_FB_VESA_STD),y)
+  obj-y				  += vesafb.o
+else
+  obj-$(CONFIG_FB_VESA)		  += vesafb-thread.o vesafb-tng.o
+endif
 obj-$(CONFIG_FB_IMAC)             += imacfb.o
 obj-$(CONFIG_FB_VGA16)            += vga16fb.o vgastate.o
 obj-$(CONFIG_FB_OF)               += offb.o
diff -urN oldtree/drivers/video/cfbsplash.c newtree/drivers/video/cfbsplash.c
--- oldtree/drivers/video/cfbsplash.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/drivers/video/cfbsplash.c	2006-07-12 19:01:05.000000000 -0400
@@ -0,0 +1,472 @@
+/*
+ *  linux/drivers/video/cfbsplash.c -- Framebuffer splash render functions
+ *  
+ *  Copyright (C) 2004 Michal Januszewski <spock@gentoo.org>
+ *
+ *  Code based upon "Bootsplash" (C) 2001-2003 
+ *       Volker Poplawski <volker@poplawski.de>,
+ *       Stefan Reinauer <stepan@suse.de>,
+ *       Steffen Winterfeldt <snwint@suse.de>,
+ *       Michael Schroeder <mls@suse.de>,
+ *       Ken Wimer <wimer@suse.de>.
+ *
+ *  This file is subject to the terms and conditions of the GNU General Public
+ *  License.  See the file COPYING in the main directory of this archive for
+ *  more details.
+ */ 
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/fb.h>
+#include <linux/selection.h>
+#include <linux/vt_kern.h>
+#include <asm/irq.h>
+#include <asm/system.h>
+
+#include "console/fbcon.h"
+#include "fbsplash.h"
+
+#define parse_pixel(shift,bpp,type)						\
+	do {									\
+		if (d & (0x80 >> (shift)))					\
+			dd2[(shift)] = fgx;					\
+		else								\
+			dd2[(shift)] = transparent ? *(type *)splash_src : bgx;	\
+		splash_src += (bpp);						\
+	} while (0)								\
+
+extern int get_color(struct vc_data *vc, struct fb_info *info,
+		     u16 c, int is_fg);
+
+void fbsplash_fix_pseudo_pal(struct fb_info *info, struct vc_data *vc)
+{
+	int i, j, k;
+	int minlen = min(min(info->var.red.length, info->var.green.length), 
+			     info->var.blue.length);
+	u32 col;
+	
+	for (j = i = 0; i < 16; i++) {
+		k = color_table[i];
+                      
+		col = ((vc->vc_palette[j++]  >> (8-minlen)) 
+			<< info->var.red.offset);
+		col |= ((vc->vc_palette[j++] >> (8-minlen)) 
+			<< info->var.green.offset);
+		col |= ((vc->vc_palette[j++] >> (8-minlen)) 
+			<< info->var.blue.offset);
+			((u32 *)info->pseudo_palette)[k] = col;
+	}
+}
+				
+void fbsplash_renderc(struct fb_info *info, int ypos, int xpos, int height, 
+		      int width, u8* src, u32 fgx, u32 bgx, u8 transparent)
+{	
+	unsigned int x, y;
+	u32 dd;
+	int bytespp = ((info->var.bits_per_pixel + 7) >> 3);
+	unsigned int d = ypos * info->fix.line_length + xpos * bytespp;
+	unsigned int ds = (ypos * info->var.xres + xpos) * bytespp;
+	u16 dd2[4];
+
+	u8* splash_src = (u8 *)(info->splash.data + ds);
+	u8* dst = (u8 *)(info->screen_base + d);
+
+	if ((ypos + height) > info->var.yres || (xpos + width) > info->var.xres)
+		return;
+	
+	for (y = 0; y < height; y++) {
+		switch (info->var.bits_per_pixel) {
+	
+		case 32:
+			for (x = 0; x < width; x++) {
+
+				if ((x & 7) == 0)
+					d = *src++;
+				if (d & 0x80)
+					dd = fgx;
+				else
+					dd = transparent ? 
+					     *(u32 *)splash_src : bgx;
+				
+				d <<= 1;
+				splash_src += 4;
+				fb_writel(dd, dst);
+				dst += 4;
+			}
+			break;
+		case 24:
+			for (x = 0; x < width; x++) {
+
+				if ((x & 7) == 0)
+					d = *src++;
+				if (d & 0x80)
+					dd = fgx;
+				else
+					dd = transparent ? 
+					     (*(u32 *)splash_src & 0xffffff) : bgx;
+				
+				d <<= 1;
+				splash_src += 3;
+#ifdef __LITTLE_ENDIAN
+				fb_writew(dd & 0xffff, dst);
+				dst += 2;
+				fb_writeb((dd >> 16), dst);
+#else
+				fb_writew(dd >> 8, dst);
+				dst += 2;
+				fb_writeb(dd & 0xff, dst);
+#endif
+				dst++;
+			}
+			break;
+		case 16:
+			for (x = 0; x < width; x += 2) {
+		    		if ((x & 7) == 0)
+					d = *src++;
+
+				parse_pixel(0, 2, u16);
+				parse_pixel(1, 2, u16);
+#ifdef __LITTLE_ENDIAN
+				dd = dd2[0] | (dd2[1] << 16);
+#else
+				dd = dd2[1] | (dd2[0] << 16);
+#endif
+				d <<= 2;
+				fb_writel(dd, dst);
+				dst += 4;
+			}
+			break;
+
+		case 8:
+			for (x = 0; x < width; x += 4) {
+				if ((x & 7) == 0)
+					d = *src++;
+	
+				parse_pixel(0, 1, u8);
+				parse_pixel(1, 1, u8);
+				parse_pixel(2, 1, u8);
+				parse_pixel(3, 1, u8);
+		
+#ifdef __LITTLE_ENDIAN
+				dd = dd2[0] | (dd2[1] << 8) | (dd2[2] << 16) | (dd2[3] << 24);
+#else
+				dd = dd2[3] | (dd2[2] << 8) | (dd2[1] << 16) | (dd2[0] << 24);
+#endif
+				d <<= 4;
+				fb_writel(dd, dst);
+				dst += 4;
+			}		
+		}
+
+		dst += info->fix.line_length - width * bytespp;
+		splash_src += (info->var.xres - width) * bytespp;
+    	}
+}
+
+#define cc2cx(a) 						\
+	((info->fix.visual == FB_VISUAL_TRUECOLOR || 		\
+	  info->fix.visual == FB_VISUAL_DIRECTCOLOR) ? 		\
+	 ((u32*)info->pseudo_palette)[a] : a)
+
+void fbsplash_putcs(struct vc_data *vc, struct fb_info *info,
+		   const unsigned short *s, int count, int yy, int xx)
+{
+	unsigned short charmask = vc->vc_hi_font_mask ? 0x1ff : 0xff;
+	struct fbcon_ops *ops = info->fbcon_par;
+	int fg_color, bg_color, transparent;
+	u8 *src;
+	u32 bgx, fgx;
+	u16 c = scr_readw(s);
+
+	fg_color = get_color(vc, info, c, 1);
+        bg_color = get_color(vc, info, c, 0);
+	
+	/* Don't paint the background image if console is blanked */
+	transparent = ops->blank_state ? 0 : 
+		(vc->vc_splash.bg_color == bg_color);
+
+	xx = xx * vc->vc_font.width + vc->vc_splash.tx;
+	yy = yy * vc->vc_font.height + vc->vc_splash.ty;
+
+	fgx = cc2cx(fg_color);
+	bgx = cc2cx(bg_color);
+
+	while (count--) {
+		c = scr_readw(s++);
+		src = vc->vc_font.data + (c & charmask) * vc->vc_font.height *
+		      ((vc->vc_font.width + 7) >> 3);
+
+		fbsplash_renderc(info, yy, xx, vc->vc_font.height, 
+			       vc->vc_font.width, src, fgx, bgx, transparent);
+		xx += vc->vc_font.width;
+	}
+}
+
+void fbsplash_cursor(struct fb_info *info, struct fb_cursor *cursor)
+{
+	int i;
+	unsigned int dsize, s_pitch;
+	struct fbcon_ops *ops = info->fbcon_par;
+	struct vc_data* vc;	
+	u8 *src;
+
+	/* we really don't need any cursors while the console is blanked */
+	if (info->state != FBINFO_STATE_RUNNING || ops->blank_state)
+		return;
+
+	vc = vc_cons[ops->currcon].d;
+
+	src = kmalloc(64 + sizeof(struct fb_image), GFP_ATOMIC);
+	if (!src)
+		return;
+
+	s_pitch = (cursor->image.width + 7) >> 3;
+	dsize = s_pitch * cursor->image.height;
+	if (cursor->enable) {	
+		switch (cursor->rop) {
+		case ROP_XOR:
+			for (i = 0; i < dsize; i++)
+				src[i] = cursor->image.data[i] ^ cursor->mask[i];
+                        break;
+		case ROP_COPY:
+		default:
+			for (i = 0; i < dsize; i++)
+				src[i] = cursor->image.data[i] & cursor->mask[i];
+			break;
+		}
+	} else
+		memcpy(src, cursor->image.data, dsize);
+
+	fbsplash_renderc(info,
+			cursor->image.dy + vc->vc_splash.ty,
+			cursor->image.dx + vc->vc_splash.tx,
+			cursor->image.height,
+			cursor->image.width,
+			(u8*)src,
+			cc2cx(cursor->image.fg_color),
+			cc2cx(cursor->image.bg_color),
+			cursor->image.bg_color == vc->vc_splash.bg_color);
+
+	kfree(src);
+}
+
+static void splashset(u8 *dst, int height, int width, int dstbytes, 
+		        u32 bgx, int bpp)
+{
+	int i;
+	
+	if (bpp == 8)
+		bgx |= bgx << 8;
+	if (bpp == 16 || bpp == 8)
+		bgx |= bgx << 16;
+	
+	while (height-- > 0) {
+		u8 *p = dst;
+		
+		switch (bpp) {
+		
+		case 32:
+			for (i=0; i < width; i++) {
+				fb_writel(bgx, p); p += 4;
+			}
+			break;
+		case 24:	
+			for (i=0; i < width; i++) {
+#ifdef __LITTLE_ENDIAN
+				fb_writew((bgx & 0xffff),(u16*)p); p += 2;
+				fb_writeb((bgx >> 16),p++);
+#else
+				fb_writew((bgx >> 8),(u16*)p); p += 2;
+				fb_writeb((bgx & 0xff),p++);
+#endif
+			}
+		case 16:
+			for (i=0; i < width/4; i++) {
+				fb_writel(bgx,p); p += 4;
+				fb_writel(bgx,p); p += 4;
+			}
+			if (width & 2) {
+				fb_writel(bgx,p); p += 4;
+			}
+			if (width & 1)
+				fb_writew(bgx,(u16*)p);
+			break;
+		case 8:
+			for (i=0; i < width/4; i++) {
+				fb_writel(bgx,p); p += 4;
+			}
+			
+			if (width & 2) {
+				fb_writew(bgx,p); p += 2;
+			}
+			if (width & 1)
+				fb_writeb(bgx,(u8*)p);
+			break;
+
+		}		
+		dst += dstbytes;
+	}
+}
+
+void fbsplash_copy(u8 *dst, u8 *src, int height, int width, int linebytes, 
+		   int srclinebytes, int bpp)
+{
+	int i;
+
+	while (height-- > 0) {
+		u32 *p = (u32 *)dst;
+		u32 *q = (u32 *)src;
+
+		switch (bpp) {
+	
+		case 32:
+			for (i=0; i < width; i++)
+				fb_writel(*q++, p++);
+			break;	
+		case 24:	
+			for (i=0; i < (width*3/4); i++)
+				fb_writel(*q++, p++);
+			if ((width*3) % 4) {
+				if (width & 2) {
+					fb_writeb(*(u8*)q, (u8*)p);
+				} else if (width & 1) {
+					fb_writew(*(u16*)q, (u16*)p);
+					fb_writeb(*(u8*)((u16*)q+1),(u8*)((u16*)p+2));
+				}
+			}
+			break;
+		case 16:
+			for (i=0; i < width/4; i++) {
+				fb_writel(*q++, p++);
+				fb_writel(*q++, p++);
+			}
+			if (width & 2)
+				fb_writel(*q++, p++);
+			if (width & 1)
+				fb_writew(*(u16*)q, (u16*)p);
+			break;
+		case 8:
+			for (i=0; i < width/4; i++) 
+				fb_writel(*q++, p++);
+				
+			if (width & 2) {
+				fb_writew(*(u16*)q, (u16*)p); 
+				q = (u32*) ((u16*)q + 1);
+				p = (u32*) ((u16*)p + 1);
+			}
+			if (width & 1)
+				fb_writeb(*(u8*)q, (u8*)p);
+			break;
+		}
+
+		dst += linebytes;
+		src += srclinebytes;
+	}
+}
+
+static void splashfill(struct fb_info *info, int sy, int sx, int height, 
+		       int width) 
+{
+	int bytespp = ((info->var.bits_per_pixel + 7) >> 3);
+	int d  = sy * info->fix.line_length + sx * bytespp;
+	int ds = (sy * info->var.xres + sx) * bytespp;
+
+	fbsplash_copy((u8 *)(info->screen_base + d), (u8 *)(info->splash.data + ds),
+		    height, width, info->fix.line_length, info->var.xres * bytespp,
+		    info->var.bits_per_pixel);
+}
+
+void fbsplash_clear(struct vc_data *vc, struct fb_info *info, int sy, int sx, 
+		    int height, int width)
+{
+	int bgshift = (vc->vc_hi_font_mask) ? 13 : 12;
+	int bg_color = attr_bgcol_ec(bgshift, vc);
+	int transparent = vc->vc_splash.bg_color == bg_color;
+	struct fbcon_ops *ops = info->fbcon_par;
+	u8 *dst;
+
+	sy = sy * vc->vc_font.height + vc->vc_splash.ty;
+	sx = sx * vc->vc_font.width + vc->vc_splash.tx;
+	height *= vc->vc_font.height;
+	width *= vc->vc_font.width;
+
+	/* Don't paint the background image if console is blanked */
+	if (transparent && !ops->blank_state) {
+		splashfill(info, sy, sx, height, width);
+	} else {
+		dst = (u8 *)(info->screen_base + sy * info->fix.line_length + 
+			     sx * ((info->var.bits_per_pixel + 7) >> 3));
+		splashset(dst, height, width, info->fix.line_length, cc2cx(bg_color), 
+			  info->var.bits_per_pixel);
+	}
+}
+
+void fbsplash_clear_margins(struct vc_data *vc, struct fb_info *info, 
+			    int bottom_only)
+{
+	unsigned int tw = vc->vc_cols*vc->vc_font.width;
+	unsigned int th = vc->vc_rows*vc->vc_font.height;
+
+	if (!bottom_only) {
+		/* top margin */
+		splashfill(info, 0, 0, vc->vc_splash.ty, info->var.xres);
+		/* left margin */
+		splashfill(info, vc->vc_splash.ty, 0, th, vc->vc_splash.tx);
+		/* right margin */
+		splashfill(info, vc->vc_splash.ty, vc->vc_splash.tx + tw, th, 
+			   info->var.xres - vc->vc_splash.tx - tw);
+	}
+	splashfill(info, vc->vc_splash.ty + th, 0, 
+		   info->var.yres - vc->vc_splash.ty - th, info->var.xres);
+}
+
+void fbsplash_bmove_redraw(struct vc_data *vc, struct fb_info *info, int y, 
+			   int sx, int dx, int width)
+{
+	u16 *d = (u16 *) (vc->vc_origin + vc->vc_size_row * y + dx * 2);
+	u16 *s = d + (dx - sx);
+	u16 *start = d;
+	u16 *ls = d;
+	u16 *le = d + width;
+	u16 c;
+	int x = dx;
+	u16 attr = 1;
+
+	do {
+		c = scr_readw(d);
+		if (attr != (c & 0xff00)) {
+			attr = c & 0xff00;
+			if (d > start) {
+				fbsplash_putcs(vc, info, start, d - start, y, x);
+				x += d - start;
+				start = d;
+			}
+		}
+		if (s >= ls && s < le && c == scr_readw(s)) {
+			if (d > start) {
+				fbsplash_putcs(vc, info, start, d - start, y, x);
+				x += d - start + 1;
+				start = d + 1;
+			} else {
+				x++;
+				start++;
+			}
+		}
+		s++;
+		d++;
+	} while (d < le);
+	if (d > start)
+		fbsplash_putcs(vc, info, start, d - start, y, x);
+}
+
+void fbsplash_blank(struct vc_data *vc, struct fb_info *info, int blank)
+{
+	if (blank) {
+		splashset((u8 *)info->screen_base, info->var.yres, info->var.xres,
+			  info->fix.line_length, 0, info->var.bits_per_pixel);
+	} else {
+		update_screen(vc);
+		fbsplash_clear_margins(vc, info, 0);
+	}
+}
+
diff -urN oldtree/drivers/video/console/bitblit.c newtree/drivers/video/console/bitblit.c
--- oldtree/drivers/video/console/bitblit.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/video/console/bitblit.c	2006-07-12 19:01:05.000000000 -0400
@@ -17,6 +17,7 @@
 #include <linux/console.h>
 #include <asm/types.h>
 #include "fbcon.h"
+#include "../fbsplash.h"
 
 /*
  * Accelerated handlers.
@@ -54,6 +55,13 @@
 	area.height = height * vc->vc_font.height;
 	area.width = width * vc->vc_font.width;
 
+	if (fbsplash_active(info, vc)) {
+ 		area.sx += vc->vc_splash.tx;
+ 		area.sy += vc->vc_splash.ty;
+ 		area.dx += vc->vc_splash.tx;
+ 		area.dy += vc->vc_splash.ty;
+ 	}
+
 	info->fbops->fb_copyarea(info, &area);
 }
 
@@ -379,11 +387,15 @@
 	cursor.image.depth = 1;
 	cursor.rop = ROP_XOR;
 
-	if (info->fbops->fb_cursor)
-		err = info->fbops->fb_cursor(info, &cursor);
+	if (fbsplash_active(info, vc)) {
+		fbsplash_cursor(info, &cursor);
+	} else {
+		if (info->fbops->fb_cursor)
+			err = info->fbops->fb_cursor(info, &cursor);
 
-	if (err)
-		soft_cursor(info, &cursor);
+		if (err)
+			soft_cursor(info, &cursor);
+	}
 
 	ops->cursor_reset = 0;
 }
diff -urN oldtree/drivers/video/console/fbcon.c newtree/drivers/video/console/fbcon.c
--- oldtree/drivers/video/console/fbcon.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/video/console/fbcon.c	2006-07-12 19:01:05.000000000 -0400
@@ -91,6 +91,7 @@
 #endif
 
 #include "fbcon.h"
+#include "../fbsplash.h"
 
 #ifdef FBCONDEBUG
 #  define DPRINTK(fmt, args...) printk(KERN_DEBUG "%s: " fmt, __FUNCTION__ , ## args)
@@ -106,7 +107,7 @@
 
 static struct display fb_display[MAX_NR_CONSOLES];
 
-static signed char con2fb_map[MAX_NR_CONSOLES];
+signed char con2fb_map[MAX_NR_CONSOLES];
 static signed char con2fb_map_boot[MAX_NR_CONSOLES];
 static int logo_height;
 static int logo_lines;
@@ -300,7 +301,7 @@
 		vc->vc_mode != KD_TEXT || ops->graphics);
 }
 
-static inline int get_color(struct vc_data *vc, struct fb_info *info,
+inline int get_color(struct vc_data *vc, struct fb_info *info,
 	      u16 c, int is_fg)
 {
 	int depth = fb_get_color_depth(&info->var, &info->fix);
@@ -409,6 +410,7 @@
 		CM_ERASE : CM_DRAW;
 	ops->cursor(vc, info, mode, softback_lines, get_color(vc, info, c, 1),
 		    get_color(vc, info, c, 0));
+	
 	release_console_sem();
 }
 
@@ -574,6 +576,8 @@
 		info_idx = -1;
 	}
 
+	fbsplash_init();
+
 	return err;
 }
 
@@ -980,6 +984,12 @@
 	rows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);
 	cols /= vc->vc_font.width;
 	rows /= vc->vc_font.height;
+
+	if (fbsplash_active(info, vc)) {
+		cols = vc->vc_splash.twidth / vc->vc_font.width;
+		rows = vc->vc_splash.theight / vc->vc_font.height;
+	}
+
 	vc_resize(vc, cols, rows);
 
 	DPRINTK("mode:   %s\n", info->fix.id);
@@ -1063,7 +1073,7 @@
 	cap = info->flags;
 
 	if (vc != svc || logo_shown == FBCON_LOGO_DONTSHOW ||
-	    (info->fix.type == FB_TYPE_TEXT))
+	    (info->fix.type == FB_TYPE_TEXT) || fbsplash_active(info, vc))
 		logo = 0;
 
 	if (var_to_display(p, &info->var, info))
@@ -1257,6 +1267,11 @@
 	if (!height || !width)
 		return;
 
+ 	if (fbsplash_active(info, vc)) {
+ 		fbsplash_clear(vc, info, sy, sx, height, width);
+ 		return;
+ 	}
+ 	
 	/* Split blits that cross physical y_wrap boundary */
 
 	y_break = p->vrows - p->yscroll;
@@ -1276,10 +1291,15 @@
 	struct display *p = &fb_display[vc->vc_num];
 	struct fbcon_ops *ops = info->fbcon_par;
 
-	if (!fbcon_is_inactive(vc, info))
-		ops->putcs(vc, info, s, count, real_y(p, ypos), xpos,
-			   get_color(vc, info, scr_readw(s), 1),
-			   get_color(vc, info, scr_readw(s), 0));
+	if (!fbcon_is_inactive(vc, info)) {
+		
+		if (fbsplash_active(info, vc))
+			fbsplash_putcs(vc, info, s, count, ypos, xpos);
+		else
+			ops->putcs(vc, info, s, count, real_y(p, ypos), xpos,
+				   get_color(vc, info, scr_readw(s), 1),
+				   get_color(vc, info, scr_readw(s), 0));
+	}
 }
 
 static void fbcon_putc(struct vc_data *vc, int c, int ypos, int xpos)
@@ -1295,8 +1315,13 @@
 	struct fb_info *info = registered_fb[con2fb_map[vc->vc_num]];
 	struct fbcon_ops *ops = info->fbcon_par;
 
-	if (!fbcon_is_inactive(vc, info))
-		ops->clear_margins(vc, info, bottom_only);
+	if (!fbcon_is_inactive(vc, info)) {
+	 	if (fbsplash_active(info, vc)) {
+	 		fbsplash_clear_margins(vc, info, bottom_only);
+ 		} else {
+			ops->clear_margins(vc, info, bottom_only);
+		}
+	}
 }
 
 static void fbcon_cursor(struct vc_data *vc, int mode)
@@ -1769,7 +1794,7 @@
 			count = vc->vc_rows;
 		if (softback_top)
 			fbcon_softback_note(vc, t, count);
-		if (logo_shown >= 0)
+		if (logo_shown >= 0 || fbsplash_active(info, vc))
 			goto redraw_up;
 		switch (p->scrollmode) {
 		case SCROLL_MOVE:
@@ -1857,6 +1882,8 @@
 			count = vc->vc_rows;
 		if (logo_shown >= 0)
 			goto redraw_down;
+		if (fbsplash_active(info, vc))
+			goto redraw_down;
 		switch (p->scrollmode) {
 		case SCROLL_MOVE:
 			ops->bmove(vc, info, t, 0, t + count, 0,
@@ -1999,6 +2026,13 @@
 		}
 		return;
 	}
+
+	if (fbsplash_active(info, vc) && sy == dy && height == 1) {
+ 		/* must use slower redraw bmove to keep background pic intact */
+ 		fbsplash_bmove_redraw(vc, info, sy, sx, dx, width);
+ 		return;
+ 	}
+	
 	ops->bmove(vc, info, real_y(p, sy), sx, real_y(p, dy), dx,
 		   height, width);
 }
@@ -2069,8 +2103,9 @@
 	var.yres = virt_h * virt_fh;
 	x_diff = info->var.xres - var.xres;
 	y_diff = info->var.yres - var.yres;
-	if (x_diff < 0 || x_diff > virt_fw ||
-	    y_diff < 0 || y_diff > virt_fh) {
+
+	if ((x_diff < 0 || x_diff > virt_fw ||
+	    y_diff < 0 || y_diff > virt_fh) && !vc->vc_splash.state) {
 		struct fb_videomode *mode;
 
 		DPRINTK("attempting resize %ix%i\n", var.xres, var.yres);
@@ -2106,6 +2141,25 @@
 
 	info = registered_fb[con2fb_map[vc->vc_num]];
 	ops = info->fbcon_par;
+	prev_console = ops->currcon;
+	if (prev_console != -1)
+		old_info = registered_fb[con2fb_map[prev_console]];
+
+	if (fbsplash_active_vc(vc)) {
+		struct vc_data *vc_curr = vc_cons[prev_console].d;
+		if (!vc_curr->vc_splash.theme || strcmp(vc->vc_splash.theme, vc_curr->vc_splash.theme)) {
+			if (fbsplash_call_helper("getpic", vc->vc_num))
+				fbsplash_disable(vc, 0);
+		}
+	} else if (info->fix.visual == FB_VISUAL_DIRECTCOLOR) { 
+		struct vc_data *vc_curr = vc_cons[prev_console].d;
+		if (vc_curr && fbsplash_active_vc(vc_curr)) {
+			/* Clear the screen to avoid displaying funky colors during
+			 * palette updates. */ 
+			memset((u8*)info->screen_base + info->fix.line_length * info->var.yoffset,
+			       0, info->var.yres * info->fix.line_length);
+		}
+	}
 
 	if (softback_top) {
 		if (softback_lines)
@@ -2124,9 +2178,6 @@
 		logo_shown = FBCON_LOGO_CANSHOW;
 	}
 
-	prev_console = ops->currcon;
-	if (prev_console != -1)
-		old_info = registered_fb[con2fb_map[prev_console]];
 	/*
 	 * FIXME: If we have multiple fbdev's loaded, we need to
 	 * update all info->currcon.  Perhaps, we can place this
@@ -2165,6 +2216,11 @@
 		if (old_info != info)
 			fbcon_del_cursor_timer(old_info);
 	}
+	
+	if (fbsplash_active_nores(info, vc) && !fbsplash_active(info, vc)) {
+		if (fbsplash_call_helper("modechange", vc->vc_num))
+			fbsplash_disable(vc, 0);
+	}
 
 	fbcon_add_cursor_timer(info);
 	set_blitting_type(vc, info);
@@ -2268,8 +2324,12 @@
 			fbcon_cursor(vc, blank ? CM_ERASE : CM_DRAW);
 			ops->cursor_flash = (!blank);
 
-			if (fb_blank(info, blank))
-				fbcon_generic_blank(vc, info, blank);
+			if (fb_blank(info, blank)) {
+				if (fbsplash_active(info, vc))
+					fbsplash_blank(vc, info, blank);
+				else 
+					fbcon_generic_blank(vc, info, blank);
+			}
 		}
 
 		if (!blank)
@@ -2419,13 +2479,22 @@
 	}
 
 	if (resize) {
+		/* reset wrap/pan */
 		int cols, rows;
 
 		cols = FBCON_SWAP(ops->rotate, info->var.xres, info->var.yres);
 		rows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);
+
+ 		info->var.xoffset = info->var.yoffset = p->yscroll = 0;
+		if (fbsplash_active(info, vc)) {
+			cols = vc->vc_splash.twidth;
+			rows = vc->vc_splash.theight;
+		}
 		cols /= w;
 		rows /= h;
+
 		vc_resize(vc, cols, rows);
+
 		if (CON_IS_VISIBLE(vc) && softback_buf)
 			fbcon_update_softback(vc);
 	} else if (CON_IS_VISIBLE(vc)
@@ -2543,7 +2612,7 @@
 	int i, j, k, depth;
 	u8 val;
 
-	if (fbcon_is_inactive(vc, info))
+	if (fbcon_is_inactive(vc, info) || vc->vc_num != fg_console)
 		return -EINVAL;
 
 	if (!CON_IS_VISIBLE(vc))
@@ -2569,7 +2638,49 @@
 	} else
 		fb_copy_cmap(fb_default_cmap(1 << depth), &palette_cmap);
 
-	return fb_set_cmap(&palette_cmap, info);
+	if (fbsplash_active(info, vc_cons[fg_console].d) &&
+	    info->fix.visual == FB_VISUAL_DIRECTCOLOR) {
+
+		u16 *red, *green, *blue;
+		int minlen = min(min(info->var.red.length, info->var.green.length), 
+				     info->var.blue.length);
+		int h;
+
+		struct fb_cmap cmap = {
+			.start = 0,
+			.len = (1 << minlen),
+			.red = NULL,
+			.green = NULL,
+			.blue = NULL,
+			.transp = NULL
+		};
+
+		red = kmalloc(256 * sizeof(u16) * 3, GFP_KERNEL);
+	
+		if (!red)
+			goto out;		
+	
+		green = red + 256;
+		blue = green + 256;
+		cmap.red = red;
+		cmap.green = green;
+		cmap.blue = blue;
+		
+		for (i = 0; i < cmap.len; i++) {
+			red[i] = green[i] = blue[i] = (0xffff * i)/(cmap.len-1);
+		}
+
+		h = fb_set_cmap(&cmap, info);
+		fbsplash_fix_pseudo_pal(info, vc_cons[fg_console].d);
+		kfree(red);
+		
+		return h;
+		
+	} else if (fbsplash_active(info, vc_cons[fg_console].d) && 
+		   info->var.bits_per_pixel == 8 && info->splash.cmap.red != NULL) 
+		fb_set_cmap(&info->splash.cmap, info);
+		
+out:	return fb_set_cmap(&palette_cmap, info);
 }
 
 static u16 *fbcon_screen_pos(struct vc_data *vc, int offset)
@@ -2795,7 +2906,14 @@
 		rows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);
 		cols /= vc->vc_font.width;
 		rows /= vc->vc_font.height;
-		vc_resize(vc, cols, rows);
+				
+		if (!fbsplash_active_nores(info, vc)) {
+			vc_resize(vc, cols, rows);
+		} else {
+			if (fbsplash_call_helper("modechange", vc->vc_num))
+				fbsplash_disable(vc, 0);
+		}
+
 		updatescrollmode(p, info, vc);
 		scrollback_max = 0;
 		scrollback_current = 0;
@@ -3235,6 +3353,7 @@
 		}
 	}
 
+	fbsplash_exit();
 	fbcon_has_exited = 1;
 }
 
diff -urN oldtree/drivers/video/fbcmap.c newtree/drivers/video/fbcmap.c
--- oldtree/drivers/video/fbcmap.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/video/fbcmap.c	2006-07-12 19:01:05.000000000 -0400
@@ -15,6 +15,7 @@
 #include <linux/module.h>
 #include <linux/fb.h>
 #include <linux/slab.h>
+#include "fbsplash.h"
 
 #include <asm/uaccess.h>
 
@@ -234,14 +235,17 @@
 			if (transp)
 				htransp = *transp++;
 			if (info->fbops->fb_setcolreg(start++,
-						      hred, hgreen, hblue,
+						      hred, hgreen, hblue, 
 						      htransp, info))
 				break;
 		}
 	}
-	if (rc == 0)
+	if (rc == 0) {
 		fb_copy_cmap(cmap, &info->cmap);
-
+		if (fbsplash_active(info, vc_cons[fg_console].d) &&
+		    info->fix.visual == FB_VISUAL_DIRECTCOLOR)
+			fbsplash_fix_pseudo_pal(info, vc_cons[fg_console].d);
+	}
 	return rc;
 }
 
@@ -249,7 +253,7 @@
 {
 	int rc, size = cmap->len * sizeof(u16);
 	struct fb_cmap umap;
-
+	
 	if (cmap->start < 0 || (!info->fbops->fb_setcolreg &&
 			        !info->fbops->fb_setcmap))
 		return -EINVAL;
diff -urN oldtree/drivers/video/fbmem.c newtree/drivers/video/fbmem.c
--- oldtree/drivers/video/fbmem.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/video/fbmem.c	2006-07-12 19:01:08.000000000 -0400
@@ -1435,6 +1435,7 @@
 		printk(KERN_WARNING "Unable to create fb class; errno = %ld\n", PTR_ERR(fb_class));
 		fb_class = NULL;
 	}
+
 	return 0;
 }
 
diff -urN oldtree/drivers/video/fbsplash.c newtree/drivers/video/fbsplash.c
--- oldtree/drivers/video/fbsplash.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/drivers/video/fbsplash.c	2006-07-12 19:01:05.000000000 -0400
@@ -0,0 +1,425 @@
+/* 
+ *  linux/drivers/video/fbsplash.c -- Framebuffer splash routines
+ *
+ *  Copyright (C) 2004 Michal Januszewski <spock@gentoo.org>
+ *
+ *  Code based upon "Bootsplash" (C) 2001-2003 
+ *       Volker Poplawski <volker@poplawski.de>,
+ *       Stefan Reinauer <stepan@suse.de>,
+ *       Steffen Winterfeldt <snwint@suse.de>,
+ *       Michael Schroeder <mls@suse.de>,
+ *       Ken Wimer <wimer@suse.de>.
+ *
+ *  Splash render routines are located in /linux/drivers/video/cfbsplash.c
+ * 
+ *  This file is subject to the terms and conditions of the GNU General Public
+ *  License.  See the file COPYING in the main directory of this archive for
+ *  more details.
+ * 
+ */
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/fb.h>
+#include <linux/vt_kern.h>
+#include <linux/vmalloc.h>
+#include <linux/unistd.h>
+#include <linux/syscalls.h>
+#include <linux/init.h>
+#include <linux/proc_fs.h>
+#include <linux/workqueue.h>
+#include <linux/kmod.h>
+#include <linux/miscdevice.h>
+#include <linux/device.h>
+#include <linux/fs.h>
+
+#include <asm/uaccess.h>
+#include <asm/irq.h>
+#include <asm/system.h>
+
+#include "console/fbcon.h"
+#include "fbsplash.h"
+
+#define SPLASH_VERSION 		"0.9.2"
+
+extern signed char con2fb_map[];
+static int fbsplash_enable(struct vc_data *vc);
+char fbsplash_path[KMOD_PATH_LEN] = "/sbin/splash_helper";
+static int initialized = 0;
+
+int fbsplash_call_helper(char* cmd, unsigned short vc)
+{
+	char *envp[] = {
+		"HOME=/",
+		"PATH=/sbin:/bin",
+		NULL
+	};
+
+	char tfb[5];
+	char tcons[5];
+	unsigned char fb = (int) con2fb_map[vc];
+
+	char *argv[] = {
+		fbsplash_path,
+		"2",
+		cmd,
+		tcons,
+		tfb,
+		vc_cons[vc].d->vc_splash.theme,
+		NULL
+	};
+
+	snprintf(tfb,5,"%d",fb);
+	snprintf(tcons,5,"%d",vc);
+
+	return call_usermodehelper(fbsplash_path, argv, envp, 1);
+}
+
+/* Disables fbsplash on a virtual console; called with console sem held. */
+int fbsplash_disable(struct vc_data *vc, unsigned char redraw)
+{
+	struct fb_info* info;
+
+	if (!vc->vc_splash.state)
+		return -EINVAL;
+
+	info = registered_fb[(int) con2fb_map[vc->vc_num]];
+
+	if (info == NULL)
+		return -EINVAL;
+
+	vc->vc_splash.state = 0; 
+	vc_resize(vc, info->var.xres / vc->vc_font.width, 
+		  info->var.yres / vc->vc_font.height);
+
+	if (fg_console == vc->vc_num && redraw) {
+		redraw_screen(vc, 0);
+		update_region(vc, vc->vc_origin + 
+			      vc->vc_size_row * vc->vc_top, 
+			      vc->vc_size_row * (vc->vc_bottom - vc->vc_top) / 2);
+	}
+
+	printk(KERN_INFO "fbsplash: switched splash state to 'off' on console %d\n", 
+			 vc->vc_num);
+
+	return 0;
+}
+
+/* Enables fbsplash on a virtual console; called with console sem held. */
+static int fbsplash_enable(struct vc_data *vc)
+{
+	struct fb_info* info;
+
+	info = registered_fb[(int) con2fb_map[vc->vc_num]];
+		
+	if (vc->vc_splash.twidth == 0 || vc->vc_splash.theight == 0 || 
+	    info == NULL || vc->vc_splash.state || (!info->splash.data &&
+	    vc->vc_num == fg_console))
+		return -EINVAL;
+	
+	vc->vc_splash.state = 1;
+	vc_resize(vc, vc->vc_splash.twidth / vc->vc_font.width, 
+		  vc->vc_splash.theight / vc->vc_font.height);
+
+	if (fg_console == vc->vc_num) {
+		redraw_screen(vc, 0);
+		update_region(vc, vc->vc_origin + 
+			      vc->vc_size_row * vc->vc_top, 
+			      vc->vc_size_row * (vc->vc_bottom - vc->vc_top) / 2);
+		fbsplash_clear_margins(vc, info, 0);
+	}
+
+	printk(KERN_INFO "fbsplash: switched splash state to 'on' on console %d\n", 
+			 vc->vc_num);
+
+	return 0;
+}
+
+static inline int fbsplash_ioctl_dosetstate(struct vc_data *vc, unsigned int __user* state, unsigned char origin)
+{
+	int tmp, ret;
+
+	if (get_user(tmp, state))
+		return -EFAULT;
+
+	if (origin == FB_SPLASH_IO_ORIG_USER)
+		acquire_console_sem();
+	if (!tmp)
+		ret = fbsplash_disable(vc, 1);
+	else
+		ret = fbsplash_enable(vc);
+	if (origin == FB_SPLASH_IO_ORIG_USER)
+		release_console_sem();
+
+	return ret;
+}
+
+static inline int fbsplash_ioctl_dogetstate(struct vc_data *vc, unsigned int __user *state)
+{
+	return put_user(vc->vc_splash.state, (unsigned int __user*) state);
+}
+
+static int fbsplash_ioctl_dosetcfg(struct vc_data *vc, struct vc_splash __user *arg, unsigned char origin)
+{
+	struct vc_splash cfg;
+	struct fb_info *info;
+	int len;
+	char *tmp;
+	
+	info = registered_fb[(int) con2fb_map[vc->vc_num]];
+
+	if (copy_from_user(&cfg, arg, sizeof(struct vc_splash)))
+		return -EFAULT;
+	if (info == NULL || !cfg.twidth || !cfg.theight || 
+	    cfg.tx + cfg.twidth  > info->var.xres ||
+	    cfg.ty + cfg.theight > info->var.yres)
+		return -EINVAL;
+
+	len = strlen_user(cfg.theme);
+	if (!len || len > FB_SPLASH_THEME_LEN)
+		return -EINVAL;
+	tmp = kmalloc(len, GFP_KERNEL);
+	if (!tmp)
+		return -ENOMEM;
+	if (copy_from_user(tmp, (void __user *)cfg.theme, len))
+		return -EFAULT;
+	cfg.theme = tmp;
+	cfg.state = 0;
+
+	/* If this ioctl is a response to a request from kernel, the console sem
+	 * is already held; we also don't need to disable splash because either the
+	 * new config and background picture will be successfully loaded, and the 
+	 * splash will stay on, or in case of a failure it'll be turned off in fbcon. */
+	if (origin == FB_SPLASH_IO_ORIG_USER) {
+		acquire_console_sem();
+		if (vc->vc_splash.state)
+			fbsplash_disable(vc, 1);
+	}
+
+	if (vc->vc_splash.theme)
+		kfree(vc->vc_splash.theme);
+
+	vc->vc_splash = cfg;
+
+	if (origin == FB_SPLASH_IO_ORIG_USER)
+		release_console_sem();
+
+	printk(KERN_INFO "fbsplash: console %d using theme '%s'\n", 
+			 vc->vc_num, vc->vc_splash.theme);
+	return 0;	
+}
+
+static int fbsplash_ioctl_dogetcfg(struct vc_data *vc, struct vc_splash __user *arg)
+{
+	struct vc_splash splash;
+	char __user *tmp;
+
+	if (get_user(tmp, &arg->theme))
+		return -EFAULT;
+	
+	splash = vc->vc_splash;
+	splash.theme = tmp;
+
+	if (vc->vc_splash.theme) {
+		if (copy_to_user(tmp, vc->vc_splash.theme, strlen(vc->vc_splash.theme) + 1))
+			return -EFAULT;
+	} else
+		if (put_user(0, tmp))
+			return -EFAULT;
+
+	if (copy_to_user(arg, &splash, sizeof(struct vc_splash)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static int fbsplash_ioctl_dosetpic(struct vc_data *vc, struct fb_image __user *arg, unsigned char origin)
+{
+	struct fb_image img;
+	struct fb_info *info;
+	int len;
+	u8 *tmp;
+	
+	if (vc->vc_num != fg_console) 
+		return -EINVAL;
+
+	info = registered_fb[(int) con2fb_map[vc->vc_num]];
+	
+	if (info == NULL)
+		return -EINVAL;
+	
+	if (copy_from_user(&img, arg, sizeof(struct fb_image)))
+		return -EFAULT;
+	
+	if (img.width != info->var.xres || img.height != info->var.yres) {
+		printk(KERN_ERR "fbsplash: picture dimensions mismatch\n");
+		return -EINVAL;
+	}
+
+	if (img.depth != info->var.bits_per_pixel) {
+		printk(KERN_ERR "fbsplash: picture depth mismatch\n");
+		return -EINVAL;
+	}
+		
+	if (img.depth == 8) {
+		if (!img.cmap.len || !img.cmap.red || !img.cmap.green || 
+		    !img.cmap.blue)
+			return -EINVAL;
+		
+		tmp = vmalloc(img.cmap.len * 3 * 2);
+		if (!tmp)
+			return -ENOMEM;
+
+		if (copy_from_user(tmp, (void __user*)img.cmap.red, img.cmap.len * 2) ||
+		    copy_from_user(tmp + (img.cmap.len << 1),
+			    	   (void __user*)img.cmap.green, (img.cmap.len << 1)) ||
+		    copy_from_user(tmp + (img.cmap.len << 2),
+			    	   (void __user*)img.cmap.blue, (img.cmap.len << 1))) {
+			vfree(tmp);
+			return -EFAULT;
+		}
+			
+		img.cmap.transp = NULL;
+		img.cmap.red = (u16*)tmp;
+		img.cmap.green = img.cmap.red + img.cmap.len;
+		img.cmap.blue = img.cmap.green + img.cmap.len;
+	} else {
+		img.cmap.red = NULL;
+	}
+		
+	len = ((img.depth + 7) >> 3) * img.width * img.height;
+	tmp = vmalloc(len);
+
+	if (!tmp)
+		goto out;
+
+	if (copy_from_user(tmp, (void __user*)img.data, len))
+		goto out;
+		
+	img.data = tmp;
+
+	/* If this ioctl is a response to a request from kernel, the console sem
+	 * is already held. */
+	if (origin == FB_SPLASH_IO_ORIG_USER)
+		acquire_console_sem();
+	
+	if (info->splash.data)
+		vfree((u8*)info->splash.data);
+	if (info->splash.cmap.red)
+		vfree(info->splash.cmap.red);
+	
+	info->splash = img;
+
+	if (origin == FB_SPLASH_IO_ORIG_USER)
+		release_console_sem();
+
+	return 0;
+
+out:	if (img.cmap.red)
+		vfree(img.cmap.red);
+	if (tmp)
+		vfree(tmp);
+	return -ENOMEM;
+}
+
+static int splash_ioctl(struct inode * inode, struct file *filp, u_int cmd, 
+			u_long arg)
+{
+	struct fb_splash_iowrapper __user *wrapper = (void __user*) arg;
+	struct vc_data *vc = NULL;
+	unsigned short vc_num = 0;
+	unsigned char origin = 0;
+	void __user *data = NULL;
+	
+	if (!access_ok(VERIFY_READ, wrapper, 
+			sizeof(struct fb_splash_iowrapper)))
+		return -EFAULT;
+	
+	__get_user(vc_num, &wrapper->vc);
+	__get_user(origin, &wrapper->origin);
+	__get_user(data, &wrapper->data);
+		
+	if (!vc_cons_allocated(vc_num))
+		return -EINVAL;
+
+	vc = vc_cons[vc_num].d;
+	
+	switch (cmd) {
+	case FBIOSPLASH_SETPIC:
+		return fbsplash_ioctl_dosetpic(vc, (struct fb_image __user*)data, origin);
+	case FBIOSPLASH_SETCFG:
+		return fbsplash_ioctl_dosetcfg(vc, (struct vc_splash*)data, origin);
+	case FBIOSPLASH_GETCFG:
+		return fbsplash_ioctl_dogetcfg(vc, (struct vc_splash*)data);
+	case FBIOSPLASH_SETSTATE:
+		return fbsplash_ioctl_dosetstate(vc, (unsigned int *)data, origin);
+	case FBIOSPLASH_GETSTATE:
+		return fbsplash_ioctl_dogetstate(vc, (unsigned int *)data);
+	default:
+		return -ENOIOCTLCMD;
+	}	
+}
+
+static struct file_operations splash_ops = {
+	.owner = THIS_MODULE,
+	.ioctl = splash_ioctl
+};
+
+static struct miscdevice splash_dev = {
+	.minor = MISC_DYNAMIC_MINOR,
+	.name = "fbsplash",
+	.fops = &splash_ops
+};
+
+void fbsplash_reset(void)
+{
+	struct fb_info *info;
+	struct vc_data *vc;
+	int i;
+
+	vc = vc_cons[0].d;
+	info = registered_fb[0];
+
+	for (i = 0; i < num_registered_fb; i++) {
+		registered_fb[i]->splash.data = NULL;
+		registered_fb[i]->splash.cmap.red = NULL;
+	}
+
+	for (i = 0; i < MAX_NR_CONSOLES && vc_cons[i].d; i++) {
+		vc_cons[i].d->vc_splash.state = vc_cons[i].d->vc_splash.twidth = 
+						vc_cons[i].d->vc_splash.theight = 0;
+		vc_cons[i].d->vc_splash.theme = NULL;
+	}
+
+	return;
+}
+
+int fbsplash_init(void)
+{
+	int i;
+
+	fbsplash_reset();
+
+	if (initialized)
+		return 0;
+
+	i = misc_register(&splash_dev);
+	if (i) {
+		printk(KERN_ERR "fbsplash: failed to register device\n");
+		return i;
+	}
+
+	fbsplash_call_helper("init", 0);
+	initialized = 1;
+	return 0;
+}
+
+int fbsplash_exit(void)
+{
+	fbsplash_reset();
+	return 0;
+}
+
+EXPORT_SYMBOL(fbsplash_path);
diff -urN oldtree/drivers/video/fbsplash.h newtree/drivers/video/fbsplash.h
--- oldtree/drivers/video/fbsplash.h	1969-12-31 19:00:00.000000000 -0500
+++ newtree/drivers/video/fbsplash.h	2006-07-12 19:01:05.000000000 -0400
@@ -0,0 +1,78 @@
+/* 
+ *  linux/drivers/video/fbsplash.h -- Framebuffer splash headers
+ *
+ *  Copyright (C) 2004 Michal Januszewski <spock@gentoo.org>
+ *
+ */
+
+#ifndef __FB_SPLASH_H
+#define __FB_SPLASH_H
+
+#ifndef _LINUX_FB_H
+#include <linux/fb.h>
+#endif
+
+/* This is needed for vc_cons in fbcmap.c */
+#include <linux/vt_kern.h>
+
+struct fb_cursor;
+struct fb_info;
+struct vc_data;
+
+#ifdef CONFIG_FB_SPLASH
+/* fbsplash.c */
+int fbsplash_init(void);
+int fbsplash_exit(void);
+int fbsplash_call_helper(char* cmd, unsigned short cons);
+int fbsplash_disable(struct vc_data *vc, unsigned char redraw);
+
+/* cfbsplash.c */
+void fbsplash_putcs(struct vc_data *vc, struct fb_info *info, const unsigned short *s, int count, int yy, int xx);
+void fbsplash_cursor(struct fb_info *info, struct fb_cursor *cursor);
+void fbsplash_clear(struct vc_data *vc, struct fb_info *info, int sy, int sx, int height, int width);
+void fbsplash_clear_margins(struct vc_data *vc, struct fb_info *info, int bottom_only);
+void fbsplash_blank(struct vc_data *vc, struct fb_info *info, int blank);
+void fbsplash_bmove_redraw(struct vc_data *vc, struct fb_info *info, int y, int sx, int dx, int width);
+void fbsplash_copy(u8 *dst, u8 *src, int height, int width, int linebytes, int srclinesbytes, int bpp);
+void fbsplash_fix_pseudo_pal(struct fb_info *info, struct vc_data *vc);
+
+/* vt.c */
+void acquire_console_sem(void);
+void release_console_sem(void);
+void do_unblank_screen(int entering_gfx);
+
+/* struct vc_data *y */
+#define fbsplash_active_vc(y) (y->vc_splash.state && y->vc_splash.theme) 
+
+/* struct fb_info *x, struct vc_data *y */
+#define fbsplash_active_nores(x,y) (x->splash.data && fbsplash_active_vc(y))
+
+/* struct fb_info *x, struct vc_data *y */
+#define fbsplash_active(x,y) (fbsplash_active_nores(x,y) &&		\
+			      x->splash.width == x->var.xres && 	\
+			      x->splash.height == x->var.yres &&	\
+			      x->splash.depth == x->var.bits_per_pixel)
+
+
+#else /* CONFIG_FB_SPLASH */
+
+static inline void fbsplash_putcs(struct vc_data *vc, struct fb_info *info, const unsigned short *s, int count, int yy, int xx) {}
+static inline void fbsplash_putc(struct vc_data *vc, struct fb_info *info, int c, int ypos, int xpos) {}
+static inline void fbsplash_cursor(struct fb_info *info, struct fb_cursor *cursor) {}
+static inline void fbsplash_clear(struct vc_data *vc, struct fb_info *info, int sy, int sx, int height, int width) {}
+static inline void fbsplash_clear_margins(struct vc_data *vc, struct fb_info *info, int bottom_only) {}
+static inline void fbsplash_blank(struct vc_data *vc, struct fb_info *info, int blank) {}
+static inline void fbsplash_bmove_redraw(struct vc_data *vc, struct fb_info *info, int y, int sx, int dx, int width) {}
+static inline void fbsplash_fix_pseudo_pal(struct fb_info *info, struct vc_data *vc) {}
+static inline int fbsplash_call_helper(char* cmd, unsigned short cons) { return 0; }
+static inline int fbsplash_init(void) { return 0; }
+static inline int fbsplash_exit(void) { return 0; }
+static inline int fbsplash_disable(struct vc_data *vc, unsigned char redraw) { return 0; }
+
+#define fbsplash_active_vc(y) (0)
+#define fbsplash_active_nores(x,y) (0)
+#define fbsplash_active(x,y) (0)
+
+#endif /* CONFIG_FB_SPLASH */
+
+#endif /* __FB_SPLASH_H */
diff -urN oldtree/drivers/video/modedb.c newtree/drivers/video/modedb.c
--- oldtree/drivers/video/modedb.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/drivers/video/modedb.c	2006-07-12 19:01:08.000000000 -0400
@@ -674,6 +674,7 @@
 {
 	u32 pixclock, hfreq, htotal, vtotal;
 
+	mode->refresh = 0;
 	mode->name = NULL;
 	mode->xres = var->xres;
 	mode->yres = var->yres;
@@ -1025,3 +1026,4 @@
 EXPORT_SYMBOL(fb_find_nearest_mode);
 EXPORT_SYMBOL(fb_videomode_to_modelist);
 EXPORT_SYMBOL(fb_find_mode);
+EXPORT_SYMBOL(fb_destroy_modelist);
diff -urN oldtree/drivers/video/vesafb-thread.c newtree/drivers/video/vesafb-thread.c
--- oldtree/drivers/video/vesafb-thread.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/drivers/video/vesafb-thread.c	2006-07-12 19:01:08.000000000 -0400
@@ -0,0 +1,727 @@
+/*
+ * Framebuffer driver for VBE 2.0+ compliant graphic boards.
+ * Kernel thread and vm86 routines.
+ *
+ * (c) 2004-2006 Michal Januszewski <spock@gentoo.org>
+ *
+ */
+
+#include <linux/config.h>
+#include <linux/slab.h>
+#include <linux/workqueue.h>
+#include <linux/completion.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/mm.h>
+#include <linux/delay.h>
+#include <linux/signal.h>
+#include <linux/suspend.h>
+#include <linux/unistd.h>
+#include <video/vesa.h>
+#include <video/edid.h>
+#include <asm/mman.h>
+#include <asm/page.h>
+#include <asm/vm86.h>
+#include <asm/thread_info.h>
+#include <asm/uaccess.h>
+#include <asm/mmu_context.h>
+#include "edid.h"
+
+#ifdef MODULE
+int errno;
+#endif
+
+static DECLARE_COMPLETION(vesafb_th_completion);
+static DECLARE_MUTEX(vesafb_task_list_sem);
+static LIST_HEAD(vesafb_task_list);
+static DECLARE_WAIT_QUEUE_HEAD(vesafb_wait);
+
+static struct vm86_struct vm86;
+static int vesafb_pid = 0;
+
+_syscall3(int,ioperm,unsigned long, a, unsigned long, b, unsigned long, c);
+_syscall1(int,vm86old,struct vm86_struct __user*, v86);
+
+#define DEFAULT_VM86_FLAGS (IF_MASK | IOPL_MASK)
+#define VM86_PUSHW(x)					\
+do { 							\
+	vm86.regs.esp -= 2; 				\
+	*(u16*)(STACK_ADDR + vm86.regs.esp) = x;	\
+} while(0);
+
+/* Stack, the return code and buffers will be put into
+ * one contiguous memory chunk:
+ *
+ * [ STACK | RET_CODE | BUFFER ]
+ *
+ * Some video BIOSes (sis6326) try to store data somewhere
+ * in 0x7000-0x7fff, so we zeromap more memory to be safe.
+ */
+#define IVTBDA_SIZE 	PAGE_SIZE
+#define RET_CODE_SIZE	0x0010
+#define STACK_SIZE	0x0500
+#define BUFFER_SIZE	0x10000
+
+/* The amount of memory that will be allocated should be a multiple
+ * of PAGE_SIZE. */
+#define __MEM_SIZE 	(RET_CODE_SIZE + STACK_SIZE + BUFFER_SIZE)
+#define REAL_MEM_SIZE	(((__MEM_SIZE / PAGE_SIZE) + 1) * PAGE_SIZE)
+
+#define IVTBDA_ADDR	0x00000
+#define STACK_ADDR	(IVTBDA_ADDR + IVTBDA_SIZE)
+#define RET_CODE_ADDR	(STACK_ADDR + STACK_SIZE)
+#define BUF_ADDR	(RET_CODE_ADDR + RET_CODE_SIZE)
+
+#define FLAG_D 		(1 << 10)
+
+/* Segment prefix opcodes */
+enum {
+	P_CS = 0x2e,
+	P_SS = 0x36,
+	P_DS = 0x3e,
+	P_ES = 0x26,
+	P_FS = 0x64,
+	P_GS = 0x65
+};
+
+/* Emulated vm86 ins instruction */
+static void vm86_ins(int size)
+{
+	u32 edx, edi;
+	edx = vm86.regs.edx & 0xffff;
+	edi = (vm86.regs.edi & 0xffff) + (u32)(vm86.regs.es << 4);
+
+	if (vm86.regs.eflags & FLAG_D)
+		asm volatile ("std\n");
+	else
+		asm volatile ("cld\n");
+
+	switch (size) {
+	case 4:
+		asm volatile ("insl\n" : "=D" (edi) : "d" (edx), "0" (edi));
+		break;
+	case 2:
+		asm volatile ("insw\n" : "=D" (edi) : "d" (edx), "0" (edi));
+		break;
+	case 1:
+		asm volatile ("insb\n" : "=D" (edi) : "d" (edx), "0" (edi));
+		break;
+	}
+
+	if (vm86.regs.eflags & FLAG_D)
+		asm volatile ("cld\n");
+
+	edi -= (u32)(vm86.regs.es << 4);
+
+	vm86.regs.edi &= 0xffff0000;
+	vm86.regs.edi |= edi & 0xffff;
+}
+
+static void vm86_rep_ins(int size)
+{
+	u16 cx = vm86.regs.ecx;
+	while (cx--)
+		vm86_ins(size);
+
+	vm86.regs.ecx &= 0xffff0000;
+}
+
+/* Emulated vm86 outs instruction */
+static void vm86_outs(int size, int segment)
+{
+	u32 edx, esi, base;
+
+	edx = vm86.regs.edx & 0xffff;
+	esi = vm86.regs.esi & 0xffff;
+
+	switch (segment) {
+	case P_CS: base = vm86.regs.cs; break;
+	case P_SS: base = vm86.regs.ss; break;
+	case P_ES: base = vm86.regs.es; break;
+	case P_FS: base = vm86.regs.fs; break;
+	case P_GS: base = vm86.regs.gs; break;
+	default:   base = vm86.regs.ds; break;
+	}
+
+	esi += base << 4;
+
+	if (vm86.regs.eflags & FLAG_D)
+		asm volatile ("std\n");
+	else
+		asm volatile ("cld\n");
+
+	switch (size) {
+	case 4:
+		asm volatile ("outsl\n" : "=S" (esi) : "d" (edx), "0" (esi));
+		break;
+	case 2:
+		asm volatile ("outsw\n" : "=S" (esi) : "d" (edx), "0" (esi));
+		break;
+	case 1:
+		asm volatile ("outsb\n" : "=S" (esi) : "d" (edx), "0" (esi));
+		break;
+	}
+
+	if (vm86.regs.eflags & FLAG_D)
+		asm volatile ("cld");
+
+	esi -= base << 4;
+	vm86.regs.esi &= 0xffff0000;
+	vm86.regs.esi |= (esi & 0xffff);
+}
+
+static void vm86_rep_outs(int size, int segment)
+{
+	u16 cx = vm86.regs.ecx;
+	while (cx--)
+		vm86_outs(size, segment);
+
+	vm86.regs.ecx &= 0xffff0000;
+}
+
+static int vm86_do_unknown(void)
+{
+	u8 data32 = 0, segment = P_DS, rep = 0;
+	u8 *instr;
+	int ret = 0, i = 0;
+
+	instr = (u8*)((vm86.regs.cs << 4) + vm86.regs.eip);
+
+	while (1) {
+		switch(instr[i]) {
+		case 0x66:	/* operand size prefix */
+			data32 = 1 - data32;
+			i++;
+			break;
+		case 0xf2:	/* repnz */
+		case 0xf3:	/* rep */
+			rep = 1;
+			i++;
+			break;
+		case P_CS:	/* segment prefix */
+		case P_SS:
+		case P_DS:
+		case P_ES:
+		case P_FS:
+		case P_GS:
+			segment = instr[i];
+			i++;
+			break;
+		case 0xf0:	/* LOCK - ignored */
+		case 0x67:	/* address size prefix - ignored */
+			i++;
+			break;
+		case 0x6c:	/* insb */
+			if (rep)
+				vm86_rep_ins(1);
+			else
+				vm86_ins(1);
+			i++;
+			goto out;
+		case 0x6d:	/* insw / insd */
+			if (rep) {
+				if (data32)
+					vm86_rep_ins(4);
+				else
+					vm86_rep_ins(2);
+			} else {
+				if (data32)
+					vm86_ins(4);
+				else
+					vm86_ins(2);
+			}
+			i++;
+			goto out;
+		case 0x6e:	/* outsb */
+			if (rep)
+				vm86_rep_outs(1, segment);
+			else
+				vm86_outs(1, segment);
+			i++;
+			goto out;
+		case 0x6f:	/* outsw / outsd */
+			if (rep) {
+				if (data32)
+					vm86_rep_outs(4, segment);
+				else
+					vm86_rep_outs(2, segment);
+			} else {
+				if (data32)
+					vm86_outs(4, segment);
+				else
+					vm86_outs(2, segment);
+			}
+			i++;
+			goto out;
+		case 0xe4:	/* inb xx */
+			asm volatile (
+				"inb %w1, %b0"
+				: "=a" (vm86.regs.eax)
+				: "d" (instr[i+1]), "0" (vm86.regs.eax));
+			i += 2;
+			goto out;
+		case 0xe5:	/* inw xx / ind xx */
+			if (data32) {
+				asm volatile (
+					"inl %w1, %0"
+					: "=a" (vm86.regs.eax)
+					: "d" (instr[i+1]),
+					  "0" (vm86.regs.eax));
+			} else {
+				asm volatile (
+					"inw %w1, %w0"
+					: "=a" (vm86.regs.eax)
+					: "d" (instr[i+1]),
+					  "0" (vm86.regs.eax));
+			}
+			i += 2;
+			goto out;
+
+		case 0xec:	/* inb dx */
+			asm volatile (
+				"inb %w1, %b0"
+	 			: "=a" (vm86.regs.eax)
+				: "d" (vm86.regs.edx), "0" (vm86.regs.eax));
+			i++;
+			goto out;
+		case 0xed:	/* inw dx / ind dx */
+			if (data32) {
+				asm volatile (
+					"inl %w1, %0"
+					: "=a" (vm86.regs.eax)
+					: "d" (vm86.regs.edx));
+			} else {
+				asm volatile (
+					"inw %w1, %w0"
+					: "=a" (vm86.regs.eax)
+					: "d" (vm86.regs.edx));
+			}
+			i++;
+			goto out;
+		case 0xe6:	/* outb xx */
+			asm volatile (
+				"outb %b0, %w1"
+				: /* no return value */
+				: "a" (vm86.regs.eax), "d" (instr[i+1]));
+			i += 2;
+			goto out;
+		case 0xe7:	/* outw xx / outd xx */
+			if (data32) {
+				asm volatile (
+					"outl %0, %w1"
+					: /* no return value */
+					: "a" (vm86.regs.eax),
+					  "d" (instr[i+1]));
+			} else {
+				asm volatile (
+					"outw %w0, %w1"
+					: /* no return value */
+					: "a" (vm86.regs.eax),
+					  "d" (instr[i+1]));
+			}
+			i += 2;
+			goto out;
+		case 0xee:	/* outb dx */
+			asm volatile (
+				"outb %b0, %w1"
+				: /* no return value */
+				: "a" (vm86.regs.eax), "d" (vm86.regs.edx));
+			i++;
+			goto out;
+		case 0xef:	/* outw dx / outd dx */
+			if (data32) {
+				asm volatile (
+					"outl %0, %w1"
+					: /* no return value */
+					: "a" (vm86.regs.eax),
+					  "d" (vm86.regs.edx));
+			} else {
+				asm volatile (
+					"outw %w0, %w1"
+					: /* no return value */
+					: "a" (vm86.regs.eax),
+					  "d" (vm86.regs.edx));
+			}
+			i++;
+			goto out;
+		default:
+			printk(KERN_ERR "vesafb: BUG, opcode 0x%x emulation "
+					"not supported (EIP: 0x%lx)\n",
+					instr[i], (u32)(vm86.regs.cs << 4) +
+					vm86.regs.eip);
+			ret = 1;
+			goto out;
+		}
+	}
+out: 	vm86.regs.eip += i;
+	return ret;
+}
+
+void vesafb_do_vm86(struct vm86_regs *regs)
+{
+	unsigned int ret;
+	u8 *retcode = (void*)RET_CODE_ADDR;
+
+	memset(&vm86,0,sizeof(vm86));
+	memcpy(&vm86.regs, regs, sizeof(struct vm86_regs));
+
+	/* The return code */
+	retcode[0] = 0xcd;  		/* int opcode */
+	retcode[1] = 0xff;		/* int number (255) */
+
+        /* We use int 0xff to get back to protected mode */
+	memset(&vm86.int_revectored, 0, sizeof(vm86.int_revectored));
+        ((unsigned char *)&vm86.int_revectored)[0xff / 8] |= (1 << (0xff % 8));
+
+	/*
+	 * We want to call int 0x10, so we set:
+	 *   CS = 0x42 = 0x10 * 4 + 2
+	 *   IP = 0x40 = 0x10 * 4
+	 * and SS:ESP. It's up to the caller to set the rest of the registers.
+	 */
+	vm86.regs.eflags = DEFAULT_VM86_FLAGS;
+	vm86.regs.cs = *(unsigned short *)0x42;
+	vm86.regs.eip = *(unsigned short *)0x40;
+	vm86.regs.ss = (STACK_ADDR >> 4);
+	vm86.regs.esp = ((STACK_ADDR & 0x0000f) + STACK_SIZE);
+
+	/* These will be fetched off the stack when we come to an iret in the
+	 * int's 0x10 code. */
+	VM86_PUSHW(DEFAULT_VM86_FLAGS);
+	VM86_PUSHW((RET_CODE_ADDR >> 4));	/* return code segment */
+	VM86_PUSHW((RET_CODE_ADDR & 0x0000f));	/* return code offset */
+
+	while(1) {
+		ret = vm86old(&vm86);
+
+		if (VM86_TYPE(ret) == VM86_INTx) {
+			int vint = VM86_ARG(ret);
+
+			/* If exit from vm86 was caused by int 0xff, then
+			 * we're done.. */
+			if (vint == 0xff)
+				goto out;
+
+			/* .. otherwise, we have to call the int handler
+			 * manually */
+			VM86_PUSHW(vm86.regs.eflags);
+			VM86_PUSHW(vm86.regs.cs);
+			VM86_PUSHW(vm86.regs.eip);
+
+			vm86.regs.cs = *(u16 *)((vint << 2) + 2);
+			vm86.regs.eip = *(u16 *)(vint << 2);
+			vm86.regs.eflags &= ~(VIF_MASK | TF_MASK);
+		} else if (VM86_TYPE(ret) == VM86_UNKNOWN) {
+			if (vm86_do_unknown())
+				goto out;
+		} else {
+			printk(KERN_ERR "vesafb: BUG, returned from "
+					"vm86 with %x (EIP: 0x%lx)\n",
+					ret, (u32)(vm86.regs.cs << 4) +
+					vm86.regs.eip);
+			goto out;
+		}
+	}
+
+out:	/* copy the registers' state back to the caller's struct */
+	memcpy(regs, &vm86.regs, sizeof(struct vm86_regs));
+}
+
+static int vesafb_remap_pfn_range(unsigned long start, unsigned long end,
+				  unsigned long pgoff, unsigned long prot,
+				  int type)
+{
+	struct vm_area_struct *vma;
+	struct mm_struct *mm = current->mm;
+	int ret = 0;
+
+	vma = kmem_cache_alloc(vm_area_cachep, SLAB_KERNEL);
+	if (!vma)
+		return -ENOMEM;
+	memset(vma, 0, sizeof(*vma));
+	down_write(&mm->mmap_sem);
+	vma->vm_mm = mm;
+	vma->vm_start = start;
+	vma->vm_end = end;
+	vma->vm_flags = VM_READ | VM_WRITE | VM_EXEC;
+	vma->vm_flags |= mm->def_flags;
+	vma->vm_page_prot.pgprot = prot;
+	vma->vm_pgoff = pgoff;
+
+	if ((ret = insert_vm_struct(mm, vma))) {
+		up_write(&mm->mmap_sem);
+		kmem_cache_free(vm_area_cachep, vma);
+		return ret;
+	}
+
+	if (type) {
+		ret = zeromap_page_range(vma,
+					 vma->vm_start,
+					 vma->vm_end - vma->vm_start,
+					 vma->vm_page_prot);
+	} else {
+		vma->vm_flags |= VM_SHARED;
+		ret = remap_pfn_range(vma,
+				      vma->vm_start,
+				      vma->vm_pgoff,
+				      vma->vm_end - vma->vm_start,
+				      vma->vm_page_prot);
+	}
+	up_write(&mm->mmap_sem);
+	return ret;
+}
+
+static inline int vesafb_init_mem(void)
+{
+	int ret = 0;
+
+	/* The memory chunks we're remapping here should be multiples
+	 * of PAGE_SIZE. */
+	ret += vesafb_remap_pfn_range(0x00000, IVTBDA_SIZE, 0,
+				      PROT_READ | PROT_EXEC | PROT_WRITE, 0);
+	ret += vesafb_remap_pfn_range(IVTBDA_SIZE, REAL_MEM_SIZE, 0,
+				      PROT_READ | PROT_EXEC | PROT_WRITE, 1);
+	ret += vesafb_remap_pfn_range(0x9f000, 0x100000, 
+				      0x9f000 >> PAGE_SHIFT,
+				      PROT_READ | PROT_EXEC | PROT_WRITE, 0);
+	if (ret)
+		printk(KERN_ERR "vesafb thread: memory remapping failed\n");
+
+	return ret;
+}
+
+#define vesafb_get_string(str) \
+{ 									\
+	/* The address is in the form ssssoooo, where oooo = offset,	\
+	 * ssss = segment */						\
+	addr = ((p_vbe(tsk->buf)->str & 0xffff0000) >> 12) +		\
+		(p_vbe(tsk->buf)->str & 0x0000ffff);			\
+									\
+	/* The data is in ROM which is shared between processes, so we 	\
+	 * just translate the real mode address into one visible from 	\
+	 * kernel space */						\
+	if (addr >= 0xa0000) {						\
+		p_vbe(tsk->buf)->str = (u32) __va(addr);		\
+									\
+	/* The data is in the buffer, we just have to convert the	\
+	 * address so that it points into the buffer user provided. */	\
+	} else if (addr > BUF_ADDR && addr < BUF_ADDR +			\
+		   sizeof(struct vesafb_vbe_ib)) {			\
+		addr -= BUF_ADDR;					\
+		p_vbe(tsk->buf)->str = (u32) (tsk->buf + addr);		\
+									\
+	/* This should never happen: someone was insane enough to put	\
+	 * the data somewhere in RAM.. */				\
+	} else {							\
+		p_vbe(tsk->buf)->str = (u32) "";			\
+	}								\
+}
+
+void vesafb_handle_getvbeib(struct vesafb_task *tsk)
+{
+	int addr, res;
+
+	tsk->regs.es  = (BUF_ADDR >> 4);
+	tsk->regs.edi = (BUF_ADDR & 0x000f);
+	strncpy(p_vbe(BUF_ADDR)->vbe_signature, "VBE2", 4);
+
+	vesafb_do_vm86(&tsk->regs);
+	memcpy(tsk->buf, (void*)(BUF_ADDR), sizeof(struct vesafb_vbe_ib));
+
+	/* The OEM fields were not defined prior to VBE 2.0 */
+	if (p_vbe(tsk->buf)->vbe_version >= 0x200) {
+		vesafb_get_string(oem_string_ptr);
+		vesafb_get_string(oem_vendor_name_ptr);
+		vesafb_get_string(oem_product_name_ptr);
+		vesafb_get_string(oem_product_rev_ptr);
+	}
+
+	/* This is basically the same as vesafb_get_string() */
+	addr = ((p_vbe(tsk->buf)->mode_list_ptr & 0xffff0000) >> 12) +
+		(p_vbe(tsk->buf)->mode_list_ptr & 0x0000ffff);
+
+	if (addr >= 0xa0000) {
+		p_vbe(tsk->buf)->mode_list_ptr = (u32) __va(addr);
+	} else if (addr > BUF_ADDR && addr < BUF_ADDR +
+		   sizeof(struct vesafb_vbe_ib)) {
+		addr -= BUF_ADDR;
+		p_vbe(tsk->buf)->mode_list_ptr = (u32) (tsk->buf + addr);
+	} else {
+		res = 0;
+		printk(KERN_WARNING "vesafb: warning, copying modelist "
+				    "from somewhere in RAM!\n");
+		while (*(u16*)(addr+res) != 0xffff &&
+		       res < (sizeof(p_vbe(tsk->buf)->reserved) - 2)) {
+			*(u16*) ((u32)&(p_vbe(tsk->buf)->reserved) + res) =
+				*(u16*)(addr+res);
+			res += 2;
+		}
+		*(u16*) ((u32)&(p_vbe(tsk->buf)->reserved) + res) = 0xffff;
+	}
+}
+
+int vesafb_handle_tasks(void)
+{
+	struct vesafb_task *tsk;
+	struct list_head *curr, *next;
+	int ret = 0;
+
+	down(&vesafb_task_list_sem);
+	list_for_each_safe(curr, next, &vesafb_task_list) {
+		tsk = list_entry(curr, struct vesafb_task, node);
+
+		if (tsk->flags & TF_EXIT) {
+			ret = 1;
+			goto task_done;
+		}
+		if (tsk->flags & TF_GETVBEIB) {
+			vesafb_handle_getvbeib(tsk);
+			goto task_done;
+		}
+		/* Do we need to store a pointer to the buffer in ES:EDI? */
+		if (tsk->flags & TF_BUF_DI) {
+			tsk->regs.es  = (BUF_ADDR >> 4);
+			tsk->regs.edi = (BUF_ADDR & 0x000f);
+		}
+		/* Sometimes the pointer has to be in ES:EBX. */
+		if (tsk->flags & TF_BUF_BX) {
+			tsk->regs.es  = (BUF_ADDR >> 4);
+			tsk->regs.ebx = (BUF_ADDR & 0x000f);
+		}
+		if (tsk->flags & (TF_BUF_DI | TF_BUF_BX))
+			memcpy((void*)BUF_ADDR, tsk->buf, tsk->buf_len);
+
+		vesafb_do_vm86(&tsk->regs);
+
+		if (tsk->flags & TF_RETURN_BUF)
+			memcpy(tsk->buf, (void*)BUF_ADDR, tsk->buf_len);
+
+task_done:	list_del(curr);
+		complete(&tsk->done);
+	}
+
+	/* If we're going to kill this thread, don't allow any elements
+	 * to be added to the task list. */
+	if (!ret)
+		up(&vesafb_task_list_sem);
+
+	return ret;
+}
+
+/*
+ * This 'hybrid' thread serves as a backend for vesafb-tng, handling all vm86
+ * calls. It is started as a kernel thread. It then creates its own mm struct,
+ * thus separating itself from any userspace processes. At this moment, it
+ * stops being a kernel thread (kernel threads have mm = NULL) and becomes
+ * a 'hybrid' thread -- one that has full access to kernel space, yet runs
+ * with its own address space.
+ *
+ * This is necessary because in order to make vm86 calls some parts of the
+ * first 1MB of RAM have to be setup to mimic the real mode. These are:
+ *  - interrupt vector table	[0x00000-0x003ff]
+ *  - BIOS data area		[0x00400-0x004ff]
+ *  - Extended BIOS data area	[0x9fc00-0x9ffff]
+ *  - the video RAM		[0xa0000-0xbffff]
+ *  - video BIOS		[0xc0000-0xcffff]
+ *  - motherboard BIOS		[0xf0000-0xfffff]
+ */
+int vesafb_thread(void *unused)
+{
+	int err = 0;
+
+	set_fs(KERNEL_DS);
+	daemonize("vesafb");
+
+	if (set_new_mm()) {
+		err = -ENOMEM;
+		goto thr_end;
+	}
+	if (vesafb_init_mem()) {
+		err = -ENOMEM;
+		goto thr_end;
+	}
+
+	DPRINTK("started vesafb thread\n");
+
+	/* Having an IO bitmap makes things faster as we avoid GPFs
+	 * when running vm86 code. We can live if it fails, though,
+	 * so don't bother checking for errors. */
+	ioperm(0,1024,1);
+	set_user_nice(current, -10);
+
+	complete(&vesafb_th_completion);
+
+	while (1) {
+		if (vesafb_handle_tasks())
+			break;
+		wait_event_interruptible(vesafb_wait,
+					 !list_empty(&vesafb_task_list));
+		try_to_freeze();
+	}
+
+out:	DPRINTK("exiting the vesafb thread\n");
+	vesafb_pid = -1;
+
+	/* Now that all callers know this thread is no longer running
+	 * (pid < 0), allow them to continue. */
+	up(&vesafb_task_list_sem);
+	return err;
+thr_end:
+	down(&vesafb_task_list_sem);
+	complete(&vesafb_th_completion);
+	goto out;	
+}
+
+int vesafb_queue_task(struct vesafb_task *tsk)
+{
+	down(&vesafb_task_list_sem);
+	if (vesafb_pid < 0)
+		return -1;
+	list_add_tail(&tsk->node, &vesafb_task_list);
+	up(&vesafb_task_list_sem);
+	wake_up(&vesafb_wait);
+	return 0;
+}
+
+int vesafb_wait_for_thread(void)
+{
+	/* PID 0 means that the thread is still initializing. */
+	if (vesafb_pid < 0)
+		return -1;
+	wait_for_completion(&vesafb_th_completion);
+	return 0;
+}
+
+int __init vesafb_init_thread(void)
+{
+	vesafb_pid = kernel_thread(vesafb_thread,NULL,0);
+	return 0;
+}
+
+#ifdef MODULE
+void __exit vesafb_kill_thread(void)
+{
+	struct vesafb_task *tsk;
+	if (vesafb_pid <= 0)
+		return;
+
+	vesafb_create_task(tsk);
+	if (!tsk)
+		return;
+	tsk->flags |= TF_EXIT;
+	vesafb_queue_task(tsk);
+	vesafb_wait_for_task(tsk);
+	kfree(tsk);
+	return;
+}
+module_exit(vesafb_kill_thread);
+#endif
+module_init(vesafb_init_thread);
+
+EXPORT_SYMBOL_GPL(vesafb_queue_task);
+EXPORT_SYMBOL_GPL(vesafb_wait_for_thread);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Michal Januszewski");
+
diff -urN oldtree/drivers/video/vesafb-tng.c newtree/drivers/video/vesafb-tng.c
--- oldtree/drivers/video/vesafb-tng.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/drivers/video/vesafb-tng.c	2006-07-12 19:01:08.000000000 -0400
@@ -0,0 +1,1598 @@
+/*
+ * Framebuffer driver for VBE 2.0+ compliant graphic boards
+ *
+ * (c) 2004-2006 Michal Januszewski <spock@gentoo.org>
+ *     Based upon vesafb code by Gerd Knorr <kraxel@goldbach.in-berlin.de>
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/mm.h>
+#include <linux/tty.h>
+#include <linux/delay.h>
+#include <linux/fb.h>
+#include <linux/ioport.h>
+#include <linux/init.h>
+#include <linux/proc_fs.h>
+#include <linux/completion.h>
+#include <linux/platform_device.h>
+#include <video/edid.h>
+#include <video/vesa.h>
+#include <video/vga.h>
+#include <asm/io.h>
+#include <asm/mtrr.h>
+#include <asm/page.h>
+#include <asm/pgtable.h>
+#include "edid.h"
+
+#define dac_reg	(0x3c8)
+#define dac_val	(0x3c9)
+
+#define VESAFB_NEED_EXACT_RES 	1
+#define VESAFB_NEED_EXACT_DEPTH 2
+
+/* --------------------------------------------------------------------- */
+
+static struct fb_var_screeninfo vesafb_defined __initdata = {
+	.activate	= FB_ACTIVATE_NOW,
+	.height		= 0,
+	.width		= 0,
+	.right_margin	= 32,
+	.upper_margin	= 16,
+	.lower_margin	= 4,
+	.vsync_len	= 4,
+	.vmode		= FB_VMODE_NONINTERLACED,
+};
+
+static struct fb_fix_screeninfo vesafb_fix __initdata = {
+	.id	= "VESA VGA",
+	.type	= FB_TYPE_PACKED_PIXELS,
+	.accel	= FB_ACCEL_NONE,
+};
+
+static int  mtrr       = 0;	/* disable mtrr by default */
+static int  blank      = 1;     /* enable blanking by default */
+static int  ypan       = 0;	/* 0 - nothing, 1 - ypan, 2 - ywrap */
+static int  pmi_setpal = 1;	/* pmi for palette changes */
+static u16  *pmi_base  = NULL;  /* protected mode interface location */
+static void (*pmi_start)(void) = NULL;
+static void (*pmi_pal)(void)   = NULL;
+static struct vesafb_vbe_ib  vbe_ib;
+static struct vesafb_mode_ib *vbe_modes;
+static int                   vbe_modes_cnt = 0;
+static struct fb_info	     *vesafb_info = NULL;
+static int  nocrtc		    = 0; /* ignore CRTC settings */
+static int  noedid       __initdata = 0; /* don't try DDC transfers */
+static int  vram_remap   __initdata = 0; /* set amount of memory to be used */
+static int  vram_total   __initdata = 0; /* set total amount of memory */
+static u16  maxclk       __initdata = 0; /* maximum pixel clock */
+static u16  maxvf        __initdata = 0; /* maximum vertical frequency */
+static u16  maxhf        __initdata = 0; /* maximum horizontal frequency */
+static int  gtf          __initdata = 0; /* forces use of the GTF */
+static char *mode_option __initdata = NULL;
+static u16  vbemode      __initdata = 0;
+
+/* --------------------------------------------------------------------- */
+
+static int vesafb_find_vbe_mode(int xres, int yres, int depth,
+				unsigned char flags)
+{
+	int i, match = -1, h = 0, d = 0x7fffffff;
+
+	for (i = 0; i < vbe_modes_cnt; i++) {
+		h = abs(vbe_modes[i].x_res - xres) +
+		    abs(vbe_modes[i].y_res - yres) +
+		    abs(depth - vbe_modes[i].depth);
+		if (h == 0)
+			return i;
+		if (h < d || (h == d && vbe_modes[i].depth > depth)) {
+			d = h;
+			match = i;
+		}
+	}
+	i = 1;
+
+	if (flags & VESAFB_NEED_EXACT_DEPTH && vbe_modes[match].depth != depth)
+		i = 0;
+	if (flags & VESAFB_NEED_EXACT_RES && d > 24)
+		i = 0;
+	if (i != 0)
+		return match;
+	else
+		return -1;
+}
+
+static int vesafb_pan_display(struct fb_var_screeninfo *var,
+                              struct fb_info *info)
+{
+	int offset;
+
+	offset = (var->yoffset * info->fix.line_length + var->xoffset) / 4;
+
+	/* It turns out it's not the best idea to do panning via vm86,
+	 * so we only allow it if we have a PMI. */
+	if (pmi_start) {
+		__asm__ __volatile__(
+			"call *(%%edi)"
+			: /* no return value */
+			: "a" (0x4f07),         /* EAX */
+			  "b" (0),              /* EBX */
+			  "c" (offset),         /* ECX */
+			  "d" (offset >> 16),   /* EDX */
+			  "D" (&pmi_start));    /* EDI */
+	}
+	return 0;
+}
+
+static int vesafb_blank(int blank, struct fb_info *info)
+{
+	struct vesafb_task *tsk;
+	int err = 1;
+
+	if (vbe_ib.capabilities & VBE_CAP_VGACOMPAT) {
+		int loop = 10000;
+		u8 seq = 0, crtc17 = 0;
+
+		if (blank == FB_BLANK_POWERDOWN) {
+			seq = 0x20;
+			crtc17 = 0x00;
+			err = 0;
+		} else {
+			seq = 0x00;
+			crtc17 = 0x80;
+			err = (blank == FB_BLANK_UNBLANK) ? 0 : -EINVAL;
+		}
+
+		vga_wseq(NULL, 0x00, 0x01);
+		seq |= vga_rseq(NULL, 0x01) & ~0x20;
+		vga_wseq(NULL, 0x00, seq);
+
+		crtc17 |= vga_rcrt(NULL, 0x17) & ~0x80;
+		while (loop--);
+		vga_wcrt(NULL, 0x17, crtc17);
+		vga_wseq(NULL, 0x00, 0x03);
+	} else {
+		vesafb_create_task (tsk);
+		if (!tsk)
+			return -ENOMEM;
+		tsk->regs.eax = 0x4f10;
+		switch (blank) {
+		case FB_BLANK_UNBLANK:
+			tsk->regs.ebx = 0x0001;
+			break;
+		case FB_BLANK_NORMAL:
+			tsk->regs.ebx = 0x0101;	/* standby */
+			break;
+		case FB_BLANK_POWERDOWN:
+			tsk->regs.ebx = 0x0401;	/* powerdown */
+			break;
+		default:
+			goto out;
+		}
+		tsk->flags = TF_CALL;
+		if (!vesafb_queue_task (tsk))
+			vesafb_wait_for_task(tsk);
+
+		if ((tsk->regs.eax & 0xffff) == 0x004f)
+			err = 0;
+out:		kfree(tsk);
+	}
+	return err;
+}
+
+static int vesafb_setpalette(struct vesafb_pal_entry *entries, int count,
+			     int start, struct fb_info *info)
+{
+	struct vesafb_task *tsk;
+	int i = ((struct vesafb_par*)info->par)->mode_idx;
+	int ret = 0;
+
+	/* We support palette modifications for 8 bpp modes only, so
+	 * there can never be more than 256 entries. */
+	if (start + count > 256)
+		return -EINVAL;
+
+	/* Use VGA registers if mode is VGA-compatible. */
+	if (i >= 0 && i < vbe_modes_cnt &&
+	    vbe_modes[i].mode_attr & VBE_MODE_VGACOMPAT) {
+		for (i = 0; i < count; i++) {
+			outb_p(start + i,        dac_reg);
+			outb_p(entries[i].red,   dac_val);
+			outb_p(entries[i].green, dac_val);
+			outb_p(entries[i].blue,  dac_val);
+		}
+	} else if (pmi_setpal) {
+		__asm__ __volatile__(
+		"call *(%%esi)"
+		: /* no return value */
+		: "a" (0x4f09),         /* EAX */
+		  "b" (0),              /* EBX */
+		  "c" (count),          /* ECX */
+		  "d" (start),          /* EDX */
+		  "D" (entries),        /* EDI */
+		  "S" (&pmi_pal));      /* ESI */
+	} else {
+		vesafb_create_task (tsk);
+		if (!tsk)
+			return -ENOMEM;
+		tsk->regs.eax = 0x4f09;
+		tsk->regs.ebx = 0x0;
+		tsk->regs.ecx = count;
+		tsk->regs.edx = start;
+		tsk->buf = entries;
+		tsk->buf_len = sizeof(struct vesafb_pal_entry) * count;
+		tsk->flags = TF_CALL | TF_BUF_DI;
+
+		if (!vesafb_queue_task (tsk))
+			vesafb_wait_for_task(tsk);
+		if ((tsk->regs.eax & 0xffff) != 0x004f)
+			ret = 1;
+		kfree(tsk);
+	}
+	return ret;
+}
+
+static int vesafb_setcolreg(unsigned regno, unsigned red, unsigned green,
+			    unsigned blue, unsigned transp,
+			    struct fb_info *info)
+{
+	struct vesafb_pal_entry entry;
+	int shift = 16 - info->var.green.length;
+	int ret = 0;
+
+	if (regno >= info->cmap.len)
+		return -EINVAL;
+
+	if (info->var.bits_per_pixel == 8) {
+		entry.red   = red   >> shift;
+		entry.green = green >> shift;
+		entry.blue  = blue  >> shift;
+		entry.pad   = 0;
+
+		ret = vesafb_setpalette(&entry, 1, regno, info);
+	} else if (regno < 16) {
+		switch (info->var.bits_per_pixel) {
+		case 16:
+			if (info->var.red.offset == 10) {
+				/* 1:5:5:5 */
+				((u32*) (info->pseudo_palette))[regno] =
+						((red   & 0xf800) >>  1) |
+						((green & 0xf800) >>  6) |
+						((blue  & 0xf800) >> 11);
+			} else {
+				/* 0:5:6:5 */
+				((u32*) (info->pseudo_palette))[regno] =
+						((red   & 0xf800)      ) |
+						((green & 0xfc00) >>  5) |
+						((blue  & 0xf800) >> 11);
+			}
+			break;
+
+		case 24:
+		case 32:
+			red   >>= 8;
+			green >>= 8;
+			blue  >>= 8;
+			((u32 *)(info->pseudo_palette))[regno] =
+				(red   << info->var.red.offset)   |
+				(green << info->var.green.offset) |
+				(blue  << info->var.blue.offset);
+			break;
+		}
+	}
+	return ret;
+}
+
+static int vesafb_setcmap(struct fb_cmap *cmap, struct fb_info *info)
+{
+	struct vesafb_pal_entry *entries;
+	int shift = 16 - info->var.green.length;
+	int i, ret = 0;
+
+	if (info->var.bits_per_pixel == 8) {
+		if (cmap->start + cmap->len > info->cmap.start +
+		    info->cmap.len || cmap->start < info->cmap.start)
+			return -EINVAL;
+
+		entries = vmalloc(sizeof(struct vesafb_pal_entry) * cmap->len);
+		if (!entries)
+			return -ENOMEM;
+		for (i = 0; i < cmap->len; i++) {
+			entries[i].red   = cmap->red[i]   >> shift;
+			entries[i].green = cmap->green[i] >> shift;
+			entries[i].blue  = cmap->blue[i]  >> shift;
+			entries[i].pad   = 0;
+		}
+		ret = vesafb_setpalette(entries, cmap->len, cmap->start, info);
+		vfree(entries);
+	} else {
+		/* For modes with bpp > 8, we only set the pseudo palette in
+		 * the fb_info struct. We rely on vesafb_setcolreg to do all
+		 * sanity checking. */
+		for (i = 0; i < cmap->len; i++) {
+			ret += vesafb_setcolreg(cmap->start + i, cmap->red[i],
+						cmap->green[i], cmap->blue[i],
+						0, info);
+		}
+	}
+	return ret;
+}
+
+static int vesafb_set_par(struct fb_info *info)
+{
+	struct vesafb_par *par = (struct vesafb_par *) info->par;
+	struct vesafb_task *tsk;
+	struct vesafb_crtc_ib *crtc = NULL;
+	struct vesafb_mode_ib *mode = NULL;
+	int i, err = 0, depth = info->var.bits_per_pixel;
+
+	if (depth > 8 && depth != 32)
+		depth = info->var.red.length + info->var.green.length +
+			info->var.blue.length;
+
+	i = vesafb_find_vbe_mode(info->var.xres, info->var.yres, depth,
+				 VESAFB_NEED_EXACT_RES |
+				 VESAFB_NEED_EXACT_DEPTH);
+	if (i >= 0)
+		mode = &vbe_modes[i];
+	else
+		return -EINVAL;
+
+	vesafb_create_task (tsk);
+	if (!tsk)
+		return -ENOMEM;
+	tsk->regs.eax = 0x4f02;
+	tsk->regs.ebx = mode->mode_id | 0x4000;		/* use LFB */
+	tsk->flags = TF_CALL;
+
+	if (vbe_ib.vbe_version >= 0x0300 && !nocrtc &&
+	    info->var.pixclock != 0) {
+		tsk->regs.ebx |= 0x0800; 		/* use CRTC data */
+		tsk->flags |= TF_BUF_DI;
+		crtc = kmalloc(sizeof(struct vesafb_crtc_ib), GFP_KERNEL);
+		if (!crtc) {
+			err = -ENOMEM;
+			goto out;
+		}
+		crtc->horiz_start = info->var.xres + info->var.right_margin;
+		crtc->horiz_end	  = crtc->horiz_start + info->var.hsync_len;
+		crtc->horiz_total = crtc->horiz_end + info->var.left_margin;
+
+		crtc->vert_start  = info->var.yres + info->var.lower_margin;
+		crtc->vert_end    = crtc->vert_start + info->var.vsync_len;
+		crtc->vert_total  = crtc->vert_end + info->var.upper_margin;
+
+		crtc->pixel_clock = PICOS2KHZ(info->var.pixclock) * 1000;
+		crtc->refresh_rate = (u16)(100 * (crtc->pixel_clock /
+				     (crtc->vert_total * crtc->horiz_total)));
+		crtc->flags = 0;
+
+		if (info->var.vmode & FB_VMODE_DOUBLE)
+			crtc->flags |= 0x1;
+		if (info->var.vmode & FB_VMODE_INTERLACED)
+			crtc->flags |= 0x2;
+		if (!(info->var.sync & FB_SYNC_HOR_HIGH_ACT))
+			crtc->flags |= 0x4;
+		if (!(info->var.sync & FB_SYNC_VERT_HIGH_ACT))
+			crtc->flags |= 0x8;
+		memcpy(&par->crtc, crtc, sizeof(struct vesafb_crtc_ib));
+	} else
+		memset(&par->crtc, 0, sizeof(struct vesafb_crtc_ib));
+
+	tsk->buf = (void*)crtc;
+	tsk->buf_len = sizeof(struct vesafb_crtc_ib);
+
+	if (vesafb_queue_task (tsk)) {
+		err = -EINVAL;
+		goto out;
+	}
+	vesafb_wait_for_task(tsk);
+
+	if ((tsk->regs.eax & 0xffff) != 0x004f) {
+		printk(KERN_ERR "vesafb: mode switch failed (eax: 0x%lx)\n",
+				tsk->regs.eax);
+		err = -EINVAL;
+		goto out;
+	}
+	par->mode_idx = i;
+
+	/* For 8bpp modes, always try to set the DAC to 8 bits. */
+	if (vbe_ib.capabilities & VBE_CAP_CAN_SWITCH_DAC &&
+	    mode->bits_per_pixel <= 8) {
+		vesafb_reset_task(tsk);
+		tsk->flags = TF_CALL;
+		tsk->regs.eax = 0x4f08;
+		tsk->regs.ebx = 0x0800;
+
+		if (!vesafb_queue_task (tsk))
+			vesafb_wait_for_task(tsk);
+
+		if ((tsk->regs.eax & 0xffff) != 0x004f ||
+		    ((tsk->regs.ebx & 0xff00) >> 8) != 8) {
+			/* We've failed to set the DAC palette format -
+			 * time to correct var. */
+			info->var.red.length    = 6;
+			info->var.green.length  = 6;
+			info->var.blue.length   = 6;
+		}
+	}
+
+	info->fix.visual = (info->var.bits_per_pixel == 8) ?
+		           FB_VISUAL_PSEUDOCOLOR : FB_VISUAL_TRUECOLOR;
+	info->fix.line_length = mode->bytes_per_scan_line;
+
+	DPRINTK("set new mode %dx%d-%d (0x%x)\n",
+		info->var.xres, info->var.yres, info->var.bits_per_pixel,
+		mode->mode_id);
+
+out:	if (crtc != NULL)
+		kfree(crtc);
+	kfree(tsk);
+
+	return err;
+}
+
+static void vesafb_setup_var(struct fb_var_screeninfo *var, struct fb_info *info,
+			     struct vesafb_mode_ib *mode)
+{
+	var->xres = mode->x_res;
+	var->yres = mode->y_res;
+	var->xres_virtual = mode->x_res;
+	var->yres_virtual = (ypan) ?
+			      info->fix.smem_len / mode->bytes_per_scan_line :
+			      mode->y_res;
+	var->xoffset = 0;
+	var->yoffset = 0;
+	var->bits_per_pixel = mode->bits_per_pixel;
+
+	if (var->bits_per_pixel == 15)
+		var->bits_per_pixel = 16;
+
+	if (var->bits_per_pixel > 8) {
+		var->red.offset    = mode->red_off;
+		var->red.length    = mode->red_len;
+		var->green.offset  = mode->green_off;
+		var->green.length  = mode->green_len;
+		var->blue.offset   = mode->blue_off;
+		var->blue.length   = mode->blue_len;
+		var->transp.offset = mode->rsvd_off;
+		var->transp.length = mode->rsvd_len;
+
+		DPRINTK("directcolor: size=%d:%d:%d:%d, shift=%d:%d:%d:%d\n",
+			mode->rsvd_len,
+			mode->red_len,
+			mode->green_len,
+			mode->blue_len,
+			mode->rsvd_off,
+			mode->red_off,
+			mode->green_off,
+			mode->blue_off);
+	} else {
+		var->red.offset    = 0;
+		var->green.offset  = 0;
+		var->blue.offset   = 0;
+		var->transp.offset = 0;
+
+		/* We're assuming that we can switch the DAC to 8 bits. If
+		 * this proves to be incorrect, we'll update the fields
+		 * later in set_par(). */
+		if (vbe_ib.capabilities & VBE_CAP_CAN_SWITCH_DAC) {
+			var->red.length    = 8;
+			var->green.length  = 8;
+			var->blue.length   = 8;
+			var->transp.length = 0;
+		} else {
+			var->red.length    = 6;
+			var->green.length  = 6;
+			var->blue.length   = 6;
+			var->transp.length = 0;
+		}
+	}
+}
+
+static void inline vesafb_check_limits(struct fb_var_screeninfo *var,
+		 		       struct fb_info *info)
+{
+	struct fb_videomode *mode;
+
+	if (!var->pixclock)
+		return;
+	if (vbe_ib.vbe_version < 0x0300) {
+		fb_get_mode(FB_VSYNCTIMINGS | FB_IGNOREMON, 60, var, info);
+		return;
+	}
+	if (!fb_validate_mode(var, info))
+		return;
+	mode = fb_find_best_mode(var, &info->modelist);
+	if (mode) {
+		DPRINTK("find_best_mode: %d %d @ %d (vmode: %d)\n",
+			mode->xres, mode->yres, mode->refresh, mode->vmode);
+		if (mode->xres == var->xres && mode->yres == var->yres &&
+		    !(mode->vmode & (FB_VMODE_INTERLACED | FB_VMODE_DOUBLE))) {
+			fb_videomode_to_var(var, mode);
+			return;
+		}
+	}
+	if (info->monspecs.gtf && !fb_get_mode(FB_MAXTIMINGS, 0, var, info))
+		return;
+	/* Use default refresh rate */
+	var->pixclock = 0;
+}
+
+static int vesafb_check_var(struct fb_var_screeninfo *var,
+			    struct fb_info *info)
+{
+	int match = -1;
+	int depth = var->red.length + var->green.length + var->blue.length;
+
+	/* Various apps will use bits_per_pixel to set the color depth,
+	 * which is theoretically incorrect, but which we'll try to handle
+	 * here. */
+	if (depth == 0 || abs(depth - var->bits_per_pixel) >= 8)
+		depth = var->bits_per_pixel;
+	match = vesafb_find_vbe_mode(var->xres, var->yres, depth,
+				     VESAFB_NEED_EXACT_RES);
+
+	if (match == -1) {
+		DPRINTK("vesafb: mode %dx%d-%d not found\n", var->xres,
+			var->yres, depth);
+		return -EINVAL;
+	}
+
+	vesafb_setup_var(var, info, &vbe_modes[match]);
+	DPRINTK("found mode 0x%x (%dx%d-%dbpp)\n",
+		vbe_modes[match].mode_id, vbe_modes[match].x_res,
+		vbe_modes[match].y_res, vbe_modes[match].depth);
+
+	/* Check whether we have remapped enough memory for this mode. */
+	if (var->yres * vbe_modes[match].bytes_per_scan_line >
+	    info->fix.smem_len) {
+		return -EINVAL;
+	}
+
+	if ((var->vmode & FB_VMODE_DOUBLE) &&
+	    !(vbe_modes[match].mode_attr & 0x100))
+		var->vmode &= ~FB_VMODE_DOUBLE;
+	if ((var->vmode & FB_VMODE_INTERLACED) &&
+	    !(vbe_modes[match].mode_attr & 0x200))
+		var->vmode &= ~FB_VMODE_INTERLACED;
+	vesafb_check_limits(var, info);
+	return 0;
+}
+
+static int vesafb_open(struct fb_info *info, int user)
+{
+	struct vesafb_task *tsk = NULL;
+	struct vesafb_par *par = info->par;
+	int cnt = atomic_read(&par->ref_count);
+
+	if (!cnt) {
+		vesafb_create_task(tsk);
+		if (!tsk)
+			goto out;
+
+		/* Get the VBE state buffer size. We want all available
+		 * hardware state data (CL = 0x0f). */
+		tsk->regs.eax = 0x4f04;
+		tsk->regs.ecx = 0x000f;
+		tsk->regs.edx = 0x0000;
+		tsk->flags = TF_CALL;
+
+		if (vesafb_queue_task(tsk))
+			goto out;
+	
+		vesafb_wait_for_task(tsk);
+		
+		if ((tsk->regs.eax & 0xffff) != 0x004f) {
+			printk(KERN_WARNING "vesafb: VBE state buffer size "
+				"cannot be determined (eax: 0x%lx)\n",
+				tsk->regs.eax);
+			goto out;
+		}
+
+		par->vbe_state_size = 64 * (tsk->regs.ebx & 0xffff);
+		par->vbe_state = kzalloc(par->vbe_state_size, GFP_KERNEL);
+		if (!par->vbe_state) 
+			goto out;
+
+		vesafb_reset_task(tsk);
+		tsk->regs.eax = 0x4f04;
+		tsk->regs.ecx = 0x000f;
+		tsk->regs.edx = 0x0001;
+		tsk->flags = TF_CALL | TF_BUF_BX | TF_RETURN_BUF;
+		tsk->buf = (void*)(par->vbe_state);
+		tsk->buf_len = par->vbe_state_size;
+
+		if (vesafb_queue_task(tsk))
+			goto getstate_failed;
+		vesafb_wait_for_task(tsk);
+
+		if ((tsk->regs.eax & 0xffff) != 0x004f) {
+			printk(KERN_WARNING "vesafb: VBE get state call "
+				"failed (eax: 0x%lx)\n", tsk->regs.eax);
+			goto getstate_failed;
+		}
+	}
+out:
+	atomic_inc(&par->ref_count);
+	if (tsk)
+		kfree(tsk);
+	return 0;
+
+getstate_failed:
+	kfree(par->vbe_state);
+	par->vbe_state = NULL;
+	par->vbe_state_size = 0;
+	goto out;
+}
+
+static int vesafb_release(struct fb_info *info, int user)
+{
+	struct vesafb_task *tsk = NULL;
+	struct vesafb_par *par = info->par;
+	int cnt = atomic_read(&par->ref_count);
+
+	if (!cnt)
+		return -EINVAL;
+	
+	if (cnt == 1 && par->vbe_state && par->vbe_state_size) {
+		vesafb_create_task(tsk);
+		if (!tsk)
+			goto out;
+
+		tsk->regs.eax = 0x0003;
+		tsk->regs.ebx = 0x0000;
+		tsk->flags = TF_CALL;
+
+		if (vesafb_queue_task(tsk))
+			goto out;
+	
+		vesafb_wait_for_task(tsk);
+
+		vesafb_reset_task(tsk);
+		tsk->regs.eax = 0x4f04;
+		tsk->regs.ecx = 0x000f;
+		tsk->regs.edx = 0x0002;
+		tsk->buf = (void*)(par->vbe_state);
+		tsk->buf_len = par->vbe_state_size;
+		tsk->flags = TF_CALL | TF_BUF_BX;
+
+		if (vesafb_queue_task(tsk))
+			goto out;
+	
+		vesafb_wait_for_task(tsk);
+
+		if ((tsk->regs.eax & 0xffff) != 0x004f)
+			printk(KERN_WARNING "vesafb: VBE state restore call "
+				"failed (eax: 0x%lx)\n",
+				tsk->regs.eax);
+	}
+out:
+	atomic_dec(&par->ref_count);
+	if (tsk)
+		kfree(tsk);
+	return 0;
+}
+
+static int __init vesafb_probe(struct platform_device *device);
+
+static struct fb_ops vesafb_ops = {
+	.owner		= THIS_MODULE,
+	.fb_open	= vesafb_open,
+	.fb_release	= vesafb_release,
+	.fb_setcolreg	= vesafb_setcolreg,
+	.fb_setcmap	= vesafb_setcmap,
+	.fb_pan_display	= vesafb_pan_display,
+	.fb_blank       = vesafb_blank,
+	.fb_fillrect	= cfb_fillrect,
+	.fb_copyarea	= cfb_copyarea,
+	.fb_imageblit	= cfb_imageblit,
+	.fb_check_var	= vesafb_check_var,
+	.fb_set_par	= vesafb_set_par
+};
+
+static struct platform_driver vesafb_driver = {
+	.probe	= vesafb_probe,
+	.driver	= {
+		.name	= "vesafb",
+	},
+};
+
+static struct platform_device *vesafb_device;
+ 
+#ifndef MODULE
+int __init vesafb_setup(char *options)
+{
+	char *this_opt;
+
+	if (!options || !*options)
+		return 0;
+
+	DPRINTK("options %s\n",options);
+
+	while ((this_opt = strsep(&options, ",")) != NULL) {
+		if (!*this_opt) continue;
+
+		DPRINTK("this_opt: %s\n",this_opt);
+
+		if (! strcmp(this_opt, "redraw"))
+			ypan=0;
+		else if (! strcmp(this_opt, "ypan"))
+			ypan=1;
+		else if (! strcmp(this_opt, "ywrap"))
+			ypan=2;
+		else if (! strcmp(this_opt, "vgapal"))
+			pmi_setpal=0;
+		else if (! strcmp(this_opt, "pmipal"))
+			pmi_setpal=1;
+		else if (! strncmp(this_opt, "mtrr:", 5))
+			mtrr = simple_strtoul(this_opt+5, NULL, 0);
+		else if (! strcmp(this_opt, "nomtrr"))
+			mtrr=0;
+		else if (! strcmp(this_opt, "nocrtc"))
+			nocrtc=1;
+		else if (! strcmp(this_opt, "noedid"))
+			noedid=1;
+		else if (! strcmp(this_opt, "noblank"))
+			blank=0;
+		else if (! strcmp(this_opt, "gtf"))
+			gtf=1;
+		else if (! strncmp(this_opt, "vtotal:", 7))
+			vram_total = simple_strtoul(this_opt + 7, NULL, 0);
+		else if (! strncmp(this_opt, "vremap:", 7))
+			vram_remap = simple_strtoul(this_opt + 7, NULL, 0);
+		else if (! strncmp(this_opt, "maxhf:", 6))
+			maxhf = simple_strtoul(this_opt + 6, NULL, 0);
+		else if (! strncmp(this_opt, "maxvf:", 6))
+			maxvf = simple_strtoul(this_opt + 6, NULL, 0);
+		else if (! strncmp(this_opt, "maxclk:", 7))
+			maxclk = simple_strtoul(this_opt + 7, NULL, 0);
+		else if (! strncmp(this_opt, "vbemode:", 8))
+			vbemode = simple_strtoul(this_opt + 8, NULL,0);
+		else if (this_opt[0] >= '0' && this_opt[0] <= '9') {
+			DPRINTK("mode_option: %s\n",this_opt);
+			mode_option = this_opt;
+		} else {
+			printk(KERN_WARNING
+			       "vesafb: unrecognized option %s\n", this_opt);
+		}
+	}
+
+	return 0;
+}
+#endif /* !MODULE */
+
+static int vesafb_read_proc_modes(char *buf, char **start, off_t offset,
+			    	  int len, int *eof, void *private)
+{
+	int clen = 0, i;
+
+	for (i = 0; i < vbe_modes_cnt; i++) {
+		clen += sprintf(buf + clen, "%dx%d-%d\n", vbe_modes[i].x_res,
+				vbe_modes[i].y_res, vbe_modes[i].depth);
+	}
+	*start = buf + offset;
+
+	if (clen > offset) {
+		clen -= offset;
+	} else {
+		clen = 0;
+	}
+	return clen;
+}
+
+static int vesafb_read_proc_vbe_info(char *buf, char **start, off_t offset,
+			    	     int len, int *eof, void *private)
+{
+	int clen = 0;
+
+	clen += sprintf(buf + clen, "Version:    %d.%d\n",
+			((vbe_ib.vbe_version & 0xff00) >> 8),
+			vbe_ib.vbe_version & 0xff);
+	clen += sprintf(buf + clen, "Vendor:     %s\n",
+			(char*)vbe_ib.oem_vendor_name_ptr);
+	clen += sprintf(buf + clen, "Product:    %s\n",
+			(char*)vbe_ib.oem_product_name_ptr);
+	clen += sprintf(buf + clen, "OEM rev:    %s\n",
+			(char*)vbe_ib.oem_product_rev_ptr);
+	clen += sprintf(buf + clen, "OEM string: %s\n",
+			(char*)vbe_ib.oem_string_ptr);
+
+	*start = buf + offset;
+
+	if (clen > offset) {
+		clen -= offset;
+	} else {
+		clen = 0;
+	}
+	return clen;
+}
+
+static int __init inline vesafb_vbe_getinfo(struct vesafb_task *tsk)
+{
+	tsk->regs.eax = 0x4f00;
+	tsk->flags = TF_CALL | TF_GETVBEIB;
+	tsk->buf = &vbe_ib;
+	tsk->buf_len = sizeof(vbe_ib);
+	if (vesafb_queue_task (tsk))
+		return -EINVAL;
+	vesafb_wait_for_task(tsk);
+
+	if (vbe_ib.vbe_version < 0x0200) {
+		printk(KERN_ERR "vesafb: Sorry, pre-VBE 2.0 cards are "
+				"not supported.\n");
+		return -EINVAL;
+	}
+
+	if ((tsk->regs.eax & 0xffff) != 0x004f) {
+		printk(KERN_ERR "vesafb: Getting mode info block failed "
+				"(eax=0x%x)\n", (u32)tsk->regs.eax);
+		return -EINVAL;
+	}
+
+	printk(KERN_INFO "vesafb: %s, %s, %s (OEM: %s)\n",
+		(char*)vbe_ib.oem_vendor_name_ptr,
+		(char*)vbe_ib.oem_product_name_ptr,
+		(char*)vbe_ib.oem_product_rev_ptr,
+		(char*)vbe_ib.oem_string_ptr);
+
+	printk(KERN_INFO "vesafb: VBE version: %d.%d\n",
+			 ((vbe_ib.vbe_version & 0xff00) >> 8),
+			 vbe_ib.vbe_version & 0xff);
+	return 0;
+}
+
+static int __init inline vesafb_vbe_getmodes(struct vesafb_task *tsk)
+{
+	u16 *mode = 0;
+	int off = 0;
+
+	/* Count available modes. */
+	mode = (u16*)vbe_ib.mode_list_ptr;
+	while (*mode != 0xffff) {
+		vbe_modes_cnt++;
+		mode++;
+	}
+
+	vbe_modes = kmalloc(sizeof(struct vesafb_mode_ib)*
+			    vbe_modes_cnt, GFP_KERNEL);
+	if (!vbe_modes)
+		return -ENOMEM;
+
+	/* Get mode info for all available modes. */
+	mode = (u16*)vbe_ib.mode_list_ptr;
+
+	while (*mode != 0xffff) {
+		struct vesafb_mode_ib *mib;
+
+		vesafb_reset_task(tsk);
+		tsk->regs.eax = 0x4f01;
+		tsk->regs.ecx = (u32) *mode;
+		tsk->flags = TF_CALL | TF_RETURN_BUF | TF_BUF_DI;
+		tsk->buf = vbe_modes+off;
+		tsk->buf_len = sizeof(struct vesafb_mode_ib);
+		if (vesafb_queue_task(tsk))
+			return -EINVAL;
+		vesafb_wait_for_task(tsk);
+		mib = p_mode(tsk->buf);
+		mib->mode_id = *mode;
+
+		/* We only want modes that are supported with the currennt
+		 * hardware configuration (D0), color (D3), graphics (D4)
+		 * and that have support for the LFB (D7). */
+		if ((mib->mode_attr & 0x99) == 0x99 &&
+		    mib->bits_per_pixel >= 8) {
+			off++;
+		} else {
+			vbe_modes_cnt--;
+		}
+		mode++;
+		mib->depth = mib->red_len + mib->green_len + mib->blue_len;
+		/* Handle 8bpp modes and modes with broken color component
+		 * lengths. */
+		if (mib->depth == 0 ||
+		    (mib->depth == 24 && mib->bits_per_pixel == 32))
+			mib->depth = mib->bits_per_pixel;
+	}
+
+	return 0;
+}
+
+static int __init inline vesafb_vbe_getpmi(struct vesafb_task *tsk)
+{
+	int i;
+
+	vesafb_reset_task(tsk);
+	tsk->regs.eax = 0x4f0a;
+	tsk->regs.ebx = 0x0;
+	tsk->flags = TF_CALL;
+	if (vesafb_queue_task(tsk))
+		return -EINVAL;
+	vesafb_wait_for_task(tsk);
+
+	if ((tsk->regs.eax & 0xffff) != 0x004f || tsk->regs.es < 0xc000) {
+		pmi_setpal = ypan = 0;
+	} else {
+		pmi_base  = (u16*)phys_to_virt(((u32)tsk->regs.es << 4) +
+			     tsk->regs.edi);
+		pmi_start = (void*)((char*)pmi_base + pmi_base[1]);
+		pmi_pal   = (void*)((char*)pmi_base + pmi_base[2]);
+		printk(KERN_INFO "vesafb: protected mode interface info at "
+				 "%04x:%04x\n",
+				 (u16)tsk->regs.es, (u16)tsk->regs.edi);
+		printk(KERN_INFO "vesafb: pmi: set display start = %p, "
+				 "set palette = %p\n", pmi_start, pmi_pal);
+
+		if (pmi_base[3]) {
+			printk(KERN_INFO "vesafb: pmi: ports = ");
+			for (i = pmi_base[3]/2; pmi_base[i] != 0xffff; i++)
+				printk("%x ",pmi_base[i]);
+			printk("\n");
+
+			/*
+			 * memory areas not supported (yet?)
+			 *
+			 * Rules are: we have to set up a descriptor for the
+			 * requested memory area and pass it in the ES register
+			 * to the BIOS function.
+			 */
+			if (pmi_base[i] != 0xffff) {
+				printk(KERN_INFO "vesafb: can't handle memory "
+						 "requests, pmi disabled\n");
+				ypan = pmi_setpal = 0;
+			}
+		}
+	}
+	return 0;
+}
+
+static int __init inline vesafb_vbe_getedid(struct vesafb_task *tsk,
+					    struct fb_info *info)
+{
+	int res = 0;
+
+	if (noedid || vbe_ib.vbe_version < 0x0300)
+		return -EINVAL;
+
+	vesafb_reset_task(tsk);
+	tsk->regs.eax = 0x4f15;
+	tsk->regs.ebx = 0;
+	tsk->regs.ecx = 0;
+	if (vesafb_queue_task(tsk))
+		return -EINVAL;
+	vesafb_wait_for_task(tsk);
+
+	if ((tsk->regs.eax & 0xffff) != 0x004f)
+		return -EINVAL;
+
+	if ((tsk->regs.ebx & 0x3) == 3) {
+		printk(KERN_INFO "vesafb: VBIOS/hardware supports both "
+				 "DDC1 and DDC2 transfers\n");
+	} else if ((tsk->regs.ebx & 0x3) == 2) {
+		printk(KERN_INFO "vesafb: VBIOS/hardware supports DDC2 "
+				 "transfers\n");
+	} else if ((tsk->regs.ebx & 0x3) == 1) {
+		printk(KERN_INFO "vesafb: VBIOS/hardware supports DDC1 "
+				 "transfers\n");
+	} else {
+		printk(KERN_INFO "vesafb: VBIOS/hardware doesn't support "
+				 "DDC transfers\n");
+		return -EINVAL;
+	}
+
+	vesafb_reset_task(tsk);
+	tsk->regs.eax = 0x4f15;
+	tsk->regs.ebx = 1;
+	tsk->regs.ecx = tsk->regs.edx = 0;
+	tsk->flags = TF_CALL | TF_RETURN_BUF | TF_BUF_DI;
+	tsk->buf = kmalloc(EDID_LENGTH, GFP_KERNEL);
+	tsk->buf_len = EDID_LENGTH;
+
+	if (vesafb_queue_task(tsk)) {
+		res = -EINVAL;
+		goto out;
+	}
+	vesafb_wait_for_task(tsk);
+
+	if ((tsk->regs.eax & 0xffff) == 0x004f) {
+		fb_edid_to_monspecs(tsk->buf, &info->monspecs);
+		fb_videomode_to_modelist(info->monspecs.modedb,
+				info->monspecs.modedb_len, &info->modelist);
+		if (info->monspecs.vfmax && info->monspecs.hfmax) {
+			/* If the maximum pixel clock wasn't specified in
+			 * the EDID block, set it to 300 MHz. */
+			if (info->monspecs.dclkmax == 0)
+				info->monspecs.dclkmax = 300 * 1000000;
+			info->monspecs.gtf = 1;
+		} else {
+			res = -EINVAL;
+		}
+	}
+
+out:	kfree(tsk->buf);
+	return res;
+}
+
+static void __init inline vesafb_vbe_getmonspecs(struct vesafb_task *tsk,
+		                                 struct fb_info *info)
+{
+	struct fb_var_screeninfo var;
+	int i;
+	memset(&info->monspecs, 0, sizeof(struct fb_monspecs));
+
+	/* If we didn't get all necessary data from the EDID block,
+	 * mark it as incompatible with the GTF. */
+	if (vesafb_vbe_getedid(tsk, info))
+		info->monspecs.gtf = 0;
+
+	/* Kernel command line overrides. */
+	if (maxclk)
+		info->monspecs.dclkmax = maxclk * 1000000;
+	if (maxvf)
+		info->monspecs.vfmax = maxvf;
+	if (maxhf)
+		info->monspecs.hfmax = maxhf * 1000;
+
+	/* In case DDC transfers are not supported the user can provide
+	 * monitor limits manually. Lower limits are set to "safe" values. */
+	if (info->monspecs.gtf == 0 && maxclk && maxvf && maxhf) {
+		info->monspecs.dclkmin = 0;
+		info->monspecs.vfmin = 60;
+		info->monspecs.hfmin = 29000;
+		info->monspecs.gtf = 1;
+	}
+
+	if (info->monspecs.gtf) {
+		printk(KERN_INFO
+		       	"vesafb: monitor limits: vf = %d Hz, hf = %d kHz, "
+			"clk = %d MHz\n", info->monspecs.vfmax,
+			(int)(info->monspecs.hfmax / 1000),
+			(int)(info->monspecs.dclkmax / 1000000));
+		/* Add valid VESA video modes to our modelist. */
+		for (i = 0; i < VESA_MODEDB_SIZE; i++) {
+			fb_videomode_to_var(&var, (struct fb_videomode *)
+					    &vesa_modes[i]);
+			if (!fb_validate_mode(&var, info))
+				fb_add_videomode((struct fb_videomode *)
+						 &vesa_modes[i],
+						 &info->modelist);
+		}
+	} else {
+		/* Add all VESA video modes to our modelist. */
+		fb_videomode_to_modelist((struct fb_videomode *)vesa_modes,
+				 	 VESA_MODEDB_SIZE, &info->modelist);
+		printk(KERN_INFO "vesafb: no monitor limits have been set\n");
+	}
+	return;
+}
+
+static int __init inline vesafb_vbe_init(struct fb_info *info)
+{
+	struct vesafb_task *tsk;
+	int res = 0;
+
+	vesafb_create_task(tsk);
+	if (!tsk)
+		return -EINVAL;
+	if ((res = vesafb_vbe_getinfo(tsk)) != 0)
+		goto out;
+	if ((res = vesafb_vbe_getmodes(tsk)) != 0)
+		goto out;
+	if (pmi_setpal || ypan)
+		vesafb_vbe_getpmi(tsk);
+
+	INIT_LIST_HEAD(&info->modelist);
+	vesafb_vbe_getmonspecs(tsk, info);
+
+out:	kfree(tsk);
+	return res;
+}
+
+static int __init decode_mode(u32 *xres, u32 *yres, u32 *bpp, u32 *refresh)
+{
+	int len = strlen(mode_option), i, err = 0;
+	u8 res_specified = 0, bpp_specified = 0, refresh_specified = 0,
+	   yres_specified = 0;
+
+	for (i = len-1; i >= 0; i--) {
+ 		switch (mode_option[i]) {
+		case '@':
+    			len = i;
+    			if (!refresh_specified && !bpp_specified &&
+			    !yres_specified) {
+				*refresh = simple_strtoul(&mode_option[i+1],
+							  NULL, 0);
+				refresh_specified = 1;
+			} else
+				goto out;
+		    	break;
+		case '-':
+			len = i;
+		    	if (!bpp_specified && !yres_specified) {
+			    	*bpp = simple_strtoul(&mode_option[i+1],
+						      NULL, 0);
+				bpp_specified = 1;
+		    	} else
+				goto out;
+		    	break;
+		case 'x':
+			if (!yres_specified) {
+				*yres = simple_strtoul(&mode_option[i+1],
+						       NULL, 0);
+				yres_specified = 1;
+		    	} else
+				goto out;
+		    	break;
+		case '0'...'9':
+			break;
+		default:
+			goto out;
+	    	}
+	}
+
+	if (i < 0 && yres_specified) {
+		*xres = simple_strtoul(mode_option, NULL, 0);
+	   	res_specified = 1;
+	}
+
+out:	if (!res_specified || !yres_specified) {
+		printk(KERN_ERR "vesafb: invalid resolution, "
+				"%s not specified\n",
+				(!res_specified) ? "width" : "height");
+		err = -EINVAL;
+	}
+
+	return err;
+}
+
+static int __init vesafb_init_set_mode(struct fb_info *info)
+{
+	struct fb_videomode *fbmode;
+	struct fb_videomode mode;
+	int i, modeid, refresh = 0;
+	u8 refresh_specified = 0;
+
+	if (!mode_option)
+		mode_option = CONFIG_FB_VESA_DEFAULT_MODE;
+
+	if (vbemode > 0) {
+		for (i = 0; i < vbe_modes_cnt; i++) {
+			if (vbe_modes[i].mode_id == vbemode) {
+				info->var.vmode = FB_VMODE_NONINTERLACED;
+				info->var.sync = FB_SYNC_VERT_HIGH_ACT;
+				vesafb_setup_var(&info->var, info,
+						 &vbe_modes[i]);
+				fb_get_mode(FB_VSYNCTIMINGS | FB_IGNOREMON,
+					    60, &info->var, info);
+				/* With pixclock set to 0, the default BIOS
+				 * timings will be used in set_par(). */
+				info->var.pixclock = 0;
+				modeid = i;
+				goto out;
+			}
+		}
+		printk(KERN_INFO "specified VBE mode %d not found\n",
+				 vbemode);
+		vbemode = 0;
+	}
+
+	/* Decode the mode specified on the kernel command line. We save
+	 * the depth into bits_per_pixel, which is wrong, but will work
+	 * anyway. */
+	if (decode_mode(&info->var.xres, &info->var.yres,
+			&info->var.bits_per_pixel, &refresh))
+		return -EINVAL;
+	if (refresh)
+		refresh_specified = 1;
+	else
+		refresh = 60;
+
+	/* Look for a matching VBE mode. We can live if an exact match
+	 * cannot be found. */
+	modeid = vesafb_find_vbe_mode(info->var.xres, info->var.yres,
+			              info->var.bits_per_pixel, 0);
+
+	if (modeid == -1) {
+		return -EINVAL;
+	} else {
+		info->var.vmode = FB_VMODE_NONINTERLACED;
+		info->var.sync = FB_SYNC_VERT_HIGH_ACT;
+		vesafb_setup_var(&info->var, info, &vbe_modes[modeid]);
+	}
+	if (vbe_ib.vbe_version < 0x0300) {
+		fb_get_mode(FB_VSYNCTIMINGS | FB_IGNOREMON, 60,
+			    &info->var, info);
+		goto out;
+	}
+	if (!gtf) {
+		struct fb_videomode tmode;
+
+		if (refresh_specified) {
+			fb_var_to_videomode(&tmode, &info->var);
+			tmode.refresh = refresh;
+			fbmode = fb_find_nearest_mode(&tmode, 
+						      &info->modelist);
+		} else
+			fbmode = fb_find_best_mode(&info->var, 
+						   &info->modelist);
+
+		if (fbmode->xres == info->var.xres &&
+		    fbmode->yres == info->var.yres &&
+		    !(fbmode->vmode & (FB_VMODE_INTERLACED | FB_VMODE_DOUBLE))
+		    && (!refresh_specified || 
+		    abs(refresh - fbmode->refresh) <= 5)) {
+			fb_videomode_to_var(&info->var, fbmode);
+			return modeid;
+		}
+	}
+	i = FB_MAXTIMINGS;
+	if (!info->monspecs.gtf)
+		i = FB_IGNOREMON | FB_VSYNCTIMINGS;
+	else if (refresh_specified)
+		i = FB_VSYNCTIMINGS;
+	if (!fb_get_mode(i, refresh, &info->var, info))
+		goto out;
+	if (info->monspecs.gtf &&
+	    !fb_get_mode(FB_MAXTIMINGS, 0, &info->var, info))
+		goto out;
+	/* Use default refresh rate */
+	printk(KERN_WARNING "vesafb: using default BIOS refresh rate\n");
+	info->var.pixclock = 0;
+
+out:
+	fb_var_to_videomode(&mode, &info->var);
+	fb_add_videomode(&mode, &info->modelist);
+	return modeid;
+}
+
+static int __init vesafb_probe(struct platform_device *dev)
+{
+	char entry[16];
+	struct fb_info *info;
+	struct vesafb_mode_ib *mode = NULL;
+	int err = 0, i, h;
+	unsigned int size_vmode;
+	unsigned int size_remap;
+	unsigned int size_total;
+
+	vesafb_info = info = framebuffer_alloc(sizeof(struct vesafb_par) +
+			                       sizeof(u32) * 256, &dev->dev);
+	if (!info)
+	 	return -ENOMEM;
+
+	if (vesafb_wait_for_thread()) {
+		printk(KERN_ERR "vesafb: vesafb thread not running\n");
+		framebuffer_release(info);
+		return -EINVAL;
+	}
+
+	if (vesafb_vbe_init(info)) {
+		printk(KERN_ERR "vesafb: vbe_init failed\n");
+		err = -EINVAL;
+		goto out;
+	}
+
+	vesafb_fix.ypanstep  = ypan     ? 1 : 0;
+	vesafb_fix.ywrapstep = (ypan>1) ? 1 : 0;
+
+	info->pseudo_palette = ((u8*)info->par + sizeof(struct vesafb_par));
+	info->fbops = &vesafb_ops;
+	info->var = vesafb_defined;
+	info->fix = vesafb_fix;
+
+	if (fb_alloc_cmap(&info->cmap, 256, 0) < 0) {
+		err = -ENXIO;
+		goto out;
+	}
+
+	i = vesafb_init_set_mode(info);
+	if (i < 0) {
+		err = -EINVAL;
+		goto out_cmap;
+	} else
+		mode = &vbe_modes[i];
+
+	/* Disable blanking if the user requested so. */
+	if (!blank) {
+		info->fbops->fb_blank = NULL;
+	}
+
+	/* Find out how much IO memory is required for the mode with
+	 * the highest resolution. */
+	size_remap = 0;
+	for (i = 0; i < vbe_modes_cnt; i++) {
+		h = vbe_modes[i].bytes_per_scan_line * vbe_modes[i].y_res;
+		if (h > size_remap)
+			size_remap = h;
+	}
+	size_remap *= 2;
+
+	/*   size_vmode -- that is the amount of memory needed for the
+	 *                 used video mode, i.e. the minimum amount of
+	 *                 memory we need. */
+	if (mode != NULL) {
+		size_vmode = info->var.yres * mode->bytes_per_scan_line;
+	} else {
+		size_vmode = info->var.yres * info->var.xres *
+			     ((info->var.bits_per_pixel + 7) >> 3);
+	}
+
+	/*   size_total -- all video memory we have. Used for mtrr
+	 *                 entries, ressource allocation and bounds
+	 *                 checking. */
+	size_total = vbe_ib.total_memory * 65536;
+	if (vram_total)
+		size_total = vram_total * 1024 * 1024;
+	if (size_total < size_vmode)
+		size_total = size_vmode;
+	((struct vesafb_par*)(info->par))->mem_total = size_total;
+
+	/*   size_remap -- the amount of video memory we are going to
+	 *                 use for vesafb.  With modern cards it is no
+	 *                 option to simply use size_total as th
+	 *                 wastes plenty of kernel address space. */
+	if (vram_remap)
+		size_remap = vram_remap * 1024 * 1024;
+	if (size_remap < size_vmode)
+		size_remap = size_vmode;
+	if (size_remap > size_total)
+		size_remap = size_total;
+
+	info->fix.smem_len = size_remap;
+	info->fix.smem_start = mode->phys_base_ptr;
+
+	/* We have to set it here, because when setup_var() was called,
+	 * smem_len wasn't defined yet. */
+	info->var.yres_virtual = info->fix.smem_len /
+				 mode->bytes_per_scan_line;
+
+	if (ypan && info->var.yres_virtual > info->var.yres) {
+		printk(KERN_INFO "vesafb: scrolling: %s "
+		       "using protected mode interface, "
+		       "yres_virtual=%d\n",
+		       (ypan > 1) ? "ywrap" : "ypan",info->var.yres_virtual);
+	} else {
+		printk(KERN_INFO "vesafb: scrolling: redraw\n");
+		info->var.yres_virtual = info->var.yres;
+		ypan = 0;
+	}
+
+	info->flags = FBINFO_FLAG_DEFAULT |
+		(ypan) ? FBINFO_HWACCEL_YPAN : 0;
+
+	if (!ypan)
+		info->fbops->fb_pan_display = NULL;
+
+	if (!request_mem_region(info->fix.smem_start, size_total, "vesafb")) {
+		printk(KERN_WARNING "vesafb: cannot reserve video memory at "
+		       "0x%lx\n", info->fix.smem_start);
+		/* We cannot make this fatal. Sometimes this comes from magic
+		   spaces our resource handlers simply don't know about. */
+	}
+
+	info->screen_base = ioremap(info->fix.smem_start, info->fix.smem_len);
+
+	if (!info->screen_base) {
+		printk(KERN_ERR
+		       "vesafb: abort, cannot ioremap video memory "
+		       "0x%x @ 0x%lx\n",
+		       info->fix.smem_len, info->fix.smem_start);
+		err = -EIO;
+		goto out_mem;
+ 	}
+
+	/* Request failure does not faze us, as vgacon probably has this
+	   region already (FIXME) */
+	request_region(0x3c0, 32, "vesafb");
+
+#ifdef CONFIG_MTRR
+	if (mtrr && !(info->fix.smem_start & (PAGE_SIZE - 1))) {
+		int temp_size = size_total;
+		unsigned int type = 0;
+
+		switch (mtrr) {
+		case 1:
+			type = MTRR_TYPE_UNCACHABLE;
+			break;
+		case 2:
+			type = MTRR_TYPE_WRBACK;
+			break;
+		case 3:
+			type = MTRR_TYPE_WRCOMB;
+			break;
+		case 4:
+			type = MTRR_TYPE_WRTHROUGH;
+			break;
+		default:
+			type = 0;
+			break;
+		}
+
+		if (type) {
+			int rc;
+
+			/* Find the largest power-of-two */
+			while (temp_size & (temp_size - 1))
+				temp_size &= (temp_size - 1);
+
+			/* Try and find a power of two to add */
+			do {
+				rc = mtrr_add(info->fix.smem_start,
+					      temp_size, type, 1);
+				temp_size >>= 1;
+			} while (temp_size >= PAGE_SIZE && rc == -EINVAL);
+  		}
+  	}
+#endif /* CONFIG_MTRR */
+
+	if (register_framebuffer(info) < 0) {
+		printk(KERN_ERR
+		       "vesafb: failed to register framebuffer device\n");
+		err = -EINVAL;
+		goto out_mem;
+	}
+
+  	printk(KERN_INFO "vesafb: framebuffer at 0x%lx, mapped to 0x%p, "
+	       "using %dk, total %dk\n", info->fix.smem_start,
+	       info->screen_base, size_remap/1024, size_total/1024);
+	printk(KERN_INFO "fb%d: %s frame buffer device\n", info->node,
+	       info->fix.id);
+
+	sprintf(entry, "fb%d", info->node);
+	proc_mkdir(entry, 0);
+
+	sprintf(entry, "fb%d/modes", info->node);
+	create_proc_read_entry(entry, 0, 0, vesafb_read_proc_modes, NULL);
+
+	sprintf(entry, "fb%d/vbe_info", info->node);
+	create_proc_read_entry(entry, 0, 0, vesafb_read_proc_vbe_info, NULL);
+	return 0;
+
+out_mem:
+	release_mem_region(info->fix.smem_start, size_total);
+	if (!list_empty(&info->modelist))
+		fb_destroy_modelist(&info->modelist);
+	fb_destroy_modedb(info->monspecs.modedb);
+out_cmap:
+	fb_dealloc_cmap(&info->cmap);
+out:
+	framebuffer_release(info);
+	vesafb_info = NULL;
+	kfree(vbe_modes);
+	vbe_modes = NULL;
+	return err;
+}
+
+int __init vesafb_init(void)
+{
+	int ret;
+#ifndef MODULE
+	char *option = NULL;
+
+	if (fb_get_options("vesafb", &option))
+		return -ENODEV;
+	vesafb_setup(option);
+#endif
+	ret = platform_driver_register(&vesafb_driver);
+
+	if (!ret) {
+		vesafb_device = platform_device_alloc("vesafb", 0);
+
+		if (vesafb_device)
+			ret = platform_device_add(vesafb_device);
+		else
+			ret = -ENOMEM;
+
+		if (ret) {
+			platform_device_put(vesafb_device);
+			platform_driver_unregister(&vesafb_driver);
+		}
+	}
+	return ret;
+}
+
+module_init(vesafb_init);
+
+#ifdef MODULE
+void __exit vesafb_exit(void)
+{
+	char entry[16];
+
+	if (vesafb_info)
+		unregister_framebuffer(vesafb_info);
+
+	platform_device_unregister(vesafb_device);
+	platform_driver_unregister(&vesafb_driver);
+
+	if (vesafb_info) {
+		struct vesafb_par *par = (struct vesafb_par*)vesafb_info->par;
+
+		sprintf(entry, "fb%d/modes", vesafb_info->node);
+		remove_proc_entry(entry, NULL);
+
+		sprintf(entry, "fb%d/vbe_info", vesafb_info->node);
+		remove_proc_entry(entry, NULL);
+
+		sprintf(entry, "fb%d", vesafb_info->node);
+		remove_proc_entry(entry, NULL);
+
+		iounmap(vesafb_info->screen_base);
+		release_mem_region(vesafb_info->fix.smem_start,
+				   par->mem_total);
+		fb_dealloc_cmap(&vesafb_info->cmap);
+		if (!list_empty(&vesafb_info->modelist))
+			fb_destroy_modelist(&vesafb_info->modelist);
+		fb_destroy_modedb(vesafb_info->monspecs.modedb);
+		framebuffer_release(vesafb_info);
+	}
+
+	if (vbe_modes != NULL)
+		kfree(vbe_modes);
+}
+
+module_exit(vesafb_exit);
+
+static inline int param_get_scroll(char *buffer, struct kernel_param *kp)
+{
+	return 0;
+}
+static inline int param_set_scroll(const char *val, struct kernel_param *kp)
+{
+	ypan = 0;
+
+	if (! strcmp(val, "redraw"))
+		ypan = 0;
+	else if (! strcmp(val, "ypan"))
+		ypan = 1;
+	else if (! strcmp(val, "ywrap"))
+		ypan = 2;
+
+	return 0;
+}
+
+#define param_check_scroll(name, p) __param_check(name, p, void);
+
+module_param_named(scroll, ypan, scroll, 0);
+MODULE_PARM_DESC(scroll,"Scrolling mode, set to 'redraw', 'ypan' or 'ywrap'");
+module_param_named(vgapal, pmi_setpal, invbool, 0);
+MODULE_PARM_DESC(vgapal,"bool: set palette using VGA registers");
+module_param_named(pmipal, pmi_setpal, bool, 0);
+MODULE_PARM_DESC(pmipal,"bool: set palette using PMI calls");
+module_param_named(nomtrr, mtrr, invbool, 0);
+MODULE_PARM_DESC(nomtrr,"bool: disable use of MTRR registers");
+module_param(blank, bool, 1);
+MODULE_PARM_DESC(blank,"bool: enable hardware blanking");
+module_param(nocrtc, bool, 0);
+MODULE_PARM_DESC(nocrtc,"bool: ignore CRTC timings when setting modes");
+module_param(noedid, bool, 0);
+MODULE_PARM_DESC(noedid,"bool: ignore EDID-provided monitor limits "
+		        "when setting modes");
+module_param(gtf, bool, 0);
+MODULE_PARM_DESC(gtf,"bool: force use of VESA GTF to calculate mode timings");
+module_param(vram_remap, uint, 0);
+MODULE_PARM_DESC(vram_remap,"Set amount of video memory to be used [MiB]");
+module_param(vram_total, uint, 0);
+MODULE_PARM_DESC(vram_total,"Set total amount of video memoery [MiB]");
+module_param(maxclk, ushort, 0);
+MODULE_PARM_DESC(maxclk,"Maximum pixelclock [MHz], overrides EDID data");
+module_param(maxhf, ushort, 0);
+MODULE_PARM_DESC(maxhf,"Maximum horizontal frequency [kHz], "
+		       "overrides EDID data");
+module_param(maxvf, ushort, 0);
+MODULE_PARM_DESC(maxvf,"Maximum vertical frequency [Hz], "
+		       "overrides EDID data");
+module_param_named(mode, mode_option, charp, 0);
+MODULE_PARM_DESC(mode, "Specify resolution as "
+		       "\"<xres>x<yres>[-<bpp>][@<refresh>]\"");
+module_param(vbemode, ushort, 0);
+MODULE_PARM_DESC(vbemode,"VBE mode number to set, overrides 'mode' setting");
+
+#endif /* MODULE */
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Michal Januszewski");
+MODULE_DESCRIPTION("Framebuffer driver for VBE2.0+ compliant graphics boards");
+
diff -urN oldtree/fs/Kconfig newtree/fs/Kconfig
--- oldtree/fs/Kconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/fs/Kconfig	2006-07-12 19:01:51.000000000 -0400
@@ -1262,6 +1262,71 @@
 
 	  If unsure, say N.
 
+config SQUASHFS
+	tristate "SquashFS 3.0 - Squashed file system support"
+	select ZLIB_INFLATE
+	help
+	  Saying Y here includes support for SquashFS 3.0 (a Compressed Read-Only File
+	  System).  Squashfs is a highly compressed read-only filesystem for Linux.
+	  It uses zlib compression to compress both files, inodes and directories.
+	  Inodes in the system are very small and all blocks are packed to minimise
+	  data overhead. Block sizes greater than 4K are supported up to a maximum of 64K.
+	  SquashFS 3.0 supports 64 bit filesystems and files (larger than 4GB), full
+	  uid/gid information, hard links and timestamps.
+
+	  Squashfs is intended for general read-only filesystem use, for archival
+	  use (i.e. in cases where a .tar.gz file may be used), and in embedded
+	  systems where low overhead is needed.  Further information and filesystem tools
+	  are available from http://squashfs.sourceforge.net.
+
+	  If you want to compile this as a module ( = code which can be
+	  inserted in and removed from the running kernel whenever you want),
+	  say M here and read <file:Documentation/modules.txt>.  The module
+	  will be called squashfs.  Note that the root file system (the one
+	  containing the directory /) cannot be compiled as a module.
+
+	  If unsure, say N.
+
+config SQUASHFS_EMBEDDED
+
+	bool "Additional options for memory-constrained systems" 
+	depends on SQUASHFS
+	default n
+	help
+	  Saying Y here allows you to specify cache sizes and how Squashfs
+	  allocates memory.  This is only intended for memory constrained
+	  systems.
+
+	  If unsure, say N.
+
+config SQUASHFS_FRAGMENT_CACHE_SIZE
+	int "Number of fragments cached" if SQUASHFS_EMBEDDED
+	depends on SQUASHFS
+	default "3"
+	help
+	  By default SquashFS caches the last 3 fragments read from
+	  the filesystem.  Increasing this amount may mean SquashFS
+	  has to re-read fragments less often from disk, at the expense
+	  of extra system memory.  Decreasing this amount will mean
+	  SquashFS uses less memory at the expense of extra reads from disk.
+
+	  Note there must be at least one cached fragment.  Anything
+	  much more than three will probably not make much difference.
+
+config SQUASHFS_VMALLOC
+	bool "Use Vmalloc rather than Kmalloc" if SQUASHFS_EMBEDDED
+	depends on SQUASHFS
+	default n
+	help
+	  By default SquashFS uses kmalloc to obtain fragment cache memory.
+	  Kmalloc memory is the standard kernel allocator, but it can fail
+	  on memory constrained systems.  Because of the way Vmalloc works,
+	  Vmalloc can succeed when kmalloc fails.  Specifying this option
+	  will make SquashFS always use Vmalloc to allocate the
+	  fragment cache memory.
+
+	  If unsure, say N.
+
 config VXFS_FS
 	tristate "FreeVxFS file system support (VERITAS VxFS(TM) compatible)"
 	help
@@ -1407,6 +1472,16 @@
 	  Y here.  This will result in _many_ additional debugging messages to be
 	  written to the system log.
 
+config UNION_FS
+	tristate "Union fs support"
+	depends on EXPERIMENTAL
+	help
+	  Unionfs is a stackable unification file system, which can
+	  appear to merge the contents of several directories (branches),
+	  while keeping their physical content separate.
+
+	  see <http://www.fsl.cs.sunysb.edu/project-unionfs.html> for details
+
 endmenu
 
 menu "Network File Systems"
diff -urN oldtree/fs/Makefile newtree/fs/Makefile
--- oldtree/fs/Makefile	2006-07-05 10:06:57.000000000 -0400
+++ newtree/fs/Makefile	2006-07-12 19:01:51.000000000 -0400
@@ -59,6 +59,7 @@
 obj-$(CONFIG_JBD)		+= jbd/
 obj-$(CONFIG_EXT2_FS)		+= ext2/
 obj-$(CONFIG_CRAMFS)		+= cramfs/
+obj-$(CONFIG_SQUASHFS)		+= squashfs/
 obj-$(CONFIG_RAMFS)		+= ramfs/
 obj-$(CONFIG_HUGETLBFS)		+= hugetlbfs/
 obj-$(CONFIG_CODA_FS)		+= coda/
@@ -106,3 +107,4 @@
 obj-$(CONFIG_DEBUG_FS)		+= debugfs/
 obj-$(CONFIG_OCFS2_FS)		+= ocfs2/
 obj-$(CONFIG_GFS2_FS)           += gfs2/
+obj-$(CONFIG_UNION_FS)          += unionfs/
diff -urN oldtree/fs/proc/array.c newtree/fs/proc/array.c
--- oldtree/fs/proc/array.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/fs/proc/array.c	2006-07-12 19:00:11.000000000 -0400
@@ -165,7 +165,12 @@
 	read_lock(&tasklist_lock);
 	buffer += sprintf(buffer,
 		"State:\t%s\n"
+#ifdef CONFIG_INGOSCHED
 		"SleepAVG:\t%lu%%\n"
+#endif
+#ifdef CONFIG_STAIRCASE
+                "Bonus:\t%d\n"
+#endif
 		"Tgid:\t%d\n"
 		"Pid:\t%d\n"
 		"PPid:\t%d\n"
@@ -173,7 +178,12 @@
 		"Uid:\t%d\t%d\t%d\t%d\n"
 		"Gid:\t%d\t%d\t%d\t%d\n",
 		get_task_state(p),
+#ifdef CONFIG_INGOSCHED
 		(p->sleep_avg/1024)*100/(1020000000/1024),
+#endif
+#ifdef CONFIG_STAIRCASE
+                p->bonus,
+#endif
 	       	p->tgid,
 		p->pid, pid_alive(p) ? p->group_leader->real_parent->tgid : 0,
 		pid_alive(p) && p->ptrace ? p->parent->pid : 0,
diff -urN oldtree/fs/squashfs/Makefile newtree/fs/squashfs/Makefile
--- oldtree/fs/squashfs/Makefile	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/squashfs/Makefile	2006-07-12 19:01:48.000000000 -0400
@@ -0,0 +1,7 @@
+#
+# Makefile for the linux squashfs routines.
+#
+
+obj-$(CONFIG_SQUASHFS) += squashfs.o
+squashfs-y += inode.o
+squashfs-y += squashfs2_0.o
diff -urN oldtree/fs/squashfs/inode.c newtree/fs/squashfs/inode.c
--- oldtree/fs/squashfs/inode.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/squashfs/inode.c	2006-07-12 19:01:48.000000000 -0400
@@ -0,0 +1,2126 @@
+/*
+ * Squashfs - a compressed read only filesystem for Linux
+ *
+ * Copyright (c) 2002, 2003, 2004, 2005, 2006
+ * Phillip Lougher <phillip@lougher.org.uk>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2,
+ * or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * inode.c
+ */
+
+#include <linux/types.h>
+#include <linux/squashfs_fs.h>
+#include <linux/module.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/smp_lock.h>
+#include <linux/slab.h>
+#include <linux/squashfs_fs_sb.h>
+#include <linux/squashfs_fs_i.h>
+#include <linux/buffer_head.h>
+#include <linux/vfs.h>
+#include <linux/init.h>
+#include <linux/dcache.h>
+#include <linux/wait.h>
+#include <linux/zlib.h>
+#include <linux/blkdev.h>
+#include <linux/vmalloc.h>
+#include <asm/uaccess.h>
+#include <asm/semaphore.h>
+
+#include "squashfs.h"
+
+static void squashfs_put_super(struct super_block *);
+static int squashfs_statfs(struct dentry *dentry, struct kstatfs *);
+static int squashfs_symlink_readpage(struct file *file, struct page *page);
+static int squashfs_readpage(struct file *file, struct page *page);
+static int squashfs_readpage4K(struct file *file, struct page *page);
+static int squashfs_readdir(struct file *, void *, filldir_t);
+static struct inode *squashfs_alloc_inode(struct super_block *sb);
+static void squashfs_destroy_inode(struct inode *inode);
+static int init_inodecache(void);
+static void destroy_inodecache(void);
+static struct dentry *squashfs_lookup(struct inode *, struct dentry *,
+				struct nameidata *);
+static struct inode *squashfs_iget(struct super_block *s, squashfs_inode_t inode);
+static long long read_blocklist(struct inode *inode, int index,
+				int readahead_blks, char *block_list,
+				unsigned short **block_p, unsigned int *bsize);
+static int squashfs_get_sb(struct file_system_type *, int,
+				const char *, void *, struct vfsmount *);
+
+
+static z_stream stream;
+
+static struct file_system_type squashfs_fs_type = {
+	.owner = THIS_MODULE,
+	.name = "squashfs",
+	.get_sb = squashfs_get_sb,
+	.kill_sb = kill_block_super,
+	.fs_flags = FS_REQUIRES_DEV
+};
+
+static unsigned char squashfs_filetype_table[] = {
+	DT_UNKNOWN, DT_DIR, DT_REG, DT_LNK, DT_BLK, DT_CHR, DT_FIFO, DT_SOCK
+};
+
+static struct super_operations squashfs_ops = {
+	.alloc_inode = squashfs_alloc_inode,
+	.destroy_inode = squashfs_destroy_inode,
+	.statfs = squashfs_statfs,
+	.put_super = squashfs_put_super,
+};
+
+SQSH_EXTERN struct address_space_operations squashfs_symlink_aops = {
+	.readpage = squashfs_symlink_readpage
+};
+
+SQSH_EXTERN struct address_space_operations squashfs_aops = {
+	.readpage = squashfs_readpage
+};
+
+SQSH_EXTERN struct address_space_operations squashfs_aops_4K = {
+	.readpage = squashfs_readpage4K
+};
+
+static struct file_operations squashfs_dir_ops = {
+	.read = generic_read_dir,
+	.readdir = squashfs_readdir
+};
+
+SQSH_EXTERN struct inode_operations squashfs_dir_inode_ops = {
+	.lookup = squashfs_lookup
+};
+
+
+static struct buffer_head *get_block_length(struct super_block *s,
+				int *cur_index, int *offset, int *c_byte)
+{
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	unsigned short temp;
+	struct buffer_head *bh;
+
+	if (!(bh = sb_bread(s, *cur_index)))
+		goto out;
+
+	if (msblk->devblksize - *offset == 1) {
+		if (msblk->swap)
+			((unsigned char *) &temp)[1] = *((unsigned char *)
+				(bh->b_data + *offset));
+		else
+			((unsigned char *) &temp)[0] = *((unsigned char *)
+				(bh->b_data + *offset));
+		brelse(bh);
+		if (!(bh = sb_bread(s, ++(*cur_index))))
+			goto out;
+		if (msblk->swap)
+			((unsigned char *) &temp)[0] = *((unsigned char *)
+				bh->b_data); 
+		else
+			((unsigned char *) &temp)[1] = *((unsigned char *)
+				bh->b_data); 
+		*c_byte = temp;
+		*offset = 1;
+	} else {
+		if (msblk->swap) {
+			((unsigned char *) &temp)[1] = *((unsigned char *)
+				(bh->b_data + *offset));
+			((unsigned char *) &temp)[0] = *((unsigned char *)
+				(bh->b_data + *offset + 1)); 
+		} else {
+			((unsigned char *) &temp)[0] = *((unsigned char *)
+				(bh->b_data + *offset));
+			((unsigned char *) &temp)[1] = *((unsigned char *)
+				(bh->b_data + *offset + 1)); 
+		}
+		*c_byte = temp;
+		*offset += 2;
+	}
+
+	if (SQUASHFS_CHECK_DATA(msblk->sblk.flags)) {
+		if (*offset == msblk->devblksize) {
+			brelse(bh);
+			if (!(bh = sb_bread(s, ++(*cur_index))))
+				goto out;
+			*offset = 0;
+		}
+		if (*((unsigned char *) (bh->b_data + *offset)) !=
+						SQUASHFS_MARKER_BYTE) {
+			ERROR("Metadata block marker corrupt @ %x\n",
+						*cur_index);
+			brelse(bh);
+			goto out;
+		}
+		(*offset)++;
+	}
+	return bh;
+
+out:
+	return NULL;
+}
+
+
+SQSH_EXTERN unsigned int squashfs_read_data(struct super_block *s, char *buffer,
+			long long index, unsigned int length,
+			long long *next_index)
+{
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	struct buffer_head *bh[((SQUASHFS_FILE_MAX_SIZE - 1) >>
+			msblk->devblksize_log2) + 2];
+	unsigned int offset = index & ((1 << msblk->devblksize_log2) - 1);
+	unsigned int cur_index = index >> msblk->devblksize_log2;
+	int bytes, avail_bytes, b = 0, k;
+	char *c_buffer;
+	unsigned int compressed;
+	unsigned int c_byte = length;
+
+	if (c_byte) {
+		bytes = msblk->devblksize - offset;
+		compressed = SQUASHFS_COMPRESSED_BLOCK(c_byte);
+		c_buffer = compressed ? msblk->read_data : buffer;
+		c_byte = SQUASHFS_COMPRESSED_SIZE_BLOCK(c_byte);
+
+		TRACE("Block @ 0x%llx, %scompressed size %d\n", index, compressed
+					? "" : "un", (unsigned int) c_byte);
+
+		if (!(bh[0] = sb_getblk(s, cur_index)))
+			goto block_release;
+
+		for (b = 1; bytes < c_byte; b++) {
+			if (!(bh[b] = sb_getblk(s, ++cur_index)))
+				goto block_release;
+			bytes += msblk->devblksize;
+		}
+		ll_rw_block(READ, b, bh);
+	} else {
+		if (!(bh[0] = get_block_length(s, &cur_index, &offset,
+								&c_byte)))
+			goto read_failure;
+
+		bytes = msblk->devblksize - offset;
+		compressed = SQUASHFS_COMPRESSED(c_byte);
+		c_buffer = compressed ? msblk->read_data : buffer;
+		c_byte = SQUASHFS_COMPRESSED_SIZE(c_byte);
+
+		TRACE("Block @ 0x%llx, %scompressed size %d\n", index, compressed
+					? "" : "un", (unsigned int) c_byte);
+
+		for (b = 1; bytes < c_byte; b++) {
+			if (!(bh[b] = sb_getblk(s, ++cur_index)))
+				goto block_release;
+			bytes += msblk->devblksize;
+		}
+		ll_rw_block(READ, b - 1, bh + 1);
+	}
+
+	if (compressed)
+		down(&msblk->read_data_mutex);
+
+	for (bytes = 0, k = 0; k < b; k++) {
+		avail_bytes = (c_byte - bytes) > (msblk->devblksize - offset) ?
+					msblk->devblksize - offset :
+					c_byte - bytes;
+		wait_on_buffer(bh[k]);
+		if (!buffer_uptodate(bh[k]))
+			goto block_release;
+		memcpy(c_buffer + bytes, bh[k]->b_data + offset, avail_bytes);
+		bytes += avail_bytes;
+		offset = 0;
+		brelse(bh[k]);
+	}
+
+	/*
+	 * uncompress block
+	 */
+	if (compressed) {
+		int zlib_err;
+
+		stream.next_in = c_buffer;
+		stream.avail_in = c_byte;
+		stream.next_out = buffer;
+		stream.avail_out = msblk->read_size;
+
+		if (((zlib_err = zlib_inflateInit(&stream)) != Z_OK) ||
+				((zlib_err = zlib_inflate(&stream, Z_FINISH))
+				 != Z_STREAM_END) || ((zlib_err =
+				zlib_inflateEnd(&stream)) != Z_OK)) {
+			ERROR("zlib_fs returned unexpected result 0x%x\n",
+				zlib_err);
+			bytes = 0;
+		} else
+			bytes = stream.total_out;
+		
+		up(&msblk->read_data_mutex);
+	}
+
+	if (next_index)
+		*next_index = index + c_byte + (length ? 0 :
+				(SQUASHFS_CHECK_DATA(msblk->sblk.flags)
+				 ? 3 : 2));
+	return bytes;
+
+block_release:
+	while (--b >= 0)
+		brelse(bh[b]);
+
+read_failure:
+	ERROR("sb_bread failed reading block 0x%x\n", cur_index);
+	return 0;
+}
+
+
+SQSH_EXTERN int squashfs_get_cached_block(struct super_block *s, char *buffer,
+				long long block, unsigned int offset,
+				int length, long long *next_block,
+				unsigned int *next_offset)
+{
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	int n, i, bytes, return_length = length;
+	long long next_index;
+
+	TRACE("Entered squashfs_get_cached_block [%llx:%x]\n", block, offset);
+
+	while ( 1 ) {
+		for (i = 0; i < SQUASHFS_CACHED_BLKS; i++) 
+			if (msblk->block_cache[i].block == block)
+				break; 
+		
+		down(&msblk->block_cache_mutex);
+
+		if (i == SQUASHFS_CACHED_BLKS) {
+			/* read inode header block */
+			for (i = msblk->next_cache, n = SQUASHFS_CACHED_BLKS;
+					n ; n --, i = (i + 1) %
+					SQUASHFS_CACHED_BLKS)
+				if (msblk->block_cache[i].block !=
+							SQUASHFS_USED_BLK)
+					break;
+
+			if (n == 0) {
+				wait_queue_t wait;
+
+				init_waitqueue_entry(&wait, current);
+				add_wait_queue(&msblk->waitq, &wait);
+				set_current_state(TASK_UNINTERRUPTIBLE);
+ 				up(&msblk->block_cache_mutex);
+				schedule();
+				set_current_state(TASK_RUNNING);
+				remove_wait_queue(&msblk->waitq, &wait);
+				continue;
+			}
+			msblk->next_cache = (i + 1) % SQUASHFS_CACHED_BLKS;
+
+			if (msblk->block_cache[i].block ==
+							SQUASHFS_INVALID_BLK) {
+				if (!(msblk->block_cache[i].data =
+						kmalloc(SQUASHFS_METADATA_SIZE,
+						GFP_KERNEL))) {
+					ERROR("Failed to allocate cache"
+							"block\n");
+					up(&msblk->block_cache_mutex);
+					goto out;
+				}
+			}
+	
+			msblk->block_cache[i].block = SQUASHFS_USED_BLK;
+			up(&msblk->block_cache_mutex);
+
+			if (!(msblk->block_cache[i].length =
+						squashfs_read_data(s,
+						msblk->block_cache[i].data,
+						block, 0, &next_index))) {
+				ERROR("Unable to read cache block [%llx:%x]\n",
+						block, offset);
+				goto out;
+			}
+
+			down(&msblk->block_cache_mutex);
+			wake_up(&msblk->waitq);
+			msblk->block_cache[i].block = block;
+			msblk->block_cache[i].next_index = next_index;
+			TRACE("Read cache block [%llx:%x]\n", block, offset);
+		}
+
+		if (msblk->block_cache[i].block != block) {
+			up(&msblk->block_cache_mutex);
+			continue;
+		}
+
+		if ((bytes = msblk->block_cache[i].length - offset) >= length) {
+			if (buffer)
+				memcpy(buffer, msblk->block_cache[i].data +
+						offset, length);
+			if (msblk->block_cache[i].length - offset == length) {
+				*next_block = msblk->block_cache[i].next_index;
+				*next_offset = 0;
+			} else {
+				*next_block = block;
+				*next_offset = offset + length;
+			}
+			up(&msblk->block_cache_mutex);
+			goto finish;
+		} else {
+			if (buffer) {
+				memcpy(buffer, msblk->block_cache[i].data +
+						offset, bytes);
+				buffer += bytes;
+			}
+			block = msblk->block_cache[i].next_index;
+			up(&msblk->block_cache_mutex);
+			length -= bytes;
+			offset = 0;
+		}
+	}
+
+finish:
+	return return_length;
+out:
+	return 0;
+}
+
+
+static int get_fragment_location(struct super_block *s, unsigned int fragment,
+				long long *fragment_start_block,
+				unsigned int *fragment_size)
+{
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	long long start_block =
+		msblk->fragment_index[SQUASHFS_FRAGMENT_INDEX(fragment)];
+	int offset = SQUASHFS_FRAGMENT_INDEX_OFFSET(fragment);
+	struct squashfs_fragment_entry fragment_entry;
+
+	if (msblk->swap) {
+		struct squashfs_fragment_entry sfragment_entry;
+
+		if (!squashfs_get_cached_block(s, (char *) &sfragment_entry,
+					start_block, offset,
+					sizeof(sfragment_entry), &start_block,
+					&offset))
+			goto out;
+		SQUASHFS_SWAP_FRAGMENT_ENTRY(&fragment_entry, &sfragment_entry);
+	} else
+		if (!squashfs_get_cached_block(s, (char *) &fragment_entry,
+					start_block, offset,
+					sizeof(fragment_entry), &start_block,
+					&offset))
+			goto out;
+
+	*fragment_start_block = fragment_entry.start_block;
+	*fragment_size = fragment_entry.size;
+
+	return 1;
+
+out:
+	return 0;
+}
+
+
+SQSH_EXTERN void release_cached_fragment(struct squashfs_sb_info *msblk, struct
+					squashfs_fragment_cache *fragment)
+{
+	down(&msblk->fragment_mutex);
+	fragment->locked --;
+	wake_up(&msblk->fragment_wait_queue);
+	up(&msblk->fragment_mutex);
+}
+
+
+SQSH_EXTERN struct squashfs_fragment_cache *get_cached_fragment(struct super_block
+					*s, long long start_block,
+					int length)
+{
+	int i, n;
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+
+	while ( 1 ) {
+		down(&msblk->fragment_mutex);
+
+		for (i = 0; i < SQUASHFS_CACHED_FRAGMENTS &&
+				msblk->fragment[i].block != start_block; i++);
+
+		if (i == SQUASHFS_CACHED_FRAGMENTS) {
+			for (i = msblk->next_fragment, n =
+				SQUASHFS_CACHED_FRAGMENTS; n &&
+				msblk->fragment[i].locked; n--, i = (i + 1) %
+				SQUASHFS_CACHED_FRAGMENTS);
+
+			if (n == 0) {
+				wait_queue_t wait;
+
+				init_waitqueue_entry(&wait, current);
+				add_wait_queue(&msblk->fragment_wait_queue,
+									&wait);
+				set_current_state(TASK_UNINTERRUPTIBLE);
+				up(&msblk->fragment_mutex);
+				schedule();
+				set_current_state(TASK_RUNNING);
+				remove_wait_queue(&msblk->fragment_wait_queue,
+									&wait);
+				continue;
+			}
+			msblk->next_fragment = (msblk->next_fragment + 1) %
+				SQUASHFS_CACHED_FRAGMENTS;
+			
+			if (msblk->fragment[i].data == NULL)
+				if (!(msblk->fragment[i].data = SQUASHFS_ALLOC
+						(SQUASHFS_FILE_MAX_SIZE))) {
+					ERROR("Failed to allocate fragment "
+							"cache block\n");
+					up(&msblk->fragment_mutex);
+					goto out;
+				}
+
+			msblk->fragment[i].block = SQUASHFS_INVALID_BLK;
+			msblk->fragment[i].locked = 1;
+			up(&msblk->fragment_mutex);
+
+			if (!(msblk->fragment[i].length = squashfs_read_data(s,
+						msblk->fragment[i].data,
+						start_block, length, NULL))) {
+				ERROR("Unable to read fragment cache block "
+							"[%llx]\n", start_block);
+				msblk->fragment[i].locked = 0;
+				goto out;
+			}
+
+			msblk->fragment[i].block = start_block;
+			TRACE("New fragment %d, start block %lld, locked %d\n",
+						i, msblk->fragment[i].block,
+						msblk->fragment[i].locked);
+			break;
+		}
+
+		msblk->fragment[i].locked++;
+		up(&msblk->fragment_mutex);
+		TRACE("Got fragment %d, start block %lld, locked %d\n", i,
+						msblk->fragment[i].block,
+						msblk->fragment[i].locked);
+		break;
+	}
+
+	return &msblk->fragment[i];
+
+out:
+	return NULL;
+}
+
+
+static struct inode *squashfs_new_inode(struct super_block *s,
+		struct squashfs_base_inode_header *inodeb)
+{
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	struct inode *i = new_inode(s);
+
+	if (i) {
+		i->i_ino = inodeb->inode_number;
+		i->i_mtime.tv_sec = inodeb->mtime;
+		i->i_atime.tv_sec = inodeb->mtime;
+		i->i_ctime.tv_sec = inodeb->mtime;
+		i->i_uid = msblk->uid[inodeb->uid];
+		i->i_mode = inodeb->mode;
+		i->i_size = 0;
+		if (inodeb->guid == SQUASHFS_GUIDS)
+			i->i_gid = i->i_uid;
+		else
+			i->i_gid = msblk->guid[inodeb->guid];
+	}
+
+	return i;
+}
+
+
+static struct inode *squashfs_iget(struct super_block *s, squashfs_inode_t inode)
+{
+	struct inode *i;
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+	long long block = SQUASHFS_INODE_BLK(inode) +
+		sblk->inode_table_start;
+	unsigned int offset = SQUASHFS_INODE_OFFSET(inode);
+	long long next_block;
+	unsigned int next_offset;
+	union squashfs_inode_header id, sid;
+	struct squashfs_base_inode_header *inodeb = &id.base,
+					  *sinodeb = &sid.base;
+
+	TRACE("Entered squashfs_iget\n");
+
+	if (msblk->swap) {
+		if (!squashfs_get_cached_block(s, (char *) sinodeb, block,
+					offset, sizeof(*sinodeb), &next_block,
+					&next_offset))
+			goto failed_read;
+		SQUASHFS_SWAP_BASE_INODE_HEADER(inodeb, sinodeb,
+					sizeof(*sinodeb));
+	} else
+		if (!squashfs_get_cached_block(s, (char *) inodeb, block,
+					offset, sizeof(*inodeb), &next_block,
+					&next_offset))
+			goto failed_read;
+
+	switch(inodeb->inode_type) {
+		case SQUASHFS_FILE_TYPE: {
+			unsigned int frag_size;
+			long long frag_blk;
+			struct squashfs_reg_inode_header *inodep = &id.reg;
+			struct squashfs_reg_inode_header *sinodep = &sid.reg;
+				
+			if (msblk->swap) {
+				if (!squashfs_get_cached_block(s, (char *)
+						sinodep, block, offset,
+						sizeof(*sinodep), &next_block,
+						&next_offset))
+					goto failed_read;
+				SQUASHFS_SWAP_REG_INODE_HEADER(inodep, sinodep);
+			} else
+				if (!squashfs_get_cached_block(s, (char *)
+						inodep, block, offset,
+						sizeof(*inodep), &next_block,
+						&next_offset))
+					goto failed_read;
+
+			frag_blk = SQUASHFS_INVALID_BLK;
+			if (inodep->fragment != SQUASHFS_INVALID_FRAG &&
+					!get_fragment_location(s,
+					inodep->fragment, &frag_blk, &frag_size))
+				goto failed_read;
+				
+			if((i = squashfs_new_inode(s, inodeb)) == NULL)
+				goto failed_read1;
+
+			i->i_nlink = 1;
+			i->i_size = inodep->file_size;
+			i->i_fop = &generic_ro_fops;
+			i->i_mode |= S_IFREG;
+			i->i_blocks = ((i->i_size - 1) >> 9) + 1;
+			SQUASHFS_I(i)->u.s1.fragment_start_block = frag_blk;
+			SQUASHFS_I(i)->u.s1.fragment_size = frag_size;
+			SQUASHFS_I(i)->u.s1.fragment_offset = inodep->offset;
+			SQUASHFS_I(i)->start_block = inodep->start_block;
+			SQUASHFS_I(i)->u.s1.block_list_start = next_block;
+			SQUASHFS_I(i)->offset = next_offset;
+			if (sblk->block_size > 4096)
+				i->i_data.a_ops = &squashfs_aops;
+			else
+				i->i_data.a_ops = &squashfs_aops_4K;
+
+			TRACE("File inode %x:%x, start_block %llx, "
+					"block_list_start %llx, offset %x\n",
+					SQUASHFS_INODE_BLK(inode), offset,
+					inodep->start_block, next_block,
+					next_offset);
+			break;
+		}
+		case SQUASHFS_LREG_TYPE: {
+			unsigned int frag_size;
+			long long frag_blk;
+			struct squashfs_lreg_inode_header *inodep = &id.lreg;
+			struct squashfs_lreg_inode_header *sinodep = &sid.lreg;
+				
+			if (msblk->swap) {
+				if (!squashfs_get_cached_block(s, (char *)
+						sinodep, block, offset,
+						sizeof(*sinodep), &next_block,
+						&next_offset))
+					goto failed_read;
+				SQUASHFS_SWAP_LREG_INODE_HEADER(inodep, sinodep);
+			} else
+				if (!squashfs_get_cached_block(s, (char *)
+						inodep, block, offset,
+						sizeof(*inodep), &next_block,
+						&next_offset))
+					goto failed_read;
+
+			frag_blk = SQUASHFS_INVALID_BLK;
+			if (inodep->fragment != SQUASHFS_INVALID_FRAG &&
+					!get_fragment_location(s,
+					inodep->fragment, &frag_blk, &frag_size))
+				goto failed_read;
+				
+			if((i = squashfs_new_inode(s, inodeb)) == NULL)
+				goto failed_read1;
+
+			i->i_nlink = inodep->nlink;
+			i->i_size = inodep->file_size;
+			i->i_fop = &generic_ro_fops;
+			i->i_mode |= S_IFREG;
+			i->i_blocks = ((i->i_size - 1) >> 9) + 1;
+			SQUASHFS_I(i)->u.s1.fragment_start_block = frag_blk;
+			SQUASHFS_I(i)->u.s1.fragment_size = frag_size;
+			SQUASHFS_I(i)->u.s1.fragment_offset = inodep->offset;
+			SQUASHFS_I(i)->start_block = inodep->start_block;
+			SQUASHFS_I(i)->u.s1.block_list_start = next_block;
+			SQUASHFS_I(i)->offset = next_offset;
+			if (sblk->block_size > 4096)
+				i->i_data.a_ops = &squashfs_aops;
+			else
+				i->i_data.a_ops = &squashfs_aops_4K;
+
+			TRACE("File inode %x:%x, start_block %llx, "
+					"block_list_start %llx, offset %x\n",
+					SQUASHFS_INODE_BLK(inode), offset,
+					inodep->start_block, next_block,
+					next_offset);
+			break;
+		}
+		case SQUASHFS_DIR_TYPE: {
+			struct squashfs_dir_inode_header *inodep = &id.dir;
+			struct squashfs_dir_inode_header *sinodep = &sid.dir;
+
+			if (msblk->swap) {
+				if (!squashfs_get_cached_block(s, (char *)
+						sinodep, block, offset,
+						sizeof(*sinodep), &next_block,
+						&next_offset))
+					goto failed_read;
+				SQUASHFS_SWAP_DIR_INODE_HEADER(inodep, sinodep);
+			} else
+				if (!squashfs_get_cached_block(s, (char *)
+						inodep, block, offset,
+						sizeof(*inodep), &next_block,
+						&next_offset))
+					goto failed_read;
+
+			if((i = squashfs_new_inode(s, inodeb)) == NULL)
+				goto failed_read1;
+
+			i->i_nlink = inodep->nlink;
+			i->i_size = inodep->file_size;
+			i->i_op = &squashfs_dir_inode_ops;
+			i->i_fop = &squashfs_dir_ops;
+			i->i_mode |= S_IFDIR;
+			SQUASHFS_I(i)->start_block = inodep->start_block;
+			SQUASHFS_I(i)->offset = inodep->offset;
+			SQUASHFS_I(i)->u.s2.directory_index_count = 0;
+			SQUASHFS_I(i)->u.s2.parent_inode = inodep->parent_inode;
+
+			TRACE("Directory inode %x:%x, start_block %x, offset "
+					"%x\n", SQUASHFS_INODE_BLK(inode),
+					offset, inodep->start_block,
+					inodep->offset);
+			break;
+		}
+		case SQUASHFS_LDIR_TYPE: {
+			struct squashfs_ldir_inode_header *inodep = &id.ldir;
+			struct squashfs_ldir_inode_header *sinodep = &sid.ldir;
+
+			if (msblk->swap) {
+				if (!squashfs_get_cached_block(s, (char *)
+						sinodep, block, offset,
+						sizeof(*sinodep), &next_block,
+						&next_offset))
+					goto failed_read;
+				SQUASHFS_SWAP_LDIR_INODE_HEADER(inodep,
+						sinodep);
+			} else
+				if (!squashfs_get_cached_block(s, (char *)
+						inodep, block, offset,
+						sizeof(*inodep), &next_block,
+						&next_offset))
+					goto failed_read;
+
+			if((i = squashfs_new_inode(s, inodeb)) == NULL)
+				goto failed_read1;
+
+			i->i_nlink = inodep->nlink;
+			i->i_size = inodep->file_size;
+			i->i_op = &squashfs_dir_inode_ops;
+			i->i_fop = &squashfs_dir_ops;
+			i->i_mode |= S_IFDIR;
+			SQUASHFS_I(i)->start_block = inodep->start_block;
+			SQUASHFS_I(i)->offset = inodep->offset;
+			SQUASHFS_I(i)->u.s2.directory_index_start = next_block;
+			SQUASHFS_I(i)->u.s2.directory_index_offset =
+								next_offset;
+			SQUASHFS_I(i)->u.s2.directory_index_count =
+								inodep->i_count;
+			SQUASHFS_I(i)->u.s2.parent_inode = inodep->parent_inode;
+
+			TRACE("Long directory inode %x:%x, start_block %x, "
+					"offset %x\n",
+					SQUASHFS_INODE_BLK(inode), offset,
+					inodep->start_block, inodep->offset);
+			break;
+		}
+		case SQUASHFS_SYMLINK_TYPE: {
+			struct squashfs_symlink_inode_header *inodep =
+								&id.symlink;
+			struct squashfs_symlink_inode_header *sinodep =
+								&sid.symlink;
+	
+			if (msblk->swap) {
+				if (!squashfs_get_cached_block(s, (char *)
+						sinodep, block, offset,
+						sizeof(*sinodep), &next_block,
+						&next_offset))
+					goto failed_read;
+				SQUASHFS_SWAP_SYMLINK_INODE_HEADER(inodep,
+								sinodep);
+			} else
+				if (!squashfs_get_cached_block(s, (char *)
+						inodep, block, offset,
+						sizeof(*inodep), &next_block,
+						&next_offset))
+					goto failed_read;
+
+			if((i = squashfs_new_inode(s, inodeb)) == NULL)
+				goto failed_read1;
+
+			i->i_nlink = inodep->nlink;
+			i->i_size = inodep->symlink_size;
+			i->i_op = &page_symlink_inode_operations;
+			i->i_data.a_ops = &squashfs_symlink_aops;
+			i->i_mode |= S_IFLNK;
+			SQUASHFS_I(i)->start_block = next_block;
+			SQUASHFS_I(i)->offset = next_offset;
+
+			TRACE("Symbolic link inode %x:%x, start_block %llx, "
+					"offset %x\n",
+					SQUASHFS_INODE_BLK(inode), offset,
+					next_block, next_offset);
+			break;
+		 }
+		 case SQUASHFS_BLKDEV_TYPE:
+		 case SQUASHFS_CHRDEV_TYPE: {
+			struct squashfs_dev_inode_header *inodep = &id.dev;
+			struct squashfs_dev_inode_header *sinodep = &sid.dev;
+
+			if (msblk->swap) {
+				if (!squashfs_get_cached_block(s, (char *)
+						sinodep, block, offset,
+						sizeof(*sinodep), &next_block,
+						&next_offset))
+					goto failed_read;
+				SQUASHFS_SWAP_DEV_INODE_HEADER(inodep, sinodep);
+			} else	
+				if (!squashfs_get_cached_block(s, (char *)
+						inodep, block, offset,
+						sizeof(*inodep), &next_block,
+						&next_offset))
+					goto failed_read;
+
+			if ((i = squashfs_new_inode(s, inodeb)) == NULL)
+				goto failed_read1;
+
+			i->i_nlink = inodep->nlink;
+			i->i_mode |= (inodeb->inode_type ==
+					SQUASHFS_CHRDEV_TYPE) ?  S_IFCHR :
+					S_IFBLK;
+			init_special_inode(i, i->i_mode,
+					old_decode_dev(inodep->rdev));
+
+			TRACE("Device inode %x:%x, rdev %x\n",
+					SQUASHFS_INODE_BLK(inode), offset,
+					inodep->rdev);
+			break;
+		 }
+		 case SQUASHFS_FIFO_TYPE:
+		 case SQUASHFS_SOCKET_TYPE: {
+			struct squashfs_ipc_inode_header *inodep = &id.ipc;
+			struct squashfs_ipc_inode_header *sinodep = &sid.ipc;
+
+			if (msblk->swap) {
+				if (!squashfs_get_cached_block(s, (char *)
+						sinodep, block, offset,
+						sizeof(*sinodep), &next_block,
+						&next_offset))
+					goto failed_read;
+				SQUASHFS_SWAP_IPC_INODE_HEADER(inodep, sinodep);
+			} else	
+				if (!squashfs_get_cached_block(s, (char *)
+						inodep, block, offset,
+						sizeof(*inodep), &next_block,
+						&next_offset))
+					goto failed_read;
+
+			if ((i = squashfs_new_inode(s, inodeb)) == NULL)
+				goto failed_read1;
+
+			i->i_nlink = inodep->nlink;
+			i->i_mode |= (inodeb->inode_type == SQUASHFS_FIFO_TYPE)
+							? S_IFIFO : S_IFSOCK;
+			init_special_inode(i, i->i_mode, 0);
+			break;
+		 }
+		 default:
+			ERROR("Unknown inode type %d in squashfs_iget!\n",
+					inodeb->inode_type);
+			goto failed_read1;
+	}
+	
+	insert_inode_hash(i);
+	return i;
+
+failed_read:
+	ERROR("Unable to read inode [%llx:%x]\n", block, offset);
+
+failed_read1:
+	return NULL;
+}
+
+
+static int read_fragment_index_table(struct super_block *s)
+{
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+
+	/* Allocate fragment index table */
+	if (!(msblk->fragment_index = kmalloc(SQUASHFS_FRAGMENT_INDEX_BYTES
+					(sblk->fragments), GFP_KERNEL))) {
+		ERROR("Failed to allocate uid/gid table\n");
+		return 0;
+	}
+   
+	if (SQUASHFS_FRAGMENT_INDEX_BYTES(sblk->fragments) &&
+					!squashfs_read_data(s, (char *)
+					msblk->fragment_index,
+					sblk->fragment_table_start,
+					SQUASHFS_FRAGMENT_INDEX_BYTES
+					(sblk->fragments) |
+					SQUASHFS_COMPRESSED_BIT_BLOCK, NULL)) {
+		ERROR("unable to read fragment index table\n");
+		return 0;
+	}
+
+	if (msblk->swap) {
+		int i;
+		long long fragment;
+
+		for (i = 0; i < SQUASHFS_FRAGMENT_INDEXES(sblk->fragments);
+									i++) {
+			SQUASHFS_SWAP_FRAGMENT_INDEXES((&fragment),
+						&msblk->fragment_index[i], 1);
+			msblk->fragment_index[i] = fragment;
+		}
+	}
+
+	return 1;
+}
+
+
+static int supported_squashfs_filesystem(struct squashfs_sb_info *msblk, int silent)
+{
+	struct squashfs_super *sblk = &msblk->sblk;
+
+	msblk->iget = squashfs_iget;
+	msblk->read_blocklist = read_blocklist;
+	msblk->read_fragment_index_table = read_fragment_index_table;
+
+	if (sblk->s_major == 1) {
+		if (!squashfs_1_0_supported(msblk)) {
+			SERROR("Major/Minor mismatch, Squashfs 1.0 filesystems "
+				"are unsupported\n");
+			SERROR("Please recompile with "
+				"Squashfs 1.0 support enabled\n");
+			return 0;
+		}
+	} else if (sblk->s_major == 2) {
+		if (!squashfs_2_0_supported(msblk)) {
+			SERROR("Major/Minor mismatch, Squashfs 2.0 filesystems "
+				"are unsupported\n");
+			SERROR("Please recompile with "
+				"Squashfs 2.0 support enabled\n");
+			return 0;
+		}
+	} else if(sblk->s_major != SQUASHFS_MAJOR || sblk->s_minor >
+			SQUASHFS_MINOR) {
+		SERROR("Major/Minor mismatch, trying to mount newer %d.%d "
+				"filesystem\n", sblk->s_major, sblk->s_minor);
+		SERROR("Please update your kernel\n");
+		return 0;
+	}
+
+	return 1;
+}
+
+
+static int squashfs_fill_super(struct super_block *s, void *data, int silent)
+{
+	struct squashfs_sb_info *msblk;
+	struct squashfs_super *sblk;
+	int i;
+	char b[BDEVNAME_SIZE];
+	struct inode *root;
+
+	TRACE("Entered squashfs_read_superblock\n");
+
+	if (!(s->s_fs_info = kmalloc(sizeof(struct squashfs_sb_info),
+						GFP_KERNEL))) {
+		ERROR("Failed to allocate superblock\n");
+		goto failure;
+	}
+	memset(s->s_fs_info, 0, sizeof(struct squashfs_sb_info));
+	msblk = s->s_fs_info;
+	sblk = &msblk->sblk;
+	
+	msblk->devblksize = sb_min_blocksize(s, BLOCK_SIZE);
+	msblk->devblksize_log2 = ffz(~msblk->devblksize);
+
+	init_MUTEX(&msblk->read_data_mutex);
+	init_MUTEX(&msblk->read_page_mutex);
+	init_MUTEX(&msblk->block_cache_mutex);
+	init_MUTEX(&msblk->fragment_mutex);
+	init_MUTEX(&msblk->meta_index_mutex);
+	
+	init_waitqueue_head(&msblk->waitq);
+	init_waitqueue_head(&msblk->fragment_wait_queue);
+
+	if (!squashfs_read_data(s, (char *) sblk, SQUASHFS_START,
+					sizeof(struct squashfs_super) |
+					SQUASHFS_COMPRESSED_BIT_BLOCK, NULL)) {
+		SERROR("unable to read superblock\n");
+		goto failed_mount;
+	}
+
+	/* Check it is a SQUASHFS superblock */
+	msblk->swap = 0;
+	if ((s->s_magic = sblk->s_magic) != SQUASHFS_MAGIC) {
+		if (sblk->s_magic == SQUASHFS_MAGIC_SWAP) {
+			struct squashfs_super ssblk;
+
+			WARNING("Mounting a different endian SQUASHFS "
+				"filesystem on %s\n", bdevname(s->s_bdev, b));
+
+			SQUASHFS_SWAP_SUPER(&ssblk, sblk);
+			memcpy(sblk, &ssblk, sizeof(struct squashfs_super));
+			msblk->swap = 1;
+		} else  {
+			SERROR("Can't find a SQUASHFS superblock on %s\n",
+							bdevname(s->s_bdev, b));
+			goto failed_mount;
+		}
+	}
+
+	/* Check the MAJOR & MINOR versions */
+	if(!supported_squashfs_filesystem(msblk, silent))
+		goto failed_mount;
+
+	TRACE("Found valid superblock on %s\n", bdevname(s->s_bdev, b));
+	TRACE("Inodes are %scompressed\n",
+					SQUASHFS_UNCOMPRESSED_INODES
+					(sblk->flags) ? "un" : "");
+	TRACE("Data is %scompressed\n",
+					SQUASHFS_UNCOMPRESSED_DATA(sblk->flags)
+					? "un" : "");
+	TRACE("Check data is %s present in the filesystem\n",
+					SQUASHFS_CHECK_DATA(sblk->flags) ?
+					"" : "not");
+	TRACE("Filesystem size %lld bytes\n", sblk->bytes_used);
+	TRACE("Block size %d\n", sblk->block_size);
+	TRACE("Number of inodes %d\n", sblk->inodes);
+	if (sblk->s_major > 1)
+		TRACE("Number of fragments %d\n", sblk->fragments);
+	TRACE("Number of uids %d\n", sblk->no_uids);
+	TRACE("Number of gids %d\n", sblk->no_guids);
+	TRACE("sblk->inode_table_start %llx\n", sblk->inode_table_start);
+	TRACE("sblk->directory_table_start %llx\n", sblk->directory_table_start);
+	if (sblk->s_major > 1)
+		TRACE("sblk->fragment_table_start %llx\n",
+					sblk->fragment_table_start);
+	TRACE("sblk->uid_start %llx\n", sblk->uid_start);
+
+	s->s_flags |= MS_RDONLY;
+	s->s_op = &squashfs_ops;
+
+	/* Init inode_table block pointer array */
+	if (!(msblk->block_cache = kmalloc(sizeof(struct squashfs_cache) *
+					SQUASHFS_CACHED_BLKS, GFP_KERNEL))) {
+		ERROR("Failed to allocate block cache\n");
+		goto failed_mount;
+	}
+
+	for (i = 0; i < SQUASHFS_CACHED_BLKS; i++)
+		msblk->block_cache[i].block = SQUASHFS_INVALID_BLK;
+
+	msblk->next_cache = 0;
+
+	/* Allocate read_data block */
+	msblk->read_size = (sblk->block_size < SQUASHFS_METADATA_SIZE) ?
+					SQUASHFS_METADATA_SIZE :
+					sblk->block_size;
+
+	if (!(msblk->read_data = kmalloc(msblk->read_size, GFP_KERNEL))) {
+		ERROR("Failed to allocate read_data block\n");
+		goto failed_mount;
+	}
+
+	/* Allocate read_page block */
+	if (!(msblk->read_page = kmalloc(sblk->block_size, GFP_KERNEL))) {
+		ERROR("Failed to allocate read_page block\n");
+		goto failed_mount;
+	}
+
+	/* Allocate uid and gid tables */
+	if (!(msblk->uid = kmalloc((sblk->no_uids + sblk->no_guids) *
+					sizeof(unsigned int), GFP_KERNEL))) {
+		ERROR("Failed to allocate uid/gid table\n");
+		goto failed_mount;
+	}
+	msblk->guid = msblk->uid + sblk->no_uids;
+   
+	if (msblk->swap) {
+		unsigned int suid[sblk->no_uids + sblk->no_guids];
+
+		if (!squashfs_read_data(s, (char *) &suid, sblk->uid_start,
+					((sblk->no_uids + sblk->no_guids) *
+					 sizeof(unsigned int)) |
+					SQUASHFS_COMPRESSED_BIT_BLOCK, NULL)) {
+			ERROR("unable to read uid/gid table\n");
+			goto failed_mount;
+		}
+
+		SQUASHFS_SWAP_DATA(msblk->uid, suid, (sblk->no_uids +
+			sblk->no_guids), (sizeof(unsigned int) * 8));
+	} else
+		if (!squashfs_read_data(s, (char *) msblk->uid, sblk->uid_start,
+					((sblk->no_uids + sblk->no_guids) *
+					 sizeof(unsigned int)) |
+					SQUASHFS_COMPRESSED_BIT_BLOCK, NULL)) {
+			ERROR("unable to read uid/gid table\n");
+			goto failed_mount;
+		}
+
+
+	if (sblk->s_major == 1 && squashfs_1_0_supported(msblk))
+		goto allocate_root;
+
+	if (!(msblk->fragment = kmalloc(sizeof(struct squashfs_fragment_cache) *
+				SQUASHFS_CACHED_FRAGMENTS, GFP_KERNEL))) {
+		ERROR("Failed to allocate fragment block cache\n");
+		goto failed_mount;
+	}
+
+	for (i = 0; i < SQUASHFS_CACHED_FRAGMENTS; i++) {
+		msblk->fragment[i].locked = 0;
+		msblk->fragment[i].block = SQUASHFS_INVALID_BLK;
+		msblk->fragment[i].data = NULL;
+	}
+
+	msblk->next_fragment = 0;
+
+	/* Allocate fragment index table */
+	if (msblk->read_fragment_index_table(s) == 0)
+		goto failed_mount;
+
+allocate_root:
+	if ((root = (msblk->iget)(s, sblk->root_inode)) == NULL)
+		goto failed_mount;
+
+	if ((s->s_root = d_alloc_root(root)) == NULL) {
+		ERROR("Root inode create failed\n");
+		iput(root);
+		goto failed_mount;
+	}
+
+	TRACE("Leaving squashfs_read_super\n");
+	return 0;
+
+failed_mount:
+	kfree(msblk->fragment_index);
+	kfree(msblk->fragment);
+	kfree(msblk->uid);
+	kfree(msblk->read_page);
+	kfree(msblk->read_data);
+	kfree(msblk->block_cache);
+	kfree(msblk->fragment_index_2);
+	kfree(s->s_fs_info);
+	s->s_fs_info = NULL;
+	return -EINVAL;
+
+failure:
+	return -ENOMEM;
+}
+
+
+static int squashfs_statfs(struct dentry *dentry, struct kstatfs *buf)
+{
+	struct squashfs_sb_info *msblk = dentry;
+	struct squashfs_super *sblk = &msblk->sblk;
+
+	TRACE("Entered squashfs_statfs\n");
+
+	buf->f_type = SQUASHFS_MAGIC;
+	buf->f_bsize = sblk->block_size;
+	buf->f_blocks = ((sblk->bytes_used - 1) >> sblk->block_log) + 1;
+	buf->f_bfree = buf->f_bavail = 0;
+	buf->f_files = sblk->inodes;
+	buf->f_ffree = 0;
+	buf->f_namelen = SQUASHFS_NAME_LEN;
+
+	return 0;
+}
+
+
+static int squashfs_symlink_readpage(struct file *file, struct page *page)
+{
+	struct inode *inode = page->mapping->host;
+	int index = page->index << PAGE_CACHE_SHIFT, length, bytes;
+	long long block = SQUASHFS_I(inode)->start_block;
+	int offset = SQUASHFS_I(inode)->offset;
+	void *pageaddr = kmap(page);
+
+	TRACE("Entered squashfs_symlink_readpage, page index %ld, start block "
+				"%llx, offset %x\n", page->index,
+				SQUASHFS_I(inode)->start_block,
+				SQUASHFS_I(inode)->offset);
+
+	for (length = 0; length < index; length += bytes) {
+		if (!(bytes = squashfs_get_cached_block(inode->i_sb, NULL,
+				block, offset, PAGE_CACHE_SIZE, &block,
+				&offset))) {
+			ERROR("Unable to read symbolic link [%llx:%x]\n", block,
+					offset);
+			goto skip_read;
+		}
+	}
+
+	if (length != index) {
+		ERROR("(squashfs_symlink_readpage) length != index\n");
+		bytes = 0;
+		goto skip_read;
+	}
+
+	bytes = (i_size_read(inode) - length) > PAGE_CACHE_SIZE ? PAGE_CACHE_SIZE :
+					i_size_read(inode) - length;
+
+	if (!(bytes = squashfs_get_cached_block(inode->i_sb, pageaddr, block,
+					offset, bytes, &block, &offset)))
+		ERROR("Unable to read symbolic link [%llx:%x]\n", block, offset);
+
+skip_read:
+	memset(pageaddr + bytes, 0, PAGE_CACHE_SIZE - bytes);
+	kunmap(page);
+	SetPageUptodate(page);
+	unlock_page(page);
+
+	return 0;
+}
+
+
+struct meta_index *locate_meta_index(struct inode *inode, int index, int offset)
+{
+	struct meta_index *meta = NULL;
+	struct squashfs_sb_info *msblk = inode->i_sb->s_fs_info;
+	int i;
+
+	down(&msblk->meta_index_mutex);
+
+	TRACE("locate_meta_index: index %d, offset %d\n", index, offset);
+
+	if(msblk->meta_index == NULL)
+		goto not_allocated;
+
+	for (i = 0; i < SQUASHFS_META_NUMBER; i ++)
+		if (msblk->meta_index[i].inode_number == inode->i_ino &&
+				msblk->meta_index[i].offset >= offset &&
+				msblk->meta_index[i].offset <= index &&
+				msblk->meta_index[i].locked == 0) {
+			TRACE("locate_meta_index: entry %d, offset %d\n", i,
+					msblk->meta_index[i].offset);
+			meta = &msblk->meta_index[i];
+			offset = meta->offset;
+		}
+
+	if (meta)
+		meta->locked = 1;
+
+not_allocated:
+	up(&msblk->meta_index_mutex);
+
+	return meta;
+}
+
+
+struct meta_index *empty_meta_index(struct inode *inode, int offset, int skip)
+{
+	struct squashfs_sb_info *msblk = inode->i_sb->s_fs_info;
+	struct meta_index *meta = NULL;
+	int i;
+
+	down(&msblk->meta_index_mutex);
+
+	TRACE("empty_meta_index: offset %d, skip %d\n", offset, skip);
+
+	if(msblk->meta_index == NULL) {
+		if (!(msblk->meta_index = kmalloc(sizeof(struct meta_index) *
+					SQUASHFS_META_NUMBER, GFP_KERNEL))) {
+			ERROR("Failed to allocate meta_index\n");
+			goto failed;
+		}
+		for(i = 0; i < SQUASHFS_META_NUMBER; i++) {
+			msblk->meta_index[i].inode_number = 0;
+			msblk->meta_index[i].locked = 0;
+		}
+		msblk->next_meta_index = 0;
+	}
+
+	for(i = SQUASHFS_META_NUMBER; i &&
+			msblk->meta_index[msblk->next_meta_index].locked; i --)
+		msblk->next_meta_index = (msblk->next_meta_index + 1) %
+			SQUASHFS_META_NUMBER;
+
+	if(i == 0) {
+		TRACE("empty_meta_index: failed!\n");
+		goto failed;
+	}
+
+	TRACE("empty_meta_index: returned meta entry %d, %p\n",
+			msblk->next_meta_index,
+			&msblk->meta_index[msblk->next_meta_index]);
+
+	meta = &msblk->meta_index[msblk->next_meta_index];
+	msblk->next_meta_index = (msblk->next_meta_index + 1) %
+			SQUASHFS_META_NUMBER;
+
+	meta->inode_number = inode->i_ino;
+	meta->offset = offset;
+	meta->skip = skip;
+	meta->entries = 0;
+	meta->locked = 1;
+
+failed:
+	up(&msblk->meta_index_mutex);
+	return meta;
+}
+
+
+void release_meta_index(struct inode *inode, struct meta_index *meta)
+{
+	meta->locked = 0;
+}
+
+
+static int read_block_index(struct super_block *s, int blocks, char *block_list,
+		long long *start_block, int *offset)
+{
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	unsigned int *block_listp;
+	int block = 0;
+	
+	if (msblk->swap) {
+		char sblock_list[blocks << 2];
+
+		if (!squashfs_get_cached_block(s, sblock_list, *start_block,
+				*offset, blocks << 2, start_block, offset)) {
+			ERROR("Unable to read block list [%llx:%x]\n",
+				*start_block, *offset);
+			goto failure;
+		}
+		SQUASHFS_SWAP_INTS(((unsigned int *)block_list),
+				((unsigned int *)sblock_list), blocks);
+	} else
+		if (!squashfs_get_cached_block(s, block_list, *start_block,
+				*offset, blocks << 2, start_block, offset)) {
+			ERROR("Unable to read block list [%llx:%x]\n",
+				*start_block, *offset);
+			goto failure;
+		}
+
+	for (block_listp = (unsigned int *) block_list; blocks;
+				block_listp++, blocks --)
+		block += SQUASHFS_COMPRESSED_SIZE_BLOCK(*block_listp);
+
+	return block;
+
+failure:
+	return -1;
+}
+
+
+#define SIZE 256
+
+static inline int calculate_skip(int blocks) {
+	int skip = (blocks - 1) / ((SQUASHFS_SLOTS * SQUASHFS_META_ENTRIES + 1) * SQUASHFS_META_INDEXES);
+	return skip >= 7 ? 7 : skip + 1;
+}
+
+
+static int get_meta_index(struct inode *inode, int index,
+		long long *index_block, int *index_offset,
+		long long *data_block, char *block_list)
+{
+	struct squashfs_sb_info *msblk = inode->i_sb->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+	int skip = calculate_skip(i_size_read(inode) >> sblk->block_log);
+	int offset = 0;
+	struct meta_index *meta;
+	struct meta_entry *meta_entry;
+	long long cur_index_block = SQUASHFS_I(inode)->u.s1.block_list_start;
+	int cur_offset = SQUASHFS_I(inode)->offset;
+	long long cur_data_block = SQUASHFS_I(inode)->start_block;
+	int i;
+ 
+	index /= SQUASHFS_META_INDEXES * skip;
+
+	while ( offset < index ) {
+		meta = locate_meta_index(inode, index, offset + 1);
+
+		if (meta == NULL) {
+			if ((meta = empty_meta_index(inode, offset + 1,
+							skip)) == NULL)
+				goto all_done;
+		} else {
+			offset = index < meta->offset + meta->entries ? index :
+				meta->offset + meta->entries - 1;
+			meta_entry = &meta->meta_entry[offset - meta->offset];
+			cur_index_block = meta_entry->index_block + sblk->inode_table_start;
+			cur_offset = meta_entry->offset;
+			cur_data_block = meta_entry->data_block;
+			TRACE("get_meta_index: offset %d, meta->offset %d, "
+				"meta->entries %d\n", offset, meta->offset,
+				meta->entries);
+			TRACE("get_meta_index: index_block 0x%llx, offset 0x%x"
+				" data_block 0x%llx\n", cur_index_block,
+				cur_offset, cur_data_block);
+		}
+
+		for (i = meta->offset + meta->entries; i <= index &&
+				i < meta->offset + SQUASHFS_META_ENTRIES; i++) {
+			int blocks = skip * SQUASHFS_META_INDEXES;
+
+			while (blocks) {
+				int block = blocks > (SIZE >> 2) ? (SIZE >> 2) :
+					blocks;
+				int res = read_block_index(inode->i_sb, block,
+					block_list, &cur_index_block,
+					&cur_offset);
+
+				if (res == -1)
+					goto failed;
+
+				cur_data_block += res;
+				blocks -= block;
+			}
+
+			meta_entry = &meta->meta_entry[i - meta->offset];
+			meta_entry->index_block = cur_index_block - sblk->inode_table_start;
+			meta_entry->offset = cur_offset;
+			meta_entry->data_block = cur_data_block;
+			meta->entries ++;
+			offset ++;
+		}
+
+		TRACE("get_meta_index: meta->offset %d, meta->entries %d\n",
+				meta->offset, meta->entries);
+
+		release_meta_index(inode, meta);
+	}
+
+all_done:
+	*index_block = cur_index_block;
+	*index_offset = cur_offset;
+	*data_block = cur_data_block;
+
+	return offset * SQUASHFS_META_INDEXES * skip;
+
+failed:
+	release_meta_index(inode, meta);
+	return -1;
+}
+
+
+static long long read_blocklist(struct inode *inode, int index,
+				int readahead_blks, char *block_list,
+				unsigned short **block_p, unsigned int *bsize)
+{
+	long long block_ptr;
+	int offset;
+	long long block;
+	int res = get_meta_index(inode, index, &block_ptr, &offset, &block,
+		block_list);
+
+	TRACE("read_blocklist: res %d, index %d, block_ptr 0x%llx, offset"
+		       " 0x%x, block 0x%llx\n", res, index, block_ptr, offset,
+		       block);
+
+	if(res == -1)
+		goto failure;
+
+	index -= res;
+
+	while ( index ) {
+		int blocks = index > (SIZE >> 2) ? (SIZE >> 2) : index;
+		int res = read_block_index(inode->i_sb, blocks, block_list,
+			&block_ptr, &offset);
+		if (res == -1)
+			goto failure;
+		block += res;
+		index -= blocks;
+	}
+
+	if (read_block_index(inode->i_sb, 1, block_list,
+			&block_ptr, &offset) == -1)
+		goto failure;
+	*bsize = *((unsigned int *) block_list);
+
+	return block;
+
+failure:
+	return 0;
+}
+
+
+static int squashfs_readpage(struct file *file, struct page *page)
+{
+	struct inode *inode = page->mapping->host;
+	struct squashfs_sb_info *msblk = inode->i_sb->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+	unsigned char block_list[SIZE];
+	long long block;
+	unsigned int bsize, i = 0, bytes = 0, byte_offset = 0;
+	int index = page->index >> (sblk->block_log - PAGE_CACHE_SHIFT);
+ 	void *pageaddr;
+	struct squashfs_fragment_cache *fragment = NULL;
+	char *data_ptr = msblk->read_page;
+	
+	int mask = (1 << (sblk->block_log - PAGE_CACHE_SHIFT)) - 1;
+	int start_index = page->index & ~mask;
+	int end_index = start_index | mask;
+
+	TRACE("Entered squashfs_readpage, page index %lx, start block %llx\n",
+					page->index,
+					SQUASHFS_I(inode)->start_block);
+
+	if (page->index >= ((i_size_read(inode) + PAGE_CACHE_SIZE - 1) >>
+					PAGE_CACHE_SHIFT))
+		goto skip_read;
+
+	if (SQUASHFS_I(inode)->u.s1.fragment_start_block == SQUASHFS_INVALID_BLK
+					|| index < (i_size_read(inode) >>
+					sblk->block_log)) {
+		if ((block = (msblk->read_blocklist)(inode, index, 1,
+					block_list, NULL, &bsize)) == 0)
+			goto skip_read;
+
+		down(&msblk->read_page_mutex);
+		
+		if (!(bytes = squashfs_read_data(inode->i_sb, msblk->read_page,
+					block, bsize, NULL))) {
+			ERROR("Unable to read page, block %llx, size %x\n", block,
+					bsize);
+			up(&msblk->read_page_mutex);
+			goto skip_read;
+		}
+	} else {
+		if ((fragment = get_cached_fragment(inode->i_sb,
+					SQUASHFS_I(inode)->
+					u.s1.fragment_start_block,
+					SQUASHFS_I(inode)->u.s1.fragment_size))
+					== NULL) {
+			ERROR("Unable to read page, block %llx, size %x\n",
+					SQUASHFS_I(inode)->
+					u.s1.fragment_start_block,
+					(int) SQUASHFS_I(inode)->
+					u.s1.fragment_size);
+			goto skip_read;
+		}
+		bytes = SQUASHFS_I(inode)->u.s1.fragment_offset +
+					(i_size_read(inode) & (sblk->block_size
+					- 1));
+		byte_offset = SQUASHFS_I(inode)->u.s1.fragment_offset;
+		data_ptr = fragment->data;
+	}
+
+	for (i = start_index; i <= end_index && byte_offset < bytes;
+					i++, byte_offset += PAGE_CACHE_SIZE) {
+		struct page *push_page;
+		int available_bytes = (bytes - byte_offset) > PAGE_CACHE_SIZE ?
+					PAGE_CACHE_SIZE : bytes - byte_offset;
+
+		TRACE("bytes %d, i %d, byte_offset %d, available_bytes %d\n",
+					bytes, i, byte_offset, available_bytes);
+
+		if (i == page->index)  {
+			pageaddr = kmap_atomic(page, KM_USER0);
+			memcpy(pageaddr, data_ptr + byte_offset,
+					available_bytes);
+			memset(pageaddr + available_bytes, 0,
+					PAGE_CACHE_SIZE - available_bytes);
+			kunmap_atomic(pageaddr, KM_USER0);
+			flush_dcache_page(page);
+			SetPageUptodate(page);
+			unlock_page(page);
+		} else if ((push_page =
+				grab_cache_page_nowait(page->mapping, i))) {
+ 			pageaddr = kmap_atomic(push_page, KM_USER0);
+
+			memcpy(pageaddr, data_ptr + byte_offset,
+					available_bytes);
+			memset(pageaddr + available_bytes, 0,
+					PAGE_CACHE_SIZE - available_bytes);
+			kunmap_atomic(pageaddr, KM_USER0);
+			flush_dcache_page(push_page);
+			SetPageUptodate(push_page);
+			unlock_page(push_page);
+			page_cache_release(push_page);
+		}
+	}
+
+	if (SQUASHFS_I(inode)->u.s1.fragment_start_block == SQUASHFS_INVALID_BLK
+					|| index < (i_size_read(inode) >>
+					sblk->block_log))
+		up(&msblk->read_page_mutex);
+	else
+		release_cached_fragment(msblk, fragment);
+
+	return 0;
+
+skip_read:
+	pageaddr = kmap_atomic(page, KM_USER0);
+	memset(pageaddr + bytes, 0, PAGE_CACHE_SIZE - bytes);
+	kunmap_atomic(pageaddr, KM_USER0);
+	flush_dcache_page(page);
+	SetPageUptodate(page);
+	unlock_page(page);
+
+	return 0;
+}
+
+
+static int squashfs_readpage4K(struct file *file, struct page *page)
+{
+	struct inode *inode = page->mapping->host;
+	struct squashfs_sb_info *msblk = inode->i_sb->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+	unsigned char block_list[SIZE];
+	long long block;
+	unsigned int bsize, bytes = 0;
+ 	void *pageaddr;
+	
+	TRACE("Entered squashfs_readpage4K, page index %lx, start block %llx\n",
+					page->index,
+					SQUASHFS_I(inode)->start_block);
+
+	if (page->index >= ((i_size_read(inode) + PAGE_CACHE_SIZE - 1) >>
+					PAGE_CACHE_SHIFT)) {
+		pageaddr = kmap_atomic(page, KM_USER0);
+		goto skip_read;
+	}
+
+	if (SQUASHFS_I(inode)->u.s1.fragment_start_block == SQUASHFS_INVALID_BLK
+					|| page->index < (i_size_read(inode) >>
+					sblk->block_log)) {
+		block = (msblk->read_blocklist)(inode, page->index, 1,
+					block_list, NULL, &bsize);
+
+		down(&msblk->read_page_mutex);
+		bytes = squashfs_read_data(inode->i_sb, msblk->read_page, block,
+					bsize, NULL);
+		pageaddr = kmap_atomic(page, KM_USER0);
+		if (bytes)
+			memcpy(pageaddr, msblk->read_page, bytes);
+		else
+			ERROR("Unable to read page, block %llx, size %x\n",
+					block, bsize);
+		up(&msblk->read_page_mutex);
+	} else {
+		struct squashfs_fragment_cache *fragment =
+			get_cached_fragment(inode->i_sb,
+					SQUASHFS_I(inode)->
+					u.s1.fragment_start_block,
+					SQUASHFS_I(inode)-> u.s1.fragment_size);
+		pageaddr = kmap_atomic(page, KM_USER0);
+		if (fragment) {
+			bytes = i_size_read(inode) & (sblk->block_size - 1);
+			memcpy(pageaddr, fragment->data + SQUASHFS_I(inode)->
+					u.s1.fragment_offset, bytes);
+			release_cached_fragment(msblk, fragment);
+		} else
+			ERROR("Unable to read page, block %llx, size %x\n",
+					SQUASHFS_I(inode)->
+					u.s1.fragment_start_block, (int)
+					SQUASHFS_I(inode)-> u.s1.fragment_size);
+	}
+
+skip_read:
+	memset(pageaddr + bytes, 0, PAGE_CACHE_SIZE - bytes);
+	kunmap_atomic(pageaddr, KM_USER0);
+	flush_dcache_page(page);
+	SetPageUptodate(page);
+	unlock_page(page);
+
+	return 0;
+}
+
+
+static int get_dir_index_using_offset(struct super_block *s, long long 
+				*next_block, unsigned int *next_offset,
+				long long index_start,
+				unsigned int index_offset, int i_count,
+				long long f_pos)
+{
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+	int i, length = 0;
+	struct squashfs_dir_index index;
+
+	TRACE("Entered get_dir_index_using_offset, i_count %d, f_pos %d\n",
+					i_count, (unsigned int) f_pos);
+
+	f_pos =- 3;
+	if (f_pos == 0)
+		goto finish;
+
+	for (i = 0; i < i_count; i++) {
+		if (msblk->swap) {
+			struct squashfs_dir_index sindex;
+			squashfs_get_cached_block(s, (char *) &sindex,
+					index_start, index_offset,
+					sizeof(sindex), &index_start,
+					&index_offset);
+			SQUASHFS_SWAP_DIR_INDEX(&index, &sindex);
+		} else
+			squashfs_get_cached_block(s, (char *) &index,
+					index_start, index_offset,
+					sizeof(index), &index_start,
+					&index_offset);
+
+		if (index.index > f_pos)
+			break;
+
+		squashfs_get_cached_block(s, NULL, index_start, index_offset,
+					index.size + 1, &index_start,
+					&index_offset);
+
+		length = index.index;
+		*next_block = index.start_block + sblk->directory_table_start;
+	}
+
+	*next_offset = (length + *next_offset) % SQUASHFS_METADATA_SIZE;
+
+finish:
+	return length + 3;
+}
+
+
+static int get_dir_index_using_name(struct super_block *s, long long
+				*next_block, unsigned int *next_offset,
+				long long index_start,
+				unsigned int index_offset, int i_count,
+				const char *name, int size)
+{
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+	int i, length = 0;
+	char buffer[sizeof(struct squashfs_dir_index) + SQUASHFS_NAME_LEN + 1];
+	struct squashfs_dir_index *index = (struct squashfs_dir_index *) buffer;
+	char str[SQUASHFS_NAME_LEN + 1];
+
+	TRACE("Entered get_dir_index_using_name, i_count %d\n", i_count);
+
+	strncpy(str, name, size);
+	str[size] = '\0';
+
+	for (i = 0; i < i_count; i++) {
+		if (msblk->swap) {
+			struct squashfs_dir_index sindex;
+			squashfs_get_cached_block(s, (char *) &sindex,
+					index_start, index_offset,
+					sizeof(sindex), &index_start,
+					&index_offset);
+			SQUASHFS_SWAP_DIR_INDEX(index, &sindex);
+		} else
+			squashfs_get_cached_block(s, (char *) index,
+					index_start, index_offset,
+					sizeof(struct squashfs_dir_index),
+					&index_start, &index_offset);
+
+		squashfs_get_cached_block(s, index->name, index_start,
+					index_offset, index->size + 1,
+					&index_start, &index_offset);
+
+		index->name[index->size + 1] = '\0';
+
+		if (strcmp(index->name, str) > 0)
+			break;
+
+		length = index->index;
+		*next_block = index->start_block + sblk->directory_table_start;
+	}
+
+	*next_offset = (length + *next_offset) % SQUASHFS_METADATA_SIZE;
+	return length + 3;
+}
+
+		
+static int squashfs_readdir(struct file *file, void *dirent, filldir_t filldir)
+{
+	struct inode *i = file->f_dentry->d_inode;
+	struct squashfs_sb_info *msblk = i->i_sb->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+	long long next_block = SQUASHFS_I(i)->start_block +
+		sblk->directory_table_start;
+	int next_offset = SQUASHFS_I(i)->offset, length = 0, dirs_read = 0,
+		dir_count;
+	struct squashfs_dir_header dirh;
+	char buffer[sizeof(struct squashfs_dir_entry) + SQUASHFS_NAME_LEN + 1];
+	struct squashfs_dir_entry *dire = (struct squashfs_dir_entry *) buffer;
+
+	TRACE("Entered squashfs_readdir [%llx:%x]\n", next_block, next_offset);
+
+	while(file->f_pos < 3) {
+		char *name;
+		int size, i_ino;
+
+		if(file->f_pos == 0) {
+			name = ".";
+			size = 1;
+			i_ino = i->i_ino;
+		} else {
+			name = "..";
+			size = 2;
+			i_ino = SQUASHFS_I(i)->u.s2.parent_inode;
+		}
+		TRACE("Calling filldir(%x, %s, %d, %d, %d, %d)\n",
+				(unsigned int) dirent, name, size, (int)
+				file->f_pos, i_ino,
+				squashfs_filetype_table[1]);
+
+		if (filldir(dirent, name, size,
+				file->f_pos, i_ino,
+				squashfs_filetype_table[1]) < 0) {
+				TRACE("Filldir returned less than 0\n");
+				goto finish;
+		}
+		file->f_pos += size;
+		dirs_read++;
+	}
+
+	length = get_dir_index_using_offset(i->i_sb, &next_block, &next_offset,
+				SQUASHFS_I(i)->u.s2.directory_index_start,
+				SQUASHFS_I(i)->u.s2.directory_index_offset,
+				SQUASHFS_I(i)->u.s2.directory_index_count,
+				file->f_pos);
+
+	while (length < i_size_read(i)) {
+		/* read directory header */
+		if (msblk->swap) {
+			struct squashfs_dir_header sdirh;
+			
+			if (!squashfs_get_cached_block(i->i_sb, (char *) &sdirh,
+					next_block, next_offset, sizeof(sdirh),
+					&next_block, &next_offset))
+				goto failed_read;
+
+			length += sizeof(sdirh);
+			SQUASHFS_SWAP_DIR_HEADER(&dirh, &sdirh);
+		} else {
+			if (!squashfs_get_cached_block(i->i_sb, (char *) &dirh,
+					next_block, next_offset, sizeof(dirh),
+					&next_block, &next_offset))
+				goto failed_read;
+
+			length += sizeof(dirh);
+		}
+
+		dir_count = dirh.count + 1;
+		while (dir_count--) {
+			if (msblk->swap) {
+				struct squashfs_dir_entry sdire;
+				if (!squashfs_get_cached_block(i->i_sb, (char *)
+						&sdire, next_block, next_offset,
+						sizeof(sdire), &next_block,
+						&next_offset))
+					goto failed_read;
+				
+				length += sizeof(sdire);
+				SQUASHFS_SWAP_DIR_ENTRY(dire, &sdire);
+			} else {
+				if (!squashfs_get_cached_block(i->i_sb, (char *)
+						dire, next_block, next_offset,
+						sizeof(*dire), &next_block,
+						&next_offset))
+					goto failed_read;
+
+				length += sizeof(*dire);
+			}
+
+			if (!squashfs_get_cached_block(i->i_sb, dire->name,
+						next_block, next_offset,
+						dire->size + 1, &next_block,
+						&next_offset))
+				goto failed_read;
+
+			length += dire->size + 1;
+
+			if (file->f_pos >= length)
+				continue;
+
+			dire->name[dire->size + 1] = '\0';
+
+			TRACE("Calling filldir(%x, %s, %d, %d, %x:%x, %d, %d)\n",
+					(unsigned int) dirent, dire->name,
+					dire->size + 1, (int) file->f_pos,
+					dirh.start_block, dire->offset,
+					dirh.inode_number + dire->inode_number,
+					squashfs_filetype_table[dire->type]);
+
+			if (filldir(dirent, dire->name, dire->size + 1,
+					file->f_pos,
+					dirh.inode_number + dire->inode_number,
+					squashfs_filetype_table[dire->type])
+					< 0) {
+				TRACE("Filldir returned less than 0\n");
+				goto finish;
+			}
+			file->f_pos = length;
+			dirs_read++;
+		}
+	}
+
+finish:
+	return dirs_read;
+
+failed_read:
+	ERROR("Unable to read directory block [%llx:%x]\n", next_block,
+		next_offset);
+	return 0;
+}
+
+
+static struct dentry *squashfs_lookup(struct inode *i, struct dentry *dentry,
+				struct nameidata *nd)
+{
+	const unsigned char *name = dentry->d_name.name;
+	int len = dentry->d_name.len;
+	struct inode *inode = NULL;
+	struct squashfs_sb_info *msblk = i->i_sb->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+	long long next_block = SQUASHFS_I(i)->start_block +
+				sblk->directory_table_start;
+	int next_offset = SQUASHFS_I(i)->offset, length = 0,
+				dir_count;
+	struct squashfs_dir_header dirh;
+	char buffer[sizeof(struct squashfs_dir_entry) + SQUASHFS_NAME_LEN];
+	struct squashfs_dir_entry *dire = (struct squashfs_dir_entry *) buffer;
+
+	TRACE("Entered squashfs_lookup [%llx:%x]\n", next_block, next_offset);
+
+	if (len > SQUASHFS_NAME_LEN)
+		goto exit_loop;
+
+	length = get_dir_index_using_name(i->i_sb, &next_block, &next_offset,
+				SQUASHFS_I(i)->u.s2.directory_index_start,
+				SQUASHFS_I(i)->u.s2.directory_index_offset,
+				SQUASHFS_I(i)->u.s2.directory_index_count, name,
+				len);
+
+	while (length < i_size_read(i)) {
+		/* read directory header */
+		if (msblk->swap) {
+			struct squashfs_dir_header sdirh;
+			if (!squashfs_get_cached_block(i->i_sb, (char *) &sdirh,
+					next_block, next_offset, sizeof(sdirh),
+					&next_block, &next_offset))
+				goto failed_read;
+
+			length += sizeof(sdirh);
+			SQUASHFS_SWAP_DIR_HEADER(&dirh, &sdirh);
+		} else {
+			if (!squashfs_get_cached_block(i->i_sb, (char *) &dirh,
+					next_block, next_offset, sizeof(dirh),
+					&next_block, &next_offset))
+				goto failed_read;
+
+			length += sizeof(dirh);
+		}
+
+		dir_count = dirh.count + 1;
+		while (dir_count--) {
+			if (msblk->swap) {
+				struct squashfs_dir_entry sdire;
+				if (!squashfs_get_cached_block(i->i_sb, (char *)
+						&sdire, next_block,next_offset,
+						sizeof(sdire), &next_block,
+						&next_offset))
+					goto failed_read;
+				
+				length += sizeof(sdire);
+				SQUASHFS_SWAP_DIR_ENTRY(dire, &sdire);
+			} else {
+				if (!squashfs_get_cached_block(i->i_sb, (char *)
+						dire, next_block,next_offset,
+						sizeof(*dire), &next_block,
+						&next_offset))
+					goto failed_read;
+
+				length += sizeof(*dire);
+			}
+
+			if (!squashfs_get_cached_block(i->i_sb, dire->name,
+					next_block, next_offset, dire->size + 1,
+					&next_block, &next_offset))
+				goto failed_read;
+
+			length += dire->size + 1;
+
+			if (name[0] < dire->name[0])
+				goto exit_loop;
+
+			if ((len == dire->size + 1) && !strncmp(name,
+						dire->name, len)) {
+				squashfs_inode_t ino =
+					SQUASHFS_MKINODE(dirh.start_block,
+					dire->offset);
+
+				TRACE("calling squashfs_iget for directory "
+					"entry %s, inode %x:%x, %d\n", name,
+					dirh.start_block, dire->offset,
+					dirh.inode_number + dire->inode_number);
+
+				inode = (msblk->iget)(i->i_sb, ino);
+
+				goto exit_loop;
+			}
+		}
+	}
+
+exit_loop:
+	d_add(dentry, inode);
+	return ERR_PTR(0);
+
+failed_read:
+	ERROR("Unable to read directory block [%llx:%x]\n", next_block,
+		next_offset);
+	goto exit_loop;
+}
+
+
+static void squashfs_put_super(struct super_block *s)
+{
+	int i;
+
+	if (s->s_fs_info) {
+		struct squashfs_sb_info *sbi = s->s_fs_info;
+		if (sbi->block_cache)
+			for (i = 0; i < SQUASHFS_CACHED_BLKS; i++)
+				if (sbi->block_cache[i].block !=
+							SQUASHFS_INVALID_BLK)
+					kfree(sbi->block_cache[i].data);
+		if (sbi->fragment)
+			for (i = 0; i < SQUASHFS_CACHED_FRAGMENTS; i++) 
+				SQUASHFS_FREE(sbi->fragment[i].data);
+		kfree(sbi->fragment);
+		kfree(sbi->block_cache);
+		kfree(sbi->read_data);
+		kfree(sbi->read_page);
+		kfree(sbi->uid);
+		kfree(sbi->fragment_index);
+		kfree(sbi->fragment_index_2);
+		kfree(sbi->meta_index);
+		kfree(s->s_fs_info);
+		s->s_fs_info = NULL;
+	}
+}
+
+
+static int squashfs_get_sb(struct file_system_type *fs_type,
+				int flags, const char *dev_name, void *data,
+                                struct vfsmount *mnt)
+{
+	return get_sb_bdev(fs_type, flags, dev_name, data, squashfs_fill_super, mnt);
+}
+
+
+static int __init init_squashfs_fs(void)
+{
+	int err = init_inodecache();
+	if (err)
+		goto out;
+
+	printk(KERN_INFO "squashfs: version 3.0 (2006/03/15) "
+		"Phillip Lougher\n");
+
+	if (!(stream.workspace = vmalloc(zlib_inflate_workspacesize()))) {
+		ERROR("Failed to allocate zlib workspace\n");
+		destroy_inodecache();
+		err = -ENOMEM;
+		goto out;
+	}
+
+	if ((err = register_filesystem(&squashfs_fs_type))) {
+		vfree(stream.workspace);
+		destroy_inodecache();
+	}
+
+out:
+	return err;
+}
+
+
+static void __exit exit_squashfs_fs(void)
+{
+	vfree(stream.workspace);
+	unregister_filesystem(&squashfs_fs_type);
+	destroy_inodecache();
+}
+
+
+static kmem_cache_t * squashfs_inode_cachep;
+
+
+static struct inode *squashfs_alloc_inode(struct super_block *sb)
+{
+	struct squashfs_inode_info *ei;
+	ei = kmem_cache_alloc(squashfs_inode_cachep, SLAB_KERNEL);
+	if (!ei)
+		return NULL;
+	return &ei->vfs_inode;
+}
+
+
+static void squashfs_destroy_inode(struct inode *inode)
+{
+	kmem_cache_free(squashfs_inode_cachep, SQUASHFS_I(inode));
+}
+
+
+static void init_once(void * foo, kmem_cache_t * cachep, unsigned long flags)
+{
+	struct squashfs_inode_info *ei = foo;
+
+	if ((flags & (SLAB_CTOR_VERIFY|SLAB_CTOR_CONSTRUCTOR)) ==
+							SLAB_CTOR_CONSTRUCTOR)
+		inode_init_once(&ei->vfs_inode);
+}
+ 
+
+static int __init init_inodecache(void)
+{
+	squashfs_inode_cachep = kmem_cache_create("squashfs_inode_cache",
+	     sizeof(struct squashfs_inode_info),
+	     0, SLAB_HWCACHE_ALIGN|SLAB_RECLAIM_ACCOUNT,
+	     init_once, NULL);
+	if (squashfs_inode_cachep == NULL)
+		return -ENOMEM;
+	return 0;
+}
+
+
+static void destroy_inodecache(void)
+{
+	if (kmem_cache_destroy(squashfs_inode_cachep))
+		printk(KERN_INFO "squashfs_inode_cache: not all structures "
+			"were freed\n");
+}
+
+
+module_init(init_squashfs_fs);
+module_exit(exit_squashfs_fs);
+MODULE_DESCRIPTION("squashfs, a compressed read-only filesystem");
+MODULE_AUTHOR("Phillip Lougher <phillip@lougher.org.uk>");
+MODULE_LICENSE("GPL");
diff -urN oldtree/fs/squashfs/squashfs.h newtree/fs/squashfs/squashfs.h
--- oldtree/fs/squashfs/squashfs.h	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/squashfs/squashfs.h	2006-07-12 19:01:48.000000000 -0400
@@ -0,0 +1,86 @@
+/*
+ * Squashfs - a compressed read only filesystem for Linux
+ *
+ * Copyright (c) 2002, 2003, 2004, 2005, 2006
+ * Phillip Lougher <phillip@lougher.org.uk>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2,
+ * or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * squashfs.h
+ */
+
+#ifdef CONFIG_SQUASHFS_1_0_COMPATIBILITY
+#undef CONFIG_SQUASHFS_1_0_COMPATIBILITY
+#endif
+
+#ifdef SQUASHFS_TRACE
+#define TRACE(s, args...)	printk(KERN_NOTICE "SQUASHFS: "s, ## args)
+#else
+#define TRACE(s, args...)	{}
+#endif
+
+#define ERROR(s, args...)	printk(KERN_ERR "SQUASHFS error: "s, ## args)
+
+#define SERROR(s, args...)	do { \
+				if (!silent) \
+				printk(KERN_ERR "SQUASHFS error: "s, ## args);\
+				} while(0)
+
+#define WARNING(s, args...)	printk(KERN_WARNING "SQUASHFS: "s, ## args)
+
+static inline struct squashfs_inode_info *SQUASHFS_I(struct inode *inode)
+{
+	return list_entry(inode, struct squashfs_inode_info, vfs_inode);
+}
+
+#if defined(CONFIG_SQUASHFS_1_0_COMPATIBILITY ) || defined(CONFIG_SQUASHFS_2_0_COMPATIBILITY)
+#define SQSH_EXTERN
+extern unsigned int squashfs_read_data(struct super_block *s, char *buffer,
+				long long index, unsigned int length,
+				long long *next_index);
+extern int squashfs_get_cached_block(struct super_block *s, char *buffer,
+				long long block, unsigned int offset,
+				int length, long long *next_block,
+				unsigned int *next_offset);
+extern void release_cached_fragment(struct squashfs_sb_info *msblk, struct
+					squashfs_fragment_cache *fragment);
+extern struct squashfs_fragment_cache *get_cached_fragment(struct super_block
+					*s, long long start_block,
+					int length);
+extern struct address_space_operations squashfs_symlink_aops;
+extern struct address_space_operations squashfs_aops;
+extern struct address_space_operations squashfs_aops_4K;
+extern struct inode_operations squashfs_dir_inode_ops;
+#else
+#define SQSH_EXTERN static
+#endif
+
+#ifdef CONFIG_SQUASHFS_1_0_COMPATIBILITY
+extern int squashfs_1_0_supported(struct squashfs_sb_info *msblk);
+#else
+static inline int squashfs_1_0_supported(struct squashfs_sb_info *msblk)
+{
+	return 0;
+}
+#endif
+
+#ifdef CONFIG_SQUASHFS_2_0_COMPATIBILITY
+extern int squashfs_2_0_supported(struct squashfs_sb_info *msblk);
+#else
+static inline int squashfs_2_0_supported(struct squashfs_sb_info *msblk)
+{
+	return 0;
+}
+#endif
diff -urN oldtree/fs/squashfs/squashfs2_0.c newtree/fs/squashfs/squashfs2_0.c
--- oldtree/fs/squashfs/squashfs2_0.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/squashfs/squashfs2_0.c	2006-07-12 19:01:48.000000000 -0400
@@ -0,0 +1,757 @@
+/*
+ * Squashfs - a compressed read only filesystem for Linux
+ *
+ * Copyright (c) 2002, 2003, 2004, 2005, 2006
+ * Phillip Lougher <phillip@lougher.org.uk>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2,
+ * or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * squashfs2_0.c
+ */
+
+#include <linux/types.h>
+#include <linux/squashfs_fs.h>
+#include <linux/module.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/smp_lock.h>
+#include <linux/slab.h>
+#include <linux/squashfs_fs_sb.h>
+#include <linux/squashfs_fs_i.h>
+#include <linux/buffer_head.h>
+#include <linux/vfs.h>
+#include <linux/init.h>
+#include <linux/dcache.h>
+#include <linux/wait.h>
+#include <linux/zlib.h>
+#include <linux/blkdev.h>
+#include <linux/vmalloc.h>
+#include <asm/uaccess.h>
+#include <asm/semaphore.h>
+
+#include "squashfs.h"
+static int squashfs_readdir_2(struct file *file, void *dirent, filldir_t filldir);
+static struct dentry *squashfs_lookup_2(struct inode *, struct dentry *,
+				struct nameidata *);
+
+static struct file_operations squashfs_dir_ops_2 = {
+	.read = generic_read_dir,
+	.readdir = squashfs_readdir_2
+};
+
+static struct inode_operations squashfs_dir_inode_ops_2 = {
+	.lookup = squashfs_lookup_2
+};
+
+static unsigned char squashfs_filetype_table[] = {
+	DT_UNKNOWN, DT_DIR, DT_REG, DT_LNK, DT_BLK, DT_CHR, DT_FIFO, DT_SOCK
+};
+
+static int read_fragment_index_table_2(struct super_block *s)
+{
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+
+	if (!(msblk->fragment_index_2 = kmalloc(SQUASHFS_FRAGMENT_INDEX_BYTES_2
+					(sblk->fragments), GFP_KERNEL))) {
+		ERROR("Failed to allocate uid/gid table\n");
+		return 0;
+	}
+   
+	if (SQUASHFS_FRAGMENT_INDEX_BYTES_2(sblk->fragments) &&
+					!squashfs_read_data(s, (char *)
+					msblk->fragment_index_2,
+					sblk->fragment_table_start,
+					SQUASHFS_FRAGMENT_INDEX_BYTES_2
+					(sblk->fragments) |
+					SQUASHFS_COMPRESSED_BIT_BLOCK, NULL)) {
+		ERROR("unable to read fragment index table\n");
+		return 0;
+	}
+
+	if (msblk->swap) {
+		int i;
+		unsigned int fragment;
+
+		for (i = 0; i < SQUASHFS_FRAGMENT_INDEXES_2(sblk->fragments);
+									i++) {
+			SQUASHFS_SWAP_FRAGMENT_INDEXES_2((&fragment),
+						&msblk->fragment_index_2[i], 1);
+			msblk->fragment_index_2[i] = fragment;
+		}
+	}
+
+	return 1;
+}
+
+
+static int get_fragment_location_2(struct super_block *s, unsigned int fragment,
+				long long *fragment_start_block,
+				unsigned int *fragment_size)
+{
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	long long start_block =
+		msblk->fragment_index_2[SQUASHFS_FRAGMENT_INDEX_2(fragment)];
+	int offset = SQUASHFS_FRAGMENT_INDEX_OFFSET_2(fragment);
+	struct squashfs_fragment_entry_2 fragment_entry;
+
+	if (msblk->swap) {
+		struct squashfs_fragment_entry_2 sfragment_entry;
+
+		if (!squashfs_get_cached_block(s, (char *) &sfragment_entry,
+					start_block, offset,
+					sizeof(sfragment_entry), &start_block,
+					&offset))
+			goto out;
+		SQUASHFS_SWAP_FRAGMENT_ENTRY_2(&fragment_entry, &sfragment_entry);
+	} else
+		if (!squashfs_get_cached_block(s, (char *) &fragment_entry,
+					start_block, offset,
+					sizeof(fragment_entry), &start_block,
+					&offset))
+			goto out;
+
+	*fragment_start_block = fragment_entry.start_block;
+	*fragment_size = fragment_entry.size;
+
+	return 1;
+
+out:
+	return 0;
+}
+
+
+static struct inode *squashfs_new_inode(struct super_block *s,
+		struct squashfs_base_inode_header_2 *inodeb, unsigned int ino)
+{
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+	struct inode *i = new_inode(s);
+
+	if (i) {
+		i->i_ino = ino;
+		i->i_mtime.tv_sec = sblk->mkfs_time;
+		i->i_atime.tv_sec = sblk->mkfs_time;
+		i->i_ctime.tv_sec = sblk->mkfs_time;
+		i->i_uid = msblk->uid[inodeb->uid];
+		i->i_mode = inodeb->mode;
+		i->i_nlink = 1;
+		i->i_size = 0;
+		if (inodeb->guid == SQUASHFS_GUIDS)
+			i->i_gid = i->i_uid;
+		else
+			i->i_gid = msblk->guid[inodeb->guid];
+	}
+
+	return i;
+}
+
+
+static struct inode *squashfs_iget_2(struct super_block *s, squashfs_inode_t inode)
+{
+	struct inode *i;
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+	unsigned int block = SQUASHFS_INODE_BLK(inode) +
+		sblk->inode_table_start;
+	unsigned int offset = SQUASHFS_INODE_OFFSET(inode);
+	unsigned int ino = SQUASHFS_MK_VFS_INODE(block
+		- sblk->inode_table_start, offset);
+	long long next_block;
+	unsigned int next_offset;
+	union squashfs_inode_header_2 id, sid;
+	struct squashfs_base_inode_header_2 *inodeb = &id.base,
+					  *sinodeb = &sid.base;
+
+	TRACE("Entered squashfs_iget\n");
+
+	if (msblk->swap) {
+		if (!squashfs_get_cached_block(s, (char *) sinodeb, block,
+					offset, sizeof(*sinodeb), &next_block,
+					&next_offset))
+			goto failed_read;
+		SQUASHFS_SWAP_BASE_INODE_HEADER_2(inodeb, sinodeb,
+					sizeof(*sinodeb));
+	} else
+		if (!squashfs_get_cached_block(s, (char *) inodeb, block,
+					offset, sizeof(*inodeb), &next_block,
+					&next_offset))
+			goto failed_read;
+
+	switch(inodeb->inode_type) {
+		case SQUASHFS_FILE_TYPE: {
+			struct squashfs_reg_inode_header_2 *inodep = &id.reg;
+			struct squashfs_reg_inode_header_2 *sinodep = &sid.reg;
+			long long frag_blk;
+			unsigned int frag_size;
+				
+			if (msblk->swap) {
+				if (!squashfs_get_cached_block(s, (char *)
+						sinodep, block, offset,
+						sizeof(*sinodep), &next_block,
+						&next_offset))
+					goto failed_read;
+				SQUASHFS_SWAP_REG_INODE_HEADER_2(inodep, sinodep);
+			} else
+				if (!squashfs_get_cached_block(s, (char *)
+						inodep, block, offset,
+						sizeof(*inodep), &next_block,
+						&next_offset))
+					goto failed_read;
+
+			frag_blk = SQUASHFS_INVALID_BLK;
+			if (inodep->fragment != SQUASHFS_INVALID_FRAG &&
+					!get_fragment_location_2(s,
+					inodep->fragment, &frag_blk, &frag_size))
+				goto failed_read;
+				
+			if((i = squashfs_new_inode(s, inodeb, ino)) == NULL)
+				goto failed_read1;
+
+			i->i_size = inodep->file_size;
+			i->i_fop = &generic_ro_fops;
+			i->i_mode |= S_IFREG;
+			i->i_mtime.tv_sec = inodep->mtime;
+			i->i_atime.tv_sec = inodep->mtime;
+			i->i_ctime.tv_sec = inodep->mtime;
+			i->i_blocks = ((i->i_size - 1) >> 9) + 1;
+			SQUASHFS_I(i)->u.s1.fragment_start_block = frag_blk;
+			SQUASHFS_I(i)->u.s1.fragment_size = frag_size;
+			SQUASHFS_I(i)->u.s1.fragment_offset = inodep->offset;
+			SQUASHFS_I(i)->start_block = inodep->start_block;
+			SQUASHFS_I(i)->u.s1.block_list_start = next_block;
+			SQUASHFS_I(i)->offset = next_offset;
+			if (sblk->block_size > 4096)
+				i->i_data.a_ops = &squashfs_aops;
+			else
+				i->i_data.a_ops = &squashfs_aops_4K;
+
+			TRACE("File inode %x:%x, start_block %x, "
+					"block_list_start %llx, offset %x\n",
+					SQUASHFS_INODE_BLK(inode), offset,
+					inodep->start_block, next_block,
+					next_offset);
+			break;
+		}
+		case SQUASHFS_DIR_TYPE: {
+			struct squashfs_dir_inode_header_2 *inodep = &id.dir;
+			struct squashfs_dir_inode_header_2 *sinodep = &sid.dir;
+
+			if (msblk->swap) {
+				if (!squashfs_get_cached_block(s, (char *)
+						sinodep, block, offset,
+						sizeof(*sinodep), &next_block,
+						&next_offset))
+					goto failed_read;
+				SQUASHFS_SWAP_DIR_INODE_HEADER_2(inodep, sinodep);
+			} else
+				if (!squashfs_get_cached_block(s, (char *)
+						inodep, block, offset,
+						sizeof(*inodep), &next_block,
+						&next_offset))
+					goto failed_read;
+
+			if((i = squashfs_new_inode(s, inodeb, ino)) == NULL)
+				goto failed_read1;
+
+			i->i_size = inodep->file_size;
+			i->i_op = &squashfs_dir_inode_ops_2;
+			i->i_fop = &squashfs_dir_ops_2;
+			i->i_mode |= S_IFDIR;
+			i->i_mtime.tv_sec = inodep->mtime;
+			i->i_atime.tv_sec = inodep->mtime;
+			i->i_ctime.tv_sec = inodep->mtime;
+			SQUASHFS_I(i)->start_block = inodep->start_block;
+			SQUASHFS_I(i)->offset = inodep->offset;
+			SQUASHFS_I(i)->u.s2.directory_index_count = 0;
+			SQUASHFS_I(i)->u.s2.parent_inode = 0;
+
+			TRACE("Directory inode %x:%x, start_block %x, offset "
+					"%x\n", SQUASHFS_INODE_BLK(inode),
+					offset, inodep->start_block,
+					inodep->offset);
+			break;
+		}
+		case SQUASHFS_LDIR_TYPE: {
+			struct squashfs_ldir_inode_header_2 *inodep = &id.ldir;
+			struct squashfs_ldir_inode_header_2 *sinodep = &sid.ldir;
+
+			if (msblk->swap) {
+				if (!squashfs_get_cached_block(s, (char *)
+						sinodep, block, offset,
+						sizeof(*sinodep), &next_block,
+						&next_offset))
+					goto failed_read;
+				SQUASHFS_SWAP_LDIR_INODE_HEADER_2(inodep,
+						sinodep);
+			} else
+				if (!squashfs_get_cached_block(s, (char *)
+						inodep, block, offset,
+						sizeof(*inodep), &next_block,
+						&next_offset))
+					goto failed_read;
+
+			if((i = squashfs_new_inode(s, inodeb, ino)) == NULL)
+				goto failed_read1;
+
+			i->i_size = inodep->file_size;
+			i->i_op = &squashfs_dir_inode_ops_2;
+			i->i_fop = &squashfs_dir_ops_2;
+			i->i_mode |= S_IFDIR;
+			i->i_mtime.tv_sec = inodep->mtime;
+			i->i_atime.tv_sec = inodep->mtime;
+			i->i_ctime.tv_sec = inodep->mtime;
+			SQUASHFS_I(i)->start_block = inodep->start_block;
+			SQUASHFS_I(i)->offset = inodep->offset;
+			SQUASHFS_I(i)->u.s2.directory_index_start = next_block;
+			SQUASHFS_I(i)->u.s2.directory_index_offset =
+								next_offset;
+			SQUASHFS_I(i)->u.s2.directory_index_count =
+								inodep->i_count;
+			SQUASHFS_I(i)->u.s2.parent_inode = 0;
+
+			TRACE("Long directory inode %x:%x, start_block %x, "
+					"offset %x\n",
+					SQUASHFS_INODE_BLK(inode), offset,
+					inodep->start_block, inodep->offset);
+			break;
+		}
+		case SQUASHFS_SYMLINK_TYPE: {
+			struct squashfs_symlink_inode_header_2 *inodep =
+								&id.symlink;
+			struct squashfs_symlink_inode_header_2 *sinodep =
+								&sid.symlink;
+	
+			if (msblk->swap) {
+				if (!squashfs_get_cached_block(s, (char *)
+						sinodep, block, offset,
+						sizeof(*sinodep), &next_block,
+						&next_offset))
+					goto failed_read;
+				SQUASHFS_SWAP_SYMLINK_INODE_HEADER_2(inodep,
+								sinodep);
+			} else
+				if (!squashfs_get_cached_block(s, (char *)
+						inodep, block, offset,
+						sizeof(*inodep), &next_block,
+						&next_offset))
+					goto failed_read;
+
+			if((i = squashfs_new_inode(s, inodeb, ino)) == NULL)
+				goto failed_read1;
+
+			i->i_size = inodep->symlink_size;
+			i->i_op = &page_symlink_inode_operations;
+			i->i_data.a_ops = &squashfs_symlink_aops;
+			i->i_mode |= S_IFLNK;
+			SQUASHFS_I(i)->start_block = next_block;
+			SQUASHFS_I(i)->offset = next_offset;
+
+			TRACE("Symbolic link inode %x:%x, start_block %llx, "
+					"offset %x\n",
+					SQUASHFS_INODE_BLK(inode), offset,
+					next_block, next_offset);
+			break;
+		 }
+		 case SQUASHFS_BLKDEV_TYPE:
+		 case SQUASHFS_CHRDEV_TYPE: {
+			struct squashfs_dev_inode_header_2 *inodep = &id.dev;
+			struct squashfs_dev_inode_header_2 *sinodep = &sid.dev;
+
+			if (msblk->swap) {
+				if (!squashfs_get_cached_block(s, (char *)
+						sinodep, block, offset,
+						sizeof(*sinodep), &next_block,
+						&next_offset))
+					goto failed_read;
+				SQUASHFS_SWAP_DEV_INODE_HEADER_2(inodep, sinodep);
+			} else	
+				if (!squashfs_get_cached_block(s, (char *)
+						inodep, block, offset,
+						sizeof(*inodep), &next_block,
+						&next_offset))
+					goto failed_read;
+
+			if ((i = squashfs_new_inode(s, inodeb, ino)) == NULL)
+				goto failed_read1;
+
+			i->i_mode |= (inodeb->inode_type ==
+					SQUASHFS_CHRDEV_TYPE) ?  S_IFCHR :
+					S_IFBLK;
+			init_special_inode(i, i->i_mode,
+					old_decode_dev(inodep->rdev));
+
+			TRACE("Device inode %x:%x, rdev %x\n",
+					SQUASHFS_INODE_BLK(inode), offset,
+					inodep->rdev);
+			break;
+		 }
+		 case SQUASHFS_FIFO_TYPE:
+		 case SQUASHFS_SOCKET_TYPE: {
+			if ((i = squashfs_new_inode(s, inodeb, ino)) == NULL)
+				goto failed_read1;
+
+			i->i_mode |= (inodeb->inode_type == SQUASHFS_FIFO_TYPE)
+							? S_IFIFO : S_IFSOCK;
+			init_special_inode(i, i->i_mode, 0);
+			break;
+		 }
+		 default:
+			ERROR("Unknown inode type %d in squashfs_iget!\n",
+					inodeb->inode_type);
+			goto failed_read1;
+	}
+	
+	insert_inode_hash(i);
+	return i;
+
+failed_read:
+	ERROR("Unable to read inode [%x:%x]\n", block, offset);
+
+failed_read1:
+	return NULL;
+}
+
+
+static int get_dir_index_using_offset(struct super_block *s, long long 
+				*next_block, unsigned int *next_offset,
+				long long index_start,
+				unsigned int index_offset, int i_count,
+				long long f_pos)
+{
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+	int i, length = 0;
+	struct squashfs_dir_index_2 index;
+
+	TRACE("Entered get_dir_index_using_offset, i_count %d, f_pos %d\n",
+					i_count, (unsigned int) f_pos);
+
+	if (f_pos == 0)
+		goto finish;
+
+	for (i = 0; i < i_count; i++) {
+		if (msblk->swap) {
+			struct squashfs_dir_index_2 sindex;
+			squashfs_get_cached_block(s, (char *) &sindex,
+					index_start, index_offset,
+					sizeof(sindex), &index_start,
+					&index_offset);
+			SQUASHFS_SWAP_DIR_INDEX_2(&index, &sindex);
+		} else
+			squashfs_get_cached_block(s, (char *) &index,
+					index_start, index_offset,
+					sizeof(index), &index_start,
+					&index_offset);
+
+		if (index.index > f_pos)
+			break;
+
+		squashfs_get_cached_block(s, NULL, index_start, index_offset,
+					index.size + 1, &index_start,
+					&index_offset);
+
+		length = index.index;
+		*next_block = index.start_block + sblk->directory_table_start;
+	}
+
+	*next_offset = (length + *next_offset) % SQUASHFS_METADATA_SIZE;
+
+finish:
+	return length;
+}
+
+
+static int get_dir_index_using_name(struct super_block *s, long long
+				*next_block, unsigned int *next_offset,
+				long long index_start,
+				unsigned int index_offset, int i_count,
+				const char *name, int size)
+{
+	struct squashfs_sb_info *msblk = s->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+	int i, length = 0;
+	char buffer[sizeof(struct squashfs_dir_index_2) + SQUASHFS_NAME_LEN + 1];
+	struct squashfs_dir_index_2 *index = (struct squashfs_dir_index_2 *) buffer;
+	char str[SQUASHFS_NAME_LEN + 1];
+
+	TRACE("Entered get_dir_index_using_name, i_count %d\n", i_count);
+
+	strncpy(str, name, size);
+	str[size] = '\0';
+
+	for (i = 0; i < i_count; i++) {
+		if (msblk->swap) {
+			struct squashfs_dir_index_2 sindex;
+			squashfs_get_cached_block(s, (char *) &sindex,
+					index_start, index_offset,
+					sizeof(sindex), &index_start,
+					&index_offset);
+			SQUASHFS_SWAP_DIR_INDEX_2(index, &sindex);
+		} else
+			squashfs_get_cached_block(s, (char *) index,
+					index_start, index_offset,
+					sizeof(struct squashfs_dir_index_2),
+					&index_start, &index_offset);
+
+		squashfs_get_cached_block(s, index->name, index_start,
+					index_offset, index->size + 1,
+					&index_start, &index_offset);
+
+		index->name[index->size + 1] = '\0';
+
+		if (strcmp(index->name, str) > 0)
+			break;
+
+		length = index->index;
+		*next_block = index->start_block + sblk->directory_table_start;
+	}
+
+	*next_offset = (length + *next_offset) % SQUASHFS_METADATA_SIZE;
+	return length;
+}
+
+		
+static int squashfs_readdir_2(struct file *file, void *dirent, filldir_t filldir)
+{
+	struct inode *i = file->f_dentry->d_inode;
+	struct squashfs_sb_info *msblk = i->i_sb->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+	long long next_block = SQUASHFS_I(i)->start_block +
+		sblk->directory_table_start;
+	int next_offset = SQUASHFS_I(i)->offset, length = 0, dirs_read = 0,
+		dir_count;
+	struct squashfs_dir_header_2 dirh;
+	char buffer[sizeof(struct squashfs_dir_entry_2) + SQUASHFS_NAME_LEN + 1];
+	struct squashfs_dir_entry_2 *dire = (struct squashfs_dir_entry_2 *) buffer;
+
+	TRACE("Entered squashfs_readdir_2 [%llx:%x]\n", next_block, next_offset);
+
+	length = get_dir_index_using_offset(i->i_sb, &next_block, &next_offset,
+				SQUASHFS_I(i)->u.s2.directory_index_start,
+				SQUASHFS_I(i)->u.s2.directory_index_offset,
+				SQUASHFS_I(i)->u.s2.directory_index_count,
+				file->f_pos);
+
+	while (length < i_size_read(i)) {
+		/* read directory header */
+		if (msblk->swap) {
+			struct squashfs_dir_header_2 sdirh;
+			
+			if (!squashfs_get_cached_block(i->i_sb, (char *) &sdirh,
+					next_block, next_offset, sizeof(sdirh),
+					&next_block, &next_offset))
+				goto failed_read;
+
+			length += sizeof(sdirh);
+			SQUASHFS_SWAP_DIR_HEADER_2(&dirh, &sdirh);
+		} else {
+			if (!squashfs_get_cached_block(i->i_sb, (char *) &dirh,
+					next_block, next_offset, sizeof(dirh),
+					&next_block, &next_offset))
+				goto failed_read;
+
+			length += sizeof(dirh);
+		}
+
+		dir_count = dirh.count + 1;
+		while (dir_count--) {
+			if (msblk->swap) {
+				struct squashfs_dir_entry_2 sdire;
+				if (!squashfs_get_cached_block(i->i_sb, (char *)
+						&sdire, next_block, next_offset,
+						sizeof(sdire), &next_block,
+						&next_offset))
+					goto failed_read;
+				
+				length += sizeof(sdire);
+				SQUASHFS_SWAP_DIR_ENTRY_2(dire, &sdire);
+			} else {
+				if (!squashfs_get_cached_block(i->i_sb, (char *)
+						dire, next_block, next_offset,
+						sizeof(*dire), &next_block,
+						&next_offset))
+					goto failed_read;
+
+				length += sizeof(*dire);
+			}
+
+			if (!squashfs_get_cached_block(i->i_sb, dire->name,
+						next_block, next_offset,
+						dire->size + 1, &next_block,
+						&next_offset))
+				goto failed_read;
+
+			length += dire->size + 1;
+
+			if (file->f_pos >= length)
+				continue;
+
+			dire->name[dire->size + 1] = '\0';
+
+			TRACE("Calling filldir(%x, %s, %d, %d, %x:%x, %d)\n",
+					(unsigned int) dirent, dire->name,
+					dire->size + 1, (int) file->f_pos,
+					dirh.start_block, dire->offset,
+					squashfs_filetype_table[dire->type]);
+
+			if (filldir(dirent, dire->name, dire->size + 1,
+					file->f_pos, SQUASHFS_MK_VFS_INODE(
+					dirh.start_block, dire->offset),
+					squashfs_filetype_table[dire->type])
+					< 0) {
+				TRACE("Filldir returned less than 0\n");
+				goto finish;
+			}
+			file->f_pos = length;
+			dirs_read++;
+		}
+	}
+
+finish:
+	return dirs_read;
+
+failed_read:
+	ERROR("Unable to read directory block [%llx:%x]\n", next_block,
+		next_offset);
+	return 0;
+}
+
+
+static struct dentry *squashfs_lookup_2(struct inode *i, struct dentry *dentry,
+				struct nameidata *nd)
+{
+	const unsigned char *name = dentry->d_name.name;
+	int len = dentry->d_name.len;
+	struct inode *inode = NULL;
+	struct squashfs_sb_info *msblk = i->i_sb->s_fs_info;
+	struct squashfs_super *sblk = &msblk->sblk;
+	long long next_block = SQUASHFS_I(i)->start_block +
+				sblk->directory_table_start;
+	int next_offset = SQUASHFS_I(i)->offset, length = 0,
+				dir_count;
+	struct squashfs_dir_header_2 dirh;
+	char buffer[sizeof(struct squashfs_dir_entry_2) + SQUASHFS_NAME_LEN];
+	struct squashfs_dir_entry_2 *dire = (struct squashfs_dir_entry_2 *) buffer;
+	int sorted = sblk->s_major == 2 && sblk->s_minor >= 1;
+
+	TRACE("Entered squashfs_lookup [%llx:%x]\n", next_block, next_offset);
+
+	if (len > SQUASHFS_NAME_LEN)
+		goto exit_loop;
+
+	length = get_dir_index_using_name(i->i_sb, &next_block, &next_offset,
+				SQUASHFS_I(i)->u.s2.directory_index_start,
+				SQUASHFS_I(i)->u.s2.directory_index_offset,
+				SQUASHFS_I(i)->u.s2.directory_index_count, name,
+				len);
+
+	while (length < i_size_read(i)) {
+		/* read directory header */
+		if (msblk->swap) {
+			struct squashfs_dir_header_2 sdirh;
+			if (!squashfs_get_cached_block(i->i_sb, (char *) &sdirh,
+					next_block, next_offset, sizeof(sdirh),
+					&next_block, &next_offset))
+				goto failed_read;
+
+			length += sizeof(sdirh);
+			SQUASHFS_SWAP_DIR_HEADER_2(&dirh, &sdirh);
+		} else {
+			if (!squashfs_get_cached_block(i->i_sb, (char *) &dirh,
+					next_block, next_offset, sizeof(dirh),
+					&next_block, &next_offset))
+				goto failed_read;
+
+			length += sizeof(dirh);
+		}
+
+		dir_count = dirh.count + 1;
+		while (dir_count--) {
+			if (msblk->swap) {
+				struct squashfs_dir_entry_2 sdire;
+				if (!squashfs_get_cached_block(i->i_sb, (char *)
+						&sdire, next_block,next_offset,
+						sizeof(sdire), &next_block,
+						&next_offset))
+					goto failed_read;
+				
+				length += sizeof(sdire);
+				SQUASHFS_SWAP_DIR_ENTRY_2(dire, &sdire);
+			} else {
+				if (!squashfs_get_cached_block(i->i_sb, (char *)
+						dire, next_block,next_offset,
+						sizeof(*dire), &next_block,
+						&next_offset))
+					goto failed_read;
+
+				length += sizeof(*dire);
+			}
+
+			if (!squashfs_get_cached_block(i->i_sb, dire->name,
+					next_block, next_offset, dire->size + 1,
+					&next_block, &next_offset))
+				goto failed_read;
+
+			length += dire->size + 1;
+
+			if (sorted && name[0] < dire->name[0])
+				goto exit_loop;
+
+			if ((len == dire->size + 1) && !strncmp(name,
+						dire->name, len)) {
+				squashfs_inode_t ino =
+					SQUASHFS_MKINODE(dirh.start_block,
+					dire->offset);
+
+				TRACE("calling squashfs_iget for directory "
+					"entry %s, inode %x:%x, %lld\n", name,
+					dirh.start_block, dire->offset, ino);
+
+				inode = (msblk->iget)(i->i_sb, ino);
+
+				goto exit_loop;
+			}
+		}
+	}
+
+exit_loop:
+	d_add(dentry, inode);
+	return ERR_PTR(0);
+
+failed_read:
+	ERROR("Unable to read directory block [%llx:%x]\n", next_block,
+		next_offset);
+	goto exit_loop;
+}
+
+
+int squashfs_2_0_supported(struct squashfs_sb_info *msblk)
+{
+	struct squashfs_super *sblk = &msblk->sblk;
+
+	msblk->iget = squashfs_iget_2;
+	msblk->read_fragment_index_table = read_fragment_index_table_2;
+
+	sblk->bytes_used = sblk->bytes_used_2;
+	sblk->uid_start = sblk->uid_start_2;
+	sblk->guid_start = sblk->guid_start_2;
+	sblk->inode_table_start = sblk->inode_table_start_2;
+	sblk->directory_table_start = sblk->directory_table_start_2;
+	sblk->fragment_table_start = sblk->fragment_table_start_2;
+
+	return 1;
+}
diff -urN oldtree/fs/unionfs/AUTHORS newtree/fs/unionfs/AUTHORS
--- oldtree/fs/unionfs/AUTHORS	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/AUTHORS	2006-07-12 19:01:51.000000000 -0400
@@ -0,0 +1,180 @@
+PRIMARY AUTHORS AND MAJOR CONTRIBUTORS TO UNIONFS:
+The primary authors work at the Filesystems and Storage Lab at Stony Brook
+University.   They also currently maintain the package.
+
+* Erez Zadok  <ezk@cs.sunysb.edu>
+- Primary Investigator
+
+* Charles P. Wright  <cwright@cs.sunysb.edu>
+- Primary maintainer (12/2004-Present)
+- Snapshotting support
+- Initial development
+
+* Dave Quigley  <dquigley@fsl.cs.sunysb.edu>
+- 2.6 Port
+- Maintenance (1/2005-Present)
+
+* Arun Krishna Kumar <arunmk@fsl.cs.sunysb.edu>
+- Maintenance (6/2005-12/2005)
+
+* Mohammad Nayyer Zubair
+- Initial development
+- Regression Suite
+
+* Puja Gupta
+- Initial development
+
+* Harikesavan Krishnan
+- Initial development
+
+* Josef "Jeff" Sipek
+- Maintenance (12/2005-Present)
+
+Other contributors:
+* Sai Suman  <suman@pantasys.com>
+January 10, 2005: NFS Export patch
+January 10, 2005: Copyup bug fix.
+
+* Alex de Landgraaf <alex@delandgraaf.com>
+January 10, 2005: Fixes for gcc 2.9.5
+
+* Anton Farygin <rider@altlinux.com>
+February 2, 2005:  Fixes for non-privileged copyup.
+March 2, 2005: vfs_readdir bug in dirhelper.c
+March 2, 2005: Fix copyup on symlinks
+March 2, 2005: Fix handling of failed whiteout lookup
+August 15, 2005: Fix possible deadlock in incgen when memory allocaiton fails.
+
+* Jaspreet Singh  <jsingh@ensim.com>
+February 8, 2005: Use security functions for xattr copyup.
+May 11, 2005: Fix for Xattr copyup on Selinux
+May 12, 2005: Selinux requires valid i_mode before d_instantiate.
+
+* Fabian Franz <fs-bugs@fabian-franz.de>
+February 22, 2005: Hardlinks should have the same inode number.
+February 22, 2005: Device copyup fix.
+February 22, 2005: Identified d_delete problem over tmpfs
+March 2, 2005: vfs_readdir bug in dirhelper.c
+
+* Terry Barnaby <terry1@beam.ltd.uk>
+March 3, 2005: Copy attributes on d_revalidate
+March 2, 2005: Fix for readdir over NFS
+March 16, 2005: Fix for unionfs_dir_llseek
+April 20, 2005: Submitted opaque directory patch, wich the current code
+is inspired by.
+
+* Lucas Correia Villa Real <lucasvr@gobolinux.org>
+March 7, 2005: Makefile uses MODDIR.
+
+* Eduard Bloch <blade@debian.org>
+March 7, 2005: Fix man page sections, improve Makefile
+
+* Fernando Freiregomez <fernando.freiregomez@telefonica.es>
+March 7, 2005: Have snapmerge fix times on created files.
+
+* Markus F.X.J. Oberhumer <markus@oberhumer.com>
+April 18, 2005: Fixes for compilation on AMD64
+
+* Tomas Matejicek
+May 10, 2005: I used his linuxrc as the basis for the Unionfs as a root
+file system instructions.
+March 22, 2006: Little updates and corrections to INSTALL
+
+* Shaya Potter
+July 19, 2005: Symbolic links should not be renamed to whiteout files, as
+that confuses Unionfs later.
+September 2, 2005: Fix copyup checking for mmap.
+October 11, 2005: Deadlock fix.
+October 20, 2005: Improved locking for branch manipulation
+October 20, 2005: Fix for removing opaque directories.
+November 18, 2005: NULL check in lookup_whiteout.
+March 5, 2006: Implemented true mmap
+
+* Jan Engelhardt <jengelh@linux01.gwdg.de>
+July 22, 2005: Support for realpath in unionctl.
+August 1, 2005: Use vprintk instead of vsnprintf/printk combo.
+August 9, 2005: Fix unionctl so that it doesn't truncate "/" to "".
+February 21, 2006: Make unionfs work with the new mutex subsystem
+
+* Malcom Lashley <malc@gentoo.org>
+August 9, 2005: AMD64 compile fixes.
+
+* Eduard Bloch <blade@debian.org>
+August 9, 2005: Debian packaging files.
+
+* Klaus Knopper <unionfs@knopper.net>
+August 22, 2005: Fix from lookup_one_len in unionfs_create.
+September 23, 2005: Fix for unionfs_permission pertaining to
+read only file systems
+
+* Junjiro Okajima <hooanon05@yahoo.co.jp>
+September 21, 2005: Fix for of by one error in KMALLOC.
+September 26, 2005: Fix for d_revalidate.
+September 28, 2005: rmdir fix.
+October 13, 2005: rename fix (Bug #425).
+October 19, 2005: NFS security hole fix.
+November 5, 2005: Fix for race b/t lookup and new_dentry_private_data.
+November 8, 2005: Fix error checking in lookup_backend.
+December 27, 2005: Fixed create whiteout bug, forgotten dput()
+December 27, 2005: Fixed unlink bug, forgotten dput()s
+December 27, 2005: Fixed create bug, forgotten dput(), extra GET_PARENT
+December 27, 2005: Fixed permission bug, creat/open truncates the running
+executable
+December 27, 2005: Properly copyup atime, mtime, and ctime.
+December 28, 2005: Fixed missing DPUT()s in unionfs_lookup_backend
+December 28, 2005: Fixed privileges-related bug in is_opaque_dir
+December 29, 2005: Fixed missing/misplaced DPUT()sg DPUT()
+January 20, 2006: Introduced per-branch nfsro flag (unionctl.c)
+January 22, 2006: Fixed persistant inode code: link, rmdir, shrinking of
+dcache, map validation
+January 24, 2006: Fixed hidden inode not being iput() since ibstart and
+ibend is not updated.
+January 31, 2006: Fixes "pseudo hardlink" via persistent inode
+February 2, 2006: Fixed minor bug in unionfs_create regarding stale atime and
+mtime
+February 5, 2006: Changed get_uin() to read_uid()
+February 10, 2006: Fixed stale inode problem of regression/bug418.sh
+February 21, 2006: Fixed some problems around permission bits of a dir
+March 4, 2006: Fixed bug - broken inode when the target entry is hard-linked
+March 4, 2006: Fixed unionfs_permission on reiserfs and xfs
+March 8, 2006: Fixed xattr "not supported" check in copyup
+April 17, 2006: Fixed link-unlink issue (i_nlink going to 0)
+
+* Bill Nottingham <notting@redhat.com>
+Match 3, 2006: Fixed tmpfs xattrs
+
+* Robert Glowczynski <roglo@op.pl>
+October 6, 2005: Fix for fsync over squashfs.
+
+* Charles Duffy <cduffy@spamcop.net
+October 21, 2005: Compile fix for x86_64
+
+* Tom Young <twyun@twyoung.com>
+October 22, 2005: readdir.sh regression script
+
+* Alessio Curri <alessio.curri@elettra.trieste.it>
+November 11, 2005: Fix for RPM spec file.
+
+* Martin Walter <mawa@uni-freiburg.de>
+November 16, 2005: Fix a thinko in NFS_SECURITY_HOLE.
+
+* Martin Kreiner <m.kreiner@levigo.de>
+January 13, 2006: Introduced mount option nfsperms, and removed NFS_SECURITY_HOLE
+January 20, 2006: Introduced per-branch nfsro flag, and removed nfsperms
+
+* Peeka J. Enberg <penberg@cs.helsinki.fi>
+January 14, 2006: Removed the use of GFP_UNIONFS and replaced it with GFP_KERNEL
+
+* Amnon Aaronsohn <bla@cs.huji.ac.il>
+January 25, 2006: Fixed patch-kernel.sh error msg
+
+* Konstantin Olchanski <olchansk@triumf.ca>
+February 8, 2006: Fixed inode_permission (check for nameidata being null)
+
+* Ed Swierk <eswierk@gmail.com>
+March 4, 2006: Install the unionimap man page along with all the other man pages
+
+* Gregory Haskins <ghaskins@novell.com>
+June 14, 2006: Updated RPM specfile, fixed up Makefiles
+
+* And many more ...
diff -urN oldtree/fs/unionfs/COPYING newtree/fs/unionfs/COPYING
--- oldtree/fs/unionfs/COPYING	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/COPYING	2006-07-12 19:01:51.000000000 -0400
@@ -0,0 +1,373 @@
+This Unionfs-1.0 release is licensed under the terms of the GNU General
+Public License (GPL).
+
+For information on commercial licensing through the SUNY Research
+Foundation, contact Erez Zadok <ezk@cs.sunysb.edu>.
+
+Copyright (c) 2003-2006 Erez Zadok
+Copyright (c) 2003-2006 Charles P. Wright
+Copyright (c) 2005-2006 Josef Sipek
+Copyright (c) 2005      Arun M. Krishnakumar
+Copyright (c) 2005-2006 David P. Quigley
+Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+Copyright (c) 2003      Puja Gupta
+Copyright (c) 2003      Harikesavan Krishnan
+Copyright (c) 2003-2006 Stony Brook University
+Copyright (c) 2003-2006 The Research Foundation of State University of New York
+
+All rights reserved.
+
+THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
+ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+SUCH DAMAGE.
+
+
+
+		    GNU GENERAL PUBLIC LICENSE
+		       Version 2, June 1991
+
+ Copyright (C) 1989, 1991 Free Software Foundation, Inc.
+                       59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+			    Preamble
+
+  The licenses for most software are designed to take away your
+freedom to share and change it.  By contrast, the GNU General Public
+License is intended to guarantee your freedom to share and change free
+software--to make sure the software is free for all its users.  This
+General Public License applies to most of the Free Software
+Foundation's software and to any other program whose authors commit to
+using it.  (Some other Free Software Foundation software is covered by
+the GNU Library General Public License instead.)  You can apply it to
+your programs, too.
+
+  When we speak of free software, we are referring to freedom, not
+price.  Our General Public Licenses are designed to make sure that you
+have the freedom to distribute copies of free software (and charge for
+this service if you wish), that you receive source code or can get it
+if you want it, that you can change the software or use pieces of it
+in new free programs; and that you know you can do these things.
+
+  To protect your rights, we need to make restrictions that forbid
+anyone to deny you these rights or to ask you to surrender the rights.
+These restrictions translate to certain responsibilities for you if you
+distribute copies of the software, or if you modify it.
+
+  For example, if you distribute copies of such a program, whether
+gratis or for a fee, you must give the recipients all the rights that
+you have.  You must make sure that they, too, receive or can get the
+source code.  And you must show them these terms so they know their
+rights.
+
+  We protect your rights with two steps: (1) copyright the software, and
+(2) offer you this license which gives you legal permission to copy,
+distribute and/or modify the software.
+
+  Also, for each author's protection and ours, we want to make certain
+that everyone understands that there is no warranty for this free
+software.  If the software is modified by someone else and passed on, we
+want its recipients to know that what they have is not the original, so
+that any problems introduced by others will not reflect on the original
+authors' reputations.
+
+  Finally, any free program is threatened constantly by software
+patents.  We wish to avoid the danger that redistributors of a free
+program will individually obtain patent licenses, in effect making the
+program proprietary.  To prevent this, we have made it clear that any
+patent must be licensed for everyone's free use or not licensed at all.
+
+  The precise terms and conditions for copying, distribution and
+modification follow.
+
+		    GNU GENERAL PUBLIC LICENSE
+   TERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION
+
+  0. This License applies to any program or other work which contains
+a notice placed by the copyright holder saying it may be distributed
+under the terms of this General Public License.  The "Program", below,
+refers to any such program or work, and a "work based on the Program"
+means either the Program or any derivative work under copyright law:
+that is to say, a work containing the Program or a portion of it,
+either verbatim or with modifications and/or translated into another
+language.  (Hereinafter, translation is included without limitation in
+the term "modification".)  Each licensee is addressed as "you".
+
+Activities other than copying, distribution and modification are not
+covered by this License; they are outside its scope.  The act of
+running the Program is not restricted, and the output from the Program
+is covered only if its contents constitute a work based on the
+Program (independent of having been made by running the Program).
+Whether that is true depends on what the Program does.
+
+  1. You may copy and distribute verbatim copies of the Program's
+source code as you receive it, in any medium, provided that you
+conspicuously and appropriately publish on each copy an appropriate
+copyright notice and disclaimer of warranty; keep intact all the
+notices that refer to this License and to the absence of any warranty;
+and give any other recipients of the Program a copy of this License
+along with the Program.
+
+You may charge a fee for the physical act of transferring a copy, and
+you may at your option offer warranty protection in exchange for a fee.
+
+  2. You may modify your copy or copies of the Program or any portion
+of it, thus forming a work based on the Program, and copy and
+distribute such modifications or work under the terms of Section 1
+above, provided that you also meet all of these conditions:
+
+    a) You must cause the modified files to carry prominent notices
+    stating that you changed the files and the date of any change.
+
+    b) You must cause any work that you distribute or publish, that in
+    whole or in part contains or is derived from the Program or any
+    part thereof, to be licensed as a whole at no charge to all third
+    parties under the terms of this License.
+
+    c) If the modified program normally reads commands interactively
+    when run, you must cause it, when started running for such
+    interactive use in the most ordinary way, to print or display an
+    announcement including an appropriate copyright notice and a
+    notice that there is no warranty (or else, saying that you provide
+    a warranty) and that users may redistribute the program under
+    these conditions, and telling the user how to view a copy of this
+    License.  (Exception: if the Program itself is interactive but
+    does not normally print such an announcement, your work based on
+    the Program is not required to print an announcement.)
+
+These requirements apply to the modified work as a whole.  If
+identifiable sections of that work are not derived from the Program,
+and can be reasonably considered independent and separate works in
+themselves, then this License, and its terms, do not apply to those
+sections when you distribute them as separate works.  But when you
+distribute the same sections as part of a whole which is a work based
+on the Program, the distribution of the whole must be on the terms of
+this License, whose permissions for other licensees extend to the
+entire whole, and thus to each and every part regardless of who wrote it.
+
+Thus, it is not the intent of this section to claim rights or contest
+your rights to work written entirely by you; rather, the intent is to
+exercise the right to control the distribution of derivative or
+collective works based on the Program.
+
+In addition, mere aggregation of another work not based on the Program
+with the Program (or with a work based on the Program) on a volume of
+a storage or distribution medium does not bring the other work under
+the scope of this License.
+
+  3. You may copy and distribute the Program (or a work based on it,
+under Section 2) in object code or executable form under the terms of
+Sections 1 and 2 above provided that you also do one of the following:
+
+    a) Accompany it with the complete corresponding machine-readable
+    source code, which must be distributed under the terms of Sections
+    1 and 2 above on a medium customarily used for software interchange; or,
+
+    b) Accompany it with a written offer, valid for at least three
+    years, to give any third party, for a charge no more than your
+    cost of physically performing source distribution, a complete
+    machine-readable copy of the corresponding source code, to be
+    distributed under the terms of Sections 1 and 2 above on a medium
+    customarily used for software interchange; or,
+
+    c) Accompany it with the information you received as to the offer
+    to distribute corresponding source code.  (This alternative is
+    allowed only for noncommercial distribution and only if you
+    received the program in object code or executable form with such
+    an offer, in accord with Subsection b above.)
+
+The source code for a work means the preferred form of the work for
+making modifications to it.  For an executable work, complete source
+code means all the source code for all modules it contains, plus any
+associated interface definition files, plus the scripts used to
+control compilation and installation of the executable.  However, as a
+special exception, the source code distributed need not include
+anything that is normally distributed (in either source or binary
+form) with the major components (compiler, kernel, and so on) of the
+operating system on which the executable runs, unless that component
+itself accompanies the executable.
+
+If distribution of executable or object code is made by offering
+access to copy from a designated place, then offering equivalent
+access to copy the source code from the same place counts as
+distribution of the source code, even though third parties are not
+compelled to copy the source along with the object code.
+
+  4. You may not copy, modify, sublicense, or distribute the Program
+except as expressly provided under this License.  Any attempt
+otherwise to copy, modify, sublicense or distribute the Program is
+void, and will automatically terminate your rights under this License.
+However, parties who have received copies, or rights, from you under
+this License will not have their licenses terminated so long as such
+parties remain in full compliance.
+
+  5. You are not required to accept this License, since you have not
+signed it.  However, nothing else grants you permission to modify or
+distribute the Program or its derivative works.  These actions are
+prohibited by law if you do not accept this License.  Therefore, by
+modifying or distributing the Program (or any work based on the
+Program), you indicate your acceptance of this License to do so, and
+all its terms and conditions for copying, distributing or modifying
+the Program or works based on it.
+
+  6. Each time you redistribute the Program (or any work based on the
+Program), the recipient automatically receives a license from the
+original licensor to copy, distribute or modify the Program subject to
+these terms and conditions.  You may not impose any further
+restrictions on the recipients' exercise of the rights granted herein.
+You are not responsible for enforcing compliance by third parties to
+this License.
+
+  7. If, as a consequence of a court judgment or allegation of patent
+infringement or for any other reason (not limited to patent issues),
+conditions are imposed on you (whether by court order, agreement or
+otherwise) that contradict the conditions of this License, they do not
+excuse you from the conditions of this License.  If you cannot
+distribute so as to satisfy simultaneously your obligations under this
+License and any other pertinent obligations, then as a consequence you
+may not distribute the Program at all.  For example, if a patent
+license would not permit royalty-free redistribution of the Program by
+all those who receive copies directly or indirectly through you, then
+the only way you could satisfy both it and this License would be to
+refrain entirely from distribution of the Program.
+
+If any portion of this section is held invalid or unenforceable under
+any particular circumstance, the balance of the section is intended to
+apply and the section as a whole is intended to apply in other
+circumstances.
+
+It is not the purpose of this section to induce you to infringe any
+patents or other property right claims or to contest validity of any
+such claims; this section has the sole purpose of protecting the
+integrity of the free software distribution system, which is
+implemented by public license practices.  Many people have made
+generous contributions to the wide range of software distributed
+through that system in reliance on consistent application of that
+system; it is up to the author/donor to decide if he or she is willing
+to distribute software through any other system and a licensee cannot
+impose that choice.
+
+This section is intended to make thoroughly clear what is believed to
+be a consequence of the rest of this License.
+
+  8. If the distribution and/or use of the Program is restricted in
+certain countries either by patents or by copyrighted interfaces, the
+original copyright holder who places the Program under this License
+may add an explicit geographical distribution limitation excluding
+those countries, so that distribution is permitted only in or among
+countries not thus excluded.  In such case, this License incorporates
+the limitation as if written in the body of this License.
+
+  9. The Free Software Foundation may publish revised and/or new versions
+of the General Public License from time to time.  Such new versions will
+be similar in spirit to the present version, but may differ in detail to
+address new problems or concerns.
+
+Each version is given a distinguishing version number.  If the Program
+specifies a version number of this License which applies to it and "any
+later version", you have the option of following the terms and conditions
+either of that version or of any later version published by the Free
+Software Foundation.  If the Program does not specify a version number of
+this License, you may choose any version ever published by the Free Software
+Foundation.
+
+  10. If you wish to incorporate parts of the Program into other free
+programs whose distribution conditions are different, write to the author
+to ask for permission.  For software which is copyrighted by the Free
+Software Foundation, write to the Free Software Foundation; we sometimes
+make exceptions for this.  Our decision will be guided by the two goals
+of preserving the free status of all derivatives of our free software and
+of promoting the sharing and reuse of software generally.
+
+			    NO WARRANTY
+
+  11. BECAUSE THE PROGRAM IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY
+FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN
+OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES
+PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED
+OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS
+TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE
+PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING,
+REPAIR OR CORRECTION.
+
+  12. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
+WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND/OR
+REDISTRIBUTE THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES,
+INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING
+OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED
+TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY
+YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER
+PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE
+POSSIBILITY OF SUCH DAMAGES.
+
+		     END OF TERMS AND CONDITIONS
+
+	    How to Apply These Terms to Your New Programs
+
+  If you develop a new program, and you want it to be of the greatest
+possible use to the public, the best way to achieve this is to make it
+free software which everyone can redistribute and change under these terms.
+
+  To do so, attach the following notices to the program.  It is safest
+to attach them to the start of each source file to most effectively
+convey the exclusion of warranty; and each file should have at least
+the "copyright" line and a pointer to where the full notice is found.
+
+    <one line to give the program's name and a brief idea of what it does.>
+    Copyright (C) <year>  <name of author>
+
+    This program is free software; you can redistribute it and/or modify
+    it under the terms of the GNU General Public License as published by
+    the Free Software Foundation; either version 2 of the License, or
+    (at your option) any later version.
+
+    This program is distributed in the hope that it will be useful,
+    but WITHOUT ANY WARRANTY; without even the implied warranty of
+    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+    GNU General Public License for more details.
+
+    You should have received a copy of the GNU General Public License
+    along with this program; if not, write to the Free Software
+    Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+
+
+Also add information on how to contact you by electronic and paper mail.
+
+If the program is interactive, make it output a short notice like this
+when it starts in an interactive mode:
+
+    Gnomovision version 69, Copyright (C) year name of author
+    Gnomovision comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
+    This is free software, and you are welcome to redistribute it
+    under certain conditions; type `show c' for details.
+
+The hypothetical commands `show w' and `show c' should show the appropriate
+parts of the General Public License.  Of course, the commands you use may
+be called something other than `show w' and `show c'; they could even be
+mouse-clicks or menu items--whatever suits your program.
+
+You should also get your employer (if you work as a programmer) or your
+school, if any, to sign a "copyright disclaimer" for the program, if
+necessary.  Here is a sample; alter the names:
+
+  Yoyodyne, Inc., hereby disclaims all copyright interest in the program
+  `Gnomovision' (which makes passes at compilers) written by James Hacker.
+
+  <signature of Ty Coon>, 1 April 1989
+  Ty Coon, President of Vice
+
+This General Public License does not permit incorporating your program into
+proprietary programs.  If your program is a subroutine library, you may
+consider it more useful to permit linking proprietary applications with the
+library.  If this is what you want to do, use the GNU Library General
+Public License instead of this License.
diff -urN oldtree/fs/unionfs/ChangeLog newtree/fs/unionfs/ChangeLog
--- oldtree/fs/unionfs/ChangeLog	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/ChangeLog	2006-07-12 19:01:51.000000000 -0400
@@ -0,0 +1,4010 @@
+2006-07-07  David P. Quigley  <dpquigl@tycho.nsa.gov>
+
+	* Makefile: Cleanedup install and uninstall targets.
+
+2006-07-03  David P. Quigley  <dpquigl@tycho.nsa.gov>
+
+	* copyup.c (copyup_xattrs): removed LSM hooks in function. They
+	are not needed since the only time we access xattrs are with the
+	vfs_*xattr functions which already have the checks. In addition
+	replaced makeshift setxattr code with appropriate vfs_setxattr
+	call.
+
+2006-06-29  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* misc/runtests.sh: Fixed inverted logic condition
+
+	* misc/runtests.sh: If build fails, provide an fallback option
+
+2006-06-29  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* Makefile, Makefile.kernel, commonfops.c, main.c, super.c,
+	unionfs.h: Updated to make 2.6.18-rc happy.
+
+2006-06-26  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* NEWS: Unionfs 1.3, copied over 1.1.5 release news
+
+	* Makefile, Makefile.kernel: Unionfs 1.3, supports 2.6.17 only
+
+2006-06-26  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* main.c, super.c, unionfs.h: use '#ifdef CONFIG_EXPORTFS'
+
+2006-06-22  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* persistent_inode.c: fix tiny bugs in __write_uin() and
+	  read_uin(), use __fread() in get_lin().
+	* super.c: new export functions. un-tested 64 bit environment
+	  (pointer and inode number).
+
+2006-06-14  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* man/uniondbg.8, man/unionimap.8: newline after .IP "-foo" to
+	make man happy
+
+2006-06-14  Gregory Haskins  <ghaskins@novell.com>
+
+	* Makefile, utils/Makefile, rpm/unionfs.spec: Updated RPM specfile,
+	fixed up Makefiles
+
+2006-06-11  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* unionfs.h: Fixed silly typo
+
+2006-06-10  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* unionfs.h (copy_inode_size): Removed copy_inode_size because it
+	was (1) ugly, and (2) unnecessary; the code can easily be in the two
+	functions that used it
+
+2006-06-09  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* Makefile: Up the version number
+
+	* README, NEWS: Sync with the 1.2 branch
+
+	* Makefile, Makefile.kernel, unionfs.h: Define supported version in
+	the Makefile; added kvers make target to display the supported
+	kernel version
+
+	* docs/OO-INDEX, docs/versions.txt: Document describing the new
+	versioning scheme
+
+2006-06-09  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* .cvsignore: ignore cscope.out
+
+2006-06-05  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* Makefile: fixed tarball target
+
+2006-06-02  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* docs/00-INDEX, docs/new_locking.txt: Proposed design for new locking
+	mechanism (work-in-progress)
+
+2006-06-01  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* branchman.c: Added comments
+	* commonfops.c: Added comments
+	* docs/locks.txt: Beginnings of a comprehensive file on lock placement
+	in unionfs
+
+2006-06-01  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* super.c: Remove reference to split-view caches
+
+	* unionfs_debug.h: Small touch ups
+
+2006-06-01  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* unionfs_debug.h: Finished debuging code (print entry/exit)
+
+2006-06-01  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* misc/runtests.sh: Nicified the output of the script
+
+2006-06-01  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* misc/runtests.sh: Little cleanup
+
+2006-06-01  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* misc/nightly-build.sh, misc/runtests.sh: Move unionfs build to after
+	reboot; added code to do test builds with different EXTRACFLAGS
+
+2006-05-31  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* docs/debug.c, docs/debug.txt: Wrong file extension
+
+2006-05-31  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* .cvsignore: ignore vi/vim swap files
+
+	* match-dget.pl, match-iget.pl, match-malloc.pl, misc/match-dget.pl,
+	misc/match-iget.pl, misc/match-malloc.pl: Moved debug scripts to misc/
+
+
+2006-05-31  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* INSTALL, Makefile.kernel, commonfops.c, copyup.c, dentry.c,
+	dirfops.c, file.c, inode.c, lookup.c, main.c, print.c, rdstate.c,
+	rename.c, subr.c, super.c, unionfs.h, unionfs_debug.h,
+	unionfs_macros.h, unlink.c, xattr.c: Simplified & cleaned up debugging
+	support
+
+2006-05-31  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* .cvsignore, utils/.cvsignore: updated ignore files
+
+2006-05-30  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* utils/Makefile: Removed some variable declarations
+	* Makefile: Passed variables down to lower file
+
+2006-05-30  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* utils/: Moved all user mode programs into this directory
+	* usils/Makefile: New Makefile for this directory
+	* Makefile: Changed file to call utils/Makefile for user mode program
+	related operations.
+
+2006-05-30  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* unionfs_macros.h (stohs{,_index}, ftohf{,_index}, itohi{,_index}):
+	make objects passed const - make GCC 4.0 happy
+
+2006-05-30  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* man/unionctl.2: Branch deletion and file querying
+
+2006-05-30  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* unionfs.h, unionfs_imap.h, unionfs_macros.h, unionfs_debugmacros.h:
+	simplified the macro files
+
+	* unlink.c, rdstate.c: minor cleanup
+
+2006-05-30  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* mmap.c (sync_page, unionfs_sync_page): Merged the two functions,
+	since (1) sync_page might be confused with kernel's own sync_page,
+	and (2) our sync_page did not do much beside an if and a function call
+
+2006-05-30  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* regression/rename-501.sh: remove obsoleted debug lines.
+
+2006-05-30  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* man/unionctl.2: About 70% of the manpage. Still need to describe
+	branch deletion, and file querying.
+
+2006-05-29  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* inode.c (inode_permission): Ignore EROFS returned by all but the
+	first branch
+
+2006-05-29  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* inode.c, rename.c, subr.c, unionfs.h: Several rename fixes by
+	Junjiro to comply with the newly established rename semantics (see
+	docs/rename.txt); includes new make_dir_opaque() function
+
+2006-05-25  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* docs/rename.txt: Enumerated thought all the possible cases of
+	logically/physicaly empty to logically/physicaly empty directories
+
+2006-05-25  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* docs/rename.txt: Rename "matrix" documented
+
+2006-05-25  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* copyup.c (copyup_named_dentry): Removed several hard-coded values
+
+	* main.c: same as above
+
+	* mmap.c (unionfs_do_readpage): same as above
+
+	* print.c (fist_print_inode, fist_print_file, __fist_print_dentry):
+	same as above
+
+	* unionimap.c (create_forwardmap, check_if_entry_exists,
+	print_forwardmap): same as above
+
+	* misc/runtests.sh: build torture tests before running them
+
+2006-05-25  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* unionfs.h: Bail out if KERNEL_VERSION is less than 2.6.17
+	(EXTRAVERSION happens to be conveniently ignored)
+
+2006-05-24  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* misc/nightly-build.sh: Make /cvs/cvsupdate after we have
+	successfully updated and re-exec-ed the script
+
+2006-05-24  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* misc/nightly-build.sh: Make kernel build on 2 CPUs
+
+2006-05-24  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* rename.c: add new function may_rename_dir() to check the rename src
+	  dir is empty or not. After rename succeeds, decrement its generation
+	  in order to force lookup at next time.
+	* regression/rename-501.sh: add test patterns.
+
+2006-05-23  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_mkdir): __dir_opaque whiteouts' creation mode
+	should not be hardcoded to an octal value
+
+	* mmap.c (unionfs_sync_page): Removed leftover line of code
+
+2006-05-22  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* regression/lookout-opaque.sh: added option to useradd so that userdelete will work
+	* regression/unlink-whiteout.sh: removed bin prefix from touch and
+	unlink
+
+2006-05-20  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* rename.c: fold too long line.
+	* subr.c: remove unused create_whiteout_parent() which was disabled by
+	  '#if 0 ... #endif'
+	* unionfs.h: remove the declaration of the unused function
+
+2006-05-19  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* regression/rename-501.sh: test for Bug #501
+	* rename.c (do_rename, unionfs_rename_whiteout, __rename_all,
+	  __rename_all_revert, __rename_all_clobber, unionfs_rename_all):
+	  create the whiteout for rename_src under the old parent dir.
+	* subr.c (create_whiteout_parent): remove create_whiteout_parent()
+	  temporary.
+
+2006-05-17  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* mmap.c (unionfs_commit_write, unionfs_copy_block): Removed incorrect
+	comment, removed completely useless function (unionfs_copy_block)
+
+2006-05-17  Shaya Potter <spotter@cs.columbia.edu>
+
+	* mmap.c (unionfs_commit_write): Shaya's commit_write fix
+
+2006-05-17  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* mmap.c (sync_page, unionfs_sync_page): Compiles with
+	2.6.17-rc3-git18
+
+2006-05-03  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* subr.c (superio_store): Fixed bizarre printk format
+
+2006-04-23  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* main.c (unionfs_parse_options, parse_dirs_option): Fixed refcounting
+	leak (if a branch dir doesn't exist during mount, the previously
+	lookedup branches were not freed properly)
+
+2006-04-17  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* unlink.c (unionfs_unlink_whiteout): Fixed link-unlink issue (i_nlink
+	going to 0)
+
+2006-04-14  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* BUGS, man/unionfs.4: Added warning of dire consequences (Bug #256)
+
+	* README: Mention latest 2.6 kernel only
+
+2006-04-11  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* file.c (__unionfs_read, unionfs_read): Fixed mtime being incorrectly
+	updated on access
+
+2006-03-29  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* main.c (parse_dirs_option): On error, NULL the pointers after free
+
+2006-03-22  Tomas Matejicek  <tomas@linux-live.org>
+
+	* INSTALL: Little updates and corrections
+
+2006-03-21  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* copyup.c, super.c, main.c, unionfs.h: removed references to
+	COPYUP_CURRENT_USER and copyup mount option since we only have one
+	copyup mode now.
+
+2006-03-21  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* commonfops.c, dirfops.c, inode.c, main.c, stale_inode.c: Remove old
+	checks for kernel version - it's always going to be the latest only
+
+2006-03-11  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* .cvsignore: Ignore generated files *.o.cmd (mmap and malloc)
+
+2006-03-08  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* copyup.c (copyup_xattr): Fixed xattr "not supported" check
+
+2006-03-07  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* misc/preprocess.pl: added UNIONFS_MMAP to list of symbols.
+
+2006-03-07  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* file.c: forgot to put an ifndef around __do_mmap
+	* mmap.c: forgot to add this file to cvs
+
+2006-03-07  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* file.c (__unionfs_read, __unionfs_write): code that was removed
+	by Shaya's patch which we fall back on if the person chooses not to
+	compile with UNIONFS_MMAP
+	* copyup.c, file.c, inode.c, main.c, super.c: Ifdefed around Shaya's
+	code and extracted some code to support not using UNIONFS_MMAP
+	* Makefile: added mmap.c
+	* INSTALL: added comment for UNIONFS_MMAP
+	* Note this code hasn't been fully tested yet and is considered highly
+	experimental.
+
+2006-03-05  Shaya Potter <spotter@cs.columbia.edu>
+
+	* copyup.c, file.c, inode.c, main.c, super.c: Implemented true mmap
+
+2006-03-04  Ed Swierk  <eswierk@gmail.com>
+
+	* Makefile: Install the unionimap man page along with all the other
+	man pages
+
+2006-03-04  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* inode.c (unionfs_permission): Fixed unionfs_permission on reiserfs
+	and xfs
+
+2006-03-04  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* main.c (unionfs_interpose): Fixed bug - when the target entry is
+	hard-linked, the unionfs inode will be broken
+
+2006-03-03  Bill Nottingham <notting@redhat.com>
+
+	* unionfs.h: added extern for temp vfs_listxattr function
+	* copyup.c: removed old style listxattr code and replaced
+	it with vfs_listxattr
+	* xattr.c: removed locks from all xattr functions since kernel
+	documentation says that getxattr and listxattr don't require
+	a lock and since vfs_setxattr and vfs_removexattr take the
+	mutex on the inode we get a deadlock if we lock the inode
+	before hand.
+
+2006-02-21  Jan Engelhardt  <jengelh@linux01.gwdg.de>
+
+	* ALL: Make unionfs work with the new mutex subsystem
+
+2006-02-21  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* dirhelper.c (delete_whiteouts): Use superio_*; small cleanup
+
+	* lookup.c (is_opaque_dir): Use superio_*
+
+	* subr.c (superio_store, superio_revert): Added superio_store and
+	superio_revert
+
+	* unionfs.h: Added struct superio
+
+	* unlink.c (unionfs_rmdir_all): Check return value of delete_whiteouts
+
+2006-02-20  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Makefile: Increment version number require 2.6.16.
+
+	* Makefile: Increment version number.
+	* INSTALL, NEWS: Add new define options.
+
+2006-02-20  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* man/unionfs.4: removed all references to copyup mount option.
+
+2006-02-20  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* main.c, rename.c, unionfs.h, super.c, unlink.c: Moved
+	DELETE_WHITEOUT to the default mode for delete and wrapped all
+	DELETE_ALL code in ifdefs.
+	* unlink.c (unionfs_unlink_whitout): Rewrote this function to use
+	unlink/create instead of the atomic rename operation we used before.
+	* misc/preprocess.pl: added entry for UNIONFS_DELETE_ALL ifdef.
+
+2006-02-15  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* main.c (unionfs_read_super, unionfs_d_alloc_root): Created a "fake"
+	d_alloc_root
+
+2006-02-15  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* unionfs_debugmacros.h: Cleanup; removed unused __FILE__, etc.
+	parameters from inline functions; removed unnecessary macro wrappers
+
+	* commonfops.c (unionfs_file_revalidate, unionfs_open): C99 doesn't
+	allow mixing of code and variable declarations
+
+2006-02-13  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* file.c, unionfs.h: Use simpler cpp directives.
+	* misc/preprocess.pl: Simple preprocessor for stripping #ifdefs and
+	KERNEL_VERSION defines.
+
+2006-02-12  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* branchman.c, commonfops.c, dentry.c, lookup.c, main.c,
+	persistent_inode.c, super.c, unionfs.h, unionfs_debugmacros.h:
+	Removed inline objects from unionfs_inode_info, unionfs_dentry_info,
+	unionfs_sb_info, and unionfs_file_info
+
+	* unionfs_macros.h: Removed inline objects from unionfs_inode_info,
+	unionfs_dentry_info, unionfs_sb_info, and unionfs_file_info; Replaced
+	many macros by inline functions
+
+2006-02-10  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* dentry.c (unionfs_d_revalidate): Merges two auto variables 'err' and
+	'invalid'. And also checks the return value of unionfs_lookup_backend
+	correctly. It solves the stale inode problem of regression/bug418.sh
+
+	* lookup.c (unionfs_partial_lookup): Same as above
+
+2006-02-09  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionfs.h: reverted change
+
+2006-02-09  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* super.c (unionfs_show_options): removed reference to mounter.
+
+	* unionfs.h: removed reference to mounter.
+
+2006-02-08  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* dirhelper.c (delete_whiteouts): Use WHLEN instead of 4
+
+	* persisten_inode.c (__fread, __fwrite): Make sparse happy
+
+	* unionfs.h, unionfs_debugmarcros.h (__dtopd): Misc cleanups
+
+2006-02-08  Konstantin Olchanski  <olchansk@triumf.ca>
+
+	* inode.c (inode_permission): nameidata can be null, add checks for it
+
+2006-02-08  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* copyup.c (copyup_permissions): Removed code referencing the
+	copyup=mounter option.
+	* main.c (unionfs_parse_options): same as above
+	* unionfs.h (unionfs_sb_info): same as above.
+
+2006-02-07  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* INSTALL: Added fistdev.mk description for UNIONFS_IMAP.
+
+2006-02-07  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* copyup.c (copyup_named_dentry,create_parent_named): ifdefed
+	persistent inode code.
+	* dirfops.c (unionfs_filldir): Same as first.
+	* main.c (unionfs_parse_options): Same as first.
+	* persistent_inode.c: ifdefed out entire file.
+	* super.c (unionfs_put_super): Same as two lines up.
+	* unionfs.h: ifdefed persistent inode variables from unionfs_sb_info
+	and the externed functions.
+
+
+2006-02-05  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* copyup.c (copyup_named_dentry): Any error, not only ENOSPC/EDQUOT,
+	should unlink the partial copyup
+
+	* ChangeLog: Clarified the meaning of a changelog entry
+
+2006-02-05  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* main.c (parse_dirs_option): Added a check for people attempting to
+	make the left most branch read-only
+
+2006-02-05  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* copyup.c (copyup_named_dentry): If the copyup failed because of
+	quota or lack of disk space, unlink the partial copyup
+
+	* rename.c (unionfs_rename_all, unionfs_rename_whiteout): Fixed rename
+	which was copying up to all rw branches to the left of the source
+
+2006-02-05  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionimap.c: we shouldn't be using such a complex open
+	statement when the creat function is exactly what we are doing
+	with open
+
+2006-02-05  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* dirfops.c (unionfs_filldir): With current get_uin() is hard to
+	distinguish the error from success. Writing persistent ino files
+	may meet the file size limit. Now, users should be careful about
+	filesystem quota.
+
+	* main.c (unionfs_interpose): See above
+
+	* persistent_inode.c (__fread, __fwrite, parse_imap_option, __get_uid,
+	__read_uin, __write_uin, get_uin, read_uin): See above
+
+	* unionfs.h: See above
+
+2006-02-05  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionimap.c: Modified open statements to create the files
+	with u+wr instead of u+wrx
+
+2006-02-04  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* README: Remove reference to 2.4 as we don't support it anymore
+
+2006-02-04  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* debug_malloc.c: Moved all malloc debugging functions to this file.
+
+	* Makefile: added debug_malloc.o to object list.
+
+	* persistent_inode.c (remove_map): removed a decrement that wasent
+	needed.
+
+2006-02-04  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_create): Fixed uid/gid/mode not being reset when
+	file is created by renaming a whiteout
+
+2006-02-02  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* inode.c (unionfs_create): Fixed minor bug regarding stale atime and
+	mtime
+
+2006-02-01  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionfs.h: Removed usi_fsnum_table since its not used and makes
+	no sense to have.
+
+	* persistent_inode.c: Removed usi_fsnum_table code. Added remove_map
+
+	* unionimap.c: Fixed code that prints forward maps since it was always
+	printing 0 for the inode number.
+
+2006-01-31  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* dentry.c (unionfs_d_revalidate): Get the hidden inode, not the
+	hidden dentry's inode; fixes "pseudo hardlink" via persistent inode
+
+2006-01-25  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* Makefile, Makefile.kernel: Incremented version
+
+	* NEWS, Makefile, Makefile.kernel: Version 1.1.2
+
+2006-01-25  Amnon Aaronsohn  <bla@cs.huji.ac.il>
+
+	* patch-kernel.sh: Fixed patch-kernel.sh error msg
+
+2006-01-24  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* subr.c (create_whiteout_parent): Hidden inode is not iput() since
+	ibstart and ibend is not updated.
+
+2006-01-22  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* copyup.c (copyup_named_dentry, create_parents_named): Fixed
+	persistant inode code: link, rmdir, shrinking of dcache, map validation
+
+	* main.c (copyup_xattrs): Fixed persistant inode code: link, rmdir,
+	shrinking of dcache, map validation
+
+	* persistent_inode.c (__fread, __fread, verify_forwardmap,
+	verify_reversemap, init_imap_data, parse_imap_option, __get_uin,
+	__write_uin, get_uin, write_uin): Fixed persistant inode code: link,
+	rmdir, shrinking of dcache, map validation
+
+	* unionfs.h: Fixed persistant inode code: link, rmdir, shrinking of
+	dcache, map validation
+
+2006-01-20  Martin Kreiner  <m.kreiner@levigo.de>
+
+	* BUGS, INSTALL, man/unionctl.8, man/unionfs.4: Documentation update
+
+	* branchman.c (unionfs_ioctl_addbranch, unionfs_ioctl_rdwrbranch):
+	Per branch nfsro option
+
+	* inode.c (inode_permission): Per branch nfsro option
+
+	* main.c (parse_dirs_option, unionfs_parse_options): Per branch nfsro
+	option
+
+	* super.c (unionfs_show_options): Per branch nfsro option
+
+	* unionctl.c (__usage, parse_rw, parse_options, dump_branches, main):
+	Per branch nfsro option
+
+	* unionfs.h: Per branch nfsro option
+
+2006-01-20  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* branchman.c (unionfs_ioctl_addbranch, unionfs_ioctl_delbranch):
+	Inode refcount debugging tool calls
+
+	* copyup.c (create_parents_named):Inode refcount debugging tool calls
+
+	* dentry.c (unionfs_d_revalidate, unionfs_d_iput):Inode refcount
+	debugging tool calls
+
+	* inode.c (unionfs_link): Inode refcount debugging tool calls
+
+	* main.c (unionfs_interpose, unionfs_reinterpose, unionfs_igrab,
+	unionfs_iput, unionfs_iget): Inode refcount debugging helper code;
+	Fixed init of atomic_t
+
+	* subr.c (create_whiteout_parent, unionfs_refresh_hidden_dentry):
+	Inode refcount debugging tool calls
+
+	* super.c (unionfs_clear_inode): Inode refcount debugging tool calls
+
+	* unionfs.h: Definitions for IGET, IGRAB, and IPUT
+
+	* match-iget.pl: Inode refcount debugging code output "matcher"
+
+2006-01-17  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* main.c, unionfs.h: Added #if'd kzalloc for kernels older than 2.6.14
+
+2006-01-14  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* branchman.c: Replaced pairs of KMALLOC and memset calls to
+	KZALLOC calls.
+
+	* commonfops.c: Replaced pairs of KMALLOC and memset calls to
+	KZALLOC calls.
+
+	* copyup.c: Replaced pairs of KMALLOC and memset calls to
+	KZALLOC calls.
+
+	* main.c: Replaced pairs of KMALLOC and memset calls to
+	KZALLOC calls.
+
+	* persistent_inode.c: Replaced pairs of KMALLOC and memset calls to
+	KZALLOC calls.
+
+	* super.c: Replaced pairs of KMALLOC and memset calls to
+	KZALLOC calls.
+
+
+2006-01-14  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* unionfs.h: Fixed unionfs_kmalloc prototype
+
+	* main.c (unionfs_kzalloc, unionfs_kmalloc): Use kzalloc, not
+	the non-existent kzmalloc
+
+2006-01-14  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* AUTHORS: added entry for Peeka J. Enberg
+
+2006-01-14  Peeka J. Enberg  <penberg@cs.helsinki.fi>
+
+	* ALL: Changed all use of GFP_UNIONFS to GFP_KERNEL
+
+	* unionfs.h: Removed definition for GFP_UNIONFS
+
+2006-01-14  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionfs.h : Included defines for KZMALLOC and extern for
+	unionfs_kzalloc. NOTE: This change makes the minimum kernel
+	version for unionfs 2.6.14.
+
+	* main.c (unionfs_kmalloc): Changed prototype to use the
+	actual type of GFP_KERNEL instead of int and removed a
+	(void *) since the kernel coding conventions say that this
+	is not necessary.
+
+	* main.c (unionfs_kzalloc): New wrapper function to track
+	kzallocs when debugging.
+
+2006-01-13  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* BUGS, INSTALL: Removed all references to NFS_SECURITY_HOLE
+
+2006-01-13  Martin Kreiner  <m.kreiner@levigo.de>
+
+	* inode.c (inode_permission): Introduce nfsperms mount option and
+	remove NFS_SECURITY_HOLE
+
+	* main.c (unionfs_dentry_info): Introduce nfsperms mount option and
+	remove NFS_SECURITY_HOLE
+
+	* super.c (unionfs_show_options): Introduce nfsperms mount option and
+	remove NFS_SECURITY_HOLE
+
+	* unionfs.h:Introduce nfsperms mount option and
+	remove NFS_SECURITY_HOLE
+
+2006-01-12  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* fist.h: Code moved to unionfs.h, unionfs_macros.h, and
+	unionfs_debugmacros.h
+
+	* ALL, misc/*.c: Removed all references to fist.h
+
+2006-01-09  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* rename.c (__rename_all{,_unlink,_revert,_clobber}): Make functions
+	static
+
+2006-01-08  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* rename.c (unionfs_rename_all,lookup_whiteout): Split up the
+	nearly 300 line unionfs_rename_all function into several more
+	understandable "double underscore" functions; removed get_whname
+	it used __getname which allocates a whole page, updated
+	lookup_whiteout to use alloc_whname instead
+
+	* lookup.c (unionfs_lookup_backend): Use this should be the last
+	alloc_whname patch
+
+2006-01-07  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* file.c: Use unlocked_ioctl iff the kernel is 2.6.11 or newer
+	(unionfs_main_fops)
+
+	* dirfops.c: Use unlocked_ioctl iff the kernel is 2.6.11 or newer
+	(for unonfs_dir_fops)
+
+	* commonfops.c (unionfs_ioctl): Use unlocked_ioctl iff the kernel
+	is 2.6.11 or newer, the prototype for unionfs_ioctl is also different
+	on 2.6.11 or newer
+
+	* inode.c (inode_permission): If kernel is older than 2.6.10, use
+	vfs_permission otherwise generic_permission
+
+2006-01-03  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* ChangeLog : fixed two gramatical errors in the changelog.
+
+2006-01-03  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionfs.h: Added a define for the first valid inode number so we
+	aren't using magic numbers in the persistent inode code.
+
+	* super.c: added a call to cleanup_imap_data to properly free
+	resources on unmount.
+
+	* persistent_inode.c: (imap_parse_options): cleanedup code,
+	(init_imap_data): new function (cleanup_imap_data): new
+	function.
+
+	* unionimap.c : cleaned up some brackets from single line if
+	statements.
+
+2006-01-03  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* unionfs.h: Renamed make_whname to alloc_whname, use strlcat instead
+	of strncat (the strcpy is safe since WHPFX will always be NULL
+	terminated), the NULL termination is dony by strlcat
+
+	* inode.c (unionfs_create, unionfs_link, unionfs_symlink,
+	unionfs_mkdir, unionfs_mknod): Renamed make_whname to alloc_whname
+
+	* rename.c (do_rename): Renamed make_whname to alloc_whname
+
+	* subr.c (create_whiteout, create_whiteout_parent): Renamed
+	make_whname to alloc_whname
+
+	* unlink.c (unionfs_unlink_whiteout): Renamed make_whname to
+	alloc_whname
+
+2006-01-03  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* branchman.c (unionfs_ioctl_branchcount, unionfs_ioctl_addbranch,
+	unionfs_ioctl_rdwrbranch, unionfs_ioctl_queryfile): Make sparse happy
+
+	* commonfops.c (unionfs_ioctl): Make sparse happy
+
+	* copyup.c (copyup_named_dentry): Make sparse happy
+
+	* file.c (unionfs_read, unionfs_write): Make sparse happy
+
+	* inode.c (unionfs_readlink, unionfs_follow_link): Make sparse happy
+
+	* main.c (unionfs_read_super): Make sparse happy
+
+	* persistent_inode.c (verify_forwardmap, verify_reversemap, get_uin,
+	get_lin): Make sparse happy
+
+2006-01-02  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* unionfs.h: added make_whname to replace scattered and duplicate
+	code that allocates memory, copies into it WHPFX and the rest of
+	the filename
+
+	* inode.c (unionfs_create, unionfs_link, unionfs_symlink,
+	unionfs_mkdir, unionfs_mknod): Use make_whname instead of manually
+	allocating, and copying data
+
+	* rename.c (do_rename): Use make_whname instead of manually
+	allocating, and copying data
+
+	* subr.c (create_whiteout, create_whiteout_parent): Use make_whname
+	instead of manually allocating, and copying data
+
+	* unlink.c (unionfs_unlink_whiteout): Use make_whname instead of
+	manually allocating, and copying data
+
+2006-01-02  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* ALL: Copyright year updated
+
+	* man/*: Date updated
+
+	* misc/split-views-2.4.26.patch: removed because 2.4 is not supported
+
+2006-01-01  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* rename.c (do_rename): use WHLEN+1 instead of 5
+
+2005-12-30  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* rename.c (do_rename): Added two missing DPUT()s
+
+	* subr.c (create_whiteout_parent): use LOOKUP_ONE_LEN instead of
+	lookup_one_len
+
+2005-12-29  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* commonfops.c, copyup.c, main.c, print.c, super.c: converted FISTBUG
+	commands to printk and BUG pairs.
+	* fist.h: removed definition for FISTBUG
+
+2005-12-29  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* lookup.c (unionfs_lookup_backend): Use WHLEN+1 instead of 5
+
+	* unionfs_debugmacros.h (__ftohf_index, __set_ftohf_index, __set_itohi_index,
+	__set_dbend, __set_dbstart, __set_dbopaque, __dtohd_index): BUG_ON with a more
+	complex condition is more optimal than "if(partial_condition) BUG_ON(...);"
+
+2005-12-29  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* lookup.c (unionfs_lookup_backend): DPUT() when done with the dentry
+	not before, added missing DPUT()
+
+2005-12-28  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* AUTHORS: Some maintenance dates.
+
+2005-12-28  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* AUTHORS: updated to reflect the patches commited in the last two days
+
+2005-12-28  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* lookup.c (unionfs_lookup_backend): Fixed missing DPUT()s
+
+2005-12-28  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* lookup.c: change the process's privilege temporally when
+	creating/searching/deleting the whiteouts (in is_opaque_dir,) and
+	forgot DPUT() after failing is_opaque_dir() (in unionfs_lookup_backend)
+
+2005-12-27  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* subr.c (create_whiteout): create whiteout bug, forgotten dput()
+
+2005-12-27  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* unlink.c (unionfs_unlink_whiteout): unlink bug, forgotten dput()s
+
+2005-12-27  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* inode.c (unionfs_create): create bug, forgotten dput(), extra GET_PARENT
+
+2005-12-27  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* lookup.c (new_dentry_private_data): Use GFP_ATOMIC instead
+	of GFP_UNIONFS (which currently is same as GFP_KERNEL) to prevent
+	sleeping while atomic bug
+
+2005-12-27  Junjiro Okajima  <hooanon05@yahoo.co.jp>
+
+	* inode.c (unionfs_permission): permission bug, creat/open truncates
+	the running executable
+
+2005-12-27  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionctl.c : Fixed check to see if a branch was already in the union and
+	fixed --before and --after logic.
+
+	* regression/branchman.sh: BUG370 will not work anymore due to code to avoid
+	duplicate branches being added. Need to reevaluate if it should be kept
+	anymore.
+
+2005-12-27  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* *.[ch]: Went through every file and replaced ASSERT and ASSERT2 with
+	BUG_ON calls. The logic for BUG_ON is the opposite of ASSERT but I believe
+	they are all converted properly.
+
+	* fist.h: Removed definitions for ASSERT and ASSERT2
+
+2005-12-27  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* *.[ch]: Went through every file and removed PASSERT and PASSERT2
+	statments. If there was an if that inclosed it then that was removed
+	also.
+
+	* fist.h: Removed definitions for PASSERT and PASSERT2.
+
+2005-12-27  Junjiro Okajima <hooanon05@yahoo.co.jp>
+
+	* copyup.c (copyup_permissions): Properly copyup atime, mtime, and
+	ctime.
+
+2005-12-27  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_lookup): Bugfix for bug #451 is not valid,
+	change reverted
+
+2005-12-20  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* lookup.c (new_dentry_private_data): Use SLAB_ATOMIC instead
+	of SLAB_KERNEL (prevent sleeping while atomic bug)
+
+2005-11-30  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* main.c: Updated module info (now includes unionfs version
+	 number)
+
+2005-11-29  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* branchman.c: Added check for addition of branches with
+	 overlapping paths. Fixes rest of Bug #374.
+
+2005-11-29  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* unionctl.c: Fixed branch addition with just branch (and
+	  neither mode nor before/after specified.
+
+2005-11-29  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Fix Coverity flagged errors.
+
+2005-11-29  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* main.c : Corrected a boundary case, so that one cannot
+	 use "/" as a branch if /ro is a branch.
+
+2005-11-28  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* main.c : Ensure that the branches getting added during a
+	 mount operation do not have overlapping branch paths. This
+	 is part of Bug #374.
+
+2005-11-28  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* inode.c: unionfs_lookup was not incrementing the reference
+	 count of the dentry. this was causing the chmod bug. added
+	 this, and fixed bug #451
+
+2005-11-26  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* unlink.c: Updated unionfs_unlink_whiteout to fix bug #434
+
+2005-11-24  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* print.c: Remove unused function.
+
+	* Makefile: Don't duplicate source list.
+
+2005-11-23  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* unionimap.c: Fix unchecked malloc.
+
+	* usercommon.c: Fix double free.
+
+2005-11-20  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* global : Changes made for "sparse"
+
+2005-11-18  Josef "Jeff" Sipek <jsipek@fsl.cs.sunysb.edu>
+
+	* rename.c (get_whname): Make constant's type clearer
+
+2005-11-18  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* unionctl.c : Changes made for "sparse"
+
+2005-11-18  Josef "Jeff" Sipek <jsipek@fsl.cs.sunysb.edu>
+
+	* rename.c (get_whname): Add NULL termination.
+
+2005-11-18  Shaya Potter <spotter@cs.columbia.edu>
+
+	* rename.c (lookup_whiteout): Add NULL Check.
+
+2005-11-18  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Remove UNIONFS_XATTR define, because 2.6 has consistent prototypes.
+
+2005-11-16  Martin Walter  <mawa@uni-freiburg.de>
+
+	* inode.c: Fix IS_NFS.
+
+2005-11-15  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* main.c: Allow debug= to be passed to the Unionfs module.
+
+2005-11-11  Allessio Curri <alessio.curri@elettra.trieste.it>
+
+	* rpm/unionfs.spec: Update RPM spec file to include unionimap.
+
+2005-11-09  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* commonfops.c (unionfs_open): Slightly rework reader/writer locks.
+
+2005-10-24  Shaya Potter <spotter@cs.columbia.edu>
+
+	* Fix scope of readers/writer locks on branch configuration.
+
+2005-11-09  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* unlink.c: Add comment w/ Junjiro's rmdir fix.
+
+2005-09-26  Junjiro Okajima <hooanon05@yahoo.co.jp>
+
+	* dentry.c (unionfs_d_revalidate): Don't copy attributes to nonexistent
+	inodes.
+
+	* lookup.c (new_dentry_private_data): Fix a race b/t lookup and d_free.
+
+2005-11-08  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Makefile: use /lib/modules/`uname -r`/kernel/fs/unionfs/unionfs.ko
+	to be consistent with other file systems.
+
+	* patch-kernel.sh: Add more double-patching checks.
+
+	* Use WHPFX and WHLEN instead of ".wh." and 4.
+
+	* dirhelper.c (delete_whiteouts): Remove useless partial lookup.
+	* unlink (unionfs_rmdir_all): Remove useless partial lookup.
+
+	* subr.c (create_whiteout): Silently succeed if the whiteout already
+	exists.
+	* inode.c (unionfs_unlink_all): Fix some coding style.
+
+2005-11-07  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionctl.c (parse_options): Added a check in to see if a branch
+	already exists in the union and if it does the operation fails.
+
+2005-11-07  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* dentry.c (dentry_revalidate): Remove extra d_deleted check.
+	* commonfops.c (unionfs_open): Undo add debug print of opened dentry.
+
+	* commonfops.c (unionfs_open): Add debug print of opened dentry.
+
+2005-10-26  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c (inode_permission): Fix typo in NFS_SECURITY_HOLE.
+
+2005-11-06  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* fist.h (lock_parent, unlock_dir): moved from now non-existent
+	missing_vfs_funcs.h
+	* inode.c (unionfs_create): use {,un}lock_rename instead of
+	double_{,un}lock
+	* rename.c (do_rename): use {,un}lock_rename instead of double_{,un}lock
+	* unlink.c (unionfs_unlink_whiteout): use {,un}lock_rename instead of
+	double_{,un}lock
+	* missing_vfs_funcs.h: removed
+
+2005-10-25  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Makefile: Increment version to 1.1.2pre.
+	* regression/readdir.sh: readdir regression script.
+	It doesn't reproduce the bug for us, but extra tests can't hurt.
+
+	* dirfops.c (unionfs_readdir): Properly update uds_dirpos,
+	which fixes directory reading operations.
+
+2005-10-24  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* compat.[ch]: Remove old files.
+
+2005-10-24  Shaya Potter <spotter@cs.columbia.edu>
+
+	* copyup.c (copyup_file): Don't fput errors.
+	* copyup.c, unionfs.h: Use loff_t for copyup size.
+
+	* copyup.c (copyup_named_dentry): Code cleanup.
+
+2005-10-21  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* AUTHORS: Update AUTHORS.
+
+2005-10-21  Charles Duffy  <cduffy@spamcop.net>
+
+	* xattr.c: Use ssize_t for xattr functions.
+
+2005-10-20  Shaya Potter <spotter@cs.columbia.edu>
+
+	* dirhelper.c (check_empty): Respect opaqueness.
+
+	* Replace lock_super around branch management operations with a
+	Unionfs read/write semaphore.  This will allow branchput/branchget
+	to operate concurrently, but prevent them from racing against
+	branchman operations.
+
+2005-10-19  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_permission): Don't call normal permission before
+	inode_permission (our modified version of permission), because it
+	just duplicates work.
+
+	* commonfops.c (branchput_gen): Lock the super when we read putmaps.
+
+	* inode.c (inode_permission): If NFS_SECURITY_HOLE is defined
+	treat -EACCESS as if we should fall back on inode_permission.
+
+	* branchman.c (unionfs_ioctl_rdwrbranch): Make super/dentry
+	lock/unlock symmetric.
+
+2005-10-18  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* NEWS: Updates NEWS file for release
+
+2005-10-14  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* rename.c : Used the create_whiteout_parent function to
+	create the whiteout in unionfs_rename_all.
+	* subr.c : Brought the create_whiteout_parent back. These
+	squash Bug #442.
+
+2005-10-13  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* rename.c : Fixed rest of Bug #425. Applied the patch
+	sent in by Junjiro Okajima.
+
+2005-10-13  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* rmdir-all.sh: added test for bug 430. Not sure if
+	we are going to patch it yet but if we do the test
+	is there.
+	* regression/Makefile: added rmdircheckinode.c
+	* regression/progs/rmdircheckinode.c: program to check
+	if the inode numbers match after a failed rmdir.
+
+2005-10-13  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* rename.c (unionfs_rename) : Fixed bug #425. The new_dentry
+	affects the unlink called by "mv" after the rename has failed.
+	It makes unlink return with -EISDIR. (partial fix)
+
+2005-10-11  Shaya Potter <spotter@cs.columbia.edu>
+
+	* dentry.c (unionfs_d_revalidate): Fix a deadlock.
+
+2005-10-06  Robert Glowczynski  <roglo@op.pl>
+
+	* file.c (unionfs_fsync): Fix check for NULL lower-level operation.
+
+2005-09-29  Patrik Weiskircher
+
+	* Makefile.kernel: Remove reference to locks.c.
+
+2005-09-28  Junjiro Okajima <hooanon05@yahoo.co.jp>
+
+	* unlink.c (unionfs_rmdir_all): Always create whiteouts on directory
+	removal.
+
+2005-09-28  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* commonfops.c (unionfs_file_revalidate): Properly update
+	generation when we combine copyup and reopening.
+	* Makefile: Remove some 2.4 cruft.
+
+2005-09-27  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* branchman.c (unionfs_ioctl_incgen): Fix print indentation bug.
+
+	* unionimap.c: AMD64 fix from Gentoo.
+
+2005-09-27  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* inode.c: Made change to use both inode_permissions and
+	permissions.
+
+2005-09-27  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* INSTALL: Document MODDIR.
+
+2005-09-23  Klaus Knopper  <unionfs@knopper.net>
+
+	* inode.c: added inode_permissions to check for permissions
+	on a file even if its on an ROFS and used it in
+	unionfs_permissions
+
+2005-09-22  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* Makefile: remove 'tags' target dependency from 'all'
+
+2005-09-21  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* Makefile: forgot to remove locks.c from source list
+
+2005-09-21  Junjiro Okajima <hooanon05@yahoo.co.jp>
+
+	* inode.c: Fixed several off-by-one kmalloc bugs.
+
+2005-09-20  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* removed locking code from codebase since we no longer
+	handle locking (vfs will do it)
+
+2005-09-18  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* removed 2.4 code from fist.h, inode.c, lookup.c,
+	persistent_inode.c, rdstate.c
+
+2005-09-18  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* removed 2.4 code from copyup.c, dentry.c, dirfops.c
+	and file.c
+
+2005-09-18  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* removed 2.4 code from stale_inode.c, subr.c, unionfs.h,
+	unlink.c, xattr.c
+
+2005-09-17  Josef "Jeff" Sipek  <jsipek@fsl.cs.sunysb.edu>
+
+	* removed 2.4 code from main.c, print.c, super.c,
+	xattr.c
+
+2005-09-17  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* removed 2.4 code from branchman.c, commonfops.c
+
+2005-09-16  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* Makefile.kernel: Use EXTRA_CFLAGS.
+
+	* Makefile: Use ${LD} instead of ld, for cross compilation.
+
+2005-09-15  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* Makefile: Updated to 1.1.0pre
+	* Makefile.kernel: Updated to 1.1.0pre
+
+2005-09-15  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* NEWS: Updated for release.
+	* Makefile: updated for release.
+	* Makefile.kernel: updated for release
+
+2005-09-12  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* locks.c: Removed locking code since its broken big define 0 around
+	it
+	* file.c: Removed locking references have the vfs handle it.
+	* unionfs.h: fixed bug with 2.4 compilation.
+
+2005-09-08  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* main.c: Remove comment leftover from templates that doesn't make
+	sense in current context.
+
+2005-09-05  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* commonfops.c: Remove lower fput debug printks.
+
+2005-09-02  Shaya Potter <spotter@cs.columbia.edu>
+
+	* file.c (unionfs_mmap): Fix flag checking for mmap.
+
+2005-09-01  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* We shouldn't use d_delete, vfs_unlink already does it.  The only
+	known remaining 2.6.13 issue is the unionfs_lock (flock.sh in the
+	regression suite).
+
+	* inode.c,stale_inode.c: Support for 2.6.13's new follow_link
+	prototype.  However, many of the regression tests fail for
+	unrelated reasons (or at least I think they are unrelated).
+
+	* unionctl.c: Allow --FOO to go before the union specifier (e.g.,
+	unionctl --list /mnt/unionfs now works).
+	* lookup.c: Don't partial lookup the root dentry.
+
+	* copyup.c: Check for permission setting errors.
+
+2005-08-31  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* lookup_one_len never returns NULL.
+
+2005-08-30  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_permission): Simplified code.
+
+	* inode.c (unionfs_link): Copy directory attrs to directory
+	(BUG391).  Unfortunately, regression test doesn't quite catch the
+	bug because of a revalidate for the stat.
+	* commit: Don't complain about .sh file's indentation.
+
+2005-08-29  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* INSTALL: tmpfs doesn't support fsync on directories
+
+	* Makefile: Update release target for new directory structure.
+
+	* patch-kernel.sh: Depend on Experimental, and move configuration
+	option to File Systems -> Miscellaneous file systems at the end,
+	not as the very first file system. Add UNIONFS_VERSION define.
+	Use tail -n +7 instead of tail +7.
+
+	* file.c (unionfs_fsync): Don't use dtohd on unlocked dentries.
+
+2005-08-28  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_link): Don't use unionfs_interpose with
+	INTERPOSE_LINK any more, because only one line of the whole
+	function was being used anyway.  This fixes an inode refcount leak
+	in unionfs_link, so the regression suite now passes with the new
+	locking, without any leaks.
+
+	* commonfops.c (unionfs_file_release): Update rdstate access time when
+	saving it in the inode (so that it won't be so quickly discarded).
+
+2005-08-26  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* dirfops.c (unionfs_readdir) : removed the changes made
+	for the special way in which vfs_llseek was handles for
+	Reiser4, as Reiser4 introduced a patch which made it
+	behave properly.
+
+2005-08-26  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Passes regression tests.
+
+	* Locking for create_parents.  Regression tests from branchman to
+	open.sh PASS.
+
+	* Basic inode operations pass sniff test with new locking (regression
+	tests not yet tried).
+
+	* Dentries need locking, and there isn't much way around it.This
+	snapshot adds some untested locking, and you won't want to use it
+	yet.  The basic principles are:
+		1. As soon as a VFS operation that touches a dentry is entered,
+		the dentry should be locked.
+		2. The lock ordering is:
+			Children before parents
+			Two children are tie broken with their address
+	There are several functions not done yet, most notably create_parents,
+	because it is going to require more thought (we walk up the parent
+	list and then back down it).  This in part is why children need to go
+	before parents (also revalidate walks up the parent list).
+
+2005-08-25  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* xattr.c: unused label causing a warning and inturn an error.
+
+2005-08-25  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* main.c (interpose,reinterpose): Interposition should lock the dentry.
+
+	* file.c: CodingStyle, and we don't need to check ftopd before ftohf.
+	* file.c (unionfs_mmap): We were checking our file instead of the lower
+	-level file for having valid operations.
+	* dirfops.c,dirhelper.c: CodyingStyle
+
+	* copyup.c: There is no need for _len at the end of functions, because
+	other versions don't exist any longer.
+
+	* lookup.c (unionfs_lookup_backend): Lock the dentry private data
+	for the whole lookup routine.
+	* branchman.c: CodingStyle fixes.
+	* rdstate.c, super.c: Use KERN_ERR if we have unfreed objects.
+
+2005-08-25  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* INSTALL: Fix losetup instructions.
+
+2005-08-24  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* print.c: Avoid kmallocs.
+	* rdstate.c,dirfops.c: Code style cleanups.
+
+2005-08-24  Anton Farygin  <rider@altlinux.com>
+
+	* dentry.c (d_revalidate):  Lock inode when freeing lower level ones.
+
+2005-08-24  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+	* unionfs.h: Updated forwardmap version and added a new data
+	structure bmapent.(Later on these should be unified into one header
+	rather than being in 2 header files).
+	* persistant_inode.c: Updated code to use bmapents now.
+
+2005-08-24  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionimap.c: Added code to ensure you dont add a filesystem with
+	the same fsid twice (no duplicate entries in the maps).
+	* unionimap.h: Added a new data structrue bmapent.
+	* The code in the kernel to read the maps hasent been changed so it
+	doesent work yet. Check back soon for that.
+
+2005-08-24  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_create): Use proper permissions when recreating
+	a deleted file (BUG383).
+
+2005-08-23  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* commonfops.c (copyup_deleted_file): Fix leak of name on subsequent
+	loop iterations.
+
+	* commonfops.c (copyup_deleted_file): Cleanup of major loop.
+
+	* unionfs.h (DPUT,KFREE): Don't try to free errors.
+
+	* main.c (unionfs_reinterpose): Remove d_unhashed assertion.
+	* unionfs.h (d_deleted): Create an inline function to tell
+	if a directory is deleted (i.e., unhashed and not the root).
+
+2005-08-23  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* main.c (unionfs_parse_options): More checking for copyupmode.
+
+2005-08-22  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* unionfs.h (get_nlinks): More intelligent link counting.
+
+	* inode.c (unionfs_create): Don't dput an IS_ERR.
+
+	* unionfs.h: sbstart is always zero
+	* main.c (unionfs_read_super): check_branch does existence checking
+
+	* unionfs_debugmacros.h: Fix stray printks.
+
+	The following fixes are so that the regression suite runs without
+	any memory or dentry leaks:
+	* branchman.c (unionfs_ioctl_addbranch): Zero newly allocated
+	arrays.
+	* copyup.c (unionfs_create_named_dirs): Fix leak of path stack.
+	* main.c: Handle get_parent.
+	* lookup.c: Make the logic to put preceding negative dentries a
+	function.
+	* unlink.c (unionfs_unlink_all): Fix dentry reference count leak.
+	* inode.c (unionfs_link): Properly handle lock_ and unlock_dir.
+	* match-dget.pl: Handle DS and DD for set and unset records.
+
+2005-08-20  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* INSTALL: sendfile conflicts w/ Unionfs.
+
+2005-08-19  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* dirfops.c (unionfs_readdir) : changed the way the
+	return value of the repeated vfs_llseek (with origin
+	=1) is used. this is to allow readdir in Reiser4,
+	which returns -ENOENT (!!!) for the llseek in same
+	cases. this is for Bug #358
+
+2005-08-18  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* match-dget.pl: Insert both gets and puts into list of actions
+	for unreleased dentries.
+
+	* match-dget.pl: Dget matching script.
+	* Use DGET, DPUT, DENTRY_OPEN, and LOOKUP_ONE_LEN to
+	record when we dget and dput dentries.
+
+2005-08-17  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* match-malloc.pl: Return number of errors.
+
+	* print.c (fist_print_generic_dentry): Print inode number.
+
+	* copyup.c (unionfs_create_named_dirs): Fix dentry reference leak.
+
+2005-08-15  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* print.c (fist_print_file): Divide into generic and Unionfs halves,
+	ASSERT if we are passed a lower-level file.
+
+	* file.c (unionfs_lock): Handle write locks on r/o branches.
+
+2005-03-03  Anton Farygin  <rider@altlinux.com>
+
+	* dentry.c (unionfs_d_revalidate): Fix possible double unlock.
+
+	* branchman.c (unionfs_ioctl_incgen): Fix possible deadlock when
+	memory allocation fails.
+
+2005-08-12  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* rename.c, commonfops.c : removed the
+	dget/dput functions sandwiching the vfs_unlink as
+	these raise the refcounts and cause the .nfsXYZ
+	files to get created after unlinks. This knocks
+	out Bug#364 for 2.6.
+
+2005-08-11  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* main.c (unionfs_parse_dirs): Prevent recursive Unionfs mounts.
+
+	* branchman.c: Plug a few memory leaks.
+
+2005-08-11  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* dentry.c (unionfs_d_release) : fixed a case that would
+	cause a reference to the lower-level nfs inode to be
+	present. This is part of #364.
+
+2005-08-11  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* unionctl.c: Support --mode ro branch or --mode branch ro
+
+	* branchman.c: Only count putmaps that exist.
+
+2005-08-10  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* man/unionctl.8: No need for warning about root directory anymore.
+	* commonfops.c: Kill branch deletion ioctl.
+
+	* super.c: Magic MS_REMOUNT to remove a branch.
+	* unionctl.c: Use remount instead of ioctl to remove a branch.
+	* branchman.c (unionfs_ioctl_delbranch): Take the super, not an inode.
+
+2005-08-10  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* dentry.c (unionfs_d_revalidate): All exit paths should go through
+	out.
+
+	* main.c (unionfs_parse_options,unionfs_parse_dirs): Simplified
+	option parsing using strsep rather than direct pointer manipulation.
+	Organized the options into types to factor out common code.
+
+2005-08-09  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* debian/*: Use official debian packaging files.
+
+	* Some AMD64 fixes.
+	* vprintk wrapper for 2.4.
+
+2005-08-09  Jan Engelhardt  <jengelh@linux01.gwdg.de>
+
+	* unionctl.c: Do not truncate "/" to "" when stripping the last "/"
+	from directories.
+
+2005-08-05  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* main: Use FS_REVAL_DOT so that directories are always revalidated
+	during lookup.
+
+	* Drop err=passup, as it is unmaintained.
+
+2005-08-05  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* commonfops.c (unionfs_flush) : changed the dput to give
+	more symmetry and readability to the code.
+
+2005-08-04  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* dentry.c (unionfs_d_revalidate) : return successfully if the
+	dentry is unhashed
+	* commonfops.c (unionfs_flush) : corrected the d_unhashed check
+
+2005-08-04  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* branchman.c (QUERY): Don't copy FD_SET before ioctl. FD_ZERO the
+	set before the call.
+
+	* branchman.c (newputmap): Subtract the count from old putmaps from
+	the brand new putmap's count.
+
+	* branchman.c (unionfs_ioctl_delbranch): Use putmaps.
+	* lookup.c (new_dentry_private_data): Fix branch removal when
+	transitioning from > UNIONFS_INLINE_OBJECTS to ==
+	UNIONFS_INLINE_OBJECTS.
+
+2005-08-03  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionfs.h,main.c,super.c,inode.c: removed all code
+	pertaining SETATTR_ALL since the feature is not necessary
+	and complicates things.
+	* man/unionfs.4: removed text pertaining to the setattr
+	mount option.
+
+2005-08-03  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* unionfs.h (putmap): We need to keep track of the mapping between
+	branch numbers for older generation numbers and the current
+	generation so that we can properly branchput.
+	* branchman.c: Add functions to manipulate putmaps.
+	* commonfops.c: Use putmaps for revalidation and close.
+
+2005-08-03  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* dentry.c (unionfs_d_release) : added a check to take care
+	of the case when the dentry coming into release is from a
+	failed lookup. This knocks bug #303 out.
+
+2005-08-03  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Remove older non-opaque directory mode.
+
+2005-08-01  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* lookup.c (unionfs_lookup_backend): Skip those hidden
+	dentries that are NULL.
+
+2005-08-01  Jan Engelhardt  <jengelh@linux01.gwdg.de>
+
+	* print.c (fist_dprint_internal): Use vprintk instead of
+	vsnprintf/printk combo.
+
+2005-08-01  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* unionfs_debugmacros.h (branchget,branchput):
+	ASSERT if a branch's count goes negative.
+	* super.c (unionfs_put_super): ASSERT if the branch
+	counts are not zero on unmount.
+	* commonfops.c (unionfs_file_release): Don't put branches
+	that we didn't get.
+	* commonfops.c (unionfs_open): Don't get a branch until
+	we open it.
+
+	* commonfops.c: Fix generation number increment.
+
+2005-07-26  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* main.c: cleaned up parse_options code for copyup,
+	copyupuid,copyupgid,copyupmode.
+
+2005-07-26  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* unlink.c, main.c, rmdir.c, rename.c: Remove obsolete
+	and unmaintained DELETE_FIRST mode.
+
+2005-07-26  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* man/unionfs.4: Added default behavior into the options
+	section for entries that didnt already have it.
+
+2005-07-26  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* main.c (unionfs_parse_options): Move directory parsing
+	out of parse options function, so we don't have as many 4+ level
+	indents.
+
+2005-07-25  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Makefile: Update version number to 1.0.14pre.
+
+2005-07-25  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* man/unionimap.8: Minor editing.
+
+2005-07-24  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* man/unionimap.8 : finished the man page: added a verbose
+	description and examples.
+
+2005-07-22  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* man/unionimap.8 : added descriptions to options in the
+	option section.
+
+2005-07-22  Jan Engelhardt  <jengelh@linux01.gwdg.de>
+
+	* unionctl.c: Use realpath so that relative pathnames are allowed
+	for mountpoints and branches.
+
+2005-07-22  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* subr.c (create_whiteout_parent) : removed this function
+	as it is not called anymore (#258).
+
+2005-07-22  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Makefile: Update version to 1.0.13.
+	* Makefile: Include new regression tests in distribution.
+	* man/unionimap.8: An empty placeholder man page.
+
+2005-07-21  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* missing_vfs_funcs.h: Removed get_parent function since the 2.6
+	kernel has a dget_parent function which is the appropriate one to use
+	also made note that the templates do not use either triple_up or triple_down
+	* unionfs.h: checked to see if we are in 2.6 and if so define get_parent to be
+	dget_parent.
+	* unionimap.c: fixed a typo in the usage example.
+	* man/unionfs.4: added an entry for imap
+
+2005-07-21  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* copyup.c (unionfs_create_named_dirs) : corrected the
+	function to get rid of problems with the open-unlink
+	regression test.
+
+2005-07-20  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* unlink.c (unionfs_rmdir_all) : added checks to ensure
+	that the delete_whiteouts is not called for files. this
+	is a fix for bug 323
+
+	* rename.c (unionfs_rename_all) : changed the whiteout
+	creation call to "create_whiteout" instead of the
+	"create_whiteout_parent". this must solve 332.
+
+	* rename.c (unionfs_rename_whiteout) : change similar
+	to above to correct #336.
+
+2005-07-20  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_file_revalidate): Handle GFS and don't try
+	to reopen files that no longer exist.
+	* subr.c (unionfs_copyup_named_dentry_len): Add a bit of debugging.
+	* Fix comments that are past 80 characters and some other
+	minor style issues.
+
+	* subr.c (unionfs_create_whiteout): Don't set the parent's
+	opaque field when creating a whiteout, set your own.
+
+2005-07-20  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* Fix things so that we compile on 2.4 again, and use the
+	older form of ioctl for kernels less than 2.6.11.
+
+	* ChangeLog: 80 character entries.
+
+2005-07-19  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* unionctl.c: Check if we are trying to remove a branch while
+	this process is causing it to be busy and print an appropriate
+	error message.
+
+	* man/unionctl.8: Minor edits.
+
+2005-07-19  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* branchman.c (unionfs_ioctl_queryfile) : changed
+	  function replacing lookup_one_len by partial lookups
+	  (second commit) : changed O_NONBLOCK to O_RDONLY
+
+2005-07-19  Shaya Potter  <spotter@cs.columbia.edu>
+
+	* unlink.c (unionfs_unlink_whiteout): Don't rename a symlink
+	to a whiteout, we need to unlink and create instead.
+
+2005-07-18  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* unlink.c (unionfs_unlink_all) : checked if the dbopaque
+	  value is set for the current dentry, as this will
+	  indicate if there is a file to the right of the current
+	  file (Fix BUG #319).
+
+2005-07-18  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* commonfops.c: Use unlocked_ioctl when it is defined.
+	* branchman.c: Change prototypes to be more consistent with
+	unlocked_ioctl.
+
+	* commit: Script for committing files
+	* Lindent: Linux indentation script.
+	* *.[ch]: Add emacs magic.
+
+2005-07-14  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* lookup.c (unionfs_lookup_backend) : fixed bug #321
+	  (that is, removed the semicolon that was introduced
+	  in the unionfs-042605-1324.tar.gz snapshot.
+
+2005-07-14  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* copyup.c (unionfs_create_named_dirs) : corrected the usage of
+	  the name and namelen arguments sent into the function. This
+	  should take care of both bugs #299 and #322.
+
+2005-07-13  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* branchman.c (unionfs_ioctl_queryfile) : added ioctl definition
+	  that lists those branches where the specified file exists(#253).
+	* commonfops.c (unionfs_ioctl) : added the new ioctl.
+	* fist.h : added an include for the fd_set helper functions
+	* unionfs.h : added a structure unionfs_queryfile_args to
+	  pass variables to the new structure.
+	* unionctl.c : added the user interface
+	* uniondbg.c : rearranged the includes to help compilation
+
+2005-07-08  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* unionfs.h : added variables uii_totalopens and uii_writeopens to
+	  help in keeping filehandles valid after unlinks until closes.
+	* fist.h : included random.h to help get random bytes for file names
+	* commonfops.c (unionfs_file_revalidate) : added functionality to copy
+	  the lower-level file into a file with a randomly generated name
+	  obtained by using the newly added function "get_random_name" in
+	  commonfops.c
+	  (unionfs_open, unionfs_flush) : updated the uii_totalopens and
+	  uii_writeopens to be set and checked during the time of opening and
+	  flushing the file.
+	* copyup.c (unionfs_copyup_dentry_len) : made this into a wrapper
+	  function which now accepts the name of the file
+	  (unionfs_copyup_named_dentry_len).
+	  (unionfs_copyup_file) : similar wrapper to unionfs_copyup_named_file
+	  as above.
+	  (unionfs_create_dirs) : similar wrapper to unionfs_create_named_dirs
+	  as above
+	  (unionfs_copyup_named_file) : added function similar to
+	  unionfs_copyup_file which takes the file name as argument as well.
+	* main.c (unionfs_reinterpose) : ensured that deleted dentries are not
+	  reinterposed.
+	* dentry.c (unionfs_d_revalidate) : changed the check for deleted
+	  dentries to use the d_unhashed function also.
+
+2005-07-07  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+	* main.c: Ohh my god the inodes are persistent. Ohh and changed
+	interpose to use get_uin if we are using persistent inode maps.
+
+2005-07-07  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+	* super.c: fixed problem with kernel version number in an ifdef
+
+2005-07-07  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* dirfops.c: removed some debugging printk statments.
+	* persistent_inode.c: finished debugging and cleanedup get_uin.
+
+2005-06-30  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionfs.h  added extern decls for get_uin and get_lin
+	* persistent_inode.c: more work on the loading code and get_uin and
+	 get_lin
+	* dirfops.c: changed to make use of the persistent inode code only
+	if maps are loaded.
+
+2005-06-28  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* persistent_inode.c: changed how files are loaded in should be
+	done still need to test
+	* unionfs.h: added a new variable into unionfs_superblock_info
+	* unionimap.c: changed called to mkfsid
+	* usercommon.c: changed fillfsid and mkfsid. No longer uses the inode
+	since it will always be 2 since we are using the root of the fs.
+
+2005-06-24  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* Makefile: added usercommon.c into the make targets
+	* usercommon.c: Moves find_union to here and fixed it up
+	* unionimap.c: uses mkfsid now if fsid comes back as 0
+	* unionimap.h: added extern for mkfsid
+	* persistent_inode.c: fixed some bugs
+	* unionctl.c: removed find_union from here and updated calls to it.
+
+2005-06-23  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* dentry.c (d_revalidate): We shouldn't re-lookup
+	non-connected dentries.
+
+2005-06-23  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* Makefile: Include Makefile.kernel.
+
+	* subr.c (create_whiteout, create_whiteout_parent): Set the dbopaque
+	value to the branch where the whiteout gets created.  This is to solve
+	bug #294.
+
+2005-06-22  Arun M. Krishnakumar  <arunmk@fsl.cs.sunysb.edu>
+
+	* commonfops.c (unionfs_open): If the highest-priority branch
+	is read-only return -EROFS for opens with O_WRITE.
+
+2005-06-21  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* persistent_inode.c: parse_imap_options and associated calls are
+	working. maps are loaded in properly. Still need to finish and test
+	calls to use the maps.
+
+2005-06-17  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* persistent_inode.c: it appears that parse_imap_options is working
+	and so is verify_forwardmap. verify_reversemap needs to be fixed.
+
+2005-06-17  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* persistent_inode.c: Fixed some bugs still not working properly
+
+	* unionfs.h: added typedef for uuid_t.
+
+	* doit.sh: added imap entry into script.
+
+2005-06-17  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* INSTALL: If you have Fedora Core 4, then you need kernel-devel.
+
+	* Makefile: Fix the Makefile so that it won't recompile every
+	object every time.
+
+	* unionimap.c: Fix printf formats.
+
+2005-06-16  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionimap.c : Fixed a bug that prevented compiling on newer
+	gcc versions
+
+2005-06-16  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionimap.c : Tested and working we can now make and print valid
+	unionfs imap files.
+
+2005-06-16  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionimap.c : finished coding starting testing and debugging.
+	* unionimap.h : minor changes
+
+2005-06-15  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionimap.c : wrote some more code not tested yet
+	* unionimap.h : added a struct that is needed.
+	* Makefile: Cleanedup a conflict.
+
+2005-06-15  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* fist.h,print.c,unionfs.h: Change NODEBUG to UNIONFS_NDEBUG to be
+	more inline with the rest of the kernel.
+	* print.c: Wrap in UNIONFS_NDEBUG, so there is no need for a separate
+	flag to not compile it.
+
+	* unionctl.c: Add Unionfs version to help message
+		(not just unionctl version).
+	* unionimap.c: Add version to help message.
+
+2005-06-12  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* main.c, match-malloc.pl: Handle KFREE(NULL).
+	* commonfops.c (unionfs_file_revalidate): NULL ftohf_ptr after freeing
+	it.
+	* commonfosp.c (unionfs_open): Allocate correctly sized ftohf_ptr.
+
+2005-06-10  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionimap.c: Beginning of a user mode program to create inode map
+	files.
+	* unionimap.h: Header for program.
+	* unionfs.h: added definitions for imap structs
+	* Makefile: added make targets for unionimap.c and .h
+
+2005-05-26  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* persistent_inode.c: Comment out 64-bit division.
+
+2005-05-25  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionfs.h : added some variables to the unionfs_sb_info struct to
+	handle persistent inodes.
+	* persistent_inode.c : more work done on the parsing functions almost
+	done but not quite there yet.
+	* main.c : added entry in parse_options to handle persistent inodes.
+
+2005-05-24  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* unlink.c (unionfs_unlink_whiteout): Missing dput in truncating fix.
+
+	* unlink.c (unionfs_unlink_whiteout): Truncate whiteout after it
+	is created.
+
+2005-05-23  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Test commit.
+
+	* inode.c (unionfs_create): Fix ASSERT that had a side-effect.
+	This fixes a dentry reference count bug if you compile with
+	-DNODEBUG.
+
+2005-05-19  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* unlink.c: Empty a directory of its whiteouts before deleting it,
+	and refresh the hidden dentry on a failed rmdir.
+
+	* unionfs.h: Always include ufi_file_i, so we can compile with zero
+	inline entries if we want.
+
+	* Test commit.
+
+	* Makefile: Define UNIONFS_VERSION to be the current version.
+	* main.c: Print the version of Unionfs, not main.c.
+
+2005-05-17  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* lookup.c (unionfs_lookup_backend): Fix partial lookups for
+	three or more branches.
+
+2005-05-16  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* lookup.c (unionfs_lookup_backend): Change UNIONFS_LOOKUP_PARTIAL
+	to UNIONFS_LOOKUP_REVAL_NEG, when a dentry magically turns positive
+	on us.
+
+2005-05-14  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* lookup.c (new_dentry_private_data): Fix memset'ing logic.
+	* branchman.c (unionfs_ioctl_addbranch): Fix copying logic.
+
+	* inode.c(unionfs_follow_link): Return 0 not the number of bytes
+	in follow_link.
+
+	* Reduce Unionfs module size when NODEBUG is set by not compiling
+	print.c, and using a different set of macros that don't require
+	__FILE__, __FUNCTION__, and __LINE__, and don't do as much checking
+	because we've aleady turned ASSERT off.
+
+	* Support for inline objects, so that we don't need more
+	than one allocation for inodes and only two for dentries (the dentry
+	itself and its private data).
+
+	* Separate directory file operations from normal file operations.
+
+2005-05-13  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* Makefile: Increment version number.
+
+	* Makefile: Release 1.0.12
+
+2005-05-13  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* file.c: Disable generic_file_sendfile, because it causes nfsd
+	to Oops.
+
+	* lookup.c (unionfs_lookup): Don't ASSERT dentry goodness along
+	the error path.
+	* dentry.c (unionfs_d_release): Handle dentries without dthod_ptrs.
+
+2005-05-11  Jaspreet Singh  <jsingh@ensim.com>
+
+	* main.c (unionfs_interpose): Move fist_copy_attr_all before
+	d_instantiate so Selinux can decide what type of inode it is.
+
+2005-05-13  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* Makefile : fixed spelling mistake
+	* persistent_inode.c : fixed typo for compilation
+
+2005-05-12  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+	* persistent_inode.c : Stupid spelling mistake in the name
+	of the file.
+
+2005-05-12  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* persistant_inode.c : Functions needed to establish and maintain
+	a persistant inode scheme for unionfs.
+	* main.c : modified parse options for persistant inodes. Not done yet
+
+2005-05-11  Jaspreet Singh  <jsingh@ensim.com>
+
+	* copyup.c (unionfs_copyup_xattrs): Don't use XATTR_CREATE,
+	instead use 0 so that it is replaced or created as needed
+	(Selinux automatically creates attributes.
+
+2005-05-11  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* print.c (fist_print_generic_dentry): ASSERT(d_count > 0) so
+	that we find dentries gone bad earlier rather than later.
+
+	* inode.c (unionfs_mkdir): bend should not go past the opaque
+	directory, but stay there.
+
+2005-05-10  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* INSTALL: Add root file system instructions based on Linux
+	Live's linuxrc.
+
+2005-05-08  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* main.c,super.c,lookup.c: Use alloc_inode in 2.6, so that only
+	two allocations need to be made per inode instead of two.  In
+	both 2.4 and 2.6 use a private kmem_cache for our dentry data,
+	so that we don't need to waste space by having kmalloc round up.
+
+	* lookup.c (new_dentry_private_data): Merge initialization and
+	reinitialization code from lookup and read_super.  Also don't
+	reallocate dentry private data if we already have enough space.
+
+	* INSTALL: Add Jaspreet Singh's selinux instructions.
+
+	* INSTALL: Add EXDEV on ro branch rename as a limitation.
+
+	* inode.c: follow_link and put_link fix for 2.6.
+
+	* lookup.c: Compile fix for 2.4.
+
+	* rename.c: If a directory is moved on a read-only branch return EXDEV,
+	so that mv essentially does a cp -r (recursive copyup in the kernel
+	would be too ugly).
+
+2005-05-06  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* lookup.c: Merge unionfs_lookup_backend and unionfs_partial_lookup,
+	partial lookups will now properly handle whiteouts.
+
+	* print.c: Fix compile error when NODEBUG is defined.
+
+	* lookup.c: Fix BUG 264.
+	* print.c: Separate generic inode printing from itohi printing.
+	Use const for print functions.
+
+2005-05-05  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* Code style fixes.
+
+2005-04-27  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* inode.c changed ops vectors to C99 style initialization.
+	* file.c changed ops vectors to C99 style initialization.
+	* dentry.c changed ops vectors to C99 style initialization.
+	* super.c changed ops vectors to C99 style initialization.
+	* stale_inode.c changed ops vectors to C99 style initialization.
+
+2005-04-21  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionfs.h: added no check versions ot set_dtohd_index
+	and dtohd_index.
+	* dentry.c: changed a function call to dtohd_index_nocheck
+
+2005-04-26  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* lookup.c (unionfs_lookup_backend): When a whiteout is encountered,
+	set opauqeness as we do with opaque directories, to prevent partial
+	lookup from seeing past the whiteout.
+
+	* lookup.c: Separate file for lookup code.
+	* subr.c: Move unionfs_partial_lookup to lookup.c.
+	* inode.c: Move unionfs_lookup to lookup.c:
+
+	* rename.c: Don't print lower-level dentries using a Unionfs
+	specific function.
+
+	* print.c: Divide fist_print_dentry into one function for Unionfs
+	specific dentries, and then a generic function.  Also ASSERT that
+	we are really printing a Unionfs dentry (so we don't dereference
+	bad private data).
+
+	* Fix indentation comments.
+
+2005-04-25  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* subr.c: Code style fixes.
+
+2005-04-22  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* NEWS: Keep NEWS up to date.
+
+2005-04-21  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_filldir): Don't strncmp for .wh., if the
+	name's length is less than 4.  Old cruft can make things
+	go horribly bad (BUG 254).
+
+	* inode.c (unionfs_lookup_backend): Properly handle file
+	transitioning from a negative to a positive dentry. (BUG 215)
+
+2005-04-21  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* file.c: Sendfile only exists on 2.6.
+	* main.c: Catch UNIONFS_REVAL_NEG specifically, still
+	need to understand/fix.
+
+2005-04-20  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* subr.c (unionfs_partial_lookup) ,unionfs.h
+	(unionfs_dentry_info), inode.c(unionfs_lookup):
+	Correctly mark a directory as opaque and respect that
+	during partial lookups.
+
+	* Exorcise struct typedefs and "fake" STATIC functions.
+
+	* inode.c: Experimental support for opaque directories,
+	not all of the loose ends are tied up, so this snapshot
+	will have some associated oddities.
+
+	* xattr.c: Use EOPNOTSUPP, not ENOTSUPP.
+
+	* Makefile, unionfs.h: Separation of rename/unlink code.
+	* rename.c: Separate rename code from inode.c
+	* unlink.c: Separate unlink code from inode.c
+
+2005-04-20  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* rdstate.c, unionfs.h: DOH, We were checking if
+	uii_rdversion <= MAXRDCOOKIE to decide whether or not
+	to wrap uii_cookie.
+
+2005-04-19  Fabian Franz  <fs-bugs@fabian-franz.de>
+
+	* applied patch to file.c that uses generic_file_sendfile
+	to implement sendfile.
+
+2005-04-18  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* removed a stray printk
+
+2005-04-18  Markus F.X.J. Oberhumer  <markus@oberhumer.com>
+
+	* file.c, locks.c: Use #ifdef for 64-bit locking
+	commands, which are not defined on amd64.
+
+2005-04-18  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* copyup.c: Extended attributes now copyup properly.
+	This has only been tested in 2.6 but I see no reason
+	that it shouldent work in 2.4
+	* copyup.c: unionfs_create_dirs is now nolonger bound
+	by MAX_DIRS_CREATE. We now use kmalloc to dynamically
+	allocate memory for it and "realloc" the memory when
+	needed.
+
+2005-04-18  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* copyup.c: Use list_size as the argument to xattr_free, not the
+	MAX_LIST_SIZE.
+
+	* file.c: ASSERT was asserting on the dentry, not the i_mode.
+
+2005-04-16  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* file.c: Delay copyup of read-write files located on read-only
+	branches, until an operation will actually write to them (BUG 225).
+
+	* INSTALL: Cleanup kernel instructions a bit.
+
+2005-04-13  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* inode.c: unionfs_mkdir now spoofes the uid and gid of the owner
+	of the whiteout file for the purpose of removing it.
+
+2005-04-11  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+	* patch-kernel.sh : Script written to move unionfs into the kernel
+	source tree. The original script was submitted by Sven Geggus however
+	the script was cleaned up and modified for various reasons. Check the
+	file for more details.
+	* Makefile: PHONY added to the utils target to aid in building them
+	for the kernel.
+	* INSTALL: Directions added for building unionfs into a monolithic kernel.
+
+2005-04-11  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* copyup.c : changed comparison from > to >= to be compatable with 2.6.0
+	* fist.h : same as above
+	* unionfs.h : same as above
+	* rdstate.c : same as above
+	* main.c : same as above
+	* inode.c : same as above
+	* super.c : same as above
+	* xattr.c : sme as above
+
+2005-03-24  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* unionfs.h : changed UNIONFS_SUPER_MAGIC. All fist based file systems
+	  will have the first 2 bytes of the MAGIC number be f15f.
+	* super.c : statfs struct is now properly filled.
+
+2005-03-23  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* INSTALL: removed the warning about NFS exports
+	* Makefile: Increases release number.
+	* NEWS: Added release news for 1.0.11
+	* unionfs.h: added a function rdstate2offset
+	* file.c: redid llseek and readdir implementation
+	* rdstate: redid find_rdstate
+
+2005-03-21  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* Add big warnings that NFS exports won't work in 1.0.10.
+
+2005-03-17  Terry Barnaby <terry1@beam.ltd.uk>
+
+	* rdstate.c (find_rdstate): Factor out rdstate search.
+	* file.c (unionfs_dir_llseek): Use rdstate if it exists.
+
+2005-03-16  Terry Barnaby <terry1@beam.ltd.uk>
+
+	* Makefile: fix clean target
+
+	* file.c (unionfs_dir_llseek): Fix mistaken use of origin
+	instead of offset.
+
+2005-03-16  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* super.c (unionfs_statfs): Use shifting instead of division.
+
+2005-03-15  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* super.c (unionfs_statfs) : df now reports the proper numbers.
+	duplicate super blocks are not factored into the calculations and
+	all the block sizes are normalized to the first partition.
+
+2005-03-14  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* main.c (unionfs_read_super) : In 2.4 the kernel expects a valid
+	super block or null we were returning an error pointer and this
+	caused an Oops. This has been fixed.
+
+2005-03-10  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* unionfs_getlk (unionfs_getlk): Add preprocessor define for
+	2.6.11+, and add a missing case.
+
+2005-03-10  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* locks.c (unionfs_setlk): fixed a typo in the 2.6 function
+
+2005-03-10  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* misc/snapmerge: Explain how to get two snapshots merged into one
+	snapshot.
+
+	* file.c (unionfs_dir_llseek), locks.c (unionfs_setlk): Fix build
+	on 2.6.
+
+2005-03-09  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_lock): function is completely rewritten to actually
+	work.
+	* locks.c (unionfs_setlk,unionfs_getlk): locks.c was added to house
+	locking functions so file.c doesent get overcrowded. unionfs_setlk
+	and unionfs_getlk were added and placed in this file with some
+	helper functions for 2.6
+
+2005-03-08  Terry Barnaby <terry1@beam.ltd.uk>
+
+	* file.c (unionfs_dir_lseek): Allow seek to beginning/end of dirs.
+
+2005-03-07  Fernando Freiregomez <fernado.freiregomez@telefonica.es>
+
+	* misc/snapmerge: Fix permissions/times after copying files.
+
+2005-03-07  Lucas Correia Villa Real <lucasvr@gobolinux.org>
+	* Makefile: Use MODDIR instead of /lib/modules/`uname -r`
+
+2005-03-07  Eduard Bloch <blade@debian.org>
+	* man/*: Fix man page sections
+	* Makefile: Add separate modules target, remove excessive uname -r's
+	so that kernel version can be overridden.
+
+2005-03-03  Anton Farygin  <rider@altlinux.com>
+
+	* Makefile: Can now build utilities without building kernel module.
+
+2005-03-03  Terry Barnaby <terry1@beam.ltd.uk>
+
+	* dentry.c (unionfs_d_revalidate): Use fist_copy_attr_all to
+	make the cache appear more coherent.
+
+2005-03-02  Fabian Franz  <fs-bugs@fabian-franz.de>
+2005-03-02  Anton Farygin  <rider@altlinux.com>
+	* dirhelper.c: Fix BUG 184: vfs_readdir is allowed to return positive
+	results.
+
+2005-03-02  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_readdir): Remove buf.error which was unused.
+
+2005-03-02  Anton Farygin  <rider@altlinux.com>
+
+	* Fix BUG 205: mv on symlinks
+
+	* Fix BUG 203: Kernel oops on creat file with len of name > 252
+
+2005-02-24  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* RPM spec file included in release.  Remove CVS directory from
+	debian directory in release.
+
+	* Add fsid= to interactions, with pointer to exports(5).
+
+2005-02-22  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* unionctl.c (find_union): Start off w/ a 1K buffer and double it
+	if the lines in /proc/mounts don't fit.  Thanks to J. H. Wilson for
+	finding and patching this bug.
+
+	* main.c (unionfs_interpose): Reorganize if statement.
+
+	* rdstate.c (add_filldir_node,find_filldir_node): Don't print the
+	names in debug mode because they aren't '\0' terminated.
+
+2005-02-22  Fabian Franz  <fs-bugs@fabian-franz.de>
+
+	* Applied patch to bug 193 Fixed the problem with
+	hardlinks not having the same inode number.
+
+2005-02-22  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* dentry.c (unionfs_d_delete): d_delete should not be defined (or
+	call the lower d_delete).  Thanks to Fabian Franz for identifying
+	this issue.
+
+2005-02-22  Fabian Franz  <fs-bugs@fabian-franz.de>
+
+	* copyup.c (unionfs_copyup_dentry_len): Applied patch provided
+	for bug #196. A device was being decoded when it didnt have to be.
+
+2005-02-22  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* README: Article URL.
+
+2005-02-18  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* dirhelper.c (create_dir_whs) : commiting chips change to make
+	sure it creates whiteouts properly.
+
+2005-02-17  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* INSTALL: Squashfs is generally OK, but we do have flock problems.
+
+	* file.c (unionfs_dir_llseek): Untested fix for BUG 187.
+
+2005-02-09  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_link): Fixed linking if the underlying file
+	system is read-only. The fix causes an issue with device being
+	busy on umount.  * rdstate.c (find_filldir_node): Print statement
+	was referencing a bad pointer.
+
+
+2005-02-08  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* copyup.c (unionfs_copyup_xattrs): Use PASSERT, other compile fixes.
+	* xattr.c (xattr_alloc, xattr_free): Shouldn't be static.
+	* Go back to GFP_KERNEL, because GFP_NOFS causes problems with vserver.
+
+2005-02-08  Jaspreet Singh  <jsingh@ensim.com>
+
+	* copyup.c (unionfs_copyup_xattrs): Use security functions.
+
+2005-02-08  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* copyup.c (unionfs_copyup_xattrs): Take the inode lock as is done in
+	setxattr.
+	* inode.c (unionfs_link): Reference counting on error paths/compile fix.
+
+2005-02-07  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_link): On copyup path directory was not unlocked.
+
+2005-02-07  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* copyup.c (unionfs_xattr_copyup): Review of code, not yet tested.
+
+	* inode.c (unionfs_link): Revert to Chip's version of
+	unionfs_link.
+
+2005-02-07  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Makefile: Allow module prefixes to be specified for
+	RPM_BUILD_ROOT
+	* rpm/unionfs.spec: Compiled, tested, and installed.
+
+2005-02-06  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* Makefile added a target for uninstall to accomodate rpm removal
+
+2005-02-02  Anton Farygin  <rider@altlinux.com>
+
+	* copyup.c (unionfs_copyup_permissions): Use ATTR_FORCE to make
+	sure any user can change permissions.
+	* copyup.c (unionfs_create_dirs): Twiddle current->fsuid/fsgid
+	so that any user's copyup will behave properly.
+
+2005-02-02  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_mmap): Don't ASSERT that the lower mmap
+	operation is there, instead return ENODEV, as is done in
+	do_mmap_pgoff.  NTFS (and others) don't support mmap, so
+	neither should we when stacked on top.
+
+2005-02-01  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* dirhelper.c : fixed 2.6 compilation. call to vfs_creat missing
+	last param in 2.6
+
+	* Makefile : namei.c is no longer needed so it was removed from
+	the make file.
+
+2005-01-28  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* subr.c, dirhelper.c: Move mkdir/rmdir helpers into separate
+	file.
+
+	* unionfs.h, fist.h: Remove unused/redundant definitions.
+
+2005-01-26  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* rdstate.c, unionfs.h: Move finding and adding filldir nodes
+	into rdstate.c.  This reduced the readdir code size, and should
+	also let us reuse the code for the subr.c functions.
+	* subr.c: Use an rdstate instead of a second readdir to remove
+	files in delete_whiteouts.
+	* namei.c: Removed vfs_unlink_nozombie.
+
+2005-01-19  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* INSTALL: Add more CFLAGS options to make things smaller.
+
+	* unionfs.h, file.c, rdstate.c: Change DIREOF from -1 to 2^31-1 to
+	fix readdir.
+
+	* unionctl.c: Don't strip trailing slash for "/" when searching for
+	the Union.
+
+	* Makefile: Include INSTALL.
+
+2005-01-14  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* branchman.c (unionfs_ioctl_addbranch): Fix error path resource leak.
+
+2005-01-14  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* copyup.c (unionfs_copyup_permissions): Changed so it actually works now
+	instead of corrupting certian bits.
+	* copyup.c (unionfs_create_dirs): Function actually uses unionfs_copyup_permissions
+	for setting directory permissions instead of setting them explicitly.
+
+2005-01-14  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_dir_llseek): Allow llseek to the current
+	offset in the directory, but nowhere else.
+	* file.c (unionfs_readdir): Save the last offset, so we
+	can lseek to it (for pretend).
+
+	* Remove all occurrences of GFP_KERNEL and replace with GFP_NOFS,
+	so our allocations can't cause file system calls.
+
+2005-01-13  Rakesh N. Iyer  <riyer@fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_readdir): Fixed deadlock in rdstate search.
+
+2005-01-13  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* Split README into INSTALL as well.
+
+	* Makefile: Spit out a message saying to read install.
+
+	* Don't build xattr functions w/o -DUNIONFS_XATTR
+
+2005-01-11  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* inode.c: Initial implementation of hardlinking a file on a read only branch.
+	  Attributes are not copied properly yet.
+
+2005-01-10  Sai Suman  <suman@pantasys.com>
+
+	* copyup.c: Fix usage of unionfs_copyup_permissions.
+
+2005-01-10  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* super.c: 2.4 compile fixes
+
+	* fist.h: Updated FISTBUG for gcc 2.9.5
+
+2005-01-10  David P. Quigley  <dquigley@fsl.cs.sunysb.edu>
+
+	* xattr.c: Format changes for size_t.
+
+	* doit.sh: Call optional postdoit script
+
+2005-01-10  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* AUTHORS file.
+
+	* README updates.
+
+	* Allow NFS exports on 2.6 (Sai Suman)
+	* Fixes to print format in xattr.c (Sai Suman)
+
+	* debian/*: Debian packaging from Alex de Landgraaf.
+
+	* Remove -Wno-unused-label for gcc 2.9.5 (alex@delandgraaf.com).
+	* Allow "EXTRACFLAGS=-DNODEBUG" in fistdev.mk to disable
+	the printing/ASSERT facility, cutting object size in half.
+
+2005-01-09  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* unionfs.h: Warn when compiling on unsupported kernels.
+	* super.c: Compile fix for 2.6.8.
+	* Makefile: Include ChangeLog in the release.
+
+2005-01-07  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Prepare 1.0.6 release.
+
+	* Fix 2.6 compile issues in new code.
+
+	* rdstate.c: Move readdir state away from subr.c.
+	Use a kmem_cache for filldir nodes.
+
+	* copyup.c: Move copyup functions away from subr.c
+
+2005-01-06  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* file.c: Working NFS readdir.  On close we store left-over
+	readdir state in the inode for up to 5 seconds.  If a readdir
+	has a matching cookie, then we pull the state out of the inode
+	and use it.  We also needed to use our own offsets, otherwise
+	the NFS client got confused.
+
+	* subr.c: Remember the number of entries in our hash table for
+	each inode, and use that as the number for the next readdir.
+
+	* Makefile: Fix dependencies on .h files.
+
+	* print.c: Consistencify printing of files and superblocks
+	and shorten some overly verbose output that added no information,
+	but reduced the effective kernel log size.
+
+	* fist.h: Make gcc check fist_dprint_internal formats.
+
+	* file.c,unionfs.h: Separate readdir state into a separate structure.
+
+	* file.c,unionfs.h,subr.c: Make readdir state use variable sized
+	hash tables based on number of pages in lower-level directories.
+
+	* Makefile: Install unionfs.ko, not unionfs.o if it exists.
+
+	* main.c: Remove MODULE_PARM because it is deprecated.
+
+2005-01-04  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_read): Fix "inifite" cat bug on 2.6
+
+	* Makefile: Auto-select 2.4 vs. 2.6 for build.
+
+	* Unify 2.4 and 2.6 Makefiles
+
+	* Fixes for printing file structures at debug level 18.
+
+	* Fixes for 2.4 compilation.
+
+2004-12-30  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* *.[ch]: Update year.
+
+2004-12-29  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c: Embed rename.txt into comment.
+
+2004-12-29  Charles P. Wright  <cwright@cs.sunysb.edu>
+
+	* inode.c, unionfs.h: Changed directory copyup function back to
+	old prototype.
+
+	* subr.c (unionfs_copyup_dentry_len): Handle directories, devices,
+	and symlinks.
+
+2004-12-27  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c: Only request copyup of parent directories.
+
+	* subr.c (unionfs_copyup_dentry_len): Handle directory copyup
+	(required for chmod).
+
+	* main.c (unionfs_parse_options): Fix along error path.
+
+2004-12-06  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* main.c (unionfs_interpose): Allow mount point crossing.
+
+	* README, Makefile, inode.c: Turn off extended attributes, unless
+	the user specifically turns them back on by defining UNIONFS_XATTR.
+	This prevents us from getting various compile errors depending on
+	vendor-specific patches.
+
+2004-11-10  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* Fix .cvsignore and release.
+
+	* README: Added warning that anything less than 2.4.20 is unsupported.
+
+	* namei.c: Removed inode_dir_notify calls when we are on less than
+	2.4.20, because the symbol is not exported.
+
+	* Fixed lots of bad casts from pointer to int in printks.
+
+	* main.c (unionfs_parse_options): Fix for badly ordered variables.
+
+	* main.c (unionfs_read_super): Remove double free along error path.
+
+	* wrapfs.h (dtopd_lhs): Remove cast from LHS of assignments.
+
+2004-11-09  Erez Zadok  <ezk@cs.sunysb.edu>
+
+	* released unionfs-1.0.2.
+
+	* README (site): fix typo. missing "`".
+
+	* Makefile (clean): remove tarball.
+
+	* README: updated.
+
+2004-11-09  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* released unionfs-1.0.1.
+
+	* Makefile: Include man pages in the release.
+
+2004-11-07  Erez Zadok  <ezk@cs.sunysb.edu>
+
+	* released unionfs-1.0.
+
+	* announce-email.txt: revised announcement text.
+
+2004-09-23  Erez Zadok  <ezk@cs.sunysb.edu>
+
+	* announce-email.txt: draft announcement email.
+
+	* README: revised.
+
+2004-09-15  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* file.c: Don't allocate directory hash table for files.
+
+	* inode.c (unionfs_setattr): Don't do partial lookup if setattr is not
+	set to all (should improve delete_whiteout).
+
+	* Some grayout code (we don't discuss rename_first anymore, so it
+	actually doesn't matter).
+
+2004-08-24  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* namei.c (vfs_create_nozombie): Whiteout creation needs a version
+	of vfs_create that won't down the i_zombie.
+
+	* subr.c (create_dir_whs_filldir):  Don't ASSERT that the whiteout
+	doesn't already exist, because we can have multiple files with
+	the same name in other branches.
+
+2004-08-23  Mohammad Nayyer Zubair  <zubair@filer.fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_mkdir, unionfs_symlink, unionfs_mknod): lookup
+	whiteout in 'bstart' not 'bindex'. We always looked up on first branch
+	as bindex was set to 0
+
+2004-08-20  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_inode_revalidate): Walk up the tree to revalidate
+	inodes like we do for dentries.
+
+	* inode.c, subr.c, unionfs.h: Use multilocks to protect against
+	unionfs_partial_lookup and revalidate operations racing against
+	each other.
+
+	* unionfs.h: Implementation of "multilocks".
+
+	* Makefile: install target
+
+	* We don't change size, so no sca_*.[ch] files.
+
+	* main.c (unionfs_hidden_dentry_index): This function is unused now
+	that Mohammad took it out, so I removed it.
+
+	* Removed unnecessary files.
+
+	* COPYING: SUNY we can't be blamed for anything notice; and the GPL.
+
+2004-08-19  Mohammad Nayyer Zubair  <zubair@filer.fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_setattr): using dtohd_index() instead of
+	unionfs_hidden_dentry() because an intermediate hidden dentry
+	could be NULL and we dont want PASSERT to oops on us
+
+2004-08-19  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_unlink_all): We shouldn't have negative dentries
+	unless they are the first one.
+
+	* inode.c (unionfs_inode_revalidate): PASSERT(hidden_inode) instead
+	of Oopsing.
+
+	* inode.c (unionfs_rename_all): Typo in the clobbering unlink.
+
+	* inode.c (unionfs_rename_all): No debug printks.
+
+	* unionfs.h (get_nlinks): Don't sum up non-directories.
+
+	* subr.c (unionfs_refresh_hidden_dentry): Re-lookup a dentry, this
+	is needed on rename reverts because vfs_rename trashes the dentry
+	of the target.
+
+	* inode.c (unionfs_rename_all): Working revert.
+
+2004-08-18  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_rename_all): New rename that handles more cases,
+	revert code doesn't work yet.
+
+2004-08-18  Mohammad Nayyer Zubair  <zubair@filer.fsl.cs.sunysb.edu>
+
+	* inode.c (rmdir_all): exit if error is not -EROFS and not -ENOTEMPTY
+
+	* inode.c (rmdir_all): exit from function and dont create whiteout
+	if vfs_rmdir returns error other than -EROFS
+
+
+2004-08-17  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_unlink_all): Unlock the directory on error.
+
+	* inode.c (unionfs_setattr): Lock on all attribute changes, not just
+	some.
+
+	* inode.c (do_rename): Check renames based on index rather than
+	new_dentry which might not exist.
+
+	* Makefile: debugging symbols are very useful for Oops tracing
+
+	* unionfs.h (CUR_MAX_BRANCH): Removed CUR_MAX_BRANCH which was always
+	sbend + 1, and replaced with sbmax (that doesn't require a redundant
+	and possbly inconsistent field within the super-block).
+
+	* unionfs.h (dtopd): Converted from a macro to an inline function
+	so that we can do more checking of the private data that we are
+	returning.
+
+	* unionfs.h (dtohd_index) Added a udi_bcount field to the private data
+	so we would Oops rather than silently overflow array bounds.
+
+	* dentry.c: Recursively revalidate the parent (the ASSERT that was
+	checking this was mis-written).  It turns out that our parent was not
+	always valid.
+
+	* file.c (unionfs_file_revalidate): Always reassign the read-ahead
+	values because the older file might have different values and llseek
+	will cause an assertion failure.
+
+	* inode.c (unionfs_lookup_backend): Treat revalidated negative and
+	positive dentries differently.
+
+2004-08-16  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c: Use an fd_set instead of an integer for tracking successful
+	renames.
+
+	* branchman.c: Don't allow more than FD_SETSIZE (1024) branches.
+
+	* unionctl.c: Specific message for exceeding FD_SETSIZE.
+
+2004-08-11  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c: Return ENAMETOOLONG when looking up ".wh.*"
+
+	* unionctl.c: Add ioctl.
+
+	* branchman.c: Don't allow branches without MAY_READ
+
+	* unionctl.c: Convert branch pathnames into index automatically.
+
+	* unionctl.c: Match the longest prefix of our path inside of
+	/proc/mounts instead of the path itself.  This is required so that
+	remove doesn't need to open the root of the union.
+
+	* unionctl.c: Remove branch ioctl and list branch configuration.
+
+2004-08-10  Mohammad Nayyer Zubair  <zubair@filer.fsl.cs.sunysb.edu>
+
+	* file.c: added CAP_SYS_ADMIN checks before calling unionfs ioctls
+
+	* uniondbg.c: now contains debugging related ioctls
+
+	* unionctl.c: now contains ioctls to add, remove and set branch
+	permissions moved here
+
+	* uniondbg.c: ioctls to add, remove and set branch permissions
+	moved here
+
+	* fist_ioctl.c: remaining ioctls stay here
+
+	* subr.c: making sure copyupuid, copyupgid and copyupmode are all
+	specified when copyup option is set to mounter.
+
+	* inode.c: implemented unionfs_rename_first()
+
+2004-08-09  Mohammad Nayyer Zubair  <zubair@filer.fsl.cs.sunysb.edu>
+
+	* inode.c: more places where we should exit if get an error other than
+	-EROFS in unlink/rmdir and related functions.
+
+	* subr.c: exit if get an error other than -EROFS when
+	creating/deleting whiteouts
+
+	* inode.c: making sure that we create/mkdir/symlink/mknod to the
+	left only if the error returned is -EROFS, otherwise passup
+
+	* subr.c: use notify_change instead of directly modifying the inode
+	fields.
+
+	* subr.c: use notify_change instead of directly modifying the inode
+	fields.
+
+	* main.c: added copyupuid, copyupgid and copyupmode mount options.
+	These options will specify the mode, uid and gid of copied-up files.
+	copyup option should be set to mounter.
+
+	* subr.c: using the above mount time values for copied-up files.
+
+	* unionfs.h added these values in the unionfs super block.
+
+2004-06-17  Charles P. Wright <cwright@fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_flush): Flush all branches (only makes a
+	difference for directories).
+
+2004-08-03  Mohammad Nayyer Zubair  <zubair@filer.fsl.cs.sunysb.edu>
+
+	* super.c: Implemented unionfs_show_options. Can now view unionfs
+	mount time options in /proc/mounts
+
+	* inode.c: using is_robranch_super instead of is_robranch in
+	create, mkdir, symlink, mknod operations, whenever we try to
+	remove whiteouts if they exist.
+
+	* main.c: while parsing options using strcmp wherever we can,
+	removed all magic numbers, copyup options are now preserve,
+	currentuser, mounter
+
+	* subr.c: when copying up the default mode should be original
+	owner (preserve)
+
+2004-08-02  Mohammad Nayyer Zubair  <zubair@filer.fsl.cs.sunysb.edu>
+
+	* main.c: unionfs mount time flag changed from being an integer
+	flag to several text options that will together make up the flag.
+
+2004-05-17  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* branchman.c: Fixed off by one error when adding more than 8
+	branches.
+
+2004-05-10  Charles P. Wright  <cwright@polarbear.fsl.cs.sunysb.edu>
+
+	* main.c (unionfs_read_super): Error check to prevent oops on bad
+	mount-time options.
+
+2004-04-21  Charles P. Wright  <cwright@polarbear.fsl.cs.sunysb.edu>
+
+	* file.c: revalidate files when the are passed into our methods,
+	this allows copy-up of open files.  Yeah snapshots.
+
+	* dentry.c: Should not use d_hash and d_compare because Unionfs is
+	in charge of the namespace, not the lower-level file systems.
+
+	* Unionfs now has split-view caches.  If compiled with
+	-DSPLIT_VIEW_CACHES, then Unionfs supports duplicating the super
+	block structure and dynamically selecting the correct structure to
+	use when crossing into the unionfs mountpoint.  Right now the
+	default behavior is that root has one view, and every other user
+	has another view.
+
+	If the super block is not duplicated then everything works as
+	before.
+
+2004-04-20  Charles P. Wright  <cwright@polarbear.fsl.cs.sunysb.edu>
+
+	* branchman.c: Super Duper works!  An ioctl can let you create a
+	super block that is a copy of the original.  When you stat the
+	mountpoint that super block is returned.  Now to fix the reference
+	counting bugs associated with unmount. :)
+
+2004-04-19  Charles P. Wright  <cwright@arcticfox.foo>
+
+	* super.c (unionfs_select_super): return s_root for now
+
+2004-03-15  Charles P. Wright  <cwright@arcticfox.foo>
+
+	* dentry.c (unionfs_d_revalidate): Fix refcounts.
+
+	* inode.c (unionfs_inode_revalidate): Fix reference counting.
+
+	* inode revalidate "works" (e.g., no Oopses or other broken f/s
+	behavior), but has broken reference counting.
+
+2004-03-14  Charles P. Wright  <cwright@arcticfox.foo>
+
+	* main.c (unionfs_interpose): Changed neg_dent_flag to which was
+	true if we did *not* have a negative dentry to is_negative_dentry
+	which *is* true if we have a negative dentry.  Fixed related
+	assert (which was ASSERT(1)).
+
+	* inode.c (unionfs_lookup): Read through and simplified lookup by
+	removing duplicated code or nasty if/else statements when if
+	continue would work(to the tune of 20%).  Clarified comments as
+	well.  Now instead of being a 248 line monster it is a 199 line
+	monster.
+
+	* UNIONFS-TODO: We still need to solve whiteouts (i.e., you can
+	sucessfully stat a whiteout in Unionfs).  Note this is not
+	introduced by my changes.
+
+2004-03-12  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* branchman.c (unionfs_ioctl_rdwrbranch): Read/write branch setting.
+
+2004-03-11  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* branchman.c (unionfs_ioctl_addbranch): Refcount fix.
+
+2004-03-04  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c (do_rename): Correctly check is_readonly.
+
+	* fist.h (print_exit_pointer): Print out pointer using %p and
+	don't convert using PTR_ERR if it is not IS_ERR.
+
+	* subr.c (unionfs_create_parent_dir): Return error as pointer
+
+	* subr.c (create_whiteout): Check branch before creating whiteouts.
+
+	* unionfs.h (VALID_MOUNT_FLAGS): Define valid mount flags in the
+	header, so main.c doesn't need to know all of them.
+
+	* doit.sh (FLAG): Default mode should be UNLINK_ALL, not
+	UNLINK_WHITEOUT.
+
+	* inode.c (unionfs_unlink): Unlink creates whiteout to the left.
+
+2004-03-03  Charles P. Wright  <cwright@polarbear.fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_create): Creation on a robranch will
+	cause the file to be created to the left.
+
+	* unionfs.h: More macros for checking if things are on read-only
+	branches.
+
+	* xattr.c: Separate xattrs from inode.c.
+
+	* ATTACH-TODO.txt: Unionfs doesn't attach.
+
+	* README.attach: Unionfs doesn't attach.
+
+	* mmap.c: Unionfs doesn't filter data.
+
+	* vm_area.c: No data filtering.
+
+2004-03-02  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* main.c: Updated parsing code so that dirs can be specified as
+	b0=rw,b1,b2=ro,b3.  Defaults to rw.  Need to actually integrate
+	the permission checking code elsewhere.
+
+2003-11-10  Harikesavan Pathangi Krishnan  <hari@a-rh72i.fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_readdir): Changed code to fix the NFS related bug.
+
+2003-10-27  Harikesavan Pathangi Krishnan  <hari@mooby.fsl.cs.sunysb.edu>
+
+	* subr.c (unionfs_copyup_dentry_len): This is a modified form of
+	unionfs_copyup_dentry() that takes in the length of file to be
+	copied up. The length is useful when the file size is changed in
+	the setattr.
+
+2003-10-01  Puja Gupta  <puja@t6.fsl.cs.sunysb.edu>, Jay, Mohammad.
+
+	* subr.c (unionfs_copyup_dentry): fixed copyup bug, update bstart, bend
+	on error on vfs_create.
+
+2003-09-27  Puja Gupta  <puja@a-rh8.fsl.cs.sunysb.edu>, Hari, Jay.
+
+	* inode.c (unionfs_rename_all): fixed to call rename and create
+	whiteout for test_rename script.
+
+2003-09-26  Puja Gupta  <puja@a-rh8.fsl.cs.sunysb.edu>, Zubair, Hari.
+
+	* subr.c (create_whiteout): removed unnecessary dput of hidden_dentry.
+
+2003-09-26  Puja Gupta  <puja@icon.fsl.cs.sunysb.edu>, Hari
+
+	* subr.c, inode.c, unionfs.h: Better function name and comments.
+
+	* inode.c (unionfs_rename_all): changed to create whiteouts
+	properly on error conditions.
+	(unionfs_rename_whiteout): initilize parent dentry.
+
+2003-09-26  Harikesavan Pathangi Krishnan  <hari@icon.fsl.cs.sunysb.edu>, Puja.
+
+	* print.c (fist_print_dentry): added print for d_parent, aligned.
+
+	* subr.c (create_whiteout_left_parent): Added function for
+	creating whiteout in a parent directory by a char * string.
+	(unionfs_create_parent_dir): initialize dtohd before checking and
+	changing the bstart and bend for new dentry.
+	(unionfs_copyup_dentry): removed extra dput, since fput internally
+	does that.
+
+	* inode.c (unionfs_rename_whiteout): fixed to support copyup for EROFS.
+
+2003-09-24  Puja Gupta  <puja@story.fsl.cs.sunysb.edu>, Hari, Jay.
+
+	* subr.c (unionfs_create_parent_dir): Fixed updating bstart for copyup.
+
+2003-09-23  Puja Gupta  <puja@idol.fsl.cs.sunysb.edu>, Hari, Jay.
+
+	* subr.c (unionfs_create_parent_dir): Updates bstart, bend, dputs
+	extra negative dentries.
+
+2003-09-23  Puja Gupta  <puja@story.fsl.cs.sunysb.edu>, Hari, Jay.
+
+	* subr.c (unionfs_create_parent_dir): Dput the previous negative
+	dentry.
+
+	* inode.c (unionfs_create, unionfs_mkdir, unionfs_mknod, unionfs_link)
+	(unionfs_symlink): Removed dgets and dputs to handle errors in
+	vfs functions.  Now handled in unionfs_create_parent_dir.
+	(do_rename): removed dputs and dgets to balance with create_parent.
+
+2003-09-22  Harikesavan Pathangi Krishnan  <hari@t1.fsl.cs.sunysb.edu>, Mohammad.
+
+	* subr.c (unionfs_create_parent_dir): Fixed a bug related to
+	igrabbing lower inode.
+
+2003-09-22  Akshat Aranya  <aaranya@t6.fsl.cs.sunysb.edu>
+
+	* main.c (unionfs_parse_options): Fix some memory leaks in error
+	paths.
+
+2003-09-22  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* main.c (unionfs_read_super): Fixed error path.
+
+2003-09-22  Erez Zadok  <ezk@ulkesh.fsl.cs.sunysb.edu>
+
+	* match-malloc.pl: document this script with a procedure telling
+	how to use malloc debugging.  Use -DFIST_MALLOC_DEBUG from now on.
+
+2003-09-22  Puja Gupta  <puja@icon.fsl.cs.sunysb.edu>
+
+	* fist.h, main.c, subr.c: turned off MEMORY_DEBUG.
+
+2003-09-22  Puja Gupta  <puja@t1.fsl.cs.sunysb.edu>
+
+	* main.c (unionfs_read_super): Memory fix.
+
+2003-09-22  Puja Gupta  <puja@mooby.fsl.cs.sunysb.edu>
+
+	* inode.c, dentry.c, super.c: Memory Leak Checks.
+
+2003-09-22  Erez Zadok  <ezk@agora.fsl.cs.sunysb.edu>
+
+	* subr.c: add transcation counter to malloc/free debugging.
+
+	* match-malloc.pl: perl script to parse log output from
+	KMALLOC/KFREE macros, and report leaks etc.
+
+2003-09-21  Mohammad Nayyer Zubair  <zubair@t1.fsl.cs.sunysb.edu>, Puja, Hari, Jay.
+
+	* inode.c (unionfs_lookup): changed the dput location of hidden
+	whiteout dentry in the code.
+
+2003-09-22  Puja Gupta  <puja@icon.fsl.cs.sunysb.edu>
+
+	* subr.c (check_empty): added list_del.
+
+	* inode.c (unionfs_lookup): kfree moved at out.
+	(unionfs_unlink_whiteout): added kfree.
+
+2003-09-21  Mohammad Nayyer Zubair  <zubair@t1.fsl.cs.sunysb.edu>, Puja, Hari, Jay
+
+	* subr.c (create_parent_dir()): added support for left to right
+	copy up.  Could be used in unionfs_link()
+
+2003-09-21  Puja Gupta  <puja@icon.fsl.cs.sunysb.edu>, Jay, Hari, Mohammad.
+
+	* subr.c (check_whiteout): check for error on kmalloc.
+	(check_empty): moved kfree after 'out' label.
+
+	* main.c (unionfs_read_super): changed to dput superblock dentries on
+	error conditions.
+
+	* file.c (unionfs_filldir): check for error on kmalloc.
+
+2003-09-21  Mohammad Nayyer Zubair  <zubair@t1.fsl.cs.sunysb.edu>, Puja, Hari, Jay.
+
+	* inode.c (unionfs_link()): unlinking .wh.foo (if exists)
+	while creating link called foo
+
+2003-09-21  Puja Gupta <pugupta@cs.sunysb.edu>, Hari, Jay, Mohammad.
+
+	* inode.c (unionfs_mkdir, unionfs_mknod): dget hidden_dentry if not
+	calling create_parent_dir.
+
+	* inode.c (unionfs_mkdir, unionfs_mknod): removed extra dget before
+	removing whiteout.
+
+2003-09-21  Mohammad Nayyer Zubair  <zubair@t1.fsl.cs.sunysb.edu>, Hari, Jay, Puja.
+
+	* inode.c (unionfs_symlink()):  removed extra dget() in symlink
+
+2003-09-21  Puja Gupta <pugupta@cs.sunysb.edu>
+
+	* inode.c, dentry.c, unionfs.h: variable declaration re-shuffled to
+        compile on local machine.
+
+2003-09-21  Mohammad Nayyer Zubair  <zubair@t1.fsl.cs.sunysb.edu>, Hari, Jay, Puja
+
+	* subr.c (create_dir_whs()):  removed extra dget on hidden dentry
+
+	* file.c (unionfs_open()): calling branchput if get an error in
+	opening hidden dentry
+
+2003-09-21  Puja Gupta  <puja@icon.fsl.cs.sunysb.edu>, Jay, Hari, Mohammad.
+
+	* inode.c: removed extra bstart, and comments.
+
+	* file.c (unionfs_readdir): Added a bindex++ to {un,re}-reverted code.
+	Works now!! ;-)
+
+2003-09-21  Mohammad Nayyer Zubair  <zubair@t1.fsl.cs.sunysb.edu>, Hari, Jay, Puja.
+
+	* subr.c (create_whiteout_left()): synched reference counts for
+	hidden_dentry; added a dget for the hidden_dentry
+
+2003-09-21  Puja Gupta  <puja@t1.fsl.cs.sunysb.edu>, Jay.
+
+	* inode.c (unionfs_create): removed extra dget, dput for vfs_rename.
+	Removed extra dputs, d_drop on error.
+
+2003-09-21  Puja Gupta  <puja@icon.fsl.cs.sunysb.edu>
+
+	* subr.c (unionfs_create_parent_dir): removed extra dput on error
+	in lookup.
+	(create_whiteout_left): mode set to create whiteout.
+	(delete_whiteouts): removed dput on NULL hidden_dentry.
+
+2003-09-20  Mohammad Nayyer Zubair  <zubair@mooby.fsl.cs.sunysb.edu> Puja, Jay
+
+	* inode.c:  removed all "dtohd_index(dentry, bindex) = NULL" references and any
+	extra dputs
+
+	* subr.c:  modified code in troublesome create_whiteout_left() function.
+	Removed extra dgets on the parent and dputting the hidden dentry
+
+2003-09-20  Puja Gupta  <puja@icon.fsl.cs.sunysb.edu>
+
+	* subr.c: added branchput for branchgets.
+
+	* print.c (fist_checkinode): fixed the count print condition.
+
+	* inode.c (unionfs_unlink_whiteout): changes to dget, notify_change.
+	match dget and dput.
+
+2003-09-20  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Makefile (tags): target for tags
+
+	* inode.c (unionfs_unlink_whiteout): Refcount fix.
+
+2003-09-20  Puja Gupta  <puja@icon.fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_readdir): reverting back to file.c before changes
+	related to readdir_called flag.  Was going into an infinite loop due
+	to some condition.  The current version works fine.
+
+	* subr.c (create_dir_whs_filldir): replaced "return err" with
+	"goto out", which was causing an infinite loop.
+	(check_whiteout, delete_whiteouts): replaced 'ret' with err.
+
+2003-09-19  Puja Gupta  <puja@icon.fsl.cs.sunysb.edu>
+
+	* subr.c (create_dir_whs): added filldir_called flag.
+
+2003-09-19  Puja Gupta  <puja@idol.fsl.cs.sunysb.edu>, Jay, Hari, Mohammad.
+
+	* subr.c (check_empty): check if filldir is called or not.
+
+2003-09-19  Harikesavan Pathangi Krishnan  <hari@t1.fsl.cs.sunysb.edu>, Jay, Chip, Puja, Mohammad
+
+	* subr.c (check_empty): Added a flag field in the callback
+	structure that makes readdir call once again if all the contents
+	of the directory are not read.
+
+2003-09-19  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_release): free resources on file close.
+
+	* file.c (unionfs_filldir): Print a warning on any duplicate, but
+	don't return -EIO.
+
+	* subr.c (create_whiteout_left): fix a refcount leak
+
+2003-09-19  Mohammad Nayyer Zubair  <zubair@mooby.fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_filldir()): fixed the rm -Rf (-EIO, ENOTEMTPY
+	error) cases.  return -EIO ONLY when get a 'foo' and 'foo' exists
+	in the same directory, which would indicate corruption in the file
+	system. Before were returning -EIO ALSO if filldir gets a
+	'.wh.foo' and 'foo', latter being in the linked list already
+
+2003-09-17  Puja Gupta  <puja@idol.fsl.cs.sunysb.edu>, Hari, Mohammed.
+
+	* Makefile: Changing back to old file.
+
+	* subr.c (create_whiteout_left): removed dput, hidden_dentry == NULL.
+
+	* inode.c (do_rename): If error, make hidden_dentry NULL.
+	(unionfs_rename_all): copyup file on err = -EROFS.
+
+	* unionfs.h: added copyup_dentry.
+
+	* subr.c (unionfs_copyup_dentry): added this function, called from
+	setattr.
+	(unionfs_copyup_file): modified, calls copyup_dentry internally. Added
+	checks before kfree.
+
+	* inode.c (unionfs_setattr): calls copyup when trying to setattr for a
+	RO branch.
+
+	* fist.h (FISTBUG): imported from ncryptfs.
+
+2003-09-17  Harikesavan Pathangi Krishnan  <hari@t1.fsl.cs.sunysb.edu>
+
+	* inode.c: Fixed the bug related to creation of whiteouts on the
+	leftmost branch when a directory on that branch is rmdir'ed.
+
+2003-09-16  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* subr.c: fixed indentation for readdir actors
+
+	* inode.c (unionfs_rmdir): create_whiteout can just not do anything
+
+	* unionfs.h (itohi, dtohd): Reference count check before returning
+	to user space.
+
+2003-09-16  Puja Gupta  <puja@a-rh8.fsl.cs.sunysb.edu>, Hari, Mohammad.
+
+	* inode.c (unionfs_create): removed a dget, was giving a seg fault.
+	(unionfs_lookup): error check for lookup_one_len added.
+	(unionfs_rmdir_all): checking if rmdir failed on leftmost, and if it
+	had whiteouts, remove them, and call vfs_rmdir again, to create
+	whiteout.
+
+2003-09-16  Mohammad Nayyer Zubair  <zubair@t6.fsl.cs.sunysb.edu> Hari <hari@fsl.cs.sunysbe.du>
+
+	* main.c (unionfs_interpose()):  removed unnecessary fist_prints()
+
+2003-09-16  Mohammad Nayyer Zubair  <zubair@t6.fsl.cs.sunysb.edu> Hari <hari@fsl.cs.sunysbe.du>
+
+	* print.c (fist_print_dentry() and fist_print_inode()): removed
+	the anding code when printing the mode of the inode. Was printing
+	a negative mode value.
+
+2003-09-15  Puja Gupta  <puja@a-rh8.fsl.cs.sunysb.edu>, Hari, Mohammed.
+
+	* subr.c: fput for copied up file not required.
+
+	* print.c: print level 9 changed to 8.  Level 9 doesn't get printed.
+
+	* unionfs.h (IS_COPYUP_ERR, IS_WRITE_FLAG): Added.
+
+	* subr.c (unionfs_copyup_file): Checks added, replaced variables, fixed.
+
+	* inode.c (unionfs_permission): One code for files and directories.
+	Error bypassed for -EROFS, but not for leftmost branch.
+
+	* file.c (unionfs_open): Check added for RO partition, and call copyup.
+
+2003-09-15  Mohammad Nayyer Zubair  <zubair@t6.fsl.cs.sunysb.edu> Hari <hari@fsl.cs.sunysbe.du>
+
+	* inode.c (unionfs_mknod): done. removes whiteout if
+	present. format similar to unionfs_mkdir(). Tested.
+
+2003-09-14  Puja Gupta  <puja@idol.fsl.cs.sunysb.edu>
+
+	* inode.c (do_rename): Fixed parent directory for lookup of whiteout.
+	(unionfs_rename_whiteout): Added case for DELETE_WHITEOUT for rename.
+        (unionfs_rename): Check destination->inode before S_ISDIR.
+
+2003-09-14  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_filldir): Remove a duplicate from the list if
+	filldir fails.
+
+	* dentry.c (unionfs_d_revalidate, unionfs_d_compare, unionfs_d_hash):
+	Turned into utility functions for printing to reduce volume of debug
+	output.
+
+2003-09-14  Puja Gupta  <puja@idol.fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_rename_all): fix for bindex counter.
+	(unionfs_rename_all): check if error not occured in bstart of
+	destination.
+	(unionfs_rename_all): whiteout is always create in the bstart of
+	source, not to go to left of it.
+
+2003-09-14  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Detect EIO on single directories by doing duplicate elimination
+	routine.
+
+2003-09-13  Puja Gupta  <puja@t3.fsl.cs.sunysb.edu>, Jay.
+
+	* inode.c: added dput for hidden_dentry, whiteout_dentry that
+	are not used after lookup.
+
+	* subr.c: added dput for hidden_dentry, whiteout_dentry that
+	are not used after lookup.
+
+2003-09-13  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* PASSERT is for pointers.  ASSERT is for non-pointer conditions.
+
+2003-09-12  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* main.c: Sanity checks on flag=.
+
+	* subr.c (create_whiteout_left): Balanced a dput w/ a dget.
+
+	* print.c: exit should be level 5, not 4.  hidden_dentries/inodes
+	are level 9 instead of 8.
+
+	* Changed ASSERT to Oops when the pointer is poisoned.
+
+	* Changed ASSERT(foo != NULL) to ASSERT(foo)
+
+	* Default flag is 0x0.
+
+2003-09-11  Puja Gupta  <puja@idol.fsl.cs.sunysb.edu>, Hari, Zubair, Jay.
+
+	* inode.c (unionfs_create, unionfs_symlink): whiteouts truncated
+	to zero and ctime updated.
+
+	* unionfs.h: added flags COPYUP_OWNER, COPYUP_FS_MOUNTER,
+	redefined flags.
+
+	* inode.c: unlock parent added.
+
+	* file.c (unionfs_open): check for error and call copyup on EROFS.
+
+	* inode.c (unionfs_unlink): changed the function like rmdir. Used
+	rename instead of unlink, create for whiteouts.
+	(unionfs_mkdir): restructured code.
+
+	* unionfs.h (GLOBAL_ERR_PASSUP): removed DELETE_ERR_PASSUP.
+	GLOBAL_ERR_PASSUP indicates whether to pass error back or create
+	whiteouts/try again.
+
+	* inode.c (unionfs_create, unionfs_symlink, unionfs_link):
+	restructured changed *_ERR_PASSUP to GLOBAL_ERR_PASSUP.
+
+	* unionfs.h (GLOBAL_ERR_PASSUP): added, for all functions,
+	whether, on error, should passup or try to make up for error
+	encountered.
+
+	* inode.c (unionfs_symlink): updating bstart, bend for symlink
+	create.  Restructured the code.
+
+2003-09-11  Mohammad Nayyer Zubair  <zubair@t6.fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_symlink): done. removes whiteout if
+	present. same format as in unionfs_create()
+
+	* inode.c (unionfs_link): creating destination path in source's
+	branch instead of vice versa
+
+	* inode.c (unionfs_rmdir_all): removed unneccessary d_drop()
+	call. unionfs_rmdir() calls it at the end
+
+2003-09-10  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_rmdir): Split into several functions.
+
+	* subr.c (delete_whiteouts): Delete all whiteouts in
+	a given directory in a given branch.
+
+	* subr.c (check_empty): Check if a directory is empty.
+
+2003-09-10  Puja Gupta  <puja@idol.fsl.cs.sunysb.edu>, Hari.
+
+	* unionfs.h: unionfs_interpose now returns void.
+
+	* subr.c (unionfs_partial_lookup): removed extra dput.
+	unionfs_reinterpose now returns void.
+
+	* main.c (unionfs_reinterpose): is now a void function and igrab
+	was called twice for already existing hidden inode. Fixed.
+
+	* file.c (single_branch_filldir): removed fist_dprint printed
+	extra newline character on commandline during readdir.
+
+	* unionfs.h (COPYUP_CURRENT): removed multiple definition.
+
+2003-09-09  Puja Gupta  <puja@idol.fsl.cs.sunysb.edu>
+
+	* subr.c (unionfs_full_lookup): Removed this function, was similar
+	to unionfs_partial_lookup.
+
+	* inode.c (unionfs_unlink): added if-else for three different
+	options for unlink, and also for error handling (passup or
+	whiteout). Replaced unionfs_full_lookup with
+	unionfs_partial_lookup (Both functions were same, just change in
+	name).  Added dget, dput at appropriate places.
+
+	* unionfs.h (DELETE_FIRST, DELETE_WHITEOUT, DELETE_ERR_PASSUP):
+	added, and removed all other unlink flags.
+
+2003-09-09  Mohammad Nayyer Zubair  <zubair@t6.fsl.cs.sunysb.edu> Hari <hari@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_lookup):  returning EIO for directory whiteout.
+
+2003-09-02 Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu> Hari <hari@fsl.cs.sunysb.edu>
+
+	* inode.c: mkdir and create revisted, restructured and tested
+
+2003-09-08  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_write): Fixed positioning code.
+
+2003-09-07  Puja Gupta  <puja@idol.fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_create): added dget, get_parent, double_lock
+	and error check for vfs_rename of whiteouts. Added d_drop for any
+	negative dentry at the end of function.
+	(unionfs_mkdir): added error check for vfs_unlink of whiteouts,
+	unionfs_interpose, create_dir_whs.  Removed multiple unlock_dir.
+
+	* subr.c (create_dir_whs_filldir, create_whiteout_left):
+	changed char name[PATH_MAX] to char *name.
+
+	* inode.c (unionfs_create, unionfs_lookup, unionfs_unlink)
+	(unionfs_mkdir): changed char name[PATH_MAX] to char *name.
+	(unionfs_rmdir): removed unused name, wh_name.
+
+2003-09-06  Puja Gupta  <puja@idol.fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_create): reverted back to the loop for create
+	from bstart to zeroth branch.
+
+2003-09-02 Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* inode.c: file to file rename done.
+
+	* major items left: dir to dir rename
+
+	* minor items left: mknod, symlink
+
+2003-09-05  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* file.c (unionfs_filldir, single_branch_filldir): Removed memory
+	leak for debug output in filldirs.
+	(unionfs_readdir): Return -failure from filldir (previously our
+	filldir had no error propagation).
+
+	* ioctls via extended attributes works.
+
+	* branchman.c (unionfs_ioctl_delbranch): Don't let people delete
+	the last remaining branch.  It would be cooler if we could, but
+	probably not worth all the effort to make the root dentry behave
+	properly when there is nothing underneath it.
+
+	* file.c: Removed branch management functions.
+
+	* inode.c): xattr functions.
+
+	* branchman.c: Branch management functions.
+
+2003-09-03  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* super.c (unionfs_clear_inode): Fixed cleanup of stale inodes.
+
+	* fist.h (ASSERT2): Macro to print out caller when we fail.
+
+	* Changed ASSERT2's to ASSERT, since they weren't really being
+	used as an ASSERT2.
+
+	* unionfs.h (itohi, dtohd): Converted to a function, added an
+	ASSERT2 to make sure we don't underflow the array.
+
+2003-09-02 Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* subr.c: removed the unnecessary vfs_rename() function call in
+	create_whiteout_left(). Doing vfs_create() directly.
+
+	* inode.c: fixes in lookup(). Started on unionfs_rename().
+
+
+2003-09-01 Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* subr.c: added function int remove_whiteouts(dentry_t *dentry,
+	dentry_t *hidden_dentry, int bindex)
+	Called by unionfs_rmdir(). Does vfs_readdir() and then unlinks all
+	whiteouts entries in it.
+
+2003-09-01  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Updated CUR_MAX_BRANCH and MOUNT_FLAG.
+
+	* dentry.c (unionfs_d_revalidate): Make negative dentries that
+	just turned negative stale.
+
+	* inode.c (unionfs_inode_revalidate): Make negative inodes stale.
+
+	* stale_inode.c: Functions to make a magic stale inode.
+
+2003-09-01 Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* inode.c: unionfs_mkdir() done and works as per updated design.
+	Creating whiteout entries for all entires to the right. Details on
+	this are in the updated function_description.html file
+
+	* file.c: a separate filldir function for single branch directory.
+	Ignoring whiteout entries.
+
+	* subr.c: added a function: int create_whs_right(dentry_t *dentry,
+	int cur_index) Does a vfs_readdir on all directories starting from
+	cur_index + 1 and creates whiteout entries in cur_index (called in
+	unionfs_mkdir()). Details on this are in the updated
+	function_description.html file
+
+	preliminary cases are working as expected
+
+	Left: rename, mknod, symlink.
+
+2003-09-01  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* file.c: Add branch ioctl and trimmed unionfs_dir_fops.
+
+	* fist.h: Structure for add branch ioctl.
+
+	* fist_ioctl.c: Add branch and cleaner branchcount ioctl.
+
+2003-08-31  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* dentry.c (unionfs_d_revalidate): Do revalidation of dentries,
+	we now can remove the leftmost branch and expose old contents.
+
+2003-08-27 Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* inode.c: unionfs_unlink() implemented as per new design specs
+	(ignoring intermiate directories for now)
+
+	* subr.c: added create_whiteout_left(dentry, index) function:
+	creates a whiteout in index, on error it proceeds to the left.
+
+	* file.c: in fill_dir() ignoring whiteout entries
+
+	* Left: mkdir, mknod, symlink, rmdir, rename
+
+2003-08-27  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* Branch removal sort of works.
+
+2003-08-27 Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* inode.c: unionfs_create() implemented as per new design specs.
+	If found a .wh.foo entry, vfs_rename it to foo.
+
+	* inode.c: unionfs_lookup() implemented as per new design specs.
+	If found a whiteout entry, just stop lookup
+
+	* inode.c: unionfs_link() implemented as per new design specs
+
+	* inode.c: unionfs_setattr() implemented as per new design specs
+
+	* inode.c: unionfs_permission() implemented as per new design specs.
+
+	* file.c: calling unionfs_copyup_file() in unionfs_open() if open
+	with specified flags fails with the current underlying file
+
+	* subr.c:  fixes in unionfs_copyup_file()
+
+	* unionfs.h: modified mount time flags
+
+	* Still left (according to new specs): mkdir, symlink, mknod,
+	unlink, rmdir, rename hard.
+
+2003-08-26  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_inode_revalidate): Refresh inode from lower level.
+
+	* fist_ioctl.c: Increment super generation number.
+
+	* file.c (unionfs_ioctl_incgen): Increment super generation number.
+
+2003-08-20  Charles P. Wright  <cwright@fsl.cs.sunysb.edu>
+
+	* file.c: Branch reference counters updated on open/close.  Need
+	to figure out how do to update them when a unionfs directory
+	becomes the cwd of a process.
+
+	* doit.sh: Source doitopts or doitopts.`uname -n` so different
+	developers can have different setups, w/o changing CVS.
+
+	* unionfs.h: Added generation number to super block, inode, and dentries.
+
+	* attach.c: Not used.
+
+2003-08-13  Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* subr.c: some fixes in unionfs_copyup_file() (untested yet)
+
+	* file.c: calling unionfs_copyup_file() if write() fails on the
+	leftmost file.  Failure could result because of read-only
+	permissions on the file so probably should do copy up only if
+	branch is mounted RO.
+
+	* unionfs.h: modified definition for unionfs_copyup_file()
+
+2003-08-13  Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* subr.c: added function: unionfs_copyup_file(inode_t *dir,
+	dentry_t *dentry, int oldbindex, int newbindex) first does
+	recursive directory creation, then does vfs_create() and then
+	reads PAGE_SIZE bytes from old file to new file
+
+	* subr.c, inode.c: moved useful functions:
+	unionfs_create_parent_dir() and unionfs_partial_lookup() to subr.c
+	from inode.c
+
+	* unionfs.h: definition for unionfs_copyup_file
+
+	* Makefile: added subr.o to the list of OBJS
+
+2003-08-13  Puja Gupta  <puja@idol.fsl.cs.sunysb.edu>
+
+	* unionfs.h: added definition for unionfs_create_whiteout.
+
+2003-08-12  Puja Gupta  <puja@mooby.fsl.cs.sunysb.edu>
+
+	* inode.c: reverting back to old copy without changes to rename.
+
+2003-08-04  Puja Gupta <pugupta@cs.sunysb.edu>
+
+	* unionfs.h (UNLINK_WHITEOUT, UNLINK_ALL_FIRST, UNLINK_ERR): New flags
+	for handling various unlink options.
+
+	* main.c (unionfs_parse_options): fixed check for getting mount flags.
+
+	* inode.c (unionfs_unlink): added unlink for whiteout. If flag is set
+	for UNLINK_WHITEOUT, try to create whiteout in branches to left, create
+	recursive subdirectories.  Also added check for various flags for
+	unlink_first or unlink_all, and how to handle error on unlink.
+	(unionfs_create_parent_dir): check in for loop fixed.
+
+2003-08-04  Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* file.c: readdir is working fine with the hashtable in the private
+	data of unionfs file.
+	diff -ru on gcc tarball is successful
+
+	* inode.c: implemented option 2 of link(), recursively creating source
+	path in destination branch when branches are different
+
+	* unionfs.h: added two mount time flags
+	LINK_EXDEV: just return -EXDEV when dentries are on different branches
+	LINK_RECURSIVE: recursively create source path in target branch
+
+2003-07-31  Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* file.c: readdir/filldir's linked list converted into a hashtable
+	of size HASHTABLE_SIZE Duplicate elimation is working. Still to
+	test the gcc tarball.
+
+	* unionfs.h: converted dir_list list head to an array of list
+	heads of size HASHTABLE_SIZE
+
+
+2003-07-30  Puja Gupta <pugupta@cs.sunysb.edu>
+
+	* unionfs.h: changed to new flag, CREATE_RW_ERR, UNLINK_WHITEOUT,
+	UNLINK_ERR.  Function definition for unionfs_partial_lookup.
+
+	* inode.c (unionfs_partial_lookup): added, called from
+	unlink. Looks up the remaining files that were not looked up in
+	unionfs_lookup.
+	(unionfs_unlink): code now transferred to unionfs_partial_lookup.
+	(unionfs_create): flag name is CREATE_RW_ERR.
+
+	* inode.c (unionfs_create): changed to new flag values.
+
+	* unionfs.h (IS_SET): changed flags to be exclusive. Also, changed
+	IS_SET to work on place value of bit.
+
+2003-07-27  Puja Gupta <pugupta@cs.sunysb.edu>
+
+	* file.c (unionfs_open): removed ftohd_index(file, bindex) = NULL.
+	unionfs_getdents_callback now has file_t *.
+	(unionfs_filldir, unionfs_readdir): changed the global list_head
+	for duplicate elimination to have a list_head in private data of
+	file. So, the list of name, namelen is stored in this new
+	list_head, and compared against this.
+
+	* unionfs.h: added struct filldir_node to private data of file.
+	Thus, every file has unique and seperate list for readdir.
+
+	* doit.sh: "flag=" instead of "flags=".
+
+2003-07-28  Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* inode.c: unionfs_link's option 1 done. Return -EXDEV if old and
+	new dentries are in different branches / starting option 2: if
+	different branches, creating destination path in the source branch
+
+	* unionfs.h: added a simple function which returns the sum of all
+	the underlying inodes' nlink value
+
+	* inode.c: calling the above function wherever nlinks value was
+	being set
+
+2003-07-24  Puja Gupta  <puja@t2.fsl.cs.sunysb.edu>
+
+	* unionfs.h: #defines for MOUNT_FLAG, CREATE_PASSUP,
+	CREATE_RW_ERR_PASSUP, CREATE_TRY_LEFT and IS_SET.
+
+	* main.c (unionfs_parse_options): parse the mount time flag and
+	initialize it.
+
+	* inode.c (unionfs_create): added handling of partial errors to
+	handle various cases: 1) Try to Left (default), 2) PassUp, 3) If
+	EPERM on RW branch, PassUp.
+
+	* doit.sh: changed mount options to include "flag=".
+
+2003-07-24  Puja Gupta  <pugupta@cs.sunysb.edu>
+
+	* inode.c (unionfs_setattr): added check for NULL hidden dentry.
+	(unionfs_inode_revalidate): added ASSERT2 for inode.
+
+2003-07-24  Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* file.c: in unionfs_open, now only opening leftmost file instead
+	of opening all of the hidden files.  Opening all directories
+	though.
+
+	* inode.c: setattr function done. Assigning 'correct' value to the
+	n_links variable in the unionfs inode.  In inode_revalidate
+	function assigning 'correct' value to the n_links variable in the
+	unionfs inode.
+
+	* main.c: just moved sum_nlinks variable to the beginning of the
+	  interpose function. diff -ru on a randomly distributed
+	  (recursive) gcc tarball is failing. Some duplicate files are
+	  being listed.  'ls' on the directory containing above duplicate
+	  files also lists the files twice. Files have same unionfs inode.
+	  Checking out whats happening.
+
+2003-07-22  Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* main.c: in interpose() added lines which compute the sum of the
+	nlinks of the underlying inodes and assigns this sum to the
+	unionfs inode.
+
+	* inode.c: in permission() added check for a null hidden inode
+
+2003-07-22  Puja Gupta <pugupta@cs.sunysb.edu>
+
+	* unionfs.h: init_file_array is now init_ftohf_ptr,
+	init_inode_array is now init_itohi_ptr, init_sb_array is now
+	init_stohs_ptr, added init_stohiddenmnt_ptr, init_dentry_info,
+	init_dentry_array is now init_dtohd_ptr.  All this makes sure that
+	all allocated memory is initialized to zero before its being used.
+
+	* super.c (unionfs_read_inode): init_inode_array is now init_itohi_ptr.
+
+	* main.c (unionfs_parse_options): init_priv_inode is now
+	init_dentry_info.
+	(unionfs_parse_options): memset, init_stohs_ptr, init_stohiddenmnt_ptr
+	added for initialize memory allocated to NULL.
+	(unionfs_read_super): init_dtohd_ptr added to initialize to NULL.
+
+	* inode.c (unionfs_lookup): init_dentry_array is now init_dtohd_ptr.
+
+	* file.c (unionfs_open): added init_ftohf_ptr, setting underlying
+	file pointer to NULL.
+
+2003-07-21  Puja Gupta <pugupta@cs.sunysb.edu>
+
+	* inode.c (unionfs_create): get rid of the hidden dentry that lead
+	to an unsuccessful attempt to create. Update bstart, bend
+	accordingly.
+	(unionfs_create_parent_dir): added "count--" for proper pointer
+	position. Updated private data, bstart for the dentry and inode of
+	intermediate directories created.  Also, updated bend for the
+	negative dentry returned from this function to create.
+
+	* main.c (unionfs_interpose): removed comments and extra line
+	spaces.
+
+2003-07-20  Puja Gupta  <puja@idol.fsl.cs.sunysb.edu>
+
+	* unionfs.h (MAX_DIR_CREATE): added MAX_DIR_CREATE for the number of
+	directories allowed to be created in recursive subdir creation.
+	Should be dynamic.
+
+	* inode.c (unionfs_create): changed to handle partial error by
+	recursively creating directory to left branches.
+	(unionfs_create_parent_dir): added this function, it creates
+	hidden directory path for a given parent inode and dentry in
+	specified branch. (Still to test)
+
+2003-07-15  Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* inode.c <unionfs_lookup>: Found a bug while mounting unionfs on
+	a randomly distributed (recursive) am-utils tarball on 5 branches.
+	Basically we were not memsetting the array of pointers to hidden
+	objects after kmallocing a CUR_MAX_BRANCH number of pointers for
+	dentry, inode, file and sb.  They were assumed to be NULL in
+	lookup and hence were not explicitly set to NULL in the array when
+	requireed.
+
+	* unionfs.h: Added macros which initialize hidden arrays to NULL
+
+2003-07-15  Puja Gupta  <puja@mooby.fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_lookup): added few debug statements.
+	(unionfs_unlink): Since lookup for file had just the leftmost
+	entry, unlink now internally calls lookup for all remaining
+	branches.  And the current policy for unlink is, unlink
+	all. (Tested).
+
+	* unionfs.h: added definition for unionfs_reinterpose.
+
+	* main.c (unionfs_reinterpose): This new function takes a dentry
+	with leftmost hidden inode interposed, and reinterposes the newly
+	lookedup hidden inodes in remaining branches.  Called from
+	unionfs_unlink.
+
+	* mount_tarball.sh, mount_tarball_random.sh: These scripts are now
+	moved to ../tools directory.
+
+2003-07-13  Puja Gupta  <puja@mooby.fsl.cs.sunysb.edu>
+
+	* inode.c (unionfs_lookup): fixed initialization of negative dentry.
+
+	* print.c (fist_print_inode), (fist_print_file),(fist_print_dentry),
+	(fist_print_sb): made more readable, pointers now printed with %p.
+
+2003-07-10  Puja Gupta  <puja@idol.fsl.cs.sunysb.edu>
+
+	* super.c (unionfs_statfs): check for duplicate superblock data being
+	added.
+	(unionfs_umount_begin): check for NULL hidden_sb, hidden_sb->s_op.
+
+2003-07-10 Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* print.c: fist_print_inode(): prints underlying inodes if any.
+	fist_print_dentry(): prints underlying dentries if any.
+	fist_print_files(): prints underlying files if any.
+	fist_print_superblock(): prints underlying sbs if any.
+
+2003-07-10 Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* print.c: improved the formatting of printing inode, dentry and
+	sb
+
+	* main.c, inode.c removed unneccessary lines printing debugging
+	info in interpose()
+
+2003-07-13 Mohammad Nayyer Zubair <mzubair@ic.sunysb.edu>
+
+	* file.c inode.c: removed debugging info lines
+
+	* created mount_tarball.sh.  right now, distributes an equal
+		number of files/dirs among N branches tar-ing am-utils
+		through unionfs mount point gives same package buildall
+		through mount point gives same structure/files
diff -urN oldtree/fs/unionfs/INSTALL newtree/fs/unionfs/INSTALL
--- oldtree/fs/unionfs/INSTALL	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/INSTALL	2006-07-12 19:01:51.000000000 -0400
@@ -0,0 +1,182 @@
+To build Unionfs, there are two main components:
+
+-  unionfs.ko: the kernel module in 2.6
+
+-  unionctl: a user utility which allows you to add and remove branches
+
+You should be able to just type "make" and Unionfs will build itself for the
+running kernel.  The Makefile will look for your running kernel sources in
+/lib/modules/`uname -r`/build/include.
+
+If your kernel sources are located in a different directory, create a
+"fistdev.mk" file along the lines of:
+LINUXSRC=/path/to/my/kernel/sources/linux-2.6.xx
+TOPINC=-I$(LINUXSRC)/include
+MODDIR=/lib/modules/2.6.xx
+
+LINUXSRC points to the root of a Linux source tree.  TOPINC should point to
+that tree's include directory.  Finally, the module will be copied to
+$MODDIR/kernel/fs/unionfs.ko.
+
+You must set all three variables together, otherwise you can have problems
+compiling, installing, or loading the module.
+
+You also need to have the headers for e2fsprogs installed, on Red Hat
+derived systems (like Fedora Core), this means that you need to have
+e2fsprogs-devel installed.
+
+There are two Makefile options related to extended attribute support,
+which is turned off by default.  You should define UNIONFS_XATTR to turn
+it on.  Vanilla kernels should work automatically, but if you (or your
+vendor) has applied the ACL/EA patches you might need to define
+FIST_SETXATTR_CONSTVOID to correct the setxattr operation's function
+prototype.
+
+Using fistdev.mk, you can also turn on the debugging print system,
+which adds to the modules code size significantly.  Just add
+"EXTRACFLAGS=-DUNIONFS_DEBUG" to fistdev.mk.
+
+The doit.sh script included in the distribution will mount unionfs
+with two branches (/branch0 and /branch1) by default.  You can use it
+as an example and edit to your tastes.
+
+To install unionfs run "make install".  This copies unionfs.ko into
+/lib/modules/`uname -r`/kernel/fs/unionfs; copies the utilities into
+/usr/local/sbin; and copies man pages into /usr/local/man;
+
+fistdev.mk summary:
+UNIONFS_OPT_CFLAG	By default -O2.  If you want a different optimization
+			level change this variable.
+UNIONFS_DEBUG_CFLAG	By default -g.  If you want to remove debug, set
+			this variable to nothing.  This will result in a
+			smaller (but harder to debug) Unionfs.
+EXTRACFLAGS		Additional stuff to pass to the compiler, this
+			is useful when combined with the definitions below.
+			(e.g., EXTRACFLAGS=-DUNIONFS_DEBUG to turn on
+			debugging).
+LINUXSRC		Where to find the kernel build directory.
+TOPINC			Where to find the kernel headers.
+PREFIX			Where to install Unionfs utilities.
+			By default /usr/local.
+MODPREFIX		What is the prefix to the root directory for modules,
+			by default this is unset.  Your modules will end up
+			in /lib/modules/`uname -r`/kernel/fs.  With
+			MODPREFIX=/foo they end up in
+			/foo/lib/modules/`uname -r`/kernel/fs.
+UNIONFS_IMAP		Compiles in persistent inode code. Mount should fail
+			if you try to use the imap option without this.
+Define options summary:
+UNIONFS_DEBUG		Turn on debugging facility (increases code size).
+UNIONFS_UNSUPPORTED	Bypass kernel versions checks.
+SUPPORT_BROKEN_SETUP	Enable sendfile, which allows you to run losetup, but
+			if you try to write to a loop file, you will cause
+			an Oops.
+UNIONFS_DELETE_ALL	Enable delete=all mode.
+UNIONFS_MMAP		Enable mmap support in unionfs still experimental.
+
+Known interactions with other kernel features/patches:
+* If you get an error like this on Fedora Core 4:
+  make: *** /lib/modules/2.6.11-1.1369_FC4/build: No such file or directory.  Stop.
+  Then you need to install the kernel-devel package.
+
+* If you get an error like this:
+  unionimap.h:9:23: uuid/uuid.h: No such file or directory
+  Followed, by more parse errors,  then you need to make sure you have
+  e2fsprogs-devel installed (or your distribution's equivalent).
+
+* Some NFS servers return -EACCES instead of -EROFS when they are exported
+  read-only.  This means that we can't legitimately determine when a user is
+  not allowed to access a file.  To enable a hack so that NFS appears to work
+  correctly (but NFS ACLs will break), mount the nfs-branch with mode nfsro.
+
+* If you want to export Unionfs over NFS, then you need to add
+  extra information to /etc/exports.  knfsd will not export Unionfs unless
+  you have an fsid option in /etc/exports.  This is because Unionfs has no
+  real device.  See man exports(5) for more information on fsid.
+
+* If you want to use Unionfs as your root file system you need to use
+  pivot_root.  www.linux-live.org provides scripts for creating live CDs with
+  Unionfs as the root.  The following commands are adapted from the linuxrc
+  from linux-live 5.0.16.  You may want to look at the original script from
+  www.linux-live.org if you are having problems.  SLAX (www.slax.org) is a
+  an actual live CD distribution based on these scripts.
+
+  UNION=/union
+  MEMORY=/memory
+  MOUNTDIR=/mnt
+  CHANGES=$MEMORY/changes
+
+  mkdir -p $UNION
+  mkdir -p $MEMORY
+  mount -t tmpfs tmpfs $MEMORY
+
+  mkdir -p $CHANGES
+  mount -t unionfs -o dirs=$CHANGES=rw unionfs $UNION
+
+  # Here other things can be added to the Union, by using unionctl.
+
+  # Finally set union as root
+  cd $UNION
+  mkdir -p initrd
+  pivot_root . initrd
+  exec chroot . sbin/init <dev/console >dev/console 2>&1
+  # You should never get here
+
+* If you want to use losetup, then you need to define -DSUPPORT_BROKEN_LOSETUP
+  You will be able to use it read-only, but when you try to use it read-write,
+  you will get an Oops.  This should eventually be fixed when we have our
+  own address-space operations (so that we can support sendfile).
+
+* Selinux requires extended attributes (but has not been tested by us).
+  You should compile Unionfs with Extended Attribute support by adding
+  EXTRACFLAGS=-DUNIONFS_XATTR to fistdev.mk.  After this, you can follow
+  the following instructions are from Jaspreet Singh:
+
+  1. Install strict/targetted selinux policy sources
+  2. Open /etc/selinux/<policy_type>/src/policy/fs_use
+  3. Append "fs_use_xattr unionfs system_u:object_r:fs_t;"
+  4. Compile, install, and reload the selinux policy
+
+  "There were a couple of issues with Unionfs but they were minor."
+
+* tmpfs does not support fsyncing directories, so if you have a Union with
+  tmpfs as the leftmost branch, fsync returns EINVAL.
+
+Other known limitations:
+* Unionfs does not provide cache coherency.  What this means to you is that
+  if you directly modify the lower-level branches, then Unionfs will get
+  confused.  You can tell Unionfs to throw out its cache and recreate all
+  of its objects (lazily), by running "uniondbg -g UNION".
+
+  It is especially dangerous to create or remove whiteouts from underneath
+  Unionfs, as there are several places where it asserts on invariants
+  that must be true (e.g., if the file exists, the whiteout should not and
+  vice versa).
+
+* Unionfs doesn't support sendfile.  This often manifests itself as apache
+  serving zero length files.  You can turn off sendfile n Apache with the
+  EnableSendfile httpd.conf directive (see
+  http://httpd.apache.org/docs/2.0/mod/core.html).   This is also the reason
+  that losetup is unsupported, Unionfs needs its own address space operations
+  otherwise upper and lower-level files and pages get mixed and matched.
+
+* If you restart an NFS server, you will get ESTALE errors on the client
+  because Unionfs does not have persistent inode numbers.  You should also
+  consider NFS over TCP, so lost packets don't cause readdir to get confused.
+
+* Renaming a directory on a read-only branch returns an EXDEV error.  The "mv"
+  utility is designed to handle this error (at least GNU coreutils and busybox
+  will handle this correctly), so a user will not see anything.  The only
+  issue will be applications that internally rename a directory, but do not
+  properly handle EXDEV (which is really a bug on the application's part).
+
+* The documentation needs to improve
+
+Integrating Unionfs into a 2.6 kernel source tree (2.4 is not supported):
+1. First run patch-kernel.sh included with Unionfs. Its first argument is the
+   path to your kernel source tree.
+2. Configure and compile your kernel as you normally would.   Unionfs is at the
+   bottom of the the File systems -> Miscellaenous filesystems menu.
+3. To install the Unionfs utilities (i.e., unionctl and uniondbg), run
+   "make utils install-utils" from the Unionfs source directory.
+4. Boot into your new kernel, and enjoy Unionfs.
diff -urN oldtree/fs/unionfs/Makefile newtree/fs/unionfs/Makefile
--- oldtree/fs/unionfs/Makefile	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/Makefile	2006-07-12 19:01:51.000000000 -0400
@@ -0,0 +1,16 @@
+UNIONFS_VERSION=20060708-1403
+SUP_MAJOR= 2
+SUP_MINOR= 6
+SUP_PATCH= 18
+
+EXTRA_CFLAGS+=-DUNIONFS_VERSION=\"${UNIONFS_VERSION}\" -DSUP_MAJOR=${SUP_MAJOR} -DSUP_MINOR=${SUP_MINOR} -DSUP_PATCH=${SUP_PATCH}
+
+# This will enable full debugging support
+# EXTRA_CFLAGS+=-DUNIONFS_DEBUG
+
+obj-$(CONFIG_UNION_FS) += unionfs.o
+
+unionfs-objs := subr.o dentry.o file.o inode.o main.o super.o \
+	stale_inode.o branchman.o xattr.o rdstate.o copyup.o  \
+	dirhelper.o rename.o unlink.o lookup.o persistent_inode.o \
+	commonfops.o dirfops.o print.o
diff -urN oldtree/fs/unionfs/NEWS newtree/fs/unionfs/NEWS
--- oldtree/fs/unionfs/NEWS	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/NEWS	2006-07-12 19:01:51.000000000 -0400
@@ -0,0 +1,241 @@
+* Unionfs 1.3 (intended for use with 2.6.17)
+- Fixed makefile release & build target
+- Fixed up uniondbg & unionimap man pages
+- Updated RPM specfile
+- Implemented export functions (for NFS)
+- Updated to compile with 2.6.17
+
+* Unionfs 1.2 (intended for use with 2.6.16)
+- Added basic mmap support (experimental, off by default)
+- Added check to see if its necessary to run the delete_all
+  regression tests
+- Cleaned up list/get xattr code
+- Created documentation directory
+- Fixed unionfs_permission on reiserfs and XFS
+- Fixed link-unlink issue (i_nlink going down to zero)
+- Fixed Bug #501: mv dir ..
+- On error, NULL the pointers after free - Fixes nasty mount bug
+- Moved userspace utilities into utils subdirectory
+- Updated to work with the new mutex subsystem
+- Simplified and cleaned up debugging support (use -DUNIONFS_DEBUG to
+  _enable_ debugging)
+- Header files simplified
+
+* Unionfs 1.1.5
+- Backport: Corrected sample mount commands
+- Backport: Fixed link-unlink issue
+- Backport: Fixed refcounting leak (if a branch doesn't exist during mount,
+  the previously looked up branches were not freed properly)
+- Backport: Fixed inode_permission returning EROFS when it wasn't supposed
+  to
+- Backport: Fixed bug #501 (mv dir ..)
+
+* Unionfs 1.1.4
+- Backport: On error, NULL the pointers after free - Fixes nasty mount bug
+- Backport: Fixed unionfs_permission on reiserfs and XFS
+- Backport: Added check to see if its necessary to run the delete_all
+  regression tests
+- Added explicit check for kernel versions newer than 2.6.15
+(If you want to use 2.6.16 or newer, look at Unionfs 1.2.x)
+
+* Unionfs 1.1.3
+- Simplified delete=whiteout
+- Switched default delete mode to the simpler whiteout mode
+- Disable delete=all by default (enable with -DUNIONFS_DELETE_ALL)
+- Removed all copy up modes except preserve (the old default)
+- Unlink partially copied up files
+- Compartmentalize malloc debugging
+- Fix uid/gid/mode not being reset when a whiteout is recreated
+- Fix stale atime/mtime bugs for recreated whiteouts
+- Cleaned up peristent inode code a bit (still unstable)
+
+* Unionfs 1.1.2
+- Added inode refcounting debugging tool
+- Fixed race between lookup and d_free
+- Fixed double free/unchecked malloc
+- Fixed permission bug, creat/open truncates the running executable
+- Fixed many reference counting bugs
+- Moved code from fist.h into unionfs*.h
+- Removed unneeded UNIONFS_XATTR
+- Removed ASSERTs in favor of BUG_ONs
+- Removed FISTBUG
+- Rename dentry locking changed to use 2.6 calls
+- Replaced NFS_SECURITY_HOLE by nfsro branch option
+- Cleaned up code to "pass" a Sparse run
+- RPM spec file updated
+- Miscellaneous cleanups
+
+* Unionfs 1.1.1
+- Directory reading is fixed
+- Improved locking for branch manipulation operations
+- Fix for removing an opaque directory
+- Some cleanups and error handling fixes to the copyup code
+
+* Unionfs 1.1.0
+- Removed excess 2.4 code (Unionfs 1.1.0 is for 2.6 only)
+- Removed file locking code, as the VFS does a better job of it.
+- Fixed off by one errors when kmallocing names in inode.c.
+- Fixed several deadlock bugs.
+- Fixed several rename and rmdir bugs.
+- Permissions checked on files that are on read-only file systems
+- AMD64 fix for compiling on gentoo.
+- Properly update generation when we combine copyup and reopening.
+- Cleanedup Makefile to remove 2.4 clutter.
+- Added documentation for the MODDIR option in the INSTALL file.
+
+* Unionfs 1.0.14
+Features:
+- Dropped unmaintained setattr,diropaque,delete=first and mount flags
+- Updated unionfs.4 to reflect default mount modes
+- You can use the root of the Union to remove branches with unionctl
+- Use official Debian packaging files.
+- Linux 2.6.13 support.
+
+Bug fixes:
+- Fixed several dentry refcount bugs introduced by new deletion framework.
+- Fixed uninitialized fd_set in the query ioctl.
+- Branch reference counting now works across insertion and removal of branches.
+  This prevents the branch counts from getting "confused".
+- Handle "/" as a branch in unionctl.
+- Removed static buffer for debug prints in favor of vprintk.
+- NFS silly renames avoided during several cases.
+- Attempting a write lock causes a copyup, so that the underying flock will
+  work.
+- Cleaned up mount option parsing.
+- Improved link counting for directories.
+
+* Unionfs 1.0.13
+- Unionfs now handles deleting an open file (and several other deletion fixes).
+  Thanks to the Knoppix people and Tomas Matejicek for finding these bugs.
+- The NODEBUG flag has been renamed to UNIONFS_NDEBUG.
+- Instead of allocating all of the lower-level pointers in dentries, inodes
+  and files, we have a fixed number of them preallocated and malloc only if
+  we exceed that number.
+- Fixed a dentry reference count bug with UNIONFS_NDEBUG.
+- Support for persistent inode numbers across mounts.
+- Added a "which-branch" ioctl and corresponding --query to unionctl, so that
+  it is possible to find out where a file or directory is coming from.
+- In 2.6.11 and onwards, don't take the BKL for ioctls.
+
+* Unionfs 1.0.12
+- Copy-up is delayed until a write operation occurs, so spurious copy-ups are
+  avoided.
+- Improved copyup UID spoofing for mkdir
+- Instructions and shell script for compiling Unionfs into a monolothic kernel.
+- Added support for opaque directories, which mask the contents of
+  lower-priority directories.  This makes recreating a removed directory an
+  O(1)  operation instead of an O(n) operation, and also makes subsequent
+  operations faster.  There were also several whiteout related fixes bundled
+  in here.
+- Fixed bug with whiteouts on Reiserfs
+- Fixed bug when a file transitioned from a negative dentry to a positive
+  dentry (i.e., if you add a branch that inserts a file you previously
+  looked up, but did not exist)
+- Fixed return code for extended attribute functions
+- amd64 compile fixes
+- Extended attribute copy-up
+- Unionfs returns EXDEV instead of copying up a directory on move, this
+  fixes a bug where Unionfs would "forget" the contents of a copied up
+  directory after unmount.
+- Fix for following symlinks
+- Improved print routines (separated generic from unionfs printing routines)
+- INSTALL has some instructions for using Unionfs as a root file system and
+  also for using Unionfs with Selinux.
+- Dentry private data uses a separate kmem_cache and inodes in 2.6 use their
+  own kmem_cache.  This should improve performance a bit because there are
+  fewer individual memory allocations (and less wasted space).
+
+* Unionfs 1.0.11
+This release fixes NFS exporting which was broken in 1.0.10.
+- f_pos and lseek's offsets are unified, so readdir works on NFS exported
+  unionfs mounts again.
+
+* Unionfs 1.0.10
+This release fixes several bugs over 1.0.9, but it will not correctly work if
+you want to NFS export it.  For that you need to use 1.0.9 on 2.4.  In 1.0.11,
+we will fix NFS exports correctly.
+
+- Don't d_delete entries that don't have positive refcounts (Fabian Franz)
+- Hardlinks should point to the same inode (Fabian Franz)
+- File locking works correctly.
+- Improved directory llseeking.
+- snapmerge preserves permissions/times after copying files
+- Improved Makefile
+- Unionfs_d_revalidate copies attributes so the cache appears more coherent.
+- Directory reading bug fixed.
+- symlink copyup fixed
+- Don't oops when creating files longer than 252 characters
+- Some readdir fixes.
+- df produces correct size results
+
+* Unionfs 1.0.9
+- Fixed copyup permissions bugs (Anton Farygin)
+- NTFS mmap will no longer cause Oops (but it won't do the mmap, as NTFS
+  doesn't support mmap).
+- Moved directory reading helper routines into dirhelper.c from subr.c
+- We no longer require hacked copies of vfs_create and vfs_unlink
+- Moved finding and adding filldir_nodes into rdstate.c
+- Added a sample RPM spec file, for those who want to package Unionfs
+  (Note, we are currently not distributing source or binary RPMS)
+- Fixed unionfs_link to allow hard-linking of files with a source of a
+  readonly file system.
+- Fixed apt-get on rw/ro branch configuration (related to unionfs_link)
+- Modified regression/link.sh to reflect the changes in unionfs_link. It
+  now includes tests to make sure linking on a read only filesystem works.
+- Fixed an issue where unionfs_dir_llseek would cause an Oops
+
+* Unionfs 1.0.8
+- Fixed bug with readdir (but oddly not ls) that prevented that last entry
+  from being shown.
+- Include INSTALL file with information about installing Unionfs.
+- Fix unionctl bug with "/" as the path to search for
+- Fix to copyup permissions on directories
+- Allow llseek to current directory position, but no others
+
+* Unionfs 1.0.7
+- Fixed locking for readdir state, this fixes a kernel Oops on preemptive
+  kernels, and deadlocks on SMP kernels.
+- Prevent readdir from giving -ESTALE on last entry.
+- Fixes for gcc 2.9.5 (Alex de Landegraaf)
+- Debian package scripts (Alex de Landegraaf)
+- NFS Export on 2.6 (Sai Suman)
+- Copyup permissions fix (Sai Suman)
+- Warn when compiling on unsupported kernels (<2.6.9 or <2.4.20)
+- Improved hardlink support (but not quite there yet)
+
+* Unionfs 1.0.6
+A lot of code has changed in the 1.0.6 release, so it may not be as stable as
+the 1.0.5 release.
+
+- Makefile fix over 1.0.5 for install target on 2.6
+- unionfs.h, fist.h Makefile dependency fixed
+- Support for readdir over NFS
+- Hash table sizes are now based on the size of the lower-level directory
+- In 2.4 inode private data resides in the inode.u field, saving memory
+- Use kmem caches for filldir nodes rather than wasting space with extra
+  large kmallocs, also move short names inline
+
+* Unionfs 1.0.5
+- Support for both 2.6 and 2.4 kernels
+
+* Unionfs 1.0.4
+- Copyup now correctly handles directories, devices, and symlinks.
+- Extended attributes are now off by default because they cause too many
+  compile problems.
+- The regression tests is now included in the release, but is not actually
+  installed.  These tests are rather primitive, but do check some fundamental
+  Unionfs behavior on multiple branches.
+
+* Unionfs 1.0.3
+- Compile fixes for older (and newer) compilers than found on Redhat 9.
+- Compile fixes for older kernels.
+- Don't Oops when passed bad directories.
+
+* Unionfs 1.0.2
+- Minor README updates
+
+* Unionfs 1.0.1
+- Fix to release target which includes manual pages.
+
+* Unionfs 1.0
+- First public release
diff -urN oldtree/fs/unionfs/README newtree/fs/unionfs/README
--- oldtree/fs/unionfs/README	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/README	2006-07-12 19:01:51.000000000 -0400
@@ -0,0 +1,84 @@
+Copyright (c) 2003-2006 Erez Zadok
+Copyright (c) 2003-2006 Charles P. Wright
+Copyright (c) 2005-2006 Josef Sipek
+Copyright (c) 2005      Arun M. Krishnakumar
+Copyright (c) 2005-2006 David P. Quigley
+Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+Copyright (c) 2003      Puja Gupta
+Copyright (c) 2003      Harikesavan Krishnan
+Copyright (c) 2003-2006 Stony Brook University
+Copyright (c) 2003-2006 The Research Foundation of State University of New York
+
+THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND ANY
+EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE FOR ANY
+DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+We're pleased to announce the first release of a unioning file system
+for Linux, called Unionfs.  To download software and documentation,
+see
+
+	http://www.fsl.cs.sunysb.edu/project-unionfs.html
+
+Unionfs is a stackable unification file system, which can appear to merge
+the contents of several directories (branches), while keeping their physical
+content separate.  Unionfs is useful for unified source tree management,
+merged contents of split CD-ROM, merged separate software package
+directories, data grids, and more.  Unionfs allows any mix of read-only and
+read-write branches, as well as insertion and deletion of branches anywhere
+in the fan-out.  To maintain Unix semantics, Unionfs handles elimination of
+duplicates, partial-error conditions, and more.  This release also includes
+additional preliminary features that were specifically designed for security
+applications, such as snapshotting and sandboxing.
+
+This Unionfs release supports only one 2.6 kernel version (run 'make kvers'
+to find out which).  For older kernels look into Unionfs version 1.0.x and
+1.1.x.  For detailed information about the new Unionfs versioning scheme,
+look at docs/versions.txt.  You also need to have the development headers
+for e2fsprogs installed.
+
+Unionfs is released under the GPL (see the COPYING file in the distribution
+for details).
+
+For more information on using Unionfs, download the tarball and see the
+following man pages:
+
+- unionfs.4:  Describes how to mount unionfs
+
+- unionctl.8: Describes how to control an already mounted Union
+
+For more information about Unionfs internals (which we think are really cool
+:-), see the following technical report at the above Web site:
+
+  C. P. Wright, J. Dave, P. Gupta, H. Krishnan, E. Zadok, and M. Zubair
+  "Versatility and Unix Semantics in a Fan-Out Unification File System"
+  Technical Report FSL-04-01b, October 2004
+  Computer Science Department, Stony Brook University
+  http://www.fsl.cs.sunysb.edu/docs/unionfs-tr/unionfs.pdf
+
+In addition, you can find an article in Linux Journal (December 2004 issue)
+titled "Unionfs: Bringing File Systems Together."  It is available online at
+http://www.linuxjournal.com/article/7714.
+
+See the INSTALL file for instructions on building Unionfs, iteractions with
+kernel features, and known bugs and limitations.
+
+To report bugs, please email them to the "unionfs@filesystems.org" list (see
+www.filesystems.org), or submit them via Bugzilla to
+https://bugzilla.filesystems.org/.  But reports with fixes are most welcome.
+
+Enjoy,
+Erez and Charles (on behalf of the Unionfs team)
+
+##############################################################################
+## For Emacs:
+# Local variables:
+# fill-column: 70
+# End:
+##############################################################################
diff -urN oldtree/fs/unionfs/branchman.c newtree/fs/unionfs/branchman.c
--- oldtree/fs/unionfs/branchman.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/branchman.c	2006-07-12 19:01:51.000000000 -0400
@@ -0,0 +1,564 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: branchman.c,v 1.64 2006/07/08 17:58:30 ezk Exp $
+ */
+
+#include "unionfs.h"
+
+struct dentry **alloc_new_dentries(int objs)
+{
+	if (!objs)
+		return NULL;
+
+	return KZALLOC(sizeof(struct dentry *) * objs, GFP_KERNEL);
+}
+
+struct unionfs_usi_data *alloc_new_data(int objs)
+{
+	if (!objs)
+		return NULL;
+
+	return KZALLOC(sizeof(struct unionfs_usi_data) * objs, GFP_KERNEL);
+}
+
+static void fixputmaps(struct super_block *sb)
+{
+	struct unionfs_sb_info *spd;
+	struct putmap *cur;
+	int gen;
+	int i;
+
+	print_entry_location();
+
+	spd = stopd(sb);
+	cur = spd->usi_putmaps[spd->usi_lastputmap - spd->usi_firstputmap];
+
+	for (gen = 0; gen < spd->usi_lastputmap - spd->usi_firstputmap; gen++) {
+		if (!spd->usi_putmaps[gen])
+			continue;
+		for (i = 0; i <= spd->usi_putmaps[gen]->bend; i++)
+			spd->usi_putmaps[gen]->map[i] =
+			    cur->map[spd->usi_putmaps[gen]->map[i]];
+	}
+
+	print_exit_location();
+}
+
+static int newputmap(struct super_block *sb)
+{
+	struct unionfs_sb_info *spd;
+	struct putmap *newmap;
+	int count = 0;
+	int i;
+
+	print_entry_location();
+
+	spd = stopd(sb);
+
+	i = sizeof(int) * (sbend(sb) + 1);
+	newmap = KMALLOC(sizeof(struct putmap) + i, GFP_KERNEL);
+	if (!newmap) {
+		print_exit_status(-ENOMEM);
+		return -ENOMEM;
+	}
+
+	if (!spd->usi_firstputmap) {
+		spd->usi_firstputmap = 1;
+		spd->usi_lastputmap = 1;
+
+		spd->usi_putmaps = KMALLOC(sizeof(struct putmap *), GFP_KERNEL);
+		if (!spd->usi_putmaps) {
+			KFREE(newmap);
+			print_exit_status(-ENOMEM);
+			return -ENOMEM;
+		}
+	} else {
+		struct putmap **newlist;
+		int newfirst = spd->usi_firstputmap;
+
+		while (!spd->usi_putmaps[newfirst - spd->usi_firstputmap] &&
+		       newfirst <= spd->usi_lastputmap) {
+			newfirst++;
+		}
+
+		newlist =
+		    KMALLOC(sizeof(struct putmap *) *
+			    (1 + spd->usi_lastputmap - newfirst), GFP_KERNEL);
+		if (!newlist) {
+			KFREE(newmap);
+			print_exit_status(-ENOMEM);
+			return -ENOMEM;
+		}
+
+		for (i = newfirst; i <= spd->usi_lastputmap; i++) {
+			newlist[i - newfirst] =
+			    spd->usi_putmaps[i - spd->usi_firstputmap];
+		}
+
+		KFREE(spd->usi_putmaps);
+		spd->usi_putmaps = newlist;
+		spd->usi_firstputmap = newfirst;
+		spd->usi_lastputmap++;
+	}
+
+	newmap->bend = sbend(sb);
+	for (i = 0; i <= sbend(sb); i++) {
+		count += branch_count(sb, i);
+		newmap->map[i] = i;
+	}
+	for (i = spd->usi_firstputmap; i < spd->usi_lastputmap; i++) {
+		struct putmap *cur;
+		cur = spd->usi_putmaps[i - spd->usi_firstputmap];
+		if (!cur)
+			continue;
+		count -= atomic_read(&cur->count);
+	}
+	atomic_set(&newmap->count, count);
+	spd->usi_putmaps[spd->usi_lastputmap - spd->usi_firstputmap] = newmap;
+
+	print_exit_status(0);
+	return 0;
+}
+
+/* XXX: this function needs to go. There is no reason for this to be here */
+int unionfs_ioctl_branchcount(struct file *file, unsigned int cmd,
+			      unsigned long arg)
+{
+	int err = 0;
+	int bstart, bend;
+	int i;
+	struct super_block *sb = file->f_dentry->d_sb;
+
+	print_entry_location();
+
+	bstart = sbstart(sb);
+	bend = sbend(sb);
+
+	err = bend + 1;
+	if (!arg)
+		goto out;
+
+	for (i = bstart; i <= bend; i++) {
+		if (put_user(branch_count(sb, i), ((int __user *)arg) + i)) {
+			err = -EFAULT;
+			goto out;
+		}
+	}
+
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+int unionfs_ioctl_incgen(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	int err = 0;
+	struct super_block *sb;
+
+	print_entry_location();
+
+	sb = file->f_dentry->d_sb;
+
+	unionfs_write_lock(sb);
+	if ((err = newputmap(sb)))
+		goto out;
+
+	atomic_inc(&stopd(sb)->usi_generation);
+	err = atomic_read(&stopd(sb)->usi_generation);
+
+	atomic_set(&dtopd(sb->s_root)->udi_generation, err);
+	atomic_set(&itopd(sb->s_root->d_inode)->uii_generation, err);
+
+      out:
+	unionfs_write_unlock(sb);
+	print_exit_status(err);
+	return err;
+}
+
+int unionfs_ioctl_addbranch(struct inode *inode, unsigned int cmd,
+			    unsigned long arg)
+{
+	int err;
+	struct unionfs_addbranch_args *addargs = NULL;
+	struct nameidata nd;
+	char *path = NULL;
+	int gen;
+	int i;
+
+	int pobjects;
+
+	struct unionfs_usi_data *new_data = NULL;
+	struct dentry **new_udi_dentry = NULL;
+	struct inode **new_uii_inode = NULL;
+
+	struct dentry *root = NULL;
+	struct dentry *hidden_root = NULL;
+
+	print_entry_location();
+
+	err = -ENOMEM;
+	addargs = KMALLOC(sizeof(struct unionfs_addbranch_args), GFP_KERNEL);
+	if (!addargs)
+		goto out;
+
+	err = -EFAULT;
+	if (copy_from_user
+	    (addargs, (const void __user *)arg,
+	     sizeof(struct unionfs_addbranch_args)))
+		goto out;
+
+	err = -EINVAL;
+	if (addargs->ab_perms & ~(MAY_READ | MAY_WRITE | MAY_NFSRO))
+		goto out;
+	if (!(addargs->ab_perms & MAY_READ))
+		goto out;
+
+	err = -E2BIG;
+	if (sbend(inode->i_sb) > FD_SETSIZE)
+		goto out;
+
+	err = -ENOMEM;
+	if (!(path = getname((const char __user *)addargs->ab_path)))
+		goto out;
+
+	err = path_lookup(path, LOOKUP_FOLLOW, &nd);
+
+	RECORD_PATH_LOOKUP(&nd);
+	if (err)
+		goto out;
+	if ((err = check_branch(&nd))) {
+		path_release(&nd);
+		RECORD_PATH_RELEASE(&nd);
+		goto out;
+	}
+
+	unionfs_write_lock(inode->i_sb);
+	lock_dentry(inode->i_sb->s_root);
+
+	root = inode->i_sb->s_root;
+	for (i = dbstart(inode->i_sb->s_root); i <= dbend(inode->i_sb->s_root);
+	     i++) {
+		hidden_root = dtohd_index(root, i);
+		if (is_branch_overlap(hidden_root, nd.dentry)) {
+			err = -EINVAL;
+			goto out;
+		}
+	}
+
+	err = -EINVAL;
+	if (addargs->ab_branch < 0
+	    || (addargs->ab_branch > (sbend(inode->i_sb) + 1)))
+		goto out;
+
+	if ((err = newputmap(inode->i_sb)))
+		goto out;
+
+	stopd(inode->i_sb)->b_end++;
+	dtopd(inode->i_sb->s_root)->udi_bcount++;
+	set_dbend(inode->i_sb->s_root, dbend(inode->i_sb->s_root) + 1);
+	itopd(inode->i_sb->s_root->d_inode)->b_end++;
+
+	atomic_inc(&stopd(inode->i_sb)->usi_generation);
+	gen = atomic_read(&stopd(inode->i_sb)->usi_generation);
+
+	pobjects = sbend(inode->i_sb) + 1;
+
+	/* Reallocate the dynamic structures. */
+	new_data = alloc_new_data(pobjects);
+	new_udi_dentry = alloc_new_dentries(pobjects);
+	new_uii_inode = KZALLOC(sizeof(struct inode *) * pobjects, GFP_KERNEL);
+
+	if (!new_udi_dentry || !new_uii_inode || !new_data) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	/* Copy the in-place values to our new structure. */
+	for (i = 0; i < addargs->ab_branch; i++) {
+		atomic_set(&(new_data[i].sbcount),
+			   branch_count(inode->i_sb, i));
+
+		new_data[i].branchperms = branchperms(inode->i_sb, i);
+		new_data[i].hidden_mnt = stohiddenmnt_index(inode->i_sb, i);
+		new_data[i].sb = stohs_index(inode->i_sb, i);
+
+		new_udi_dentry[i] = dtohd_index(inode->i_sb->s_root, i);
+		new_uii_inode[i] = itohi_index(inode->i_sb->s_root->d_inode, i);
+	}
+
+	/* Shift the ends to the right (only handle reallocated bits). */
+	for (i = sbend(inode->i_sb) - 1; i >= (int)addargs->ab_branch; i--) {
+		int j = i + 1;
+		int pmindex;
+
+		atomic_set(&new_data[j].sbcount, branch_count(inode->i_sb, i));
+
+		new_data[j].branchperms = branchperms(inode->i_sb, i);
+		new_data[j].hidden_mnt = stohiddenmnt_index(inode->i_sb, i);
+		new_data[j].sb = stohs_index(inode->i_sb, i);
+		new_udi_dentry[j] = dtohd_index(inode->i_sb->s_root, i);
+		new_uii_inode[j] = itohi_index(inode->i_sb->s_root->d_inode, i);
+
+		/* Update the newest putmap, so it is correct for later. */
+		pmindex = stopd(inode->i_sb)->usi_lastputmap;
+		pmindex -= stopd(inode->i_sb)->usi_firstputmap;
+		stopd(inode->i_sb)->usi_putmaps[pmindex]->map[i] = j;
+
+	}
+
+	/* Now we can free the old ones. */
+	KFREE(dtopd(inode->i_sb->s_root)->udi_dentry);
+	KFREE(itopd(inode->i_sb->s_root->d_inode)->uii_inode);
+	KFREE(stopd(inode->i_sb)->usi_data);
+
+	/* Update the real pointers. */
+	dtohd_ptr(inode->i_sb->s_root) = new_udi_dentry;
+	itohi_ptr(inode->i_sb->s_root->d_inode) = new_uii_inode;
+	stopd(inode->i_sb)->usi_data = new_data;
+
+	/* Re-NULL the new ones so we don't try to free them. */
+	new_data = NULL;
+	new_udi_dentry = NULL;
+	new_uii_inode = NULL;
+
+	/* Put the new dentry information into it's slot. */
+	set_dtohd_index(inode->i_sb->s_root, addargs->ab_branch, nd.dentry);
+	set_itohi_index(inode->i_sb->s_root->d_inode, addargs->ab_branch,
+			IGRAB(nd.dentry->d_inode));
+	set_branchperms(inode->i_sb, addargs->ab_branch, addargs->ab_perms);
+	set_branch_count(inode->i_sb, addargs->ab_branch, 0);
+	set_stohiddenmnt_index(inode->i_sb, addargs->ab_branch, nd.mnt);
+	set_stohs_index(inode->i_sb, addargs->ab_branch, nd.dentry->d_sb);
+
+	atomic_set(&dtopd(inode->i_sb->s_root)->udi_generation, gen);
+	atomic_set(&itopd(inode->i_sb->s_root->d_inode)->uii_generation, gen);
+
+	fixputmaps(inode->i_sb);
+
+      out:
+	unlock_dentry(inode->i_sb->s_root);
+	unionfs_write_unlock(inode->i_sb);
+
+	KFREE(new_udi_dentry);
+	KFREE(new_uii_inode);
+	KFREE(new_data);
+	KFREE(addargs);
+	if (path)
+		putname(path);
+
+	print_exit_status(err);
+
+	return err;
+}
+
+/* This must be called with the super block already locked. */
+int unionfs_ioctl_delbranch(struct super_block *sb, unsigned long arg)
+{
+	struct dentry *hidden_dentry;
+	struct inode *hidden_inode;
+	struct vfsmount *hidden_mnt;
+	struct dentry *root_dentry;
+	struct inode *root_inode;
+	int err = 0;
+	int pmindex, i, gen;
+
+	print_entry("branch = %lu ", arg);
+	lock_dentry(sb->s_root);
+
+	err = -EBUSY;
+	if (sbmax(sb) == 1)
+		goto out;
+	err = -EINVAL;
+	if (arg < 0 || arg > stopd(sb)->b_end)
+		goto out;
+	err = -EBUSY;
+	if (branch_count(sb, arg))
+		goto out;
+
+	if ((err = newputmap(sb)))
+		goto out;
+
+	pmindex = stopd(sb)->usi_lastputmap;
+	pmindex -= stopd(sb)->usi_firstputmap;
+
+	atomic_inc(&stopd(sb)->usi_generation);
+	gen = atomic_read(&stopd(sb)->usi_generation);
+
+	root_dentry = sb->s_root;
+	root_inode = sb->s_root->d_inode;
+
+	hidden_dentry = dtohd_index(root_dentry, arg);
+	hidden_mnt = stohiddenmnt_index(sb, arg);
+	hidden_inode = itohi_index(root_inode, arg);
+
+	DPUT(hidden_dentry);
+	IPUT(hidden_inode);
+	mntput(hidden_mnt);
+
+	for (i = arg; i <= (sbend(sb) - 1); i++) {
+		set_branch_count(sb, i, branch_count(sb, i + 1));
+		set_stohiddenmnt_index(sb, i, stohiddenmnt_index(sb, i + 1));
+		set_stohs_index(sb, i, stohs_index(sb, i + 1));
+		set_branchperms(sb, i, branchperms(sb, i + 1));
+		set_dtohd_index(root_dentry, i,
+				dtohd_index(root_dentry, i + 1));
+		set_itohi_index(root_inode, i, itohi_index(root_inode, i + 1));
+		stopd(sb)->usi_putmaps[pmindex]->map[i + 1] = i;
+	}
+
+	set_dtohd_index(root_dentry, sbend(sb), NULL);
+	set_itohi_index(root_inode, sbend(sb), NULL);
+	set_stohiddenmnt_index(sb, sbend(sb), NULL);
+	set_stohs_index(sb, sbend(sb), NULL);
+
+	//XXX: Place check for inode maps and removal of branch here
+
+	stopd(sb)->b_end--;
+	set_dbend(root_dentry, dbend(root_dentry) - 1);
+	dtopd(root_dentry)->udi_bcount--;
+	itopd(root_inode)->b_end--;
+
+	atomic_set(&dtopd(root_dentry)->udi_generation, gen);
+	atomic_set(&itopd(root_inode)->uii_generation, gen);
+
+	fixputmaps(sb);
+
+	/* This doesn't open a file, so we might have to free the map here. */
+	if (atomic_read(&stopd(sb)->usi_putmaps[pmindex]->count) == 0) {
+		KFREE(stopd(sb)->usi_putmaps[pmindex]);
+		stopd(sb)->usi_putmaps[pmindex] = NULL;
+	}
+
+      out:
+	unlock_dentry(sb->s_root);
+	print_exit_status(err);
+
+	return err;
+}
+
+int unionfs_ioctl_rdwrbranch(struct inode *inode, unsigned int cmd,
+			     unsigned long arg)
+{
+	int err;
+	struct unionfs_rdwrbranch_args *rdwrargs = NULL;
+	int gen;
+
+	print_entry_location();
+
+	unionfs_write_lock(inode->i_sb);
+	lock_dentry(inode->i_sb->s_root);
+
+	if ((err = newputmap(inode->i_sb)))
+		goto out;
+
+	err = -ENOMEM;
+	rdwrargs = KMALLOC(sizeof(struct unionfs_rdwrbranch_args), GFP_KERNEL);
+	if (!rdwrargs)
+		goto out;
+
+	err = -EFAULT;
+	if (copy_from_user
+	    (rdwrargs, (const void __user *)arg,
+	     sizeof(struct unionfs_rdwrbranch_args)))
+		goto out;
+
+	err = -EINVAL;
+	if (rdwrargs->rwb_branch < 0
+	    || (rdwrargs->rwb_branch > (sbend(inode->i_sb) + 1)))
+		goto out;
+	if (rdwrargs->rwb_perms & ~(MAY_READ | MAY_WRITE | MAY_NFSRO))
+		goto out;
+	if (!(rdwrargs->rwb_perms & MAY_READ))
+		goto out;
+
+	set_branchperms(inode->i_sb, rdwrargs->rwb_branch, rdwrargs->rwb_perms);
+
+	atomic_inc(&stopd(inode->i_sb)->usi_generation);
+	gen = atomic_read(&stopd(inode->i_sb)->usi_generation);
+	atomic_set(&dtopd(inode->i_sb->s_root)->udi_generation, gen);
+	atomic_set(&itopd(inode->i_sb->s_root->d_inode)->uii_generation, gen);
+
+	err = 0;
+
+      out:
+	unlock_dentry(inode->i_sb->s_root);
+	unionfs_write_unlock(inode->i_sb);
+	KFREE(rdwrargs);
+
+	print_exit_status(err);
+
+	return err;
+}
+
+int unionfs_ioctl_queryfile(struct file *file, unsigned int cmd,
+			    unsigned long arg)
+{
+	int err = 0;
+	fd_set branchlist;
+
+	int bstart = 0, bend = 0, bindex = 0;
+	struct dentry *dentry, *hidden_dentry;
+
+	print_entry_location();
+
+	dentry = file->f_dentry;
+	lock_dentry(dentry);
+	if ((err = unionfs_partial_lookup(dentry)))
+		goto out;
+	bstart = dbstart(dentry);
+	bend = dbend(dentry);
+
+	FD_ZERO(&branchlist);
+
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		if (!hidden_dentry)
+			continue;
+		if (hidden_dentry->d_inode)
+			FD_SET(bindex, &branchlist);
+	}
+
+	err = copy_to_user((void __user *)arg, &branchlist, sizeof(fd_set));
+	if (err) {
+		err = -EFAULT;
+		goto out;
+	}
+
+      out:
+	unlock_dentry(dentry);
+	err = err < 0 ? err : bend;
+	print_exit_status(err);
+	return (err);
+}
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/commonfops.c newtree/fs/unionfs/commonfops.c
--- oldtree/fs/unionfs/commonfops.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/commonfops.c	2006-07-12 19:01:51.000000000 -0400
@@ -0,0 +1,708 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: commonfops.c,v 1.60 2006/06/30 00:45:13 jsipek Exp $
+ */
+
+#include "unionfs.h"
+
+/* We only need this function here, but it could get promoted to unionfs.h, if
+ * other things need a generation specific branch putting function. */
+static inline void branchput_gen(int generation, struct super_block *sb,
+				 int index)
+{
+	struct putmap *putmap;
+
+	if (generation == atomic_read(&stopd(sb)->usi_generation)) {
+		branchput(sb, index);
+		return;
+	}
+
+	BUG_ON(stopd(sb)->usi_firstputmap > generation);
+	BUG_ON(stopd(sb)->usi_lastputmap < generation);
+
+	putmap =
+	    stopd(sb)->usi_putmaps[generation - stopd(sb)->usi_firstputmap];
+	BUG_ON(index < 0);
+	BUG_ON(index > putmap->bend);
+	BUG_ON(putmap->map[index] < 0);
+	branchput(sb, putmap->map[index]);
+	if (atomic_dec_and_test(&putmap->count)) {
+		stopd(sb)->usi_putmaps[generation - stopd(sb)->usi_firstputmap]
+		    = NULL;
+		dprint(PRINT_DEBUG, "Freeing putmap %d.\n", generation);
+		KFREE(putmap);
+	}
+}
+
+static char *get_random_name(int size, unsigned char *name)
+{
+	int i;
+	int j;
+	unsigned char *tmpbuf = NULL;
+
+	if (size <= WHLEN)
+		return NULL;
+
+	if (!name)
+		name = KMALLOC(size + 1, GFP_KERNEL);
+	if (!name) {
+		name = ERR_PTR(-ENOMEM);
+		goto out;
+	}
+	strncpy(name, WHPFX, WHLEN);
+
+	tmpbuf = KMALLOC(size, GFP_KERNEL);
+	if (!tmpbuf) {
+		KFREE(name);
+		name = ERR_PTR(-ENOMEM);
+		goto out;
+	}
+
+	get_random_bytes((void *)tmpbuf, (size - 3) / 2);
+
+	j = WHLEN;
+	i = 0;
+	while ((i < (size - 3) / 2) && (j < size)) {
+		/* get characters in the 0-9, A-F range */
+
+		name[j] =
+		    (tmpbuf[i] % 16) <
+		    10 ? (tmpbuf[i] % 16) + '0' : (tmpbuf[i] % 16) + 'a';
+		j++;
+		if (j == size)
+			break;
+		name[j] =
+		    (tmpbuf[i] >> 4) <
+		    10 ? (tmpbuf[i] >> 4) + '0' : (tmpbuf[i] >> 4) + 'a';
+		j++;
+
+		i++;
+	}
+
+	name[size] = '\0';
+
+      out:
+	KFREE(tmpbuf);
+	return (name);
+
+}
+
+static int copyup_deleted_file(struct file *file, struct dentry *dentry,
+			       int bstart, int bindex)
+{
+	int attempts = 0;
+	int err;
+	int exists = 1;
+	char *name = NULL;
+	struct dentry *tmp_dentry = NULL;
+	struct dentry *hidden_dentry = NULL;
+	struct dentry *hidden_dir_dentry = NULL;
+
+	print_entry_location();
+
+	/* Try five times to get a unique file name, fail after that.  Five is
+	 * simply a magic number, because we shouldn't try forever.  */
+	while (exists) {
+		/* The first call allocates, the subsequent ones reuse. */
+		name = get_random_name(UNIONFS_TMPNAM_LEN, name);
+		err = -ENOMEM;
+		if (!name)
+			goto out;
+		//XXX: Why do we do this every time? bstart never changes?
+		hidden_dentry = dtohd_index(dentry, bstart);
+
+		tmp_dentry = LOOKUP_ONE_LEN(name, hidden_dentry->d_parent,
+					    UNIONFS_TMPNAM_LEN);
+		err = PTR_ERR(tmp_dentry);
+		if (IS_ERR(tmp_dentry))
+			goto out;
+		exists = tmp_dentry->d_inode ? 1 : 0;
+		DPUT(tmp_dentry);
+
+		err = -EEXIST;
+		if (++attempts > 5)
+			goto out;
+	}
+
+	err = copyup_named_file(dentry->d_parent->d_inode, file, name, bstart,
+				bindex, file->f_dentry->d_inode->i_size);
+	if (err)
+		goto out;
+
+	/* bring it to the same state as an unlinked file */
+	hidden_dentry = dtohd_index(dentry, dbstart(dentry));
+	hidden_dir_dentry = lock_parent(hidden_dentry);
+	err = vfs_unlink(hidden_dir_dentry->d_inode, hidden_dentry);
+	unlock_dir(hidden_dir_dentry);
+
+      out:
+	KFREE(name);
+	print_exit_status(err);
+	return err;
+}
+
+int unionfs_file_revalidate(struct file *file, int willwrite)
+{
+	struct super_block *sb;
+	struct dentry *dentry;
+	int sbgen, fgen, dgen;
+	int bindex, bstart, bend;
+	struct file *hidden_file;
+	struct dentry *hidden_dentry;
+	int size;
+
+	int err = 0;
+
+	print_entry(" file = %p", file);
+
+	dentry = file->f_dentry;
+	lock_dentry(dentry);
+	sb = dentry->d_sb;
+	unionfs_read_lock(sb);
+	if (!unionfs_d_revalidate(dentry, NULL) && !d_deleted(dentry)) {
+		err = -ESTALE;
+		goto out;
+	}
+	print_dentry("file revalidate in", dentry);
+
+	sbgen = atomic_read(&stopd(sb)->usi_generation);
+	dgen = atomic_read(&dtopd(dentry)->udi_generation);
+	fgen = atomic_read(&ftopd(file)->ufi_generation);
+
+	BUG_ON(sbgen > dgen);
+
+	/* There are two cases we are interested in.  The first is if the
+	 * generation is lower than the super-block.  The second is if someone
+	 * has copied up this file from underneath us, we also need to refresh
+	 * things. */
+	if (!d_deleted(dentry) &&
+	    ((sbgen > fgen) || (dbstart(dentry) != fbstart(file)))) {
+		/* First we throw out the existing files. */
+		bstart = fbstart(file);
+		bend = fbend(file);
+		for (bindex = bstart; bindex <= bend; bindex++) {
+			if (ftohf_index(file, bindex)) {
+				branchput_gen(fgen, dentry->d_sb, bindex);
+				fput(ftohf_index(file, bindex));
+			}
+		}
+
+		if (ftohf_ptr(file)) {
+			KFREE(ftohf_ptr(file));
+			ftohf_ptr(file) = NULL;
+		}
+
+		/* Now we reopen the file(s) as in unionfs_open. */
+		bstart = fbstart(file) = dbstart(dentry);
+		bend = fbend(file) = dbend(dentry);
+
+		size = sizeof(struct file *) * sbmax(sb);
+		ftohf_ptr(file) = KZALLOC(size, GFP_KERNEL);
+		if (!ftohf_ptr(file)) {
+			err = -ENOMEM;
+			goto out;
+		}
+
+		if (S_ISDIR(dentry->d_inode->i_mode)) {
+			/* We need to open all the files. */
+			for (bindex = bstart; bindex <= bend; bindex++) {
+				hidden_dentry = dtohd_index(dentry, bindex);
+				if (!hidden_dentry)
+					continue;
+
+				DGET(hidden_dentry);
+				mntget(stohiddenmnt_index(sb, bindex));
+				branchget(sb, bindex);
+
+				hidden_file =
+				    DENTRY_OPEN(hidden_dentry,
+						stohiddenmnt_index(sb, bindex),
+						file->f_flags);
+				if (IS_ERR(hidden_file)) {
+					err = PTR_ERR(hidden_file);
+					goto out;
+				} else {
+					set_ftohf_index(file, bindex,
+							hidden_file);
+				}
+			}
+		} else {
+			/* We only open the highest priority branch. */
+			hidden_dentry = dtohd(dentry);
+			if (willwrite && IS_WRITE_FLAG(file->f_flags)
+			    && is_robranch(dentry)) {
+				for (bindex = bstart - 1; bindex >= 0; bindex--) {
+
+					err = copyup_file(dentry->
+							  d_parent->
+							  d_inode,
+							  file,
+							  bstart,
+							  bindex,
+							  file->
+							  f_dentry->
+							  d_inode->i_size);
+
+					if (!err)
+						break;
+					else
+						continue;
+
+				}
+				atomic_set(&ftopd(file)->ufi_generation,
+					   atomic_read(&itopd(dentry->d_inode)->
+						       uii_generation));
+				goto out;
+			}
+
+			DGET(hidden_dentry);
+			mntget(stohiddenmnt_index(sb, bstart));
+			branchget(sb, bstart);
+			hidden_file =
+			    DENTRY_OPEN(hidden_dentry,
+					stohiddenmnt_index(sb, bstart),
+					file->f_flags);
+			if (IS_ERR(hidden_file)) {
+				err = PTR_ERR(hidden_file);
+				goto out;
+			}
+			set_ftohf(file, hidden_file);
+			/* Fix up the position. */
+			hidden_file->f_pos = file->f_pos;
+
+			memcpy(&(hidden_file->f_ra), &(file->f_ra),
+			       sizeof(struct file_ra_state));
+		}
+		atomic_set(&ftopd(file)->ufi_generation,
+			   atomic_read(&itopd(dentry->d_inode)->
+				       uii_generation));
+	}
+
+	/* Copyup on the first write to a file on a readonly branch. */
+	if (willwrite && IS_WRITE_FLAG(file->f_flags)
+	    && !IS_WRITE_FLAG(ftohf(file)->f_flags) && is_robranch(dentry)) {
+		dprint(PRINT_DEBUG,
+		       "Doing delayed copyup of a read-write file on a read-only branch.\n");
+		bstart = fbstart(file);
+		bend = fbend(file);
+
+		BUG_ON(!S_ISREG(file->f_dentry->d_inode->i_mode));
+
+		for (bindex = bstart - 1; bindex >= 0; bindex--) {
+			if (!d_deleted(file->f_dentry)) {
+				err =
+				    copyup_file(dentry->d_parent->
+						d_inode, file, bstart,
+						bindex,
+						file->f_dentry->
+						d_inode->i_size);
+			} else {
+				err =
+				    copyup_deleted_file(file, dentry, bstart,
+							bindex);
+			}
+
+			if (!err)
+				break;
+			else
+				continue;
+
+		}
+		if (!err && (bstart > fbstart(file))) {
+			bend = fbend(file);
+			for (bindex = bstart; bindex <= bend; bindex++) {
+				if (ftohf_index(file, bindex)) {
+					branchput(dentry->d_sb, bindex);
+					fput(ftohf_index(file, bindex));
+					set_ftohf_index(file, bindex, NULL);
+				}
+			}
+			fbend(file) = bend;
+		}
+	}
+
+      out:
+	print_dentry("file revalidate out", dentry);
+	unlock_dentry(dentry);
+	unionfs_read_unlock(dentry->d_sb);
+	print_exit_status(err);
+	return err;
+}
+
+int unionfs_open(struct inode *inode, struct file *file)
+{
+	int err = 0;
+	int hidden_flags;
+	struct file *hidden_file = NULL;
+	struct dentry *hidden_dentry = NULL;
+	struct dentry *dentry = NULL;
+	int bindex = 0, bstart = 0, bend = 0;
+	int locked = 0;
+	int size;
+
+	print_entry_location();
+
+	ftopd_lhs(file) = KZALLOC(sizeof(struct unionfs_file_info), GFP_KERNEL);
+	if (!ftopd(file)) {
+		err = -ENOMEM;
+		goto out;
+	}
+	fbstart(file) = -1;
+	fbend(file) = -1;
+	atomic_set(&ftopd(file)->ufi_generation,
+		   atomic_read(&itopd(inode)->uii_generation));
+
+	size = sizeof(struct file *) * sbmax(inode->i_sb);
+	ftohf_ptr(file) = KZALLOC(size, GFP_KERNEL);
+	if (!ftohf_ptr(file)) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	hidden_flags = file->f_flags;
+
+	dentry = file->f_dentry;
+	dprint(PRINT_DEBUG, "dentry to open is %p\n", dentry);
+	lock_dentry(dentry);
+	unionfs_read_lock(inode->i_sb);
+	locked = 1;
+
+	bstart = fbstart(file) = dbstart(dentry);
+	bend = fbend(file) = dbend(dentry);
+
+	/* increment to show the kind of open, so that we can
+	 * flush appropriately
+	 */
+	atomic_inc(&itopd(dentry->d_inode)->uii_totalopens);
+
+	/* open all directories and make the unionfs file struct point to these hidden file structs */
+	if (S_ISDIR(inode->i_mode)) {
+		for (bindex = bstart; bindex <= bend; bindex++) {
+			hidden_dentry = dtohd_index(dentry, bindex);
+			if (!hidden_dentry)
+				continue;
+
+			DGET(hidden_dentry);
+			mntget(stohiddenmnt_index(inode->i_sb, bindex));
+			hidden_file =
+			    DENTRY_OPEN(hidden_dentry,
+					stohiddenmnt_index(inode->i_sb, bindex),
+					hidden_flags);
+			if (IS_ERR(hidden_file)) {
+				err = PTR_ERR(hidden_file);
+				goto out;
+			}
+
+			set_ftohf_index(file, bindex, hidden_file);
+			/* The branchget goes after the open, because otherwise
+			 * we would miss the reference on release. */
+			branchget(inode->i_sb, bindex);
+		}
+	} else {
+		/* open a file */
+		hidden_dentry = dtohd(dentry);
+
+		/* check for the permission for hidden file.  If the error is COPYUP_ERR,
+		 * copyup the file.
+		 */
+		if (hidden_dentry->d_inode && is_robranch(dentry)) {
+			/* if the open will change the file, copy it up otherwise defer it. */
+			if (hidden_flags & O_TRUNC) {
+				int size = 0;
+
+				err = -EROFS;
+				/* copyup the file */
+				for (bindex = bstart - 1; bindex >= 0; bindex--) {
+					err =
+					    copyup_file(dentry->
+							d_parent->
+							d_inode, file,
+							bstart, bindex, size);
+					if (!err) {
+						break;
+					}
+				}
+				goto out;
+			} else {
+				hidden_flags &= ~(OPEN_WRITE_FLAGS);
+			}
+		}
+
+		DGET(hidden_dentry);
+		/* dentry_open will decrement mnt refcnt if err.
+		 * otherwise fput() will do an mntput() for us upon file close.
+		 */
+		mntget(stohiddenmnt_index(inode->i_sb, bstart));
+		hidden_file = DENTRY_OPEN(hidden_dentry,
+					  stohiddenmnt_index(inode->i_sb,
+							     bstart),
+					  hidden_flags);
+		if (IS_ERR(hidden_file)) {
+			err = PTR_ERR(hidden_file);
+			goto out;
+		} else {
+			set_ftohf(file, hidden_file);
+			branchget(inode->i_sb, bstart);
+		}
+	}
+
+      out:
+	/* freeing the allocated resources, and fput the opened files */
+	if (err < 0 && ftopd(file)) {
+		if (!locked)
+			unionfs_read_lock(file->f_dentry->d_sb);
+		for (bindex = bstart; bindex <= bend; bindex++) {
+			hidden_file = ftohf_index(file, bindex);
+			if (hidden_file) {
+				branchput(file->f_dentry->d_sb, bindex);
+				/* fput calls dput for hidden_dentry */
+				fput(hidden_file);
+			}
+		}
+		if (!locked)
+			unionfs_read_unlock(file->f_dentry->d_sb);
+		KFREE(ftohf_ptr(file));
+		KFREE(ftopd(file));
+	}
+
+	print_file("OUT: unionfs_open", file);
+
+	if (locked) {
+		unlock_dentry(dentry);
+		unionfs_read_unlock(inode->i_sb);
+	}
+	print_exit_status(err);
+	return err;
+}
+
+int unionfs_file_release(struct inode *inode, struct file *file)
+{
+	int err = 0;
+	struct file *hidden_file = NULL;
+	int bindex, bstart, bend;
+	int fgen;
+
+	print_entry_location();
+
+	checkinode(inode, "unionfs_release");
+
+	/* fput all the hidden files */
+	fgen = atomic_read(&ftopd(file)->ufi_generation);
+	bstart = fbstart(file);
+	bend = fbend(file);
+
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		hidden_file = ftohf_index(file, bindex);
+
+		if (hidden_file) {
+			fput(hidden_file);
+			unionfs_read_lock(inode->i_sb);
+			branchput_gen(fgen, inode->i_sb, bindex);
+			unionfs_read_unlock(inode->i_sb);
+		}
+	}
+	KFREE(ftohf_ptr(file));
+
+	if (ftopd(file)->rdstate) {
+		ftopd(file)->rdstate->uds_access = jiffies;
+		dprint(PRINT_DEBUG, "Saving rdstate with cookie %u [%d.%lld]\n",
+		       ftopd(file)->rdstate->uds_cookie,
+		       ftopd(file)->rdstate->uds_bindex,
+		       (long long)ftopd(file)->rdstate->uds_dirpos);
+		spin_lock(&itopd(inode)->uii_rdlock);
+		itopd(inode)->uii_rdcount++;
+		list_add_tail(&ftopd(file)->rdstate->uds_cache,
+			      &itopd(inode)->uii_readdircache);
+		mark_inode_dirty(inode);
+		spin_unlock(&itopd(inode)->uii_rdlock);
+		ftopd(file)->rdstate = NULL;
+	}
+	KFREE(ftopd(file));
+
+	checkinode(inode, "post unionfs_release");
+
+	print_exit_status(err);
+	return err;
+}
+
+long unionfs_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	long err = 0;		/* don't fail by default */
+	struct file *hidden_file = NULL;
+	int val;
+
+	print_entry_location();
+
+	if ((err = unionfs_file_revalidate(file, 1)))
+		goto out;
+
+	/* check if asked for local commands */
+	switch (cmd) {
+	case FIST_IOCTL_GET_DEBUG_VALUE:
+		if (!capable(CAP_SYS_ADMIN)) {
+			err = -EACCES;
+			goto out;
+		}
+		val = get_debug_mask();
+		err = put_user(val, (int __user *)arg);
+		break;
+
+	case FIST_IOCTL_SET_DEBUG_VALUE:
+		if (!capable(CAP_SYS_ADMIN)) {
+			err = -EACCES;
+			goto out;
+		}
+		err = get_user(val, (int __user *)arg);
+		if (err)
+			break;
+		dprint(PRINT_DEBUG, "IOCTL SET: got arg %d\n", val);
+		if (val < 0 || val > PRINT_MAX) {
+			err = -EINVAL;
+			break;
+		}
+		set_debug_mask(val);
+		break;
+
+		/* add non-debugging fist ioctl's here */
+
+	case UNIONFS_IOCTL_BRANCH_COUNT:
+		if (!capable(CAP_SYS_ADMIN)) {
+			err = -EACCES;
+			goto out;
+		}
+		err = unionfs_ioctl_branchcount(file, cmd, arg);
+		break;
+
+	case UNIONFS_IOCTL_INCGEN:
+		if (!capable(CAP_SYS_ADMIN)) {
+			err = -EACCES;
+			goto out;
+		}
+		err = unionfs_ioctl_incgen(file, cmd, arg);
+		break;
+
+	case UNIONFS_IOCTL_ADDBRANCH:
+		if (!capable(CAP_SYS_ADMIN)) {
+			err = -EACCES;
+			goto out;
+		}
+		err =
+		    unionfs_ioctl_addbranch(file->f_dentry->d_inode, cmd, arg);
+		break;
+
+	case UNIONFS_IOCTL_RDWRBRANCH:
+		if (!capable(CAP_SYS_ADMIN)) {
+			err = -EACCES;
+			goto out;
+		}
+		err =
+		    unionfs_ioctl_rdwrbranch(file->f_dentry->d_inode, cmd, arg);
+		break;
+
+	case UNIONFS_IOCTL_QUERYFILE:
+		/* XXX: This should take the file. */
+		err = unionfs_ioctl_queryfile(file, cmd, arg);
+		break;
+
+	default:
+		hidden_file = ftohf(file);
+
+		err = security_file_ioctl(hidden_file, cmd, arg);
+		if (err)
+			goto out;
+		err = -ENOTTY;
+		if (!hidden_file || !hidden_file->f_op)
+			goto out;
+		if (hidden_file->f_op->unlocked_ioctl) {
+			err =
+			    hidden_file->f_op->unlocked_ioctl(hidden_file, cmd,
+							      arg);
+		} else if (hidden_file->f_op->ioctl) {
+			lock_kernel();
+			err =
+			    hidden_file->f_op->ioctl(hidden_file->f_dentry->
+						     d_inode, hidden_file, cmd,
+						     arg);
+			unlock_kernel();
+		}
+	}			/* end of outer switch statement */
+
+      out:
+	print_exit_status((int)err);
+	return err;
+}
+
+int unionfs_flush(struct file *file, fl_owner_t id)
+{
+	int err = 0;		/* assume ok (see open.c:close_fp) */
+	struct file *hidden_file = NULL;
+	int bindex, bstart, bend;
+
+	print_entry_location();
+
+	if ((err = unionfs_file_revalidate(file, 1)))
+		goto out;
+	if (!atomic_dec_and_test
+	    (&itopd(file->f_dentry->d_inode)->uii_totalopens))
+		goto out;
+
+	lock_dentry(file->f_dentry);
+
+	bstart = fbstart(file);
+	bend = fbend(file);
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		hidden_file = ftohf_index(file, bindex);
+
+		if (hidden_file && hidden_file->f_op
+		    && hidden_file->f_op->flush) {
+			err = hidden_file->f_op->flush(hidden_file, id);
+			if (err)
+				goto out_lock;
+			/* This was earlier done in the unlink_all function in unlink.c */
+			/* if there are no more references to the dentry, dput it */
+			if (d_deleted(file->f_dentry)) {
+				DPUT(dtohd_index(file->f_dentry, bindex));
+				set_dtohd_index(file->f_dentry, bindex, NULL);
+			}
+		}
+
+	}
+
+      out_lock:
+	unlock_dentry(file->f_dentry);
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/copyup.c newtree/fs/unionfs/copyup.c
--- oldtree/fs/unionfs/copyup.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/copyup.c	2006-07-12 19:01:51.000000000 -0400
@@ -0,0 +1,724 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York*
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: copyup.c,v 1.74 2006/07/08 17:58:30 ezk Exp $
+ */
+
+#include "unionfs.h"
+
+/*Not Working Yet*/
+static int copyup_xattrs(struct dentry *old_hidden_dentry,
+			 struct dentry *new_hidden_dentry)
+{
+	int err = 0;
+	ssize_t list_size = -1;
+	char *name_list = NULL;
+	char *attr_value = NULL;
+	char *name_list_orig = NULL;
+
+	print_entry_location();
+
+	list_size = vfs_listxattr(old_hidden_dentry, NULL, 0);
+
+	if (list_size <= 0) {
+		err = list_size;
+		goto out;
+	}
+
+	name_list = xattr_alloc(list_size + 1, XATTR_LIST_MAX);
+	if (!name_list || IS_ERR(name_list)) {
+		err = PTR_ERR(name_list);
+		goto out;
+	}
+	list_size = vfs_listxattr(old_hidden_dentry, name_list, list_size);
+	attr_value = xattr_alloc(XATTR_SIZE_MAX, XATTR_SIZE_MAX);
+	if (!attr_value || IS_ERR(attr_value)) {
+		err = PTR_ERR(name_list);
+		goto out;
+	}
+	name_list_orig = name_list;
+	while (*name_list) {
+		ssize_t size;
+
+		//We need to lock here since vfs_getxattr doesn't lock for us.
+		mutex_lock(&old_hidden_dentry->d_inode->i_mutex);
+		size = vfs_getxattr(old_hidden_dentry, name_list,
+				    attr_value, XATTR_SIZE_MAX);
+		mutex_unlock(&old_hidden_dentry->d_inode->i_mutex);
+		if (size < 0) {
+			err = size;
+			goto out;
+		}
+
+		if (size > XATTR_SIZE_MAX) {
+			err = -E2BIG;
+			goto out;
+		}
+		//We don't need to lock here since vfs_setxattr does it for us.
+		err = vfs_setxattr(new_hidden_dentry, name_list, attr_value,
+				   size, 0);
+
+		if (err < 0)
+			goto out;
+		name_list += strlen(name_list) + 1;
+	}
+      out:
+	name_list = name_list_orig;
+
+	if (name_list)
+		xattr_free(name_list, list_size + 1);
+	if (attr_value)
+		xattr_free(attr_value, XATTR_SIZE_MAX);
+	/* It is no big deal if this fails, we just roll with the punches. */
+	if (err == -ENOTSUPP || err == -EOPNOTSUPP)
+		err = 0;
+	return err;
+}
+
+/* Determine the mode based on the copyup flags, and the existing dentry. */
+static int copyup_permissions(struct super_block *sb,
+			      struct dentry *old_hidden_dentry,
+			      struct dentry *new_hidden_dentry)
+{
+	struct iattr newattrs;
+	int err;
+
+	print_entry_location();
+
+	newattrs.ia_atime = old_hidden_dentry->d_inode->i_atime;
+	newattrs.ia_mtime = old_hidden_dentry->d_inode->i_mtime;
+	newattrs.ia_ctime = old_hidden_dentry->d_inode->i_ctime;
+	newattrs.ia_valid = ATTR_CTIME | ATTR_ATIME | ATTR_MTIME |
+	    ATTR_ATIME_SET | ATTR_MTIME_SET;
+	/* original mode of old file */
+	newattrs.ia_mode = old_hidden_dentry->d_inode->i_mode;
+	newattrs.ia_gid = old_hidden_dentry->d_inode->i_gid;
+	newattrs.ia_uid = old_hidden_dentry->d_inode->i_uid;
+	newattrs.ia_valid |= ATTR_FORCE | ATTR_GID | ATTR_UID | ATTR_MODE;
+	if (newattrs.ia_valid & ATTR_MODE) {
+		newattrs.ia_mode =
+		    (newattrs.ia_mode & S_IALLUGO) | (old_hidden_dentry->
+						      d_inode->
+						      i_mode & ~S_IALLUGO);
+	}
+
+	err = notify_change(new_hidden_dentry, &newattrs);
+
+	print_exit_status(err);
+	return err;
+}
+
+int copyup_dentry(struct inode *dir, struct dentry *dentry,
+		  int bstart, int new_bindex,
+		  struct file **copyup_file, loff_t len)
+{
+	return copyup_named_dentry(dir, dentry, bstart, new_bindex,
+				   dentry->d_name.name,
+				   dentry->d_name.len, copyup_file, len);
+}
+
+int copyup_named_dentry(struct inode *dir, struct dentry *dentry,
+			int bstart, int new_bindex, const char *name,
+			int namelen, struct file **copyup_file, loff_t len)
+{
+	struct dentry *new_hidden_dentry;
+	struct dentry *old_hidden_dentry = NULL;
+	struct super_block *sb;
+	struct file *input_file = NULL;
+	struct file *output_file = NULL;
+	ssize_t read_bytes, write_bytes;
+	mm_segment_t old_fs;
+	int err = 0;
+	char *buf;
+	int old_bindex;
+	int got_branch_input = -1;
+	int got_branch_output = -1;
+	int old_bstart;
+	int old_bend;
+	int size = len;
+	struct dentry *new_hidden_parent_dentry = NULL;
+	mm_segment_t oldfs;
+	char *symbuf = NULL;
+	uid_t saved_uid = current->fsuid;
+	gid_t saved_gid = current->fsgid;
+
+	print_entry_location();
+	verify_locked(dentry);
+	print_dentry("IN: copyup_named_dentry", dentry);
+
+	old_bindex = bstart;
+	old_bstart = dbstart(dentry);
+	old_bend = dbend(dentry);
+
+	BUG_ON(new_bindex < 0);
+	BUG_ON(new_bindex >= old_bindex);
+
+	sb = dir->i_sb;
+
+	unionfs_read_lock(sb);
+
+	if ((err = is_robranch_super(sb, new_bindex)))
+		goto out;
+
+	/* Create the directory structure above this dentry. */
+	new_hidden_dentry = create_parents_named(dir, dentry, name, new_bindex);
+	if (IS_ERR(new_hidden_dentry)) {
+		err = PTR_ERR(new_hidden_dentry);
+		goto out;
+	}
+
+	print_dentry("Copyup Object", new_hidden_dentry);
+
+	/* Now we actually create the object. */
+	old_hidden_dentry = dtohd_index(dentry, old_bindex);
+	DGET(old_hidden_dentry);
+
+	/* For symlinks, we must read the link before we lock the directory. */
+	if (S_ISLNK(old_hidden_dentry->d_inode->i_mode)) {
+
+		symbuf = KMALLOC(PATH_MAX, GFP_KERNEL);
+		if (!symbuf) {
+			err = -ENOMEM;
+			goto copyup_readlink_err;
+		}
+
+		oldfs = get_fs();
+		set_fs(KERNEL_DS);
+		err =
+		    old_hidden_dentry->d_inode->i_op->
+		    readlink(old_hidden_dentry, (char __user *)symbuf,
+			     PATH_MAX);
+		set_fs(oldfs);
+		if (err < 0)
+			goto copyup_readlink_err;
+		symbuf[err] = '\0';
+	}
+
+	/* Now we lock the parent, and create the object in the new branch. */
+	new_hidden_parent_dentry = lock_parent(new_hidden_dentry);
+	current->fsuid = new_hidden_parent_dentry->d_inode->i_uid;
+	current->fsgid = new_hidden_parent_dentry->d_inode->i_gid;
+	if (S_ISDIR(old_hidden_dentry->d_inode->i_mode)) {
+		err = vfs_mkdir(new_hidden_parent_dentry->d_inode,
+				new_hidden_dentry, S_IRWXU);
+	} else if (S_ISLNK(old_hidden_dentry->d_inode->i_mode)) {
+		err = vfs_symlink(new_hidden_parent_dentry->d_inode,
+				  new_hidden_dentry, symbuf, S_IRWXU);
+	} else if (S_ISBLK(old_hidden_dentry->d_inode->i_mode)
+		   || S_ISCHR(old_hidden_dentry->d_inode->i_mode)
+		   || S_ISFIFO(old_hidden_dentry->d_inode->i_mode)
+		   || S_ISSOCK(old_hidden_dentry->d_inode->i_mode)) {
+		err = vfs_mknod(new_hidden_parent_dentry->d_inode,
+				new_hidden_dentry,
+				old_hidden_dentry->d_inode->i_mode,
+				old_hidden_dentry->d_inode->i_rdev);
+	} else if (S_ISREG(old_hidden_dentry->d_inode->i_mode)) {
+		err = vfs_create(new_hidden_parent_dentry->d_inode,
+				 new_hidden_dentry, S_IRWXU, NULL);
+	} else {
+		printk(KERN_ERR "Unknown inode type %d\n",
+		       old_hidden_dentry->d_inode->i_mode);
+		BUG();
+	}
+	current->fsuid = saved_uid;
+	current->fsgid = saved_gid;
+      copyup_readlink_err:
+	KFREE(symbuf);
+	if (err) {
+		/* get rid of the hidden dentry and all its traces */
+		DPUT(new_hidden_dentry);
+		set_dtohd_index(dentry, new_bindex, NULL);
+		set_dbstart(dentry, old_bstart);
+		set_dbend(dentry, old_bend);
+		goto out_dir;
+	}
+#ifdef UNIONFS_IMAP
+	if (stopd(sb)->usi_persistent) {
+		err = write_uin(dentry->d_sb, dentry->d_inode->i_ino,
+				new_bindex, new_hidden_dentry->d_inode->i_ino);
+		if (err)
+			goto out_dir;
+	}
+#endif
+	/* We actually copyup the file here. */
+	if (S_ISREG(old_hidden_dentry->d_inode->i_mode)) {
+		mntget(stohiddenmnt_index(sb, old_bindex));
+		branchget(sb, old_bindex);
+		got_branch_input = old_bindex;
+		input_file =
+		    DENTRY_OPEN(old_hidden_dentry,
+				stohiddenmnt_index(sb, old_bindex), O_RDONLY);
+		if (IS_ERR(input_file)) {
+			err = PTR_ERR(input_file);
+			goto out_dir;
+		}
+		if (!input_file->f_op || !input_file->f_op->read) {
+			err = -EINVAL;
+			goto out_dir;
+		}
+
+		/* copy the new file */
+		DGET(new_hidden_dentry);
+		mntget(stohiddenmnt_index(sb, new_bindex));
+		branchget(sb, new_bindex);
+		got_branch_output = new_bindex;
+		output_file =
+		    DENTRY_OPEN(new_hidden_dentry,
+				stohiddenmnt_index(sb, new_bindex), O_WRONLY);
+		if (IS_ERR(output_file)) {
+			err = PTR_ERR(output_file);
+			goto out_dir;
+		}
+		if (!output_file->f_op || !output_file->f_op->write) {
+			err = -EINVAL;
+			goto out_dir;
+		}
+
+		/* allocating a buffer */
+		buf = (char *)KMALLOC(PAGE_SIZE, GFP_KERNEL);
+		if (!buf) {
+			err = -ENOMEM;
+			goto out_dir;
+		}
+
+		/* now read PAGE_SIZE bytes from offset 0 in a loop */
+		old_fs = get_fs();
+
+		input_file->f_pos = 0;
+		output_file->f_pos = 0;
+
+		err = 0;	// reset error just in case
+		set_fs(KERNEL_DS);
+		do {
+			if (len >= PAGE_SIZE)
+				size = PAGE_SIZE;
+			else if ((len < PAGE_SIZE) && (len > 0))
+				size = len;
+
+			len -= PAGE_SIZE;
+
+			read_bytes =
+			    input_file->f_op->read(input_file,
+						   (char __user *)buf, size,
+						   &input_file->f_pos);
+			if (read_bytes <= 0) {
+				err = read_bytes;
+				break;
+			}
+
+			write_bytes =
+			    output_file->f_op->write(output_file,
+						     (char __user *)buf,
+						     read_bytes,
+						     &output_file->f_pos);
+			if (write_bytes < 0 || (write_bytes < read_bytes)) {
+				err = write_bytes;
+				break;
+			}
+		} while ((read_bytes > 0) && (len > 0));
+		set_fs(old_fs);
+		KFREE(buf);
+#ifdef UNIONFS_MMAP
+		/* SP: Now that we copied up the file, have to sync its data
+		 * as otherwise when we do a read_cache_page(), we'll possibly
+		 * read crap.
+		 *
+		 * another posisble solution would be in the address op code
+		 * would be to check the "lower" page to see if its dirty,
+		 * and if it's dirty, use it directl
+		 */
+		if (!err) {
+			err =
+			    output_file->f_op->fsync(output_file,
+						     new_hidden_dentry, 0);
+		}
+#endif
+		if (err) {
+			/* copyup failed, because we ran out of space or quota,
+			 * or something else happened so let's unlink; we don't
+			 * really care about the return value of vfs_unlink */
+			vfs_unlink(new_hidden_parent_dentry->d_inode,
+				   new_hidden_dentry);
+
+			goto out_dir;
+		}
+	}
+
+	/* Set permissions. */
+	if ((err =
+	     copyup_permissions(sb, old_hidden_dentry, new_hidden_dentry)))
+		goto out_dir;
+	/* Selinux uses extended attributes for permissions. */
+	if ((err = copyup_xattrs(old_hidden_dentry, new_hidden_dentry)))
+		goto out_dir;
+
+	/* do not allow files getting deleted to be reinterposed */
+	if (!d_deleted(dentry))
+		unionfs_reinterpose(dentry);
+
+      out_dir:
+	if (new_hidden_parent_dentry)
+		unlock_dir(new_hidden_parent_dentry);
+
+      out:
+	if (input_file && !IS_ERR(input_file)) {
+		fput(input_file);
+	} else {
+		/* since input file was not opened, we need to explicitly
+		 * dput the old_hidden_dentry
+		 */
+		DPUT(old_hidden_dentry);
+	}
+
+	/* in any case, we have to branchput */
+	if (got_branch_input >= 0)
+		branchput(sb, got_branch_input);
+
+	if (output_file) {
+		if (copyup_file && !err) {
+			*copyup_file = output_file;
+		} else {
+			/* close the file if there was no error, or if we ran
+			 * out of space in which case we unlinked the file */
+			if (!IS_ERR(output_file))
+				fput(output_file);
+			branchput(sb, got_branch_output);
+		}
+	}
+
+	unionfs_read_unlock(sb);
+
+	print_dentry("OUT: copyup_dentry", dentry);
+	print_inode("OUT: copyup_dentry", dentry->d_inode);
+
+	print_exit_status(err);
+	return err;
+}
+
+/* This function creates a copy of a file represented by 'file' which currently
+ * resides in branch 'bstart' to branch 'new_bindex.  The copy will be named
+ * "name".  */
+int copyup_named_file(struct inode *dir, struct file *file, char *name,
+		      int bstart, int new_bindex, loff_t len)
+{
+	int err = 0;
+	struct file *output_file = NULL;
+
+	print_entry_location();
+
+	err = copyup_named_dentry(dir, file->f_dentry, bstart,
+				  new_bindex, name, strlen(name), &output_file,
+				  len);
+	if (!err) {
+		fbstart(file) = new_bindex;
+		set_ftohf_index(file, new_bindex, output_file);
+	}
+
+	print_exit_status(err);
+	return err;
+}
+
+/* This function creates a copy of a file represented by 'file' which currently
+ * resides in branch 'bstart' to branch 'new_bindex.
+ */
+int copyup_file(struct inode *dir, struct file *file, int bstart,
+		int new_bindex, loff_t len)
+{
+	int err = 0;
+	struct file *output_file = NULL;
+
+	print_entry_location();
+
+	err = copyup_dentry(dir, file->f_dentry, bstart, new_bindex,
+			    &output_file, len);
+	if (!err) {
+		fbstart(file) = new_bindex;
+		set_ftohf_index(file, new_bindex, output_file);
+	}
+
+	print_exit_status(err);
+	return err;
+}
+
+/* This function replicates the directory structure upto given dentry
+ * in the bindex branch. Can create directory structure recursively to the right
+ * also.
+ */
+struct dentry *create_parents(struct inode *dir, struct dentry *dentry,
+			      int bindex)
+{
+	struct dentry *hidden_dentry;
+
+	print_entry_location();
+	hidden_dentry =
+	    create_parents_named(dir, dentry, dentry->d_name.name, bindex);
+	print_exit_location();
+
+	return (hidden_dentry);
+}
+
+/* This function replicates the directory structure upto given dentry
+ * in the bindex branch.  */
+struct dentry *create_parents_named(struct inode *dir, struct dentry *dentry,
+				    const char *name, int bindex)
+{
+	int err;
+	struct dentry *child_dentry;
+	struct dentry *parent_dentry;
+	struct dentry *hidden_parent_dentry = NULL;
+	struct dentry *hidden_dentry = NULL;
+	const char *childname;
+	unsigned int childnamelen;
+
+	int old_kmalloc_size;
+	int kmalloc_size;
+	int num_dentry;
+	int count;
+
+	int old_bstart;
+	int old_bend;
+	struct dentry **path = NULL;
+	struct dentry **tmp_path;
+	struct super_block *sb;
+#ifdef UNIONFS_IMAP
+	int persistent;
+#endif
+	print_entry_location();
+
+	verify_locked(dentry);
+
+	/* There is no sense allocating any less than the minimum. */
+	kmalloc_size = malloc_sizes[0].cs_size;
+	num_dentry = kmalloc_size / sizeof(struct dentry *);
+
+	if ((err = is_robranch_super(dir->i_sb, bindex))) {
+		hidden_dentry = ERR_PTR(err);
+		goto out;
+	}
+
+	print_dentry("IN: create_parents_named", dentry);
+	dprint(PRINT_DEBUG, "name = %s\n", name);
+
+	old_bstart = dbstart(dentry);
+	old_bend = dbend(dentry);
+
+	hidden_dentry = ERR_PTR(-ENOMEM);
+	path = (struct dentry **)KZALLOC(kmalloc_size, GFP_KERNEL);
+	if (!path)
+		goto out;
+
+	/* assume the negative dentry of unionfs as the parent dentry */
+	parent_dentry = dentry;
+
+	count = 0;
+	/* This loop finds the first parent that exists in the given branch.
+	 * We start building the directory structure from there.  At the end
+	 * of the loop, the following should hold:
+	 *      child_dentry is the first nonexistent child
+	 *      parent_dentry is the first existent parent
+	 *      path[0] is the = deepest child
+	 *      path[count] is the first child to create
+	 */
+	do {
+		child_dentry = parent_dentry;
+
+		/* find the parent directory dentry in unionfs */
+		parent_dentry = child_dentry->d_parent;
+		lock_dentry(parent_dentry);
+
+		/* find out the hidden_parent_dentry in the given branch */
+		hidden_parent_dentry = dtohd_index(parent_dentry, bindex);
+
+		/* store the child dentry */
+		path[count++] = child_dentry;
+		if (count == num_dentry) {
+			old_kmalloc_size = kmalloc_size;
+			kmalloc_size *= 2;
+			num_dentry = kmalloc_size / sizeof(struct dentry *);
+
+			tmp_path =
+			    (struct dentry **)KZALLOC(kmalloc_size, GFP_KERNEL);
+			if (!tmp_path) {
+				hidden_dentry = ERR_PTR(-ENOMEM);
+				goto out;
+			}
+			memcpy(tmp_path, path, old_kmalloc_size);
+			KFREE(path);
+			path = tmp_path;
+			tmp_path = NULL;
+		}
+
+	} while (!hidden_parent_dentry);
+	count--;
+
+	sb = dentry->d_sb;
+#ifdef UNIONFS_IMAP
+	persistent = stopd(sb)->usi_persistent;
+#endif
+	/* This is basically while(child_dentry != dentry).  This loop is
+	 * horrible to follow and should be replaced with cleaner code. */
+	while (1) {
+		// get hidden parent dir in the current branch
+		hidden_parent_dentry = dtohd_index(parent_dentry, bindex);
+		unlock_dentry(parent_dentry);
+
+		// init the values to lookup
+		childname = child_dentry->d_name.name;
+		childnamelen = child_dentry->d_name.len;
+
+		if (child_dentry != dentry) {
+			// lookup child in the underlying file system
+			hidden_dentry =
+			    LOOKUP_ONE_LEN(childname, hidden_parent_dentry,
+					   childnamelen);
+			if (IS_ERR(hidden_dentry))
+				goto out;
+		} else {
+			int loop_start;
+			int loop_end;
+			int new_bstart = -1;
+			int new_bend = -1;
+			int i;
+
+			/* is the name a whiteout of the childname ? */
+			//lookup the whiteout child in the underlying file system
+			hidden_dentry =
+			    LOOKUP_ONE_LEN(name, hidden_parent_dentry,
+					   strlen(name));
+			if (IS_ERR(hidden_dentry))
+				goto out;
+
+			/* Replace the current dentry (if any) with the new one. */
+			DPUT(dtohd_index(dentry, bindex));
+			set_dtohd_index(dentry, bindex, hidden_dentry);
+
+			loop_start =
+			    (old_bstart < bindex) ? old_bstart : bindex;
+			loop_end = (old_bend > bindex) ? old_bend : bindex;
+
+			/* This loop sets the bstart and bend for the new
+			 * dentry by traversing from left to right.
+			 * It also dputs all negative dentries except
+			 * bindex (the newly looked dentry
+			 */
+			for (i = loop_start; i <= loop_end; i++) {
+				if (!dtohd_index(dentry, i))
+					continue;
+
+				if (i == bindex) {
+					new_bend = i;
+					if (new_bstart < 0)
+						new_bstart = i;
+					continue;
+				}
+
+				if (!dtohd_index(dentry, i)->d_inode) {
+					DPUT(dtohd_index(dentry, i));
+					set_dtohd_index(dentry, i, NULL);
+				} else {
+					if (new_bstart < 0)
+						new_bstart = i;
+					new_bend = i;
+				}
+			}
+
+			if (new_bstart < 0)
+				new_bstart = bindex;
+			if (new_bend < 0)
+				new_bend = bindex;
+			set_dbstart(dentry, new_bstart);
+			set_dbend(dentry, new_bend);
+			break;
+		}
+
+		if (hidden_dentry->d_inode) {
+			/* since this already exists we dput to avoid
+			 * multiple references on the same dentry */
+			DPUT(hidden_dentry);
+		} else {
+			uid_t saved_uid = current->fsuid;
+			gid_t saved_gid = current->fsgid;
+
+			/* its a negative dentry, create a new dir */
+			hidden_parent_dentry = lock_parent(hidden_dentry);
+			current->fsuid = hidden_parent_dentry->d_inode->i_uid;
+			current->fsgid = hidden_parent_dentry->d_inode->i_gid;
+			err = vfs_mkdir(hidden_parent_dentry->d_inode,
+					hidden_dentry, S_IRWXUGO);
+			current->fsuid = saved_uid;
+			current->fsgid = saved_gid;
+			if (!err)
+				err = copyup_permissions
+				    (dir->i_sb, child_dentry, hidden_dentry);
+			unlock_dir(hidden_parent_dentry);
+			if (err) {
+				DPUT(hidden_dentry);
+				hidden_dentry = ERR_PTR(err);
+				goto out;
+			}
+#ifdef UNIONFS_IMAP
+			if (persistent) {
+				err = write_uin
+				    (sb, child_dentry->d_inode->i_ino,
+				     bindex, hidden_dentry->d_inode->i_ino);
+				if (err) {
+					DPUT(hidden_dentry);
+					hidden_dentry = ERR_PTR(err);
+					goto out;
+				}
+			}
+#endif
+			set_itohi_index(child_dentry->d_inode, bindex,
+					IGRAB(hidden_dentry->d_inode));
+			if (ibstart(child_dentry->d_inode) > bindex)
+				ibstart(child_dentry->d_inode) = bindex;
+			if (ibend(child_dentry->d_inode) < bindex)
+				ibend(child_dentry->d_inode) = bindex;
+
+			set_dtohd_index(child_dentry, bindex, hidden_dentry);
+			if (dbstart(child_dentry) > bindex)
+				set_dbstart(child_dentry, bindex);
+			if (dbend(child_dentry) < bindex)
+				set_dbend(child_dentry, bindex);
+		}
+
+		parent_dentry = child_dentry;
+		child_dentry = path[--count];
+	}
+      out:
+	KFREE(path);
+	print_dentry("OUT: create_parents_named", dentry);
+	print_exit_pointer(hidden_dentry);
+	return hidden_dentry;
+}
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/dentry.c newtree/fs/unionfs/dentry.c
--- oldtree/fs/unionfs/dentry.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/dentry.c	2006-07-12 19:01:51.000000000 -0400
@@ -0,0 +1,274 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: dentry.c,v 1.76 2006/06/01 03:11:02 jsipek Exp $
+ */
+
+#include "unionfs.h"
+
+/* declarations added for "sparse" */
+extern int unionfs_d_revalidate_wrap(struct dentry *dentry,
+				     struct nameidata *nd);
+extern void unionfs_d_release(struct dentry *dentry);
+extern void unionfs_d_iput(struct dentry *dentry, struct inode *inode);
+
+/*
+ * THIS IS A BOOLEAN FUNCTION: returns 1 if valid, 0 otherwise.
+ */
+int unionfs_d_revalidate(struct dentry *dentry, struct nameidata *nd)
+{
+	int valid = 1;		/* default is valid (1); invalid is 0. */
+	struct dentry *hidden_dentry;
+	int bindex, bstart, bend;
+	int sbgen, dgen;
+	int positive = 0;
+	int locked = 0;
+	int restart = 0;
+	int interpose_flag;
+
+	print_util_entry_location();
+
+      restart:
+	verify_locked(dentry);
+
+	/* if the dentry is unhashed, do NOT revalidate */
+	if (d_deleted(dentry)) {
+		dprint(PRINT_DEBUG, "unhashed dentry being revalidated: %*s\n",
+			    dentry->d_name.len, dentry->d_name.name);
+		goto out;
+	}
+
+	BUG_ON(dbstart(dentry) == -1);
+	if (dentry->d_inode)
+		positive = 1;
+	dgen = atomic_read(&dtopd(dentry)->udi_generation);
+	sbgen = atomic_read(&stopd(dentry->d_sb)->usi_generation);
+	/* If we are working on an unconnected dentry, then there is no
+	 * revalidation to be done, because this file does not exist within the
+	 * namespace, and Unionfs operates on the namespace, not data.
+	 */
+	if (sbgen != dgen) {
+		struct dentry *result;
+		int pdgen;
+
+		unionfs_read_lock(dentry->d_sb);
+		locked = 1;
+
+		/* The root entry should always be valid */
+		BUG_ON(IS_ROOT(dentry));
+
+		/* We can't work correctly if our parent isn't valid. */
+		pdgen = atomic_read(&dtopd(dentry->d_parent)->udi_generation);
+		if (!restart && (pdgen != sbgen)) {
+			unionfs_read_unlock(dentry->d_sb);
+			locked = 0;
+			/* We must be locked before our parent. */
+			if (!
+			    (dentry->d_parent->d_op->
+			     d_revalidate(dentry->d_parent, nd))) {
+				valid = 0;
+				goto out;
+			}
+			restart = 1;
+			goto restart;
+		}
+		BUG_ON(pdgen != sbgen);
+
+		/* Free the pointers for our inodes and this dentry. */
+		bstart = dbstart(dentry);
+		bend = dbend(dentry);
+		if (bstart >= 0) {
+			struct dentry *hidden_dentry;
+			for (bindex = bstart; bindex <= bend; bindex++) {
+				hidden_dentry =
+				    dtohd_index_nocheck(dentry, bindex);
+				if (!hidden_dentry)
+					continue;
+				DPUT(hidden_dentry);
+			}
+		}
+		set_dbstart(dentry, -1);
+		set_dbend(dentry, -1);
+
+		interpose_flag = INTERPOSE_REVAL_NEG;
+		if (positive) {
+			interpose_flag = INTERPOSE_REVAL;
+			mutex_lock(&dentry->d_inode->i_mutex);
+			bstart = ibstart(dentry->d_inode);
+			bend = ibend(dentry->d_inode);
+			if (bstart >= 0) {
+				struct inode *hidden_inode;
+				for (bindex = bstart; bindex <= bend; bindex++) {
+					hidden_inode =
+					    itohi_index(dentry->d_inode,
+							bindex);
+					if (!hidden_inode)
+						continue;
+					IPUT(hidden_inode);
+				}
+			}
+			KFREE(itohi_ptr(dentry->d_inode));
+			itohi_ptr(dentry->d_inode) = NULL;
+			ibstart(dentry->d_inode) = -1;
+			ibend(dentry->d_inode) = -1;
+			mutex_unlock(&dentry->d_inode->i_mutex);
+		}
+
+		result = unionfs_lookup_backend(dentry, interpose_flag);
+		if (result) {
+			if (IS_ERR(result)) {
+				valid = 0;
+				goto out;
+			}
+			/* current unionfs_lookup_backend() doesn't return
+			   a valid dentry */
+			DPUT(dentry);
+			dentry = result;
+		}
+
+		if (positive && itopd(dentry->d_inode)->uii_stale) {
+			make_stale_inode(dentry->d_inode);
+			d_drop(dentry);
+			valid = 0;
+			goto out;
+		}
+		goto out;
+	}
+
+	/* The revalidation must occur across all branches */
+	bstart = dbstart(dentry);
+	bend = dbend(dentry);
+	BUG_ON(bstart == -1);
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		if (!hidden_dentry || !hidden_dentry->d_op
+		    || !hidden_dentry->d_op->d_revalidate)
+			continue;
+
+		if (!hidden_dentry->d_op->d_revalidate(hidden_dentry, nd))
+			valid = 0;
+	}
+
+	if (!dentry->d_inode)
+		valid = 0;
+	if (valid)
+		fist_copy_attr_all(dentry->d_inode, itohi(dentry->d_inode));
+
+      out:
+	if (locked)
+		unionfs_read_unlock(dentry->d_sb);
+	print_dentry("revalidate out", dentry);
+	print_util_exit_status(valid);
+	return valid;
+}
+
+int unionfs_d_revalidate_wrap(struct dentry *dentry, struct nameidata *nd)
+{
+	int err;
+
+	print_entry_location();
+	lock_dentry(dentry);
+
+	err = unionfs_d_revalidate(dentry, nd);
+
+	unlock_dentry(dentry);
+	print_exit_status(err);
+	return err;
+}
+
+void unionfs_d_release(struct dentry *dentry)
+{
+	struct dentry *hidden_dentry;
+	int bindex, bstart, bend;
+
+	print_entry_location();
+	/* There is no reason to lock the dentry, because we have the only
+	 * reference, but the printing functions verify that we have a lock
+	 * on the dentry before calling dbstart, etc. */
+	lock_dentry(dentry);
+	print_dentry_nocheck("unionfs_d_release IN dentry", dentry);
+
+	/* this could be a negative dentry, so check first */
+	if (!dtopd(dentry)) {
+		dprint(PRINT_DEBUG, "dentry without private data: %*s",
+			    dentry->d_name.len, dentry->d_name.name);
+		goto out;
+	} else if (dbstart(dentry) < 0) {
+		/* this is due to a failed lookup */
+		/* the failed lookup has a dtohd_ptr set to null,
+		   but this is a better check */
+		dprint(PRINT_DEBUG, "dentry without hidden dentries : %*s",
+			    dentry->d_name.len, dentry->d_name.name);
+		goto out_free;
+	}
+
+	/* Release all the hidden dentries */
+	bstart = dbstart(dentry);
+	bend = dbend(dentry);
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		DPUT(hidden_dentry);
+		set_dtohd_index(dentry, bindex, NULL);
+	}
+	/* free private data (unionfs_dentry_info) here */
+	KFREE(dtohd_ptr(dentry));
+	dtohd_ptr(dentry) = NULL;
+      out_free:
+	/* No need to unlock it, because it is disappeared. */
+#ifdef TRACKLOCK
+	printk("DESTROYLOCK:%p\n", dentry);
+#endif
+	free_dentry_private_data(dtopd(dentry));
+	dtopd_lhs(dentry) = NULL;	/* just to be safe */
+      out:
+	print_exit_location();
+}
+
+/*
+ * we don't really need unionfs_d_iput, because dentry_iput will call iput() if
+ * unionfs_d_iput is not defined. We left this implemented for ease of
+ * tracing/debugging.
+ */
+void unionfs_d_iput(struct dentry *dentry, struct inode *inode)
+{
+	print_entry_location();
+	IPUT(inode);
+	print_exit_location();
+}
+
+struct dentry_operations unionfs_dops = {
+	.d_revalidate = unionfs_d_revalidate_wrap,
+	.d_release = unionfs_d_release,
+	.d_iput = unionfs_d_iput,
+};
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/dirfops.c newtree/fs/unionfs/dirfops.c
--- oldtree/fs/unionfs/dirfops.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/dirfops.c	2006-07-12 19:01:51.000000000 -0400
@@ -0,0 +1,329 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: dirfops.c,v 1.24 2006/06/01 03:11:02 jsipek Exp $
+ */
+
+#include "unionfs.h"
+
+/* Make sure our rdstate is playing by the rules. */
+static void verify_rdstate_offset(struct unionfs_dir_state *rdstate)
+{
+	BUG_ON(rdstate->uds_offset >= DIREOF);
+	BUG_ON(rdstate->uds_cookie >= MAXRDCOOKIE);
+}
+
+struct unionfs_getdents_callback {
+	struct unionfs_dir_state *rdstate;
+	void *dirent;
+	int entries_written;
+	int filldir_called;
+	int filldir_error;
+	filldir_t filldir;
+	struct super_block *sb;
+};
+
+/* copied from generic filldir in fs/readir.c */
+static int unionfs_filldir(void *dirent, const char *name, int namelen,
+			   loff_t offset, ino_t ino, unsigned int d_type)
+{
+	struct unionfs_getdents_callback *buf =
+	    (struct unionfs_getdents_callback *)dirent;
+	struct filldir_node *found = NULL;
+	int err = 0;
+	int is_wh_entry = 0;
+
+	dprint(PRINT_DEBUG, "unionfs_filldir name=%*s\n", namelen, name);
+
+	buf->filldir_called++;
+
+	if ((namelen > WHLEN) && !strncmp(name, WHPFX, WHLEN)) {
+		name += WHLEN;
+		namelen -= WHLEN;
+		is_wh_entry = 1;
+	}
+
+	found = find_filldir_node(buf->rdstate, name, namelen);
+
+	if (found)
+		goto out;
+
+	/* if 'name' isn't a whiteout filldir it. */
+	if (!is_wh_entry) {
+		off_t pos = rdstate2offset(buf->rdstate);
+		ino_t unionfs_ino = ino;
+#ifdef UNIONFS_IMAP
+		if (stopd(buf->sb)->usi_persistent)
+			err = read_uin(buf->sb, buf->rdstate->uds_bindex,
+				       ino, O_CREAT, &unionfs_ino);
+#endif
+		if (!err) {
+			err = buf->filldir(buf->dirent, name, namelen, pos,
+					   unionfs_ino, d_type);
+			buf->rdstate->uds_offset++;
+			verify_rdstate_offset(buf->rdstate);
+		}
+	}
+	/* If we did fill it, stuff it in our hash, otherwise return an error */
+	if (err) {
+		buf->filldir_error = err;
+		goto out;
+	}
+	buf->entries_written++;
+	if ((err = add_filldir_node(buf->rdstate, name, namelen,
+				    buf->rdstate->uds_bindex, is_wh_entry)))
+		buf->filldir_error = err;
+
+      out:
+	return err;
+}
+
+static int unionfs_readdir(struct file *file, void *dirent, filldir_t filldir)
+{
+	int err = 0;
+	struct file *hidden_file = NULL;
+	struct inode *inode = NULL;
+	struct unionfs_getdents_callback buf;
+	struct unionfs_dir_state *uds;
+	int bend;
+	loff_t offset;
+
+	print_entry("file = %p, pos = %llx", file, file->f_pos);
+
+	print_file("In unionfs_readdir()", file);
+
+	if ((err = unionfs_file_revalidate(file, 0)))
+		goto out;
+
+	inode = file->f_dentry->d_inode;
+	checkinode(inode, "unionfs_readdir");
+
+	uds = ftopd(file)->rdstate;
+	if (!uds) {
+		if (file->f_pos == DIREOF) {
+			goto out;
+		} else if (file->f_pos > 0) {
+			uds = find_rdstate(inode, file->f_pos);
+			if (!uds) {
+				err = -ESTALE;
+				goto out;
+			}
+			ftopd(file)->rdstate = uds;
+		} else {
+			init_rdstate(file);
+			uds = ftopd(file)->rdstate;
+		}
+	}
+	bend = fbend(file);
+
+	while (uds->uds_bindex <= bend) {
+		hidden_file = ftohf_index(file, uds->uds_bindex);
+		if (!hidden_file) {
+			dprint(PRINT_DEBUG,
+				    "Incremented bindex to %d of %d,"
+				    " because hidden file is NULL.\n",
+				    uds->uds_bindex, bend);
+			uds->uds_bindex++;
+			uds->uds_dirpos = 0;
+			continue;
+		}
+
+		/* prepare callback buffer */
+		buf.filldir_called = 0;
+		buf.filldir_error = 0;
+		buf.entries_written = 0;
+		buf.dirent = dirent;
+		buf.filldir = filldir;
+		buf.rdstate = uds;
+		buf.sb = inode->i_sb;
+
+		/* Read starting from where we last left off. */
+		offset = vfs_llseek(hidden_file, uds->uds_dirpos, 0);
+		if (offset < 0) {
+			err = offset;
+			goto out;
+		}
+		dprint(PRINT_DEBUG, "calling readdir for %d.%lld (offset = %lld)\n",
+			    uds->uds_bindex, uds->uds_dirpos, offset);
+		err = vfs_readdir(hidden_file, unionfs_filldir, (void *)&buf);
+		dprint(PRINT_DEBUG,
+			    "readdir on %d.%lld = %d (entries written %d, filldir called %d)\n",
+			    uds->uds_bindex, (long long)uds->uds_dirpos, err,
+			    buf.entries_written, buf.filldir_called);
+		/* Save the position for when we continue. */
+
+		offset = vfs_llseek(hidden_file, 0, 1);
+		if (offset < 0) {
+			err = offset;
+			goto out;
+		}
+		uds->uds_dirpos = offset;
+
+		/* Copy the atime. */
+		fist_copy_attr_atime(inode, hidden_file->f_dentry->d_inode);
+
+		if (err < 0) {
+			goto out;
+		}
+
+		if (buf.filldir_error) {
+			break;
+		}
+
+		if (!buf.entries_written) {
+			uds->uds_bindex++;
+			uds->uds_dirpos = 0;
+		}
+	}
+
+	if (!buf.filldir_error && uds->uds_bindex >= bend) {
+		dprint(PRINT_DEBUG,
+			    "Discarding rdstate because readdir is over (hashsize = %d)\n",
+			    uds->uds_hashentries);
+		/* Save the number of hash entries for next time. */
+		itopd(inode)->uii_hashsize = uds->uds_hashentries;
+		free_rdstate(uds);
+		ftopd(file)->rdstate = NULL;
+		file->f_pos = DIREOF;
+	} else {
+		file->f_pos = rdstate2offset(uds);
+		dprint(PRINT_DEBUG, "rdstate now has a cookie of %u (err = %d)\n",
+			    uds->uds_cookie, err);
+	}
+
+      out:
+	checkinode(inode, "post unionfs_readdir");
+	print_exit_status(err);
+	return err;
+}
+
+/* This is not meant to be a generic repositioning function.  If you do
+ * things that aren't supported, then we return EINVAL.
+ *
+ * What is allowed:
+ *  (1) seeking to the same position that you are currently at
+ *	This really has no effect, but returns where you are.
+ *  (2) seeking to the end of the file, if you've read everything
+ *	This really has no effect, but returns where you are.
+ *  (3) seeking to the beginning of the file
+ *	This throws out all state, and lets you begin again.
+ */
+static loff_t unionfs_dir_llseek(struct file *file, loff_t offset, int origin)
+{
+	struct unionfs_dir_state *rdstate;
+	loff_t err;
+
+	print_entry(" file=%p, offset=0x%llx, origin = %d", file, offset,
+		    origin);
+
+	if ((err = unionfs_file_revalidate(file, 0)))
+		goto out;
+
+	rdstate = ftopd(file)->rdstate;
+
+	/* We let users seek to their current position, but not anywhere else. */
+	if (!offset) {
+		switch (origin) {
+		case SEEK_SET:
+			if (rdstate) {
+				free_rdstate(rdstate);
+				ftopd(file)->rdstate = NULL;
+			}
+			init_rdstate(file);
+			err = 0;
+			break;
+		case SEEK_CUR:
+			if (file->f_pos) {
+				if (file->f_pos == DIREOF)
+					err = DIREOF;
+				else
+					BUG_ON(file->f_pos !=
+					       rdstate2offset(rdstate));
+				err = file->f_pos;
+			} else {
+				err = 0;
+			}
+			break;
+		case SEEK_END:
+			/* Unsupported, because we would break everything.  */
+			err = -EINVAL;
+			break;
+		}
+	} else {
+		switch (origin) {
+		case SEEK_SET:
+			if (rdstate) {
+				if (offset == rdstate2offset(rdstate)) {
+					err = offset;
+				} else if (file->f_pos == DIREOF) {
+					err = DIREOF;
+				} else {
+					err = -EINVAL;
+				}
+			} else {
+				if ((rdstate =
+				     find_rdstate(file->f_dentry->d_inode,
+						  offset))) {
+					ftopd(file)->rdstate = rdstate;
+					err = rdstate->uds_offset;
+				} else {
+					err = -EINVAL;
+				}
+			}
+			break;
+		case SEEK_CUR:
+		case SEEK_END:
+			/* Unsupported, because we would break everything.  */
+			err = -EINVAL;
+			break;
+		}
+	}
+
+      out:
+	print_exit_status((int)err);
+	return err;
+}
+
+/* Trimmed directory options, we shouldn't pass everything down since
+ * we don't want to operate on partial directories.
+ */
+struct file_operations unionfs_dir_fops = {
+	.llseek = unionfs_dir_llseek,
+	.read = generic_read_dir,
+	.readdir = unionfs_readdir,
+	.unlocked_ioctl = unionfs_ioctl,
+	.open = unionfs_open,
+	.release = unionfs_file_release,
+	.flush = unionfs_flush,
+};
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/dirhelper.c newtree/fs/unionfs/dirhelper.c
--- oldtree/fs/unionfs/dirhelper.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/dirhelper.c	2006-07-12 19:01:51.000000000 -0400
@@ -0,0 +1,273 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: dirhelper.c,v 1.29 2006/02/21 08:36:24 jsipek Exp $
+ */
+
+#include "unionfs.h"
+
+/* Delete all of the whiteouts in a given directory for rmdir. */
+int delete_whiteouts(struct dentry *dentry, int bindex,
+		     struct unionfs_dir_state *namelist)
+{
+	int err = 0;
+	struct dentry *hidden_dir_dentry = NULL;
+	struct dentry *hidden_dentry;
+	struct super_block *sb;
+	char *name = NULL, *p;
+	struct inode *hidden_dir;
+	struct superio sio;
+	int do_superio;
+
+	int i;
+	struct list_head *pos;
+	struct filldir_node *cursor;
+
+	print_entry_location();
+
+	sb = dentry->d_sb;
+	unionfs_read_lock(sb);
+
+	BUG_ON(!S_ISDIR(dentry->d_inode->i_mode));
+	BUG_ON(bindex < dbstart(dentry));
+	BUG_ON(bindex > dbend(dentry));
+	err = is_robranch_super(sb, bindex);
+	if (err)
+		goto out;
+
+	/* Find out hidden parent dentry */
+	hidden_dir_dentry = dtohd_index(dentry, bindex);
+	BUG_ON(!S_ISDIR(hidden_dir_dentry->d_inode->i_mode));
+	hidden_dir = hidden_dir_dentry->d_inode;
+	BUG_ON(!S_ISDIR(hidden_dir->i_mode));
+
+	err = -ENOMEM;
+	name = __getname();
+	if (!name)
+		goto out;
+	strcpy(name, WHPFX);
+	p = name + WHLEN;
+
+	err = 0;
+	mutex_lock(&hidden_dir->i_mutex);
+	do_superio = permission(hidden_dir, MAY_WRITE | MAY_EXEC, NULL);
+	if (do_superio)
+		superio_store(&sio);
+	for (i = 0; !err && i < namelist->uds_size; i++) {
+		list_for_each(pos, &namelist->uds_list[i]) {
+			cursor =
+			    list_entry(pos, struct filldir_node, file_list);
+			/* Only operate on whiteouts in this branch. */
+			if (cursor->bindex != bindex)
+				continue;
+			if (!cursor->whiteout)
+				continue;
+
+			strcpy(p, cursor->name);
+			hidden_dentry =
+			    LOOKUP_ONE_LEN(name, hidden_dir_dentry,
+					   cursor->namelen + WHLEN);
+			if (IS_ERR(hidden_dentry)) {
+				err = PTR_ERR(hidden_dentry);
+				break;
+			}
+			if (hidden_dentry->d_inode)
+				err = vfs_unlink(hidden_dir, hidden_dentry);
+			DPUT(hidden_dentry);
+			if (err)
+				break;
+		}
+	}
+	mutex_unlock(&hidden_dir->i_mutex);
+	if (do_superio)
+		superio_revert(&sio);
+
+	__putname(name);
+
+	/* After all of the removals, we should copy the attributes once. */
+	fist_copy_attr_times(dentry->d_inode, hidden_dir_dentry->d_inode);
+
+      out:
+	unionfs_read_unlock(sb);
+	print_exit_status(err);
+	return err;
+}
+
+#define RD_NONE 0
+#define RD_CHECK_EMPTY 1
+/* The callback structure for check_empty. */
+struct unionfs_rdutil_callback {
+	int err;
+	int filldir_called;
+	struct unionfs_dir_state *rdstate;
+	int mode;
+};
+
+/* This filldir function makes sure only whiteouts exist within a directory. */
+static int readdir_util_callback(void *dirent, const char *name, int namelen,
+				 loff_t offset, ino_t ino, unsigned int d_type)
+{
+	int err = 0;
+	struct unionfs_rdutil_callback *buf =
+	    (struct unionfs_rdutil_callback *)dirent;
+	int whiteout = 0;
+	struct filldir_node *found;
+
+	print_entry_location();
+
+	buf->filldir_called = 1;
+
+	if (name[0] == '.'
+	    && (namelen == 1 || (name[1] == '.' && namelen == 2)))
+		goto out;
+
+	if ((namelen > WHLEN) && !strncmp(name, WHPFX, WHLEN)) {
+		namelen -= WHLEN;
+		name += WHLEN;
+		whiteout = 1;
+	}
+
+	found = find_filldir_node(buf->rdstate, name, namelen);
+	/* If it was found in the table there was a previous whiteout. */
+	if (found)
+		goto out;
+
+	/* If it wasn't found and isn't a whiteout, the directory isn't empty. */
+	err = -ENOTEMPTY;
+	if ((buf->mode == RD_CHECK_EMPTY) && !whiteout)
+		goto out;
+
+	err = add_filldir_node(buf->rdstate, name, namelen,
+			       buf->rdstate->uds_bindex, whiteout);
+
+      out:
+	buf->err = err;
+	print_exit_status(err);
+	return err;
+}
+
+/* Is a directory logically empty? */
+int check_empty(struct dentry *dentry, struct unionfs_dir_state **namelist)
+{
+	int err = 0;
+	struct dentry *hidden_dentry = NULL;
+	struct super_block *sb;
+	struct file *hidden_file;
+	struct unionfs_rdutil_callback *buf = NULL;
+	int bindex, bstart, bend, bopaque;
+
+	print_entry_location();
+
+	sb = dentry->d_sb;
+
+	unionfs_read_lock(sb);
+
+	BUG_ON(!S_ISDIR(dentry->d_inode->i_mode));
+
+	if ((err = unionfs_partial_lookup(dentry)))
+		goto out;
+
+	bstart = dbstart(dentry);
+	bend = dbend(dentry);
+	bopaque = dbopaque(dentry);
+	if (0 <= bopaque && bopaque < bend)
+		bend = bopaque;
+
+	buf = KMALLOC(sizeof(struct unionfs_rdutil_callback), GFP_KERNEL);
+	if (!buf) {
+		err = -ENOMEM;
+		goto out;
+	}
+	buf->err = 0;
+	buf->mode = RD_CHECK_EMPTY;
+	buf->rdstate = alloc_rdstate(dentry->d_inode, bstart);
+	if (!buf->rdstate) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	/* Process the hidden directories with rdutil_callback as a filldir. */
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		if (!hidden_dentry)
+			continue;
+		if (!hidden_dentry->d_inode)
+			continue;
+		if (!S_ISDIR(hidden_dentry->d_inode->i_mode))
+			continue;
+
+		DGET(hidden_dentry);
+		mntget(stohiddenmnt_index(sb, bindex));
+		branchget(sb, bindex);
+		hidden_file =
+		    DENTRY_OPEN(hidden_dentry, stohiddenmnt_index(sb, bindex),
+				O_RDONLY);
+		if (IS_ERR(hidden_file)) {
+			err = PTR_ERR(hidden_file);
+			DPUT(hidden_dentry);
+			branchput(sb, bindex);
+			goto out;
+		}
+
+		do {
+			buf->filldir_called = 0;
+			buf->rdstate->uds_bindex = bindex;
+			err = vfs_readdir(hidden_file,
+					  readdir_util_callback, buf);
+			if (buf->err)
+				err = buf->err;
+		} while ((err >= 0) && buf->filldir_called);
+
+		/* fput calls dput for hidden_dentry */
+		fput(hidden_file);
+		branchput(sb, bindex);
+
+		if (err < 0)
+			goto out;
+	}
+
+      out:
+	if (buf) {
+		if (namelist && !err)
+			*namelist = buf->rdstate;
+		else if (buf->rdstate)
+			free_rdstate(buf->rdstate);
+		KFREE(buf);
+	}
+
+	unionfs_read_unlock(sb);
+
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/file.c newtree/fs/unionfs/file.c
--- oldtree/fs/unionfs/file.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/file.c	2006-07-12 19:01:51.000000000 -0400
@@ -0,0 +1,392 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: file.c,v 1.142 2006/07/08 17:58:30 ezk Exp $
+ */
+
+#include "unionfs.h"
+
+/* declarations for sparse */
+extern ssize_t unionfs_read(struct file *, char __user *, size_t, loff_t *);
+extern ssize_t unionfs_write(struct file *, const char __user *, size_t,
+			     loff_t *);
+
+/*******************
+ * File Operations *
+ *******************/
+
+#ifndef UNIONFS_MMAP
+/* SP: Disable unionfs_llseek, as use generic_file_llseek on upper file */
+static loff_t unionfs_llseek(struct file *file, loff_t offset, int origin)
+{
+	loff_t err;
+	struct file *hidden_file = NULL;
+
+	print_entry_location();
+
+	dprint(PRINT_DEBUG, "unionfs_llseek: file=%p, offset=0x%llx, origin=%d\n",
+		    file, offset, origin);
+
+	if ((err = unionfs_file_revalidate(file, 0)))
+		goto out;
+
+	hidden_file = ftohf(file);
+	/* always set hidden position to this one */
+	hidden_file->f_pos = file->f_pos;
+
+	memcpy(&(hidden_file->f_ra), &(file->f_ra),
+	       sizeof(struct file_ra_state));
+
+	if (hidden_file->f_op && hidden_file->f_op->llseek)
+		err = hidden_file->f_op->llseek(hidden_file, offset, origin);
+	else
+		err = generic_file_llseek(hidden_file, offset, origin);
+
+	if (err < 0)
+		goto out;
+	if (err != file->f_pos) {
+		file->f_pos = err;
+		// ION maybe this?
+		//      file->f_pos = hidden_file->f_pos;
+
+		file->f_version++;
+	}
+      out:
+	print_exit_status((int)err);
+	return err;
+}
+#endif
+ssize_t __unionfs_read(struct file * file, char __user * buf, size_t count,
+		       loff_t * ppos)
+{
+	int err = -EINVAL;
+	struct file *hidden_file = NULL;
+	loff_t pos = *ppos;
+
+	print_file("entering __unionfs_read()", file);
+
+	hidden_file = ftohf(file);
+	if (!hidden_file->f_op || !hidden_file->f_op->read)
+		goto out;
+
+	err = hidden_file->f_op->read(hidden_file, buf, count, &pos);
+	*ppos = pos;
+
+      out:
+	print_file("leaving __unionfs_read()", file);
+
+	print_exit_status(err);
+	return err;
+}
+
+ssize_t unionfs_read(struct file * file, char __user * buf, size_t count,
+		     loff_t * ppos)
+{
+	int err = -EINVAL;
+
+	print_entry_location();
+
+	if ((err = unionfs_file_revalidate(file, 0)))
+		goto out;
+
+#ifdef UNIONFS_MMAP
+	err = generic_file_read(file, buf, count, ppos);
+	if (err >= 0)
+		file_accessed(ftohf(file));
+#else
+	err = __unionfs_read(file, buf, count, ppos);
+#endif
+
+      out:
+
+	print_exit_status(err);
+	return err;
+}
+
+/* SP: Sendfile code not updated, but should be able to use
+ * generic_file_sendfile, as it would use readpage, which we now have */
+#ifdef SUPPORT_BROKEN_LOSETUP
+static ssize_t unionfs_sendfile(struct file *file, loff_t * ppos,
+				size_t count, read_actor_t actor, void *target)
+{
+	ssize_t err;
+	struct file *hidden_file = NULL;
+
+	print_entry_location();
+
+	if ((err = unionfs_file_revalidate(file, 0)))
+		goto out;
+
+	hidden_file = ftohf(file);
+
+	err = -EINVAL;
+	if (!hidden_file->f_op || !hidden_file->f_op->sendfile)
+		goto out;
+
+	err = hidden_file->f_op->sendfile(hidden_file, ppos, count, actor,
+					  target);
+
+      out:
+	print_exit_status(err);
+	return err;
+}
+#endif
+ssize_t __unionfs_write(struct file * file, const char __user * buf,
+			size_t count, loff_t * ppos)
+{
+	int err = -EINVAL;
+	struct file *hidden_file = NULL;
+	struct inode *inode;
+	struct inode *hidden_inode;
+	loff_t pos = *ppos;
+	int bstart, bend;
+
+	print_entry_location();
+
+	inode = file->f_dentry->d_inode;
+
+	bstart = fbstart(file);
+	bend = fbend(file);
+
+	BUG_ON(bstart == -1);
+
+	hidden_file = ftohf(file);
+	hidden_inode = hidden_file->f_dentry->d_inode;
+
+	if (!hidden_file->f_op || !hidden_file->f_op->write)
+		goto out;
+
+	/* adjust for append -- seek to the end of the file */
+	if (file->f_flags & O_APPEND)
+		pos = inode->i_size;
+
+	err = hidden_file->f_op->write(hidden_file, buf, count, &pos);
+
+	/*
+	 * copy ctime and mtime from lower layer attributes
+	 * atime is unchanged for both layers
+	 */
+	if (err >= 0)
+		fist_copy_attr_times(inode, hidden_inode);
+
+	*ppos = pos;
+
+	/* update this inode's size */
+	if (pos > inode->i_size)
+		inode->i_size = pos;
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+ssize_t unionfs_write(struct file * file, const char __user * buf, size_t count,
+		      loff_t * ppos)
+{
+	int err = 0;
+
+	print_entry_location();
+
+	if ((err = unionfs_file_revalidate(file, 1)))
+		goto out;
+#ifdef UNIONFS_MMAP
+	err = generic_file_write(file, buf, count, ppos);
+#else
+	err = __unionfs_write(file, buf, count, ppos);
+#endif
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+static int unionfs_file_readdir(struct file *file, void *dirent,
+				filldir_t filldir)
+{
+	int err = -ENOTDIR;
+	print_entry_location();
+	print_exit_status(err);
+	return err;
+}
+
+static unsigned int unionfs_poll(struct file *file, poll_table * wait)
+{
+	unsigned int mask = DEFAULT_POLLMASK;
+	struct file *hidden_file = NULL;
+
+	print_entry_location();
+
+	if (unionfs_file_revalidate(file, 0)) {
+		/* We should pretend an error happend. */
+		mask = POLLERR | POLLIN | POLLOUT;
+		goto out;
+	}
+
+	hidden_file = ftohf(file);
+
+	if (!hidden_file->f_op || !hidden_file->f_op->poll)
+		goto out;
+
+	mask = hidden_file->f_op->poll(hidden_file, wait);
+
+      out:
+	print_exit_status(mask);
+	return mask;
+}
+
+#ifndef UNIONFS_MMAP
+static int __do_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	int err;
+	struct file *hidden_file;
+
+	print_entry_location();
+	hidden_file = ftohf(file);
+
+	err = -ENODEV;
+	if (!hidden_file->f_op || !hidden_file->f_op->mmap)
+		goto out;
+
+	vma->vm_file = hidden_file;
+	err = hidden_file->f_op->mmap(hidden_file, vma);
+	get_file(hidden_file);	/* make sure it doesn't get freed on us */
+	fput(file);		/* no need to keep extra ref on ours */
+      out:
+	print_exit_status(err);
+	return err;
+}
+#endif
+/* SP: mmap code now maps upper file
+ * like old code, will only copyup at this point, it's possible to copyup
+ * in writepage(), but I haven't bothered with that, as only apt-get seem
+ * to want to write to a shared/write mapping
+ */
+static int unionfs_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	int err = 0;
+	int willwrite;
+
+	print_entry_location();
+
+	/* This might could be deferred to mmap's writepage. */
+	willwrite = ((vma->vm_flags | VM_SHARED | VM_WRITE) == vma->vm_flags);
+	if ((err = unionfs_file_revalidate(file, willwrite)))
+		goto out;
+#ifdef UNIONFS_MMAP
+	err = generic_file_mmap(file, vma);
+	if (err) {
+		printk("unionfs_mmap: generic_file_mmap failed\n");
+	}
+#else
+	err = __do_mmap(file, vma);
+#endif
+
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+/* SP: disabled as use the generic file_fsync */
+#ifndef UNIONFS_MMAP
+static int unionfs_fsync(struct file *file, struct dentry *dentry, int datasync)
+{
+	int err;
+	struct file *hidden_file = NULL;
+
+	print_entry_location();
+
+	if ((err = unionfs_file_revalidate(file, 1)))
+		goto out;
+
+	hidden_file = ftohf(file);
+
+	err = -EINVAL;
+	if (!hidden_file->f_op || !hidden_file->f_op->fsync)
+		goto out;
+
+	mutex_lock(&hidden_file->f_dentry->d_inode->i_mutex);
+	err = hidden_file->f_op->fsync(hidden_file, hidden_file->f_dentry,
+				       datasync);
+	mutex_unlock(&hidden_file->f_dentry->d_inode->i_mutex);
+
+      out:
+	print_exit_status(err);
+	return err;
+}
+#endif
+
+/* SP: disabled as none of the other in kernel fs's seem to use it */
+static int unionfs_fasync(int fd, struct file *file, int flag)
+{
+	int err = 0;
+	struct file *hidden_file = NULL;
+
+	print_entry_location();
+
+	if ((err = unionfs_file_revalidate(file, 1)))
+		goto out;
+
+	hidden_file = ftohf(file);
+
+	if (hidden_file->f_op && hidden_file->f_op->fasync)
+		err = hidden_file->f_op->fasync(fd, hidden_file, flag);
+
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+struct file_operations unionfs_main_fops = {
+#ifdef UNIONFS_MMAP
+	.llseek = generic_file_llseek,
+#else
+	.llseek = unionfs_llseek,
+#endif
+	.read = unionfs_read,
+	.write = unionfs_write,
+	.readdir = unionfs_file_readdir,
+	.poll = unionfs_poll,
+	.unlocked_ioctl = unionfs_ioctl,
+	.mmap = unionfs_mmap,
+	.open = unionfs_open,
+	.flush = unionfs_flush,
+	.release = unionfs_file_release,
+#ifdef UNIONFS_MMAP
+	.fsync = file_fsync,
+#else
+	.fsync = unionfs_fsync,
+#endif
+	.fasync = unionfs_fasync,
+#ifdef SUPPORT_BROKEN_LOSETUP
+	.sendfile = unionfs_sendfile,
+#endif
+};
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/inode.c newtree/fs/unionfs/inode.c
--- oldtree/fs/unionfs/inode.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/inode.c	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,1020 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: inode.c,v 1.272 2006/07/08 17:58:30 ezk Exp $
+ */
+
+#include "unionfs.h"
+
+/* declarations added for "sparse" */
+extern struct dentry *unionfs_lookup(struct inode *, struct dentry *,
+				     struct nameidata *);
+extern int unionfs_readlink(struct dentry *dentry, char __user * buf,
+			    int bufsiz);
+extern void unionfs_put_link(struct dentry *dentry, struct nameidata *nd,
+			     void *cookie);
+
+static int unionfs_create(struct inode *parent, struct dentry *dentry,
+			  int mode, struct nameidata *nd)
+{
+	int err = 0;
+	struct dentry *hidden_dentry = NULL;
+	struct dentry *whiteout_dentry = NULL;
+	struct dentry *new_hidden_dentry;
+	struct dentry *hidden_parent_dentry = NULL;
+	int bindex = 0, bstart;
+	char *name = NULL;
+
+	print_entry_location();
+	lock_dentry(dentry);
+	print_dentry("IN unionfs_create", dentry);
+
+	/* We start out in the leftmost branch. */
+	bstart = dbstart(dentry);
+	hidden_dentry = dtohd(dentry);
+
+	/* check if whiteout exists in this branch, i.e. lookup .wh.foo first */
+	name = alloc_whname(dentry->d_name.name, dentry->d_name.len);
+	if (IS_ERR(name)) {
+		err = PTR_ERR(name);
+		goto out;
+	}
+
+	whiteout_dentry =
+	    LOOKUP_ONE_LEN(name, hidden_dentry->d_parent,
+			   dentry->d_name.len + WHLEN);
+	if (IS_ERR(whiteout_dentry)) {
+		err = PTR_ERR(whiteout_dentry);
+		whiteout_dentry = NULL;
+		goto out;
+	}
+
+	if (whiteout_dentry->d_inode) {
+		/* .wh.foo has been found. */
+		/* First truncate it and then rename it to foo (hence having
+		 * the same overall effect as a normal create.
+		 *
+		 * XXX: This is not strictly correct.  If we have unlinked the
+		 * file and it still has a reference count, then we should
+		 * actually unlink the whiteout so that user's data isn't
+		 * hosed over.
+		 */
+		struct dentry *hidden_dir_dentry;
+		struct iattr newattrs;
+
+		mutex_lock(&whiteout_dentry->d_inode->i_mutex);
+		newattrs.ia_valid = ATTR_CTIME | ATTR_MODE | ATTR_ATIME
+		    | ATTR_MTIME | ATTR_UID | ATTR_GID | ATTR_FORCE
+		    | ATTR_KILL_SUID | ATTR_KILL_SGID;
+
+		newattrs.ia_mode = mode & ~current->fs->umask;
+		newattrs.ia_uid = current->fsuid;
+		newattrs.ia_gid = current->fsgid;
+
+		if (whiteout_dentry->d_inode->i_size != 0) {
+			newattrs.ia_valid |= ATTR_SIZE;
+			newattrs.ia_size = 0;
+		}
+
+		err = notify_change(whiteout_dentry, &newattrs);
+
+		mutex_unlock(&whiteout_dentry->d_inode->i_mutex);
+
+		if (err)
+			printk(KERN_WARNING
+			       "unionfs: %s:%d: notify_change failed: %d, ignoring..\n",
+			       __FILE__, __LINE__, err);
+
+		new_hidden_dentry = dtohd(dentry);
+		DGET(new_hidden_dentry);
+
+		hidden_dir_dentry = GET_PARENT(whiteout_dentry);
+		lock_rename(hidden_dir_dentry, hidden_dir_dentry);
+
+		if (!(err = is_robranch_super(dentry->d_sb, bstart))) {
+			err =
+			    vfs_rename(hidden_dir_dentry->d_inode,
+				       whiteout_dentry,
+				       hidden_dir_dentry->d_inode,
+				       new_hidden_dentry);
+		}
+		if (!err) {
+			fist_copy_attr_timesizes(parent,
+						 new_hidden_dentry->d_parent->
+						 d_inode);
+			parent->i_nlink = get_nlinks(parent);
+		}
+
+		unlock_rename(hidden_dir_dentry, hidden_dir_dentry);
+		DPUT(hidden_dir_dentry);
+
+		DPUT(new_hidden_dentry);
+
+		if (err) {
+			/* exit if the error returned was NOT -EROFS */
+			if (!IS_COPYUP_ERR(err))
+				goto out;
+			/* We were not able to create the file in this branch,
+			 * so, we try to create it in one branch to left
+			 */
+			bstart--;
+		} else {
+			/* reset the unionfs dentry to point to the .wh.foo entry. */
+
+			/* Discard any old reference. */
+			DPUT(dtohd(dentry));
+
+			/* Trade one reference to another. */
+			set_dtohd_index(dentry, bstart, whiteout_dentry);
+			whiteout_dentry = NULL;
+
+			err = unionfs_interpose(dentry, parent->i_sb, 0);
+			goto out;
+		}
+	}
+
+	for (bindex = bstart; bindex >= 0; bindex--) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		if (!hidden_dentry) {
+			/* if hidden_dentry is NULL, create the entire
+			 * dentry directory structure in branch 'bindex'.
+			 * hidden_dentry will NOT be null when bindex == bstart
+			 * because lookup passed as a negative unionfs dentry
+			 * pointing to a lone negative underlying dentry */
+			hidden_dentry = create_parents(parent, dentry, bindex);
+			if (!hidden_dentry || IS_ERR(hidden_dentry)) {
+				if (IS_ERR(hidden_dentry))
+					err = PTR_ERR(hidden_dentry);
+				continue;
+			}
+		}
+
+		checkinode(parent, "unionfs_create");
+
+		hidden_parent_dentry = lock_parent(hidden_dentry);
+		if (IS_ERR(hidden_parent_dentry)) {
+			err = PTR_ERR(hidden_parent_dentry);
+			goto out;
+		}
+		/* We shouldn't create things in a read-only branch. */
+		if (!(err = is_robranch_super(dentry->d_sb, bindex))) {
+			//DQ: vfs_create has a different prototype in 2.6
+			err = vfs_create(hidden_parent_dentry->d_inode,
+					 hidden_dentry, mode, nd);
+		}
+		if (err || !hidden_dentry->d_inode) {
+			unlock_dir(hidden_parent_dentry);
+
+			/* break out of for loop if the error wasn't  -EROFS */
+			if (!IS_COPYUP_ERR(err))
+				break;
+		} else {
+			err = unionfs_interpose(dentry, parent->i_sb, 0);
+			if (!err) {
+				fist_copy_attr_timesizes(parent,
+							 hidden_parent_dentry->
+							 d_inode);
+				/* update number of links on parent directory */
+				parent->i_nlink = get_nlinks(parent);
+			}
+			unlock_dir(hidden_parent_dentry);
+			break;
+		}
+	}
+
+      out:
+	DPUT(whiteout_dentry);
+	KFREE(name);
+
+	print_dentry("OUT unionfs_create :", dentry);
+	unlock_dentry(dentry);
+	print_exit_status(err);
+	return err;
+}
+
+struct dentry *unionfs_lookup(struct inode *parent, struct dentry *dentry,
+			      struct nameidata *nd)
+{
+	/* The locking is done by unionfs_lookup_backend. */
+	return unionfs_lookup_backend(dentry, INTERPOSE_LOOKUP);
+}
+
+static int unionfs_link(struct dentry *old_dentry, struct inode *dir,
+			struct dentry *new_dentry)
+{
+	int err = 0;
+	struct dentry *hidden_old_dentry = NULL;
+	struct dentry *hidden_new_dentry = NULL;
+	struct dentry *hidden_dir_dentry = NULL;
+	struct dentry *whiteout_dentry;
+	char *name = NULL;
+
+	print_entry_location();
+	double_lock_dentry(new_dentry, old_dentry);
+
+	hidden_new_dentry = dtohd(new_dentry);
+
+	/* check if whiteout exists in the branch of new dentry, i.e. lookup
+	 * .wh.foo first. If present, delete it */
+	name = alloc_whname(new_dentry->d_name.name, new_dentry->d_name.len);
+	if (IS_ERR(name)) {
+		err = PTR_ERR(name);
+		goto out;
+	}
+
+	whiteout_dentry =
+	    LOOKUP_ONE_LEN(name, hidden_new_dentry->d_parent,
+			   new_dentry->d_name.len + WHLEN);
+	if (IS_ERR(whiteout_dentry)) {
+		err = PTR_ERR(whiteout_dentry);
+		goto out;
+	}
+
+	if (!whiteout_dentry->d_inode) {
+		DPUT(whiteout_dentry);
+		whiteout_dentry = NULL;
+	} else {
+		/* found a .wh.foo entry, unlink it and then call vfs_link() */
+		hidden_dir_dentry = lock_parent(whiteout_dentry);
+		if (!
+		    (err =
+		     is_robranch_super(new_dentry->d_sb,
+				       dbstart(new_dentry)))) {
+			err =
+			    vfs_unlink(hidden_dir_dentry->d_inode,
+				       whiteout_dentry);
+		}
+		fist_copy_attr_times(dir, hidden_dir_dentry->d_inode);
+		dir->i_nlink = get_nlinks(dir);
+		unlock_dir(hidden_dir_dentry);
+		hidden_dir_dentry = NULL;
+		DPUT(whiteout_dentry);
+		if (err)
+			goto out;
+	}
+
+	if (dbstart(old_dentry) != dbstart(new_dentry)) {
+		hidden_new_dentry =
+		    create_parents(dir, new_dentry, dbstart(old_dentry));
+		err = PTR_ERR(hidden_new_dentry);
+		if (IS_COPYUP_ERR(err))
+			goto docopyup;
+		if (!hidden_new_dentry || IS_ERR(hidden_new_dentry))
+			goto out;
+	}
+	hidden_new_dentry = dtohd(new_dentry);
+	hidden_old_dentry = dtohd(old_dentry);
+
+	BUG_ON(dbstart(old_dentry) != dbstart(new_dentry));
+	hidden_dir_dentry = lock_parent(hidden_new_dentry);
+	if (!(err = is_robranch(old_dentry)))
+		err =
+		    vfs_link(hidden_old_dentry, hidden_dir_dentry->d_inode,
+			     hidden_new_dentry);
+	unlock_dir(hidden_dir_dentry);
+
+      docopyup:
+	if (IS_COPYUP_ERR(err)) {
+		int old_bstart = dbstart(old_dentry);
+		int bindex;
+
+		for (bindex = old_bstart - 1; bindex >= 0; bindex--) {
+			err =
+			    copyup_dentry(old_dentry->d_parent->
+					  d_inode, old_dentry,
+					  old_bstart, bindex, NULL,
+					  old_dentry->d_inode->i_size);
+			if (!err) {
+				hidden_new_dentry =
+				    create_parents(dir, new_dentry, bindex);
+				hidden_old_dentry = dtohd(old_dentry);
+				hidden_dir_dentry =
+				    lock_parent(hidden_new_dentry);
+				/* do vfs_link */
+				err =
+				    vfs_link(hidden_old_dentry,
+					     hidden_dir_dentry->d_inode,
+					     hidden_new_dentry);
+				unlock_dir(hidden_dir_dentry);
+				goto check_link;
+			}
+		}
+		goto out;
+	}
+      check_link:
+	if (err || !hidden_new_dentry->d_inode)
+		goto out;
+
+	/* Its a hard link, so use the same inode */
+	new_dentry->d_inode = IGRAB(old_dentry->d_inode);
+	d_instantiate(new_dentry, new_dentry->d_inode);
+	fist_copy_attr_all(dir, hidden_new_dentry->d_parent->d_inode);
+	/* propagate number of hard-links */
+	old_dentry->d_inode->i_nlink = get_nlinks(old_dentry->d_inode);
+
+      out:
+	if (!new_dentry->d_inode)
+		d_drop(new_dentry);
+
+	KFREE(name);
+
+	unlock_dentry(new_dentry);
+	unlock_dentry(old_dentry);
+
+	print_exit_status(err);
+	return err;
+}
+
+static int unionfs_symlink(struct inode *dir, struct dentry *dentry,
+			   const char *symname)
+{
+	int err = 0;
+	struct dentry *hidden_dentry = NULL;
+	struct dentry *whiteout_dentry = NULL;
+	struct dentry *hidden_dir_dentry = NULL;
+	umode_t mode;
+	int bindex = 0, bstart;
+	char *name = NULL;
+
+	print_entry_location();
+	lock_dentry(dentry);
+	print_dentry("IN unionfs_symlink", dentry);
+
+	/* We start out in the leftmost branch. */
+	bstart = dbstart(dentry);
+
+	hidden_dentry = dtohd(dentry);
+
+	/* check if whiteout exists in this branch, i.e. lookup .wh.foo first. If present, delete it */
+	name = alloc_whname(dentry->d_name.name, dentry->d_name.len);
+	if (IS_ERR(name)) {
+		err = PTR_ERR(name);
+		goto out;
+	}
+
+	whiteout_dentry =
+	    LOOKUP_ONE_LEN(name, hidden_dentry->d_parent,
+			   dentry->d_name.len + WHLEN);
+	if (IS_ERR(whiteout_dentry)) {
+		err = PTR_ERR(whiteout_dentry);
+		goto out;
+	}
+
+	if (!whiteout_dentry->d_inode) {
+		DPUT(whiteout_dentry);
+		whiteout_dentry = NULL;
+	} else {
+		/* found a .wh.foo entry, unlink it and then call vfs_symlink() */
+		hidden_dir_dentry = lock_parent(whiteout_dentry);
+
+		print_dentry("HDD", hidden_dir_dentry);
+		print_dentry("WD", whiteout_dentry);
+
+		if (!(err = is_robranch_super(dentry->d_sb, bstart))) {
+			err =
+			    vfs_unlink(hidden_dir_dentry->d_inode,
+				       whiteout_dentry);
+		}
+		DPUT(whiteout_dentry);
+
+		fist_copy_attr_times(dir, hidden_dir_dentry->d_inode);
+		/* propagate number of hard-links */
+		dir->i_nlink = get_nlinks(dir);
+
+		unlock_dir(hidden_dir_dentry);
+
+		if (err) {
+			/* exit if the error returned was NOT -EROFS */
+			if (!IS_COPYUP_ERR(err))
+				goto out;
+			/* should now try to create symlink in the another branch */
+			bstart--;
+		}
+	}
+
+	/* deleted whiteout if it was present, now do a normal vfs_symlink() with
+	   possible recursive directory creation */
+	for (bindex = bstart; bindex >= 0; bindex--) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		if (!hidden_dentry) {
+			/* if hidden_dentry is NULL, create the entire
+			 * dentry directory structure in branch 'bindex'. hidden_dentry will NOT be null when
+			 * bindex == bstart because lookup passed as a negative unionfs dentry pointing to a
+			 * lone negative underlying dentry */
+			hidden_dentry = create_parents(dir, dentry, bindex);
+			if (!hidden_dentry || IS_ERR(hidden_dentry)) {
+				if (IS_ERR(hidden_dentry)) {
+					err = PTR_ERR(hidden_dentry);
+				}
+				dprint(PRINT_DEBUG,
+					    "hidden dentry NULL (or error) for bindex = %d\n",
+					    bindex);
+				continue;
+			}
+		}
+
+		hidden_dir_dentry = lock_parent(hidden_dentry);
+
+		if (!(err = is_robranch_super(dentry->d_sb, bindex))) {
+			mode = S_IALLUGO;
+			err =
+			    vfs_symlink(hidden_dir_dentry->d_inode,
+					hidden_dentry, symname, mode);
+		}
+		unlock_dir(hidden_dir_dentry);
+
+		if (err || !hidden_dentry->d_inode) {
+			/* break out of for loop if error returned was NOT -EROFS */
+			if (!IS_COPYUP_ERR(err))
+				break;
+		} else {
+			err = unionfs_interpose(dentry, dir->i_sb, 0);
+			if (!err) {
+				fist_copy_attr_timesizes(dir,
+							 hidden_dir_dentry->
+							 d_inode);
+				/* update number of links on parent directory */
+				dir->i_nlink = get_nlinks(dir);
+			}
+			break;
+		}
+	}
+
+      out:
+	if (!dentry->d_inode)
+		d_drop(dentry);
+
+	KFREE(name);
+	print_dentry("OUT unionfs_symlink :", dentry);
+	unlock_dentry(dentry);
+	print_exit_status(err);
+	return err;
+}
+
+static int unionfs_mkdir(struct inode *parent, struct dentry *dentry, int mode)
+{
+	int err = 0;
+	struct dentry *hidden_dentry = NULL, *whiteout_dentry = NULL;
+	struct dentry *hidden_parent_dentry = NULL;
+	int bindex = 0, bstart;
+	char *name = NULL;
+	int whiteout_unlinked = 0;
+	uid_t saved_uid = current->fsuid;
+	gid_t saved_gid = current->fsgid;
+
+	print_entry_location();
+	lock_dentry(dentry);
+	print_dentry("IN unionfs_mkdir", dentry);
+	bstart = dbstart(dentry);
+
+	hidden_dentry = dtohd(dentry);
+
+	// check if whiteout exists in this branch, i.e. lookup .wh.foo first
+	name = alloc_whname(dentry->d_name.name, dentry->d_name.len);
+	if (IS_ERR(name)) {
+		err = PTR_ERR(name);
+		goto out;
+	}
+
+	whiteout_dentry =
+	    LOOKUP_ONE_LEN(name, hidden_dentry->d_parent,
+			   dentry->d_name.len + WHLEN);
+	if (IS_ERR(whiteout_dentry)) {
+		err = PTR_ERR(whiteout_dentry);
+		goto out;
+	}
+
+	if (!whiteout_dentry->d_inode) {
+		DPUT(whiteout_dentry);
+		whiteout_dentry = NULL;
+	} else {
+		hidden_parent_dentry = lock_parent(whiteout_dentry);
+
+		/* Set the uid and gid to trick the fs into allowing us to create
+		 * the file */
+		current->fsuid = hidden_parent_dentry->d_inode->i_uid;
+		current->fsgid = hidden_parent_dentry->d_inode->i_gid;
+		//found a.wh.foo entry, remove it then do vfs_mkdir
+		if (!(err = is_robranch_super(dentry->d_sb, bstart))) {
+			err =
+			    vfs_unlink(hidden_parent_dentry->d_inode,
+				       whiteout_dentry);
+		}
+		DPUT(whiteout_dentry);
+
+		current->fsuid = saved_uid;
+		current->fsgid = saved_gid;
+
+		unlock_dir(hidden_parent_dentry);
+
+		if (err) {
+			/* exit if the error returned was NOT -EROFS */
+			if (!IS_COPYUP_ERR(err))
+				goto out;
+			bstart--;
+		} else {
+			whiteout_unlinked = 1;
+		}
+	}
+
+	for (bindex = bstart; bindex >= 0; bindex--) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		if (!hidden_dentry) {
+			hidden_dentry = create_parents(parent, dentry, bindex);
+			if (!hidden_dentry || IS_ERR(hidden_dentry)) {
+				dprint(PRINT_DEBUG,
+					    "hidden dentry NULL for bindex = %d\n",
+					    bindex);
+				continue;
+			}
+		}
+
+		hidden_parent_dentry = lock_parent(hidden_dentry);
+		if (IS_ERR(hidden_parent_dentry)) {
+			err = PTR_ERR(hidden_parent_dentry);
+			goto out;
+		}
+		if (!(err = is_robranch_super(dentry->d_sb, bindex))) {
+			err =
+			    vfs_mkdir(hidden_parent_dentry->d_inode,
+				      hidden_dentry, mode);
+		}
+		unlock_dir(hidden_parent_dentry);
+
+		/* XXX this could potentially return a negative hidden_dentry! */
+		if (err || !hidden_dentry->d_inode) {
+			/* break out of for loop if error returned was NOT -EROFS */
+			if (!IS_COPYUP_ERR(err))
+				break;
+		} else {
+			int i;
+			int bend = dbend(dentry);
+
+			for (i = bindex + 1; i < bend; i++) {
+				if (dtohd_index(dentry, i)) {
+					DPUT(dtohd_index(dentry, i));
+					set_dtohd_index(dentry, i, NULL);
+				}
+			}
+			bend = bindex;
+			set_dbend(dentry, bend);
+
+			err = unionfs_interpose(dentry, parent->i_sb, 0);
+			if (!err) {
+				fist_copy_attr_timesizes(parent,
+							 hidden_parent_dentry->
+							 d_inode);
+				/* update number of links on parent directory */
+				parent->i_nlink = get_nlinks(parent);
+			}
+
+			err = make_dir_opaque(dentry, dbstart(dentry));
+			if (err) {
+				dprint(PRINT_DEBUG,
+					    "mkdir: error creating directory override entry: %d\n",
+					    err);
+				goto out;
+			}
+			break;
+		}
+	}
+
+      out:
+	if (!dentry->d_inode)
+		d_drop(dentry);
+
+	KFREE(name);
+
+	print_dentry("OUT unionfs_mkdir :", dentry);
+	unlock_dentry(dentry);
+	print_exit_status(err);
+	return err;
+}
+
+static int unionfs_mknod(struct inode *dir, struct dentry *dentry, int mode,
+			 dev_t dev)
+{
+	int err = 0;
+	struct dentry *hidden_dentry = NULL, *whiteout_dentry = NULL;
+	struct dentry *hidden_parent_dentry = NULL;
+	int bindex = 0, bstart;
+	char *name = NULL;
+	int whiteout_unlinked = 0;
+
+	print_entry_location();
+	lock_dentry(dentry);
+	print_dentry("IN unionfs_mknod", dentry);
+	bstart = dbstart(dentry);
+
+	hidden_dentry = dtohd(dentry);
+
+	// check if whiteout exists in this branch, i.e. lookup .wh.foo first
+	name = alloc_whname(dentry->d_name.name, dentry->d_name.len);
+	if (IS_ERR(name)) {
+		err = PTR_ERR(name);
+		goto out;
+	}
+
+	whiteout_dentry =
+	    LOOKUP_ONE_LEN(name, hidden_dentry->d_parent,
+			   dentry->d_name.len + WHLEN);
+	if (IS_ERR(whiteout_dentry)) {
+		err = PTR_ERR(whiteout_dentry);
+		goto out;
+	}
+
+	if (!whiteout_dentry->d_inode) {
+		DPUT(whiteout_dentry);
+		whiteout_dentry = NULL;
+	} else {
+		/* found .wh.foo, unlink it */
+		hidden_parent_dentry = lock_parent(whiteout_dentry);
+
+		//found a.wh.foo entry, remove it then do vfs_mkdir
+		if (!(err = is_robranch_super(dentry->d_sb, bstart)))
+			err = vfs_unlink(hidden_parent_dentry->d_inode,
+					 whiteout_dentry);
+		DPUT(whiteout_dentry);
+
+		unlock_dir(hidden_parent_dentry);
+
+		if (err) {
+			if (!IS_COPYUP_ERR(err))
+				goto out;
+
+			bstart--;
+		} else {
+			whiteout_unlinked = 1;
+		}
+	}
+
+	for (bindex = bstart; bindex >= 0; bindex--) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		if (!hidden_dentry) {
+			hidden_dentry = create_parents(dir, dentry, bindex);
+			if (!hidden_dentry || IS_ERR(hidden_dentry)) {
+				dprint(PRINT_DEBUG,
+					    "hidden dentry NULL for bindex = %d\n",
+					    bindex);
+				continue;
+			}
+		}
+
+		hidden_parent_dentry = lock_parent(hidden_dentry);
+		if (IS_ERR(hidden_parent_dentry)) {
+			err = PTR_ERR(hidden_parent_dentry);
+			goto out;
+		}
+		if (!(err = is_robranch_super(dentry->d_sb, bindex))) {
+			err = vfs_mknod(hidden_parent_dentry->d_inode,
+					hidden_dentry, mode, dev);
+		}
+		/* XXX this could potentially return a negative hidden_dentry! */
+		if (err || !hidden_dentry->d_inode) {
+			unlock_dir(hidden_parent_dentry);
+			/* break out of for, if error was NOT -EROFS */
+			if (!IS_COPYUP_ERR(err))
+				break;
+		} else {
+			err = unionfs_interpose(dentry, dir->i_sb, 0);
+			if (!err) {
+				fist_copy_attr_timesizes(dir,
+							 hidden_parent_dentry->
+							 d_inode);
+				/* update number of links on parent directory */
+				dir->i_nlink = get_nlinks(dir);
+			}
+			unlock_dir(hidden_parent_dentry);
+
+			break;
+		}
+	}
+
+      out:
+	if (!dentry->d_inode)
+		d_drop(dentry);
+
+	if (name) {
+		KFREE(name);
+	}
+
+	print_dentry("OUT unionfs_mknod :", dentry);
+	unlock_dentry(dentry);
+	print_exit_status(err);
+	return err;
+}
+
+int unionfs_readlink(struct dentry *dentry, char __user * buf, int bufsiz)
+{
+	int err;
+	struct dentry *hidden_dentry;
+
+	print_entry_location();
+	lock_dentry(dentry);
+	hidden_dentry = dtohd(dentry);
+	print_dentry("unionfs_readlink IN", dentry);
+
+	if (!hidden_dentry->d_inode->i_op ||
+	    !hidden_dentry->d_inode->i_op->readlink) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	err = hidden_dentry->d_inode->i_op->readlink(hidden_dentry,
+						     buf, bufsiz);
+	if (err > 0)
+		fist_copy_attr_atime(dentry->d_inode, hidden_dentry->d_inode);
+
+      out:
+	unlock_dentry(dentry);
+	print_exit_status(err);
+	return err;
+}
+
+/* We don't lock the dentry here, because readlink does the heavy lifting. */
+static void *unionfs_follow_link(struct dentry *dentry, struct nameidata *nd)
+{
+	char *buf;
+	int len = PAGE_SIZE, err;
+	mm_segment_t old_fs;
+
+	print_entry_location();
+
+	/* This is freed by the put_link method assuming a successful call. */
+	buf = (char *)KMALLOC(len, GFP_KERNEL);
+	if (!buf) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	/* read the symlink, and then we will follow it */
+	old_fs = get_fs();
+	set_fs(KERNEL_DS);
+	err = dentry->d_inode->i_op->readlink(dentry, (char __user *)buf, len);
+	set_fs(old_fs);
+	if (err < 0) {
+		KFREE(buf);
+		buf = NULL;
+		goto out;
+	}
+	buf[err] = 0;
+	nd_set_link(nd, buf);
+	err = 0;
+
+      out:
+	print_exit_status(err);
+	return ERR_PTR(err);
+}
+
+void unionfs_put_link(struct dentry *dentry, struct nameidata *nd, void *cookie)
+{
+	char *link;
+	print_entry_location();
+	link = nd_get_link(nd);
+	KFREE(link);
+	print_exit_location();
+}
+
+/* Basically copied from the kernel vfs permission(), but we've changed
+ * the following: (1) the IS_RDONLY check is skipped, and (2) if you set
+ * the mount option `nfsperms=insceure', we assume that -EACCES means that
+ * the export is read-only and we should check standard Unix permissions.
+ * This means that NFS ACL checks (or other advanced permission features)
+ * are bypassed.
+ */
+static int inode_permission(struct inode *inode, int mask, struct nameidata *nd,
+			    int bindex)
+{
+	int retval, submask;
+
+	if (mask & MAY_WRITE) {
+		/* The first branch is allowed to be really readonly. */
+		if (bindex == 0) {
+			umode_t mode = inode->i_mode;
+			if (IS_RDONLY(inode) && (S_ISREG(mode) || S_ISDIR(mode)
+						 || S_ISLNK(mode)))
+				return -EROFS;
+		}
+		/*
+		 * Nobody gets write access to an immutable file.
+		 */
+		if (IS_IMMUTABLE(inode))
+			return -EACCES;
+	}
+
+	/* Ordinary permission routines do not understand MAY_APPEND. */
+	submask = mask & ~MAY_APPEND;
+	if (inode->i_op && inode->i_op->permission) {
+		retval = inode->i_op->permission(inode, submask, nd);
+		if ((retval == -EACCES) && (submask & MAY_WRITE) &&
+		    (!strcmp("nfs", (inode)->i_sb->s_type->name)) &&
+		    (nd) && (nd->mnt) && (nd->mnt->mnt_sb) &&
+		    (branchperms(nd->mnt->mnt_sb, bindex) & MAY_NFSRO)) {
+			retval = generic_permission(inode, submask, NULL);
+		}
+	} else {
+		retval = generic_permission(inode, submask, NULL);
+	}
+
+	if (retval && retval != -EROFS) /* ignore EROFS */
+		return retval;
+
+	retval = security_inode_permission(inode, mask, nd);
+	return ((retval == -EROFS) ? 0 : retval); /* ignore EROFS */
+}
+
+static int unionfs_permission(struct inode *inode, int mask,
+			      struct nameidata *nd)
+{
+	struct inode *hidden_inode = NULL;
+	int err = 0;
+	int bindex, bstart, bend;
+	const int is_file = !S_ISDIR(inode->i_mode);
+	const int write_mask = (mask & MAY_WRITE) && !(mask & MAY_READ);
+
+	print_entry_location();
+
+	bstart = ibstart(inode);
+	bend = ibend(inode);
+
+	print_inode("IN unionfs_permission", inode);
+
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		hidden_inode = itohi_index(inode, bindex);
+		if (!hidden_inode)
+			continue;
+
+		/* check the condition for D-F-D underlying files/directories,
+		 * we dont have to check for files, if we are checking for
+		 * directories.
+		 */
+		if (!is_file && !S_ISDIR(hidden_inode->i_mode))
+			continue;
+		/* We use our own special version of permission, such that
+		 * only the first branch returns -EROFS. */
+		err = inode_permission(hidden_inode, mask, nd, bindex);
+		/* The permissions are an intersection of the overall directory
+		 * permissions, so we fail if one fails. */
+		if (err)
+			goto out;
+		/* only the leftmost file matters. */
+		if (is_file || write_mask) {
+			if (is_file && write_mask) {
+				err = get_write_access(hidden_inode);
+				if (!err)
+					put_write_access(hidden_inode);
+			}
+			break;
+		}
+	}
+
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+static int unionfs_setattr(struct dentry *dentry, struct iattr *ia)
+{
+	int err = 0;
+	struct dentry *hidden_dentry;
+	struct inode *inode = NULL;
+	struct inode *hidden_inode = NULL;
+	int bstart, bend, bindex;
+	int i;
+	int copyup = 0;
+
+	print_entry_location();
+	lock_dentry(dentry);
+	bstart = dbstart(dentry);
+	bend = dbend(dentry);
+	inode = dentry->d_inode;
+
+	for (bindex = bstart; (bindex <= bend) || (bindex == bstart); bindex++) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		if (!hidden_dentry)
+			continue;
+		BUG_ON(hidden_dentry->d_inode == NULL);
+
+		/* If the file is on a read only branch */
+		if (is_robranch_super(dentry->d_sb, bindex)
+		    || IS_RDONLY(hidden_dentry->d_inode)) {
+			if (copyup || (bindex != bstart))
+				continue;
+			/* Only if its the leftmost file, copyup the file */
+			for (i = bstart - 1; i >= 0; i--) {
+				size_t size = dentry->d_inode->i_size;
+				if (ia->ia_valid & ATTR_SIZE)
+					size = ia->ia_size;
+				err = copyup_dentry(dentry->d_parent->d_inode,
+						    dentry, bstart, i, NULL,
+						    size);
+
+				if (!err) {
+					copyup = 1;
+					hidden_dentry = dtohd(dentry);
+					break;
+				}
+				/* if error is in the leftmost f/s, pass it up */
+				if (i == 0)
+					goto out;
+			}
+
+		}
+		err = notify_change(hidden_dentry, ia);
+		if (err)
+			goto out;
+		break;
+	}
+#ifdef UNIONFS_MMAP
+	/*
+	 * SP: notify_change will change the lower file's size,
+	 * but we need to truncate the page tables, so need to call
+	 * vmtruncate()
+	 */
+
+	if (ia->ia_valid & ATTR_SIZE) {
+		if (ia->ia_size != i_size_read(inode)) {
+			err = vmtruncate(inode, ia->ia_size);
+			if (err) {
+				printk("unionfs_setattr: vmtruncate failed\n");
+			}
+		}
+	}
+#endif
+	/* get the size from the first hidden inode */
+	hidden_inode = itohi(dentry->d_inode);
+	checkinode(inode, "unionfs_setattr");
+	fist_copy_attr_all(inode, hidden_inode);
+
+      out:
+	unlock_dentry(dentry);
+	checkinode(inode, "post unionfs_setattr");
+	print_exit_status(err);
+	return err;
+}
+
+struct inode_operations unionfs_symlink_iops = {
+	.readlink = unionfs_readlink,
+	.permission = unionfs_permission,
+	.follow_link = unionfs_follow_link,
+	.setattr = unionfs_setattr,
+	.put_link = unionfs_put_link,
+};
+
+struct inode_operations unionfs_dir_iops = {
+	.create = unionfs_create,
+	.lookup = unionfs_lookup,
+	.link = unionfs_link,
+	.unlink = unionfs_unlink,
+	.symlink = unionfs_symlink,
+	.mkdir = unionfs_mkdir,
+	.rmdir = unionfs_rmdir,
+	.mknod = unionfs_mknod,
+	.rename = unionfs_rename,
+	.permission = unionfs_permission,
+	.setattr = unionfs_setattr,
+	.setxattr = unionfs_setxattr,
+	.getxattr = unionfs_getxattr,
+	.removexattr = unionfs_removexattr,
+	.listxattr = unionfs_listxattr,
+};
+
+struct inode_operations unionfs_main_iops = {
+	.permission = unionfs_permission,
+	.setattr = unionfs_setattr,
+	.setxattr = unionfs_setxattr,
+	.getxattr = unionfs_getxattr,
+	.removexattr = unionfs_removexattr,
+	.listxattr = unionfs_listxattr,
+};
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/lookup.c newtree/fs/unionfs/lookup.c
--- oldtree/fs/unionfs/lookup.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/lookup.c	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,508 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: lookup.c,v 1.46 2006/06/01 03:11:03 jsipek Exp $
+ */
+
+#include "unionfs.h"
+
+static int is_opaque_dir(struct dentry *dentry, int bindex);
+static int is_validname(const char *name);
+
+struct dentry *unionfs_lookup_backend(struct dentry *dentry, int lookupmode)
+{
+	int err = 0;
+	struct dentry *hidden_dentry = NULL;
+	struct dentry *wh_hidden_dentry = NULL;
+	struct dentry *hidden_dir_dentry = NULL;
+	struct dentry *parent_dentry = NULL;
+	int bindex, bstart, bend, bopaque;
+	int dentry_count = 0;	/* Number of positive dentries. */
+	int first_dentry_offset = -1;
+	struct dentry *first_hidden_dentry = NULL;
+	int locked_parent = 0;
+	int locked_child = 0;
+
+	int opaque;
+	char *whname = NULL;
+	const char *name;
+	int namelen;
+
+	print_entry("mode = %d", lookupmode);
+
+	/* We should already have a lock on this dentry in the case of a
+	 * partial lookup, or a revalidation. Otherwise it is returned from
+	 * new_dentry_private_data already locked.  */
+	if (lookupmode == INTERPOSE_PARTIAL || lookupmode == INTERPOSE_REVAL
+	    || lookupmode == INTERPOSE_REVAL_NEG) {
+		verify_locked(dentry);
+	} else {
+		BUG_ON(dtopd_nocheck(dentry) != NULL);
+		locked_child = 1;
+	}
+	if (lookupmode != INTERPOSE_PARTIAL)
+		if ((err = new_dentry_private_data(dentry)))
+			goto out;
+	/* must initialize dentry operations */
+	dentry->d_op = &unionfs_dops;
+
+	parent_dentry = GET_PARENT(dentry);
+	/* We never partial lookup the root directory. */
+	if (parent_dentry != dentry) {
+		lock_dentry(parent_dentry);
+		locked_parent = 1;
+	} else {
+		DPUT(parent_dentry);
+		parent_dentry = NULL;
+		goto out;
+	}
+
+	print_dentry("IN unionfs_lookup (parent)", parent_dentry);
+	print_dentry("IN unionfs_lookup (child)", dentry);
+
+	name = dentry->d_name.name;
+	namelen = dentry->d_name.len;
+
+	/* No dentries should get created for possible whiteout names. */
+	if (!is_validname(name)) {
+		err = -EPERM;
+		goto out_free;
+	}
+
+	/* Now start the actual lookup procedure. */
+	bstart = dbstart(parent_dentry);
+	bend = dbend(parent_dentry);
+	bopaque = dbopaque(parent_dentry);
+	BUG_ON(bstart < 0);
+
+	/* It would be ideal if we could convert partial lookups to only have
+	 * to do this work when they really need to.  It could probably improve
+	 * performance quite a bit, and maybe simplify the rest of the code. */
+	if (lookupmode == INTERPOSE_PARTIAL) {
+		bstart++;
+		if ((bopaque != -1) && (bopaque < bend))
+			bend = bopaque;
+	}
+
+	dprint(PRINT_DEBUG, "bstart = %d, bend = %d\n", bstart, bend);
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		if (lookupmode == INTERPOSE_PARTIAL && hidden_dentry)
+			continue;
+		BUG_ON(hidden_dentry != NULL);
+
+		hidden_dir_dentry = dtohd_index(parent_dentry, bindex);
+
+		/* if the parent hidden dentry does not exist skip this */
+		if (!(hidden_dir_dentry && hidden_dir_dentry->d_inode))
+			continue;
+
+		/* also skip it if the parent isn't a directory. */
+		if (!S_ISDIR(hidden_dir_dentry->d_inode->i_mode))
+			continue;
+
+		/* Reuse the whiteout name because its value doesn't change. */
+		if (!whname) {
+			whname = alloc_whname(name, namelen);
+			if (IS_ERR(whname)) {
+				err = PTR_ERR(whname);
+				goto out_free;
+			}
+		}
+
+		/* check if whiteout exists in this branch: lookup .wh.foo */
+		wh_hidden_dentry = LOOKUP_ONE_LEN(whname, hidden_dir_dentry,
+						  namelen + WHLEN);
+		if (IS_ERR(wh_hidden_dentry)) {
+			DPUT(first_hidden_dentry);
+			err = PTR_ERR(wh_hidden_dentry);
+			goto out_free;
+		}
+
+		if (wh_hidden_dentry->d_inode) {
+			/* We found a whiteout so lets give up. */
+			dprint(PRINT_DEBUG, "whiteout found in %d\n", bindex);
+			if (S_ISREG(wh_hidden_dentry->d_inode->i_mode)) {
+				set_dbend(dentry, bindex);
+				set_dbopaque(dentry, bindex);
+				DPUT(wh_hidden_dentry);
+				break;
+			}
+			err = -EIO;
+			printk(KERN_NOTICE "EIO: Invalid whiteout entry type"
+			       " %d.\n", wh_hidden_dentry->d_inode->i_mode);
+			DPUT(wh_hidden_dentry);
+			DPUT(first_hidden_dentry);
+			goto out_free;
+		}
+
+		DPUT(wh_hidden_dentry);
+		wh_hidden_dentry = NULL;
+
+		/* Now do regular lookup; lookup foo */
+		hidden_dentry = LOOKUP_ONE_LEN(name, hidden_dir_dentry,
+					       namelen);
+		print_dentry("hidden result", hidden_dentry);
+		if (IS_ERR(hidden_dentry)) {
+			DPUT(first_hidden_dentry);
+			err = PTR_ERR(hidden_dentry);
+			goto out_free;
+		}
+
+		/* Store the first negative dentry specially, because if they
+		 * are all negative we need this for future creates. */
+		if (!hidden_dentry->d_inode) {
+			if (!first_hidden_dentry && (dbstart(dentry) == -1)) {
+				first_hidden_dentry = hidden_dentry;
+				first_dentry_offset = bindex;
+			} else {
+				DPUT(hidden_dentry);
+			}
+			continue;
+		}
+
+		/* number of positive dentries */
+		dentry_count++;
+
+		/* store underlying dentry */
+		if (dbstart(dentry) == -1)
+			set_dbstart(dentry, bindex);
+		set_dtohd_index(dentry, bindex, hidden_dentry);
+		set_dbend(dentry, bindex);
+
+		/* update parent directory's atime with the bindex */
+		fist_copy_attr_atime(parent_dentry->d_inode,
+				     hidden_dir_dentry->d_inode);
+
+		/* We terminate file lookups here. */
+		if (!S_ISDIR(hidden_dentry->d_inode->i_mode)) {
+			if (lookupmode == INTERPOSE_PARTIAL)
+				continue;
+			if (dentry_count == 1)
+				goto out_positive;
+			/* This can only happen with mixed D-*-F-* */
+			BUG_ON(!S_ISDIR(dtohd(dentry)->d_inode->i_mode));
+			continue;
+		}
+
+		opaque = is_opaque_dir(dentry, bindex);
+		if (opaque < 0) {
+			DPUT(first_hidden_dentry);
+			err = opaque;
+			goto out_free;
+		}
+		if (opaque) {
+			set_dbend(dentry, bindex);
+			set_dbopaque(dentry, bindex);
+			break;
+		}
+	}
+
+	if (dentry_count)
+		goto out_positive;
+	else
+		goto out_negative;
+
+      out_negative:
+	if (lookupmode == INTERPOSE_PARTIAL)
+		goto out;
+
+	/* If we've only got negative dentries, then use the leftmost one. */
+	if (lookupmode == INTERPOSE_REVAL) {
+		if (dentry->d_inode) {
+			itopd(dentry->d_inode)->uii_stale = 1;
+		}
+		goto out;
+	}
+	/* This should only happen if we found a whiteout. */
+	if (first_dentry_offset == -1) {
+		first_hidden_dentry = LOOKUP_ONE_LEN(name, hidden_dir_dentry,
+						     namelen);
+		first_dentry_offset = bindex;
+		if (IS_ERR(first_hidden_dentry)) {
+			err = PTR_ERR(first_hidden_dentry);
+			goto out;
+		}
+	}
+	set_dtohd_index(dentry, first_dentry_offset, first_hidden_dentry);
+	set_dbstart(dentry, first_dentry_offset);
+	set_dbend(dentry, first_dentry_offset);
+
+	if (lookupmode == INTERPOSE_REVAL_NEG)
+		BUG_ON(dentry->d_inode != NULL);
+	else
+		d_add(dentry, NULL);
+	goto out;
+
+/* This part of the code is for positive dentries. */
+      out_positive:
+	BUG_ON(dentry_count <= 0);
+
+	/* If we're holding onto the first negative dentry throw it out. */
+	DPUT(first_hidden_dentry);
+
+	/* Partial lookups need to reinterpose, or throw away older negs. */
+	if (lookupmode == INTERPOSE_PARTIAL) {
+		if (dentry->d_inode) {
+			unionfs_reinterpose(dentry);
+			goto out;
+		}
+
+		/* This somehow turned positive, so it is as if we had a
+		 * negative revalidation.  */
+		lookupmode = INTERPOSE_REVAL_NEG;
+
+		update_bstart(dentry);
+		bstart = dbstart(dentry);
+		bend = dbend(dentry);
+	}
+
+	err = unionfs_interpose(dentry, dentry->d_sb, lookupmode);
+	if (err)
+		goto out_drop;
+
+	checkinode(dentry->d_inode, "unionfs_lookup OUT: child");
+	checkinode(parent_dentry->d_inode, "unionfs_lookup OUT: dir");
+	goto out;
+
+      out_drop:
+	d_drop(dentry);
+
+      out_free:
+	/* should dput all the underlying dentries on error condition */
+	bstart = dbstart(dentry);
+	if (bstart >= 0) {
+		bend = dbend(dentry);
+		for (bindex = bstart; bindex <= bend; bindex++)
+			DPUT(dtohd_index(dentry, bindex));
+	}
+	KFREE(dtohd_ptr(dentry));
+	dtohd_ptr(dentry) = NULL;
+	set_dbstart(dentry, -1);
+	set_dbend(dentry, -1);
+
+      out:
+	if (!err && dtopd(dentry)) {
+		BUG_ON(dbend(dentry) > dtopd(dentry)->udi_bcount);
+		BUG_ON(dbend(dentry) > sbmax(dentry->d_sb));
+		BUG_ON(dbstart(dentry) < 0);
+	}
+	KFREE(whname);
+	print_dentry("OUT unionfs_lookup (parent)", parent_dentry);
+	print_dentry("OUT unionfs_lookup (child)", dentry);
+	if (locked_parent)
+		unlock_dentry(parent_dentry);
+	DPUT(parent_dentry);
+	if (locked_child)
+		unlock_dentry(dentry);
+	print_exit_status(err);
+	return ERR_PTR(err);
+}
+
+/* This is a utility function that fills in a unionfs dentry.*/
+int unionfs_partial_lookup(struct dentry *dentry)
+{
+	struct dentry *tmp;
+
+	tmp = unionfs_lookup_backend(dentry, INTERPOSE_PARTIAL);
+	if (!tmp)
+		return 0;
+	if (IS_ERR(tmp))
+		return PTR_ERR(tmp);
+	/* need to change the interface */
+	BUG_ON(tmp != dentry);
+	return -ENOSYS;
+}
+
+/* The rest of these are utility functions for lookup. */
+static int is_opaque_dir(struct dentry *dentry, int bindex)
+{
+	int err = 0;
+	struct dentry *hidden_dentry;
+	struct dentry *wh_hidden_dentry;
+	struct inode *hidden_inode;
+	struct superio sio;
+	int do_superio;
+
+	print_entry_location();
+
+	hidden_dentry = dtohd_index(dentry, bindex);
+	hidden_inode = hidden_dentry->d_inode;
+
+	BUG_ON(!S_ISDIR(hidden_inode->i_mode));
+
+	mutex_lock(&hidden_inode->i_mutex);
+	do_superio = permission(hidden_inode, MAY_EXEC, NULL);
+	if (do_superio)
+		superio_store(&sio);
+	wh_hidden_dentry = LOOKUP_ONE_LEN(UNIONFS_DIR_OPAQUE, hidden_dentry,
+					  sizeof(UNIONFS_DIR_OPAQUE) - 1);
+	if (do_superio)
+		superio_revert(&sio);
+	mutex_unlock(&hidden_inode->i_mutex);
+	if (IS_ERR(wh_hidden_dentry)) {
+		err = PTR_ERR(wh_hidden_dentry);
+		dprint(PRINT_DEBUG, "LOOKUP_ONE_LEN returned: %d\n", err);
+		goto out;
+	}
+	if (wh_hidden_dentry->d_inode)
+		err = 1;
+	DPUT(wh_hidden_dentry);
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+static int is_validname(const char *name)
+{
+	if (!strncmp(name, WHPFX, WHLEN))
+		return 0;
+	if (!strncmp(name, UNIONFS_DIR_OPAQUE_NAME,
+		     sizeof(UNIONFS_DIR_OPAQUE_NAME) - 1))
+		return 0;
+	return 1;
+}
+
+/* The dentry cache is just so we have properly sized dentries. */
+static kmem_cache_t *unionfs_dentry_cachep;
+int init_dentry_cache(void)
+{
+	unionfs_dentry_cachep =
+	    kmem_cache_create("unionfs_dentry",
+			      sizeof(struct unionfs_dentry_info), 0,
+			      SLAB_RECLAIM_ACCOUNT, NULL, NULL);
+
+	if (!unionfs_dentry_cachep)
+		return -ENOMEM;
+	return 0;
+}
+
+void destroy_dentry_cache(void)
+{
+	if (!unionfs_dentry_cachep)
+		return;
+	if (kmem_cache_destroy(unionfs_dentry_cachep))
+		printk(KERN_ERR
+		       "unionfs_dentry_cache: not all structures were freed\n");
+	return;
+}
+
+void free_dentry_private_data(struct unionfs_dentry_info *udi)
+{
+	if (!udi)
+		return;
+	kmem_cache_free(unionfs_dentry_cachep, udi);
+}
+
+int new_dentry_private_data(struct dentry *dentry)
+{
+	int newsize;
+	int oldsize = 0;
+
+	spin_lock(&dentry->d_lock);
+	if (!dtopd_nocheck(dentry)) {
+		dtopd_lhs(dentry) = (struct unionfs_dentry_info *)
+		    kmem_cache_alloc(unionfs_dentry_cachep, SLAB_ATOMIC);
+		if (!dtopd_nocheck(dentry))
+			goto out;
+		init_MUTEX_LOCKED(&dtopd_nocheck(dentry)->udi_sem);
+#ifdef TRACKLOCK
+		printk("INITLOCK:%p\n", dentry);
+#endif
+		dtohd_ptr(dentry) = NULL;
+	} else {
+		oldsize = sizeof(struct dentry *) * dtopd(dentry)->udi_bcount;
+	}
+
+	dtopd_nocheck(dentry)->udi_bstart = -1;
+	dtopd_nocheck(dentry)->udi_bend = -1;
+	dtopd_nocheck(dentry)->udi_bopaque = -1;
+	dtopd_nocheck(dentry)->udi_bcount = sbmax(dentry->d_sb);
+	atomic_set(&dtopd_nocheck(dentry)->udi_generation,
+		   atomic_read(&stopd(dentry->d_sb)->usi_generation));
+	newsize = sizeof(struct dentry *) * sbmax(dentry->d_sb);
+
+	/* Don't reallocate when we already have enough space. */
+	/* It would be ideal if we could actually use the slab macros to
+	 * determine what our object sizes is, but those are not exported.
+	 */
+	if (oldsize) {
+		int minsize = malloc_sizes[0].cs_size;
+
+		if (!newsize || ((oldsize < newsize) && (newsize > minsize))) {
+			KFREE(dtohd_ptr(dentry));
+			dtohd_ptr(dentry) = NULL;
+		}
+	}
+
+	if (!dtohd_ptr(dentry) && newsize) {
+		dtohd_ptr(dentry) = KMALLOC(newsize, GFP_ATOMIC);
+		if (!dtohd_ptr(dentry))
+			goto out;
+	}
+
+	if (oldsize > newsize)
+		memset(dtohd_ptr(dentry), 0, oldsize);
+	else
+		memset(dtohd_ptr(dentry), 0, newsize);
+
+	spin_unlock(&dentry->d_lock);
+	return 0;
+
+      out:
+	free_dentry_private_data(dtopd_nocheck(dentry));
+	dtopd_lhs(dentry) = NULL;
+	spin_unlock(&dentry->d_lock);
+	return -ENOMEM;
+}
+
+void update_bstart(struct dentry *dentry)
+{
+	int bindex;
+	int bstart = dbstart(dentry);
+	int bend = dbend(dentry);
+	struct dentry *hidden_dentry;
+
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		if (!hidden_dentry)
+			continue;
+		if (hidden_dentry->d_inode) {
+			set_dbstart(dentry, bindex);
+			break;
+		}
+		DPUT(hidden_dentry);
+		set_dtohd_index(dentry, bindex, NULL);
+	}
+}
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/main.c newtree/fs/unionfs/main.c
--- oldtree/fs/unionfs/main.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/main.c	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,796 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: main.c,v 1.172 2006/06/30 00:45:13 jsipek Exp $
+ */
+
+#include "unionfs.h"
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+
+/* declarations added for "sparse" */
+extern void unionfs_kill_block_super(struct super_block *sb);
+
+/* declarations added for malloc_debugging */
+
+#ifdef FIST_MALLOC_DEBUG
+extern atomic_t unionfs_malloc_counter;
+extern atomic_t unionfs_mallocs_outstanding;
+#endif
+/* sb we pass is unionfs's super_block */
+int unionfs_interpose(struct dentry *dentry, struct super_block *sb, int flag)
+{
+	struct inode *hidden_inode;
+	struct dentry *hidden_dentry;
+	int err = 0;
+	struct inode *inode;
+	int is_negative_dentry = 1;
+	int bindex, bstart, bend;
+
+	print_entry("flag = %d", flag);
+
+	verify_locked(dentry);
+
+	print_dentry("In unionfs_interpose", dentry);
+
+	bstart = dbstart(dentry);
+	bend = dbend(dentry);
+
+	/* Make sure that we didn't get a negative dentry. */
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		if (dtohd_index(dentry, bindex) &&
+		    dtohd_index(dentry, bindex)->d_inode) {
+			is_negative_dentry = 0;
+			break;
+		}
+	}
+	BUG_ON(is_negative_dentry);
+
+	/* We allocate our new inode below, by calling iget.
+	 * iget will call our read_inode which will initialize some
+	 * of the new inode's fields
+	 */
+
+	/* On revalidate we've already got our own inode and just need
+	 * to fix it up. */
+	if (flag == INTERPOSE_REVAL) {
+		inode = dentry->d_inode;
+		itopd(inode)->b_start = -1;
+		itopd(inode)->b_end = -1;
+		atomic_set(&itopd(inode)->uii_generation,
+			   atomic_read(&stopd(sb)->usi_generation));
+
+		itohi_ptr(inode) =
+		    KZALLOC(sbmax(sb) * sizeof(struct inode *), GFP_KERNEL);
+		if (!itohi_ptr(inode)) {
+			err = -ENOMEM;
+			goto out;
+		}
+		mutex_lock(&inode->i_mutex);
+	} else {
+		ino_t ino;
+		/* get unique inode number for unionfs */
+#ifdef UNIONFS_IMAP
+		if (stopd(sb)->usi_persistent) {
+			err = read_uin(sb, bindex,
+				       dtohd_index(dentry,
+						   bindex)->d_inode->i_ino,
+				       O_CREAT, &ino);
+			if (err)
+				goto out;
+		} else
+#endif
+			ino = iunique(sb, UNIONFS_ROOT_INO);
+
+		inode = IGET(sb, ino);
+		if (!inode) {
+			err = -EACCES;	/* should be impossible??? */
+			goto out;
+		}
+
+		mutex_lock(&inode->i_mutex);
+		if (atomic_read(&inode->i_count) > 1)
+			goto skip;
+	}
+
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		if (!hidden_dentry) {
+			set_itohi_index(inode, bindex, NULL);
+			continue;
+		}
+		/* Initialize the hidden inode to the new hidden inode. */
+		if (!hidden_dentry->d_inode)
+			continue;
+		set_itohi_index(inode, bindex, IGRAB(hidden_dentry->d_inode));
+	}
+
+	ibstart(inode) = dbstart(dentry);
+	ibend(inode) = dbend(dentry);
+
+	/* Use attributes from the first branch. */
+	hidden_inode = itohi(inode);
+
+	/* Use different set of inode ops for symlinks & directories */
+	if (S_ISLNK(hidden_inode->i_mode))
+		inode->i_op = &unionfs_symlink_iops;
+	else if (S_ISDIR(hidden_inode->i_mode))
+		inode->i_op = &unionfs_dir_iops;
+
+	/* Use different set of file ops for directories */
+	if (S_ISDIR(hidden_inode->i_mode))
+		inode->i_fop = &unionfs_dir_fops;
+
+	/* properly initialize special inodes */
+	if (S_ISBLK(hidden_inode->i_mode) || S_ISCHR(hidden_inode->i_mode) ||
+	    S_ISFIFO(hidden_inode->i_mode) || S_ISSOCK(hidden_inode->i_mode))
+		init_special_inode(inode, hidden_inode->i_mode,
+				   hidden_inode->i_rdev);
+#ifndef UNIONFS_MMAP
+	/* Fix our inode's address operations to that of the lower inode (Unionfs is FiST-Lite) */
+	if (inode->i_mapping->a_ops != hidden_inode->i_mapping->a_ops) {
+		dprint(PRINT_DEBUG, "fixing inode 0x%p a_ops (0x%p -> 0x%p)\n",
+		       inode, inode->i_mapping->a_ops,
+		       hidden_inode->i_mapping->a_ops);
+		inode->i_mapping->a_ops = hidden_inode->i_mapping->a_ops;
+	}
+#endif
+	/* all well, copy inode attributes */
+	fist_copy_attr_all(inode, hidden_inode);
+
+      skip:
+	/* only (our) lookup wants to do a d_add */
+	switch (flag) {
+	case INTERPOSE_DEFAULT:
+	case INTERPOSE_REVAL_NEG:
+		d_instantiate(dentry, inode);
+		break;
+	case INTERPOSE_LOOKUP:
+		err = PTR_ERR(d_splice_alias(inode, dentry));
+		break;
+	case INTERPOSE_REVAL:
+		/* Do nothing. */
+		break;
+	default:
+		printk(KERN_ERR "Invalid interpose flag passed!");
+		BUG();
+	}
+
+	print_dentry("Leaving unionfs_interpose", dentry);
+	print_inode("Leaving unionfs_interpose", inode);
+	mutex_unlock(&inode->i_mutex);
+
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+void unionfs_reinterpose(struct dentry *dentry)
+{
+	struct dentry *hidden_dentry;
+	struct inode *inode;
+	int bindex, bstart, bend;
+
+	print_entry_location();
+	verify_locked(dentry);
+	print_dentry("IN: unionfs_reinterpose: ", dentry);
+
+	/* This is pre-allocated inode */
+	inode = dentry->d_inode;
+
+	bstart = dbstart(dentry);
+	bend = dbend(dentry);
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		if (!hidden_dentry)
+			continue;
+
+		if (!hidden_dentry->d_inode)
+			continue;
+		if (itohi_index(inode, bindex))
+			continue;
+		set_itohi_index(inode, bindex, IGRAB(hidden_dentry->d_inode));
+	}
+	ibstart(inode) = dbstart(dentry);
+	ibend(inode) = dbend(dentry);
+
+	print_dentry("OUT: unionfs_reinterpose: ", dentry);
+	print_inode("OUT: unionfs_reinterpose: ", inode);
+
+	print_exit_location();
+}
+
+int check_branch(struct nameidata *nd)
+{
+	if (!strcmp(nd->dentry->d_sb->s_type->name, "unionfs"))
+		return -EINVAL;
+	if (!nd->dentry->d_inode)
+		return -ENOENT;
+	if (!S_ISDIR(nd->dentry->d_inode->i_mode))
+		return -ENOTDIR;
+	return 0;
+}
+
+/* checks if two hidden_dentries have overlapping branches */
+int is_branch_overlap(struct dentry *dent1, struct dentry *dent2)
+{
+	struct dentry *dent = NULL;
+
+	dent = dent1;
+	while ((dent != dent2) && (dent->d_parent != dent)) {
+		dent = dent->d_parent;
+	}
+	if (dent == dent2) {
+		return 1;
+	}
+
+	dent = dent2;
+	while ((dent != dent1) && (dent->d_parent != dent)) {
+		dent = dent->d_parent;
+	}
+	if (dent == dent1) {
+		return 1;
+	}
+
+	return 0;
+}
+static int parse_branch_mode(char *name)
+{
+	int perms;
+	int l = strlen(name);
+	if (!strcmp(name + l - 3, "=ro")) {
+		perms = MAY_READ;
+		name[l - 3] = '\0';
+	} else if (!strcmp(name + l - 6, "=nfsro")) {
+		perms = MAY_READ | MAY_NFSRO;
+		name[l - 6] = '\0';
+	} else if (!strcmp(name + l - 3, "=rw")) {
+		perms = MAY_READ | MAY_WRITE;
+		name[l - 3] = '\0';
+	} else {
+		perms = MAY_READ | MAY_WRITE;
+	}
+
+	return perms;
+}
+
+static int parse_dirs_option(struct super_block *sb, struct unionfs_dentry_info
+			     *hidden_root_info, char *options)
+{
+	struct nameidata nd;
+	char *name;
+	int err = 0;
+	int branches = 1;
+	int bindex = 0;
+	int i = 0;
+	int j = 0;
+
+	struct dentry *dent1 = NULL;
+	struct dentry *dent2 = NULL;
+
+	if (options[0] == '\0') {
+		printk(KERN_WARNING "unionfs: no branches specified\n");
+		err = -EINVAL;
+		goto out;
+	}
+
+	/* Each colon means we have a separator, this is really just a rough
+	 * guess, since strsep will handle empty fields for us. */
+	for (i = 0; options[i]; i++) {
+		if (options[i] == ':')
+			branches++;
+	}
+
+	/* allocate space for underlying pointers to hidden dentry */
+	if (!(stopd(sb)->usi_data = alloc_new_data(branches))) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	if (!(hidden_root_info->udi_dentry = alloc_new_dentries(branches))) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	/* now parsing the string b1:b2=rw:b3=ro:b4 */
+	branches = 0;
+	while ((name = strsep(&options, ":")) != NULL) {
+		int perms;
+
+		if (!*name)
+			continue;
+
+		branches++;
+
+		/* strip off =rw or =ro if it is specified. */
+		perms = parse_branch_mode(name);
+		if (!bindex && !(perms & MAY_WRITE)) {
+			err = -EINVAL;
+			goto out;
+		}
+
+		dprint(PRINT_DEBUG, "using directory: %s (%c%c%c)\n",
+		       name, perms & MAY_READ ? 'r' : '-',
+		       perms & MAY_WRITE ? 'w' : '-',
+		       perms & MAY_NFSRO ? 'n' : '-');
+
+		err = path_lookup(name, LOOKUP_FOLLOW, &nd);
+		RECORD_PATH_LOOKUP(&nd);
+		if (err) {
+			printk(KERN_WARNING "unionfs: error accessing "
+			       "hidden directory '%s' (error %d)\n", name, err);
+			goto out;
+		}
+
+		if ((err = check_branch(&nd))) {
+			printk(KERN_WARNING "unionfs: hidden directory "
+			       "'%s' is not a valid branch\n", name);
+			path_release(&nd);
+			RECORD_PATH_RELEASE(&nd);
+			goto out;
+		}
+
+		hidden_root_info->udi_dentry[bindex] = nd.dentry;
+
+		set_stohiddenmnt_index(sb, bindex, nd.mnt);
+		set_branchperms(sb, bindex, perms);
+		set_branch_count(sb, bindex, 0);
+
+		if (hidden_root_info->udi_bstart < 0)
+			hidden_root_info->udi_bstart = bindex;
+		hidden_root_info->udi_bend = bindex;
+		bindex++;
+	}
+
+	if (branches == 0) {
+		printk(KERN_WARNING "unionfs: no branches specified\n");
+		err = -EINVAL;
+		goto out;
+	}
+
+	BUG_ON(branches != (hidden_root_info->udi_bend + 1));
+
+	/* ensure that no overlaps exist in the branches */
+	for (i = 0; i < branches; i++) {
+		for (j = i + 1; j < branches; j++) {
+			dent1 = hidden_root_info->udi_dentry[i];
+			dent2 = hidden_root_info->udi_dentry[j];
+
+			if (is_branch_overlap(dent1, dent2)) {
+				goto out_overlap;
+			}
+		}
+	}
+
+      out_overlap:
+
+	if (i != branches) {
+		printk(KERN_WARNING "unionfs: branches %d and %d overlap\n", i,
+		       j);
+		err = -EINVAL;
+		goto out;
+	}
+
+      out:
+	if (err) {
+		for (i = 0; i < branches; i++) {
+			if (hidden_root_info->udi_dentry[i])
+				DPUT(hidden_root_info->udi_dentry[i]);
+		}
+
+		KFREE(hidden_root_info->udi_dentry);
+		KFREE(stopd(sb)->usi_data);
+
+		/* MUST clear the pointers to prevent potential double free if
+		 * the caller dies later on
+		 */
+		hidden_root_info->udi_dentry = NULL;
+		stopd(sb)->usi_data = NULL;
+	}
+	return err;
+}
+
+/*
+ * Parse mount options.  See the manual page for usage instructions.
+ *
+ * Returns the dentry object of the lower-level (hidden) directory;
+ * We want to mount our stackable file system on top of that hidden directory.
+ *
+ * Sets default debugging level to N, if any.
+ */
+static struct unionfs_dentry_info *unionfs_parse_options(struct super_block *sb,
+							 char *options)
+{
+	struct unionfs_dentry_info *hidden_root_info;
+	char *optname;
+	int err = 0;
+	int bindex;
+	int dirsfound = 0;
+#ifdef UNIONFS_IMAP
+	int imapfound = 0;
+#endif
+	print_entry_location();
+
+	/* allocate private data area */
+	err = -ENOMEM;
+	hidden_root_info =
+	    KZALLOC(sizeof(struct unionfs_dentry_info), GFP_KERNEL);
+	if (!hidden_root_info)
+		goto out_error;
+	hidden_root_info->udi_bstart = -1;
+	hidden_root_info->udi_bend = -1;
+	hidden_root_info->udi_bopaque = -1;
+
+	while ((optname = strsep(&options, ",")) != NULL) {
+		char *optarg;
+		char *endptr;
+		int intval;
+
+		if (!*optname) {
+			continue;
+		}
+
+		optarg = strchr(optname, '=');
+		if (optarg) {
+			*optarg++ = '\0';
+		}
+
+		/* All of our options take an argument now. Insert ones that
+		 * don't, above this check.  */
+		if (!optarg) {
+			printk("unionfs: %s requires an argument.\n", optname);
+			err = -EINVAL;
+			goto out_error;
+		}
+
+		if (!strcmp("dirs", optname)) {
+			if (++dirsfound > 1) {
+				printk(KERN_WARNING
+				       "unionfs: multiple dirs specified\n");
+				err = -EINVAL;
+				goto out_error;
+			}
+			err = parse_dirs_option(sb, hidden_root_info, optarg);
+			if (err)
+				goto out_error;
+			continue;
+		}
+#ifdef UNIONFS_IMAP
+		if (!strcmp("imap", optname)) {
+			if (++imapfound > 1) {
+				printk(KERN_WARNING
+				       "unionfs: multiple imap specified\n");
+				err = -EINVAL;
+				goto out_error;
+			}
+			err = parse_imap_option(sb, hidden_root_info, optarg);
+			if (err)
+				goto out_error;
+			continue;
+		}
+#endif
+		if (!strcmp("delete", optname)) {
+			if (!strcmp("whiteout", optarg)) {
+				/* default */
+#ifdef UNIONFS_DELETE_ALL
+			} else if (!strcmp("all", optarg)) {
+				MOUNT_FLAG(sb) |= DELETE_ALL;
+#endif
+			} else {
+				printk(KERN_WARNING
+				       "unionfs: invalid delete option '%s'\n",
+				       optarg);
+				err = -EINVAL;
+				goto out_error;
+			}
+			continue;
+		}
+		/* All of these options require an integer argument. */
+		intval = simple_strtoul(optarg, &endptr, 0);
+		if (*endptr) {
+			printk(KERN_WARNING
+			       "unionfs: invalid %s option '%s'\n",
+			       optname, optarg);
+			err = -EINVAL;
+			goto out_error;
+		}
+
+		if (!strcmp("debug", optname)) {
+			set_debug_mask(intval);
+			continue;
+		}
+
+		err = -EINVAL;
+		printk(KERN_WARNING
+		       "unionfs: unrecognized option '%s'\n", optname);
+		goto out_error;
+	}
+	if (dirsfound != 1) {
+		printk(KERN_WARNING "unionfs: dirs option required\n");
+		err = -EINVAL;
+		goto out_error;
+	}
+	goto out;
+
+      out_error:
+	if (hidden_root_info && hidden_root_info->udi_dentry) {
+		for (bindex = hidden_root_info->udi_bstart;
+		     bindex >= 0 && bindex <= hidden_root_info->udi_bend;
+		     bindex++) {
+			struct dentry *d;
+			d = hidden_root_info->udi_dentry[bindex];
+			DPUT(d);
+			if (stohiddenmnt_index(sb, bindex))
+				mntput(stohiddenmnt_index(sb, bindex));
+		}
+	}
+
+	KFREE(hidden_root_info->udi_dentry);
+	KFREE(hidden_root_info);
+
+	KFREE(stopd(sb)->usi_data);
+	stopd(sb)->usi_data = NULL;
+
+	hidden_root_info = ERR_PTR(err);
+      out:
+	print_exit_location();
+	return hidden_root_info;
+}
+
+static struct dentry *unionfs_d_alloc_root(struct super_block *sb)
+{
+	struct dentry *ret = NULL;
+
+	if (sb) {
+		static const struct qstr name = {.name = "/",.len = 1 };
+
+		ret = d_alloc(NULL, &name);
+		if (ret) {
+			ret->d_op = &unionfs_dops;
+			ret->d_sb = sb;
+			ret->d_parent = ret;
+		}
+	}
+	return ret;
+}
+
+static int unionfs_read_super(struct super_block *sb, void *raw_data,
+			      int silent)
+{
+	int err = 0;
+
+	struct unionfs_dentry_info *hidden_root_info = NULL;
+	int bindex, bstart, bend;
+	unsigned long long maxbytes;
+
+	print_entry_location();
+
+	if (!raw_data) {
+		printk(KERN_WARNING
+		       "unionfs_read_super: missing data argument\n");
+		err = -EINVAL;
+		goto out;
+	}
+
+	/*
+	 * Allocate superblock private data
+	 */
+	stopd_lhs(sb) = KZALLOC(sizeof(struct unionfs_sb_info), GFP_KERNEL);
+	if (!stopd(sb)) {
+		printk(KERN_WARNING "%s: out of memory\n", __FUNCTION__);
+		err = -ENOMEM;
+		goto out;
+	}
+	stopd(sb)->b_end = -1;
+	atomic_set(&stopd(sb)->usi_generation, 1);
+	init_rwsem(&stopd(sb)->usi_rwsem);
+
+	hidden_root_info = unionfs_parse_options(sb, raw_data);
+	if (IS_ERR(hidden_root_info)) {
+		printk(KERN_WARNING
+		       "unionfs_read_super: error while parsing options (err = %ld)\n",
+		       PTR_ERR(hidden_root_info));
+		err = PTR_ERR(hidden_root_info);
+		hidden_root_info = NULL;
+		goto out_free;
+	}
+	if (hidden_root_info->udi_bstart == -1) {
+		err = -ENOENT;
+		goto out_free;
+	}
+
+	/* set the hidden superblock field of upper superblock */
+	bstart = hidden_root_info->udi_bstart;
+	BUG_ON(bstart != 0);
+	sbend(sb) = bend = hidden_root_info->udi_bend;
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		struct dentry *d;
+
+		d = hidden_root_info->udi_dentry[bindex];
+
+		set_stohs_index(sb, bindex, d->d_sb);
+	}
+
+	/* Unionfs: Max Bytes is the maximum bytes from among all the branches */
+	maxbytes = -1;
+	for (bindex = bstart; bindex <= bend; bindex++)
+		if (maxbytes < stohs_index(sb, bindex)->s_maxbytes)
+			maxbytes = stohs_index(sb, bindex)->s_maxbytes;
+	sb->s_maxbytes = maxbytes;
+
+	sb->s_op = &unionfs_sops;
+#ifdef CONFIG_EXPORTFS
+	sb->s_export_op = &unionfs_export_ops;
+#endif
+
+	/*
+	 * we can't use d_alloc_root if we want to use
+	 * our own interpose function unchanged,
+	 * so we simply call our own "fake" d_alloc_root
+	 */
+	sb->s_root = unionfs_d_alloc_root(sb);
+	if (!sb->s_root) {
+		err = -ENOMEM;
+		goto out_dput;
+	}
+
+	/* link the upper and lower dentries */
+	dtopd_lhs(sb->s_root) = NULL;
+	if ((err = new_dentry_private_data(sb->s_root)))
+		goto out_freedpd;
+
+	/* Set the hidden dentries for s_root */
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		struct dentry *d;
+
+		d = hidden_root_info->udi_dentry[bindex];
+
+		set_dtohd_index(sb->s_root, bindex, d);
+	}
+	set_dbstart(sb->s_root, bstart);
+	set_dbend(sb->s_root, bend);
+
+	/* Set the generation number to one, since this is for the mount. */
+	atomic_set(&dtopd(sb->s_root)->udi_generation, 1);
+
+	/* call interpose to create the upper level inode */
+	if ((err = unionfs_interpose(sb->s_root, sb, 0)))
+		goto out_freedpd;
+	unlock_dentry(sb->s_root);
+	goto out;
+
+      out_freedpd:
+	if (dtopd(sb->s_root)) {
+		KFREE(dtohd_ptr(sb->s_root));
+		free_dentry_private_data(dtopd(sb->s_root));
+	}
+	DPUT(sb->s_root);
+      out_dput:
+	if (hidden_root_info && !IS_ERR(hidden_root_info)) {
+		for (bindex = hidden_root_info->udi_bstart;
+		     bindex <= hidden_root_info->udi_bend; bindex++) {
+			struct dentry *d;
+
+			d = hidden_root_info->udi_dentry[bindex];
+
+			if (d)
+				DPUT(d);
+
+			if (stopd(sb) && stohiddenmnt_index(sb, bindex))
+				mntput(stohiddenmnt_index(sb, bindex));
+		}
+		KFREE(hidden_root_info->udi_dentry);
+		KFREE(hidden_root_info);
+		hidden_root_info = NULL;
+	}
+      out_free:
+	KFREE(stopd(sb)->usi_data);
+	KFREE(stopd(sb));
+	stopd_lhs(sb) = NULL;
+      out:
+	if (hidden_root_info && !IS_ERR(hidden_root_info)) {
+		KFREE(hidden_root_info->udi_dentry);
+		KFREE(hidden_root_info);
+	}
+	print_exit_status(err);
+	return err;
+}
+
+static int unionfs_get_sb(struct file_system_type *fs_type,
+					  int flags, const char *dev_name,
+					  void *raw_data, struct vfsmount *mnt)
+{
+	return get_sb_nodev(fs_type, flags, raw_data, unionfs_read_super, mnt);
+}
+
+void unionfs_kill_block_super(struct super_block *sb)
+{
+	generic_shutdown_super(sb);
+}
+
+static struct file_system_type unionfs_fs_type = {
+	.owner = THIS_MODULE,
+	.name = "unionfs",
+	.get_sb = unionfs_get_sb,
+	.kill_sb = unionfs_kill_block_super,
+	.fs_flags = FS_REVAL_DOT,
+};
+
+static int init_debug = 0;
+module_param_named(debug, init_debug, int, S_IRUGO);
+MODULE_PARM_DESC(debug, "Initial Unionfs debug value.");
+
+static int __init init_unionfs_fs(void)
+{
+	int err;
+	printk("Registering unionfs " UNIONFS_VERSION "\n");
+
+	set_debug_mask(init_debug);
+
+#ifdef FIST_MALLOC_DEBUG
+	atomic_set(&unionfs_malloc_counter, 0);
+	atomic_set(&unionfs_mallocs_outstanding, 0);
+#endif				/* FIST_MALLOC_DEBUG */
+
+	if ((err = init_filldir_cache()))
+		goto out;
+	if ((err = init_inode_cache()))
+		goto out;
+	if ((err = init_dentry_cache()))
+		goto out;
+	err = register_filesystem(&unionfs_fs_type);
+      out:
+	if (err) {
+		destroy_filldir_cache();
+		destroy_inode_cache();
+		destroy_dentry_cache();
+	}
+	return err;
+}
+static void __exit exit_unionfs_fs(void)
+{
+	destroy_filldir_cache();
+	destroy_inode_cache();
+	destroy_dentry_cache();
+	unregister_filesystem(&unionfs_fs_type);
+	printk("Completed unionfs module unload.\n");
+}
+
+MODULE_AUTHOR
+    ("Filesystems and Storage Lab, Stony Brook University (http://www.fsl.cs.sunysb.edu/)");
+MODULE_DESCRIPTION("Unionfs " UNIONFS_VERSION
+		   " (http://unionfs.filesystems.org/)");
+MODULE_LICENSE("GPL");
+
+module_init(init_unionfs_fs);
+module_exit(exit_unionfs_fs);
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/malloc_debug.c newtree/fs/unionfs/malloc_debug.c
--- oldtree/fs/unionfs/malloc_debug.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/malloc_debug.c	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,199 @@
+#ifdef FIST_MALLOC_DEBUG
+
+#include "unionfs.h"
+
+/* for malloc debugging */
+atomic_t unionfs_malloc_counter = ATOMIC_INIT(0);
+atomic_t unionfs_mallocs_outstanding = ATOMIC_INIT(0);
+atomic_t unionfs_dget_counter = ATOMIC_INIT(0);
+atomic_t unionfs_dgets_outstanding = ATOMIC_INIT(0);
+atomic_t unionfs_iget_counter = ATOMIC_INIT(0);
+atomic_t unionfs_igets_outstanding = ATOMIC_INIT(0);
+
+void *unionfs_kzalloc(size_t size, gfp_t flags, int line, const char *file)
+{
+	void *ptr = kzalloc(size, flags);
+	if (ptr) {
+		atomic_inc(&unionfs_malloc_counter);
+		atomic_inc(&unionfs_mallocs_outstanding);
+		printk("KZA:%d:%d:%p:%d:%s\n",
+		       atomic_read(&unionfs_malloc_counter),
+		       atomic_read(&unionfs_mallocs_outstanding), ptr, line,
+		       file);
+	}
+	return ptr;
+}
+void *unionfs_kmalloc(size_t size, gfp_t flags, int line, const char *file)
+{
+	void *ptr = kmalloc(size, flags);
+	if (ptr) {
+		atomic_inc(&unionfs_malloc_counter);
+		atomic_inc(&unionfs_mallocs_outstanding);
+		printk("KM:%d:%d:%p:%d:%s\n",
+		       atomic_read(&unionfs_malloc_counter),
+		       atomic_read(&unionfs_mallocs_outstanding), ptr, line,
+		       file);
+	}
+	return ptr;
+}
+
+void unionfs_kfree(void *ptr, int line, const char *file)
+{
+	atomic_inc(&unionfs_malloc_counter);
+	if (ptr) {
+		BUG_ON(IS_ERR(ptr));
+		atomic_dec(&unionfs_mallocs_outstanding);
+	}
+	printk("KF:%d:%d:%p:%d:%s\n", atomic_read(&unionfs_malloc_counter),
+	       atomic_read(&unionfs_mallocs_outstanding), ptr, line, file);
+	kfree(ptr);
+}
+
+void record_set(struct dentry *upper, int index, struct dentry *ptr,
+		struct dentry *old, int line, const char *file)
+{
+	atomic_inc(&unionfs_dget_counter);
+	printk("DD:%d:%d:%d:%p:%d:%s %p, %d\n",
+	       atomic_read(&unionfs_dget_counter),
+	       atomic_read(&unionfs_dgets_outstanding),
+	       old ? atomic_read(&old->d_count) : 0, old, line, file, upper,
+	       index);
+	atomic_inc(&unionfs_dget_counter);
+	printk("DS:%d:%d:%d:%p:%d:%s %p, %d\n",
+	       atomic_read(&unionfs_dget_counter),
+	       atomic_read(&unionfs_dgets_outstanding),
+	       ptr ? atomic_read(&ptr->d_count) : 0, ptr, line, file, upper,
+	       index);
+}
+
+void record_path_lookup(struct nameidata *nd, int line, const char *file)
+{
+	struct dentry *ptr = nd->dentry;
+	if (ptr) {
+		atomic_inc(&unionfs_dget_counter);
+		atomic_inc(&unionfs_dgets_outstanding);
+		printk("DL:%d:%d:%d:%p:%d:%s\n",
+		       atomic_read(&unionfs_dget_counter),
+		       atomic_read(&unionfs_dgets_outstanding),
+		       atomic_read(&ptr->d_count), ptr, line, file);
+	}
+}
+
+void record_path_release(struct nameidata *nd, int line, const char *file)
+{
+	struct dentry *ptr = nd->dentry;
+
+	atomic_inc(&unionfs_dget_counter);
+	if (ptr)
+		atomic_dec(&unionfs_dgets_outstanding);
+	printk("DP:%d:%d:%d:%p:%d:%s\n", atomic_read(&unionfs_dget_counter),
+	       atomic_read(&unionfs_dgets_outstanding),
+	       ptr ? atomic_read(&ptr->d_count) : 0, ptr, line, file);
+}
+
+struct file *unionfs_dentry_open(struct dentry *ptr, struct vfsmount *mnt,
+				 int flags, int line, const char *file)
+{
+	atomic_inc(&unionfs_dget_counter);
+	if (ptr)
+		atomic_dec(&unionfs_dgets_outstanding);
+	printk("DO:%d:%d:%d:%p:%d:%s\n", atomic_read(&unionfs_dget_counter),
+	       atomic_read(&unionfs_dgets_outstanding),
+	       ptr ? atomic_read(&ptr->d_count) : 0, ptr, line, file);
+	return dentry_open(ptr, mnt, flags);
+}
+
+struct dentry *unionfs_dget(struct dentry *ptr, int line, const char *file)
+{
+	ptr = dget(ptr);
+	if (ptr) {
+		atomic_inc(&unionfs_dget_counter);
+		atomic_inc(&unionfs_dgets_outstanding);
+		printk("DG:%d:%d:%d:%p:%d:%s\n",
+		       atomic_read(&unionfs_dget_counter),
+		       atomic_read(&unionfs_dgets_outstanding),
+		       atomic_read(&ptr->d_count), ptr, line, file);
+	}
+	return ptr;
+}
+
+struct dentry *unionfs_dget_parent(struct dentry *child, int line,
+				   const char *file)
+{
+	struct dentry *ptr;
+
+	ptr = dget_parent(child);
+	atomic_inc(&unionfs_dget_counter);
+	atomic_inc(&unionfs_dgets_outstanding);
+	printk("DG:%d:%d:%d:%p:%d:%s\n",
+	       atomic_read(&unionfs_dget_counter),
+	       atomic_read(&unionfs_dgets_outstanding),
+	       atomic_read(&ptr->d_count), ptr, line, file);
+
+	return ptr;
+}
+
+struct dentry *unionfs_lookup_one_len(const char *name, struct dentry *parent,
+				      int len, int line, const char *file)
+{
+	struct dentry *ptr = lookup_one_len(name, parent, len);
+	if (ptr && !IS_ERR(ptr)) {
+		atomic_inc(&unionfs_dget_counter);
+		atomic_inc(&unionfs_dgets_outstanding);
+		printk("DL:%d:%d:%d:%p:%d:%s\n",
+		       atomic_read(&unionfs_dget_counter),
+		       atomic_read(&unionfs_dgets_outstanding),
+		       atomic_read(&ptr->d_count), ptr, line, file);
+	}
+	return ptr;
+}
+
+void unionfs_dput(struct dentry *ptr, int line, const char *file)
+{
+	atomic_inc(&unionfs_dget_counter);
+	if (ptr) {
+		BUG_ON(IS_ERR(ptr));
+		atomic_dec(&unionfs_dgets_outstanding);
+	}
+	printk("DP:%d:%d:%d:%p:%d:%s\n", atomic_read(&unionfs_dget_counter),
+	       atomic_read(&unionfs_dgets_outstanding),
+	       ptr ? atomic_read(&ptr->d_count) : 0, ptr, line, file);
+	dput(ptr);
+}
+
+struct inode *unionfs_igrab(struct inode *inode, int line, char *file)
+{
+	atomic_inc(&unionfs_iget_counter);
+	if (inode)
+		atomic_inc(&unionfs_igets_outstanding);
+	printk("IR:%d:%d:%d:%p:%d:%s\n", atomic_read(&unionfs_iget_counter),
+	       atomic_read(&unionfs_igets_outstanding),
+	       inode ? atomic_read(&inode->i_count) : 0, inode, line, file);
+	return igrab(inode);
+}
+
+void unionfs_iput(struct inode *inode, int line, char *file)
+{
+	atomic_inc(&unionfs_iget_counter);
+	if (inode)
+		atomic_dec(&unionfs_igets_outstanding);
+	printk("IP:%d:%d:%d:%p:%d:%s\n", atomic_read(&unionfs_iget_counter),
+	       atomic_read(&unionfs_igets_outstanding),
+	       inode ? atomic_read(&inode->i_count) : 0, inode, line, file);
+	iput(inode);
+}
+
+struct inode *unionfs_iget(struct super_block *sb, unsigned long ino, int line,
+			   char *file)
+{
+	struct inode *inode = iget(sb, ino);
+	atomic_inc(&unionfs_iget_counter);
+	if (inode)
+		atomic_inc(&unionfs_igets_outstanding);
+	printk("IG:%d:%d:%d:%p:%d:%s\n", atomic_read(&unionfs_iget_counter),
+	       atomic_read(&unionfs_igets_outstanding),
+	       inode ? atomic_read(&inode->i_count) : 0, inode, line, file);
+	return inode;
+}
+
+#endif				/* FIST_MALLOC_DEBUG */
diff -urN oldtree/fs/unionfs/mmap.c newtree/fs/unionfs/mmap.c
--- oldtree/fs/unionfs/mmap.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/mmap.c	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,326 @@
+/*
+ * Copyright (c) 1997-2005 Erez Zadok <ezk@cs.stonybrook.edu>
+ * Copyright (c) 2001-2005 Stony Brook University
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package, or get one from ftp://ftp.filesystems.org/pub/fistgen/COPYING.
+ *
+ * This Copyright notice must be kept intact and distributed with all
+ * fistgen sources INCLUDING sources generated by fistgen.
+ */
+/*
+ *  $Id: mmap.c,v 1.11 2006/07/08 17:58:31 ezk Exp $
+ */
+
+#include "unionfs.h"
+
+/* SP: writepage doesn't handle copyup yet
+ * a possible solution
+ *
+ * inode = d_find_alias(inode);
+ * dentry = d_find_alias(inode);
+ *
+ * then can determine if dentry needs to be copied up
+ *
+ * Questions:
+ *
+ * 1) will d_find_alias() always return a dentry?
+ * 2) hard links? (generic problem)
+ */
+int unionfs_writepage(struct page *page, struct writeback_control *wbc)
+{
+	int err = -EIO;
+	struct inode *inode;
+	struct inode *lower_inode;
+	struct page *lower_page;
+	char *kaddr, *lower_kaddr;
+
+	print_entry_location();
+
+	inode = page->mapping->host;
+	lower_inode = itohi(inode);
+
+	/* find lower page (returns a locked page) */
+	lower_page = grab_cache_page(lower_inode->i_mapping, page->index);
+	if (!lower_page)
+		goto out;
+
+	/* get page address, and encode it */
+	kaddr = (char *)kmap(page);
+	lower_kaddr = (char *)kmap(lower_page);
+
+	memcpy(lower_kaddr, kaddr, PAGE_CACHE_SIZE);
+
+	kunmap(page);
+	kunmap(lower_page);
+
+	/* call lower writepage (expects locked page) */
+	err = lower_inode->i_mapping->a_ops->writepage(lower_page, wbc);
+
+	/*
+	 * update mtime and ctime of lower level file system
+	 * unionfs' mtime and ctime are updated by generic_file_write
+	 */
+	lower_inode->i_mtime = lower_inode->i_ctime = CURRENT_TIME;
+
+	page_cache_release(lower_page);	/* b/c grab_cache_page increased refcnt */
+
+	if (err)
+		ClearPageUptodate(page);
+	else
+		SetPageUptodate(page);
+      out:
+	unlock_page(page);
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ * readpage is called from generic_page_read and the fault handler.
+ * If your file system uses generic_page_read for the read op, it
+ * must implement readpage.
+ *
+ * Readpage expects a locked page, and must unlock it.
+ */
+int unionfs_do_readpage(struct file *file, struct page *page)
+{
+	int err = -EIO;
+	struct dentry *dentry;
+	struct file *lower_file = NULL;
+	struct inode *inode, *lower_inode;
+	char *page_data;
+	struct page *lower_page;
+	char *lower_page_data;
+
+	print_entry_location();
+
+	dentry = file->f_dentry;
+	if (ftopd(file) == NULL) {
+		err = -ENOENT;
+		goto out_err;
+	}
+	lower_file = ftohf(file);
+	inode = dentry->d_inode;
+	lower_inode = itohi(inode);
+
+	lower_page = NULL;
+
+	/* find lower page (returns a locked page) */
+	lower_page = read_cache_page(lower_inode->i_mapping,
+				     page->index,
+				     (filler_t *) lower_inode->i_mapping->
+				     a_ops->readpage, (void *)lower_file);
+
+	if (IS_ERR(lower_page)) {
+		err = PTR_ERR(lower_page);
+		lower_page = NULL;
+		goto out_release;
+	}
+
+	/*
+	 * wait for the page data to show up
+	 * (signaled by readpage as unlocking the page)
+	 */
+	wait_on_page_locked(lower_page);
+	if (!PageUptodate(lower_page)) {
+		/*
+		 * call readpage() again if we returned from wait_on_page with a
+		 * page that's not up-to-date; that can happen when a partial
+		 * page has a few buffers which are ok, but not the whole
+		 * page.
+		 */
+		lock_page(lower_page);
+		err = lower_inode->i_mapping->a_ops->readpage(lower_file,
+							      lower_page);
+		if (err) {
+			lower_page = NULL;
+			goto out_release;
+		}
+		wait_on_page_locked(lower_page);
+		if (!PageUptodate(lower_page)) {
+			err = -EIO;
+			goto out_release;
+		}
+	}
+
+	/* map pages, get their addresses */
+	page_data = (char *)kmap(page);
+	lower_page_data = (char *)kmap(lower_page);
+
+	memcpy(page_data, lower_page_data, PAGE_CACHE_SIZE);
+
+	err = 0;
+
+	kunmap(lower_page);
+	kunmap(page);
+
+      out_release:
+	if (lower_page)
+		page_cache_release(lower_page);	/* undo read_cache_page */
+
+	if (err == 0)
+		SetPageUptodate(page);
+	else
+		ClearPageUptodate(page);
+
+      out_err:
+	print_exit_status(err);
+	return err;
+}
+
+int unionfs_readpage(struct file *file, struct page *page)
+{
+	int err;
+	print_entry_location();
+
+	err = unionfs_do_readpage(file, page);
+
+	/*
+	 * we have to unlock our page, b/c we _might_ have gotten a locked page.
+	 * but we no longer have to wakeup on our page here, b/c UnlockPage does
+	 * it
+	 */
+
+	unlock_page(page);
+
+	print_exit_status(err);
+	return err;
+}
+
+int unionfs_prepare_write(struct file *file, struct page *page, unsigned from,
+			  unsigned to)
+{
+	int err = 0;
+
+	print_entry_location();
+
+	print_exit_status(err);
+	return err;
+}
+
+int unionfs_commit_write(struct file *file, struct page *page, unsigned from,
+			 unsigned to)
+{
+	int err = -ENOMEM;
+	struct inode *inode, *lower_inode;
+	struct file *lower_file = NULL;
+	loff_t pos;
+	unsigned bytes = to - from;
+	char *page_data = NULL;
+	mm_segment_t old_fs;
+
+	print_entry_location();
+
+	BUG_ON(file == NULL);
+
+	inode = page->mapping->host;	/* CPW: Moved below print_entry_location */
+	lower_inode = itohi(inode);
+
+	if (ftopd(file) != NULL)
+		lower_file = ftohf(file);
+
+	BUG_ON(lower_file == NULL);	/* XXX: is this assertion right here? */
+
+	page_data = (char *)kmap(page);
+	lower_file->f_pos = (page->index << PAGE_CACHE_SHIFT) + from;
+
+	/* SP: I use vfs_write instead of copying page data and the
+	 * prepare_write/commit_write combo because file system's like
+	 * GFS/OCFS2 don't like things touching those directly,
+	 * calling the underlying write op, while a little bit slower, will
+	 * call all the FS specific code as well
+	 */
+	old_fs = get_fs();
+	set_fs(KERNEL_DS);
+	err =
+	    vfs_write(lower_file, page_data + from, bytes, &lower_file->f_pos);
+	set_fs(old_fs);
+
+	kunmap(page);
+
+	if (err < 0)
+		goto out;
+
+	inode->i_blocks = lower_inode->i_blocks;
+	/* we may have to update i_size */
+	pos = ((loff_t) page->index << PAGE_CACHE_SHIFT) + to;
+	if (pos > i_size_read(inode))
+		i_size_write(inode, pos);
+
+	/*
+	 * update mtime and ctime of lower level file system
+	 * unionfs' mtime and ctime are updated by generic_file_write
+	 */
+	lower_inode->i_mtime = lower_inode->i_ctime = CURRENT_TIME;
+
+	mark_inode_dirty_sync(inode);
+
+      out:
+	if (err < 0)
+		ClearPageUptodate(page);
+
+	print_exit_status(err);
+	return err;		/* assume all is ok */
+}
+
+sector_t unionfs_bmap(struct address_space * mapping, sector_t block)
+{
+	int err = 0;
+	struct inode *inode, *lower_inode;
+
+	print_entry_location();
+
+	inode = (struct inode *)mapping->host;
+	lower_inode = itohi(inode);
+
+	if (lower_inode->i_mapping->a_ops->bmap)
+		err =
+		    lower_inode->i_mapping->a_ops->bmap(lower_inode->i_mapping,
+							block);
+	print_exit_location();
+	return err;
+}
+
+void unionfs_sync_page(struct page *page)
+{
+	struct inode *inode;
+	struct inode *lower_inode;
+	struct page *lower_page;
+	struct address_space *mapping = page->mapping;
+
+	print_entry_location();
+
+	inode = page->mapping->host;	/* CPW: Moved below print_entry_location */
+	lower_inode = itohi(inode);
+
+	/* find lower page (returns a locked page) */
+	lower_page = grab_cache_page(lower_inode->i_mapping, page->index);
+	if (!lower_page)
+		goto out;
+
+	/* do the actual sync */
+	if (mapping && mapping->a_ops && mapping->a_ops->sync_page)
+		mapping->a_ops->sync_page(page);
+
+	unlock_page(lower_page);	/* b/c grab_cache_page locked it */
+	page_cache_release(lower_page);	/* b/c grab_cache_page increased refcnt */
+
+      out:
+	print_exit_status(0);
+	return;
+}
+
+struct address_space_operations unionfs_aops = {
+      writepage:unionfs_writepage,
+      readpage:unionfs_readpage,
+      prepare_write:unionfs_prepare_write,
+      commit_write:unionfs_commit_write,
+      bmap:unionfs_bmap,
+      sync_page:unionfs_sync_page,
+};
+
+/*
+ * Local variables:
+ * c-basic-offset: 4
+ * End:
+ */
diff -urN oldtree/fs/unionfs/persistent_inode.c newtree/fs/unionfs/persistent_inode.c
--- oldtree/fs/unionfs/persistent_inode.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/persistent_inode.c	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,658 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: persistent_inode.c,v 1.36 2006/07/08 17:58:31 ezk Exp $
+ */
+#ifdef UNIONFS_IMAP
+
+#include "unionfs.h"
+
+static ssize_t __fread(struct file *filp, void *buf, size_t size, loff_t * pos)
+{
+	int err;
+	mm_segment_t oldfs;
+	ssize_t(*func) (struct file *, char __user *, size_t, loff_t *);
+
+	func = do_sync_read;
+	if (filp->f_op && filp->f_op->read)
+		func = filp->f_op->read;
+
+	oldfs = get_fs();
+	set_fs(KERNEL_DS);
+	do {
+		err = func(filp, (char __user *)buf, size, pos);
+	} while (err == -EAGAIN || err == -EINTR);
+	set_fs(oldfs);
+	return err;
+}
+
+static ssize_t __fwrite(struct file *filp, void *buf, size_t size, loff_t * pos)
+{
+	int err;
+	mm_segment_t oldfs;
+	unsigned long flim;
+	struct rlimit *rl;
+	ssize_t(*func) (struct file *, const char __user *, size_t, loff_t *);
+
+	func = do_sync_write;
+	if (filp->f_op && filp->f_op->write)
+		func = filp->f_op->write;
+
+	/*
+	 * it breaks RLIMIT_FSIZE,
+	 * but users should be careful to quota.
+	 */
+	rl = current->signal->rlim + RLIMIT_FSIZE;
+	flim = rl->rlim_cur;
+	rl->rlim_cur = RLIM_INFINITY;
+	oldfs = get_fs();
+	set_fs(KERNEL_DS);
+	do {
+		err = func(filp, (const char __user *)buf, size, pos);
+	} while (err == -EAGAIN || err == -EINTR);
+	set_fs(oldfs);
+	rl->rlim_cur = flim;
+	return err;
+}
+
+/*
+ * verify_forwardmap(super_block *sb)
+ * sb: pointer to a superblock containing the forwardmap.
+ * returns: 0 on success EINVAL or ENOMEM on failure;
+ */
+static int verify_forwardmap(struct super_block *sb)
+{
+	int err = 0, bytesread = 0, bindex = 0, mallocsize = 0;
+	loff_t readpos = 0;
+	struct file *forwardmap = NULL;
+	struct fmaphdr header;
+	struct unionfs_sb_info *spd = NULL;
+	print_entry_location();
+
+	spd = stopd(sb);
+	BUG_ON(!spd);
+
+	forwardmap = spd->usi_forwardmap;
+	if (!forwardmap) {
+		err = -EINVAL;
+		goto out;
+	}
+	bytesread = __fread(forwardmap, &header, sizeof(struct fmaphdr),
+			    &readpos);
+	if (bytesread < sizeof(struct fmaphdr)) {
+		err = -EINVAL;
+		goto out;
+	}
+	if (header.magic != FORWARDMAP_MAGIC
+	    || header.version != FORWARDMAP_VERSION) {
+		err = -EINVAL;
+		goto out;
+	}
+	spd->usi_bmap =
+	    KMALLOC(sizeof(struct bmapent) * header.usedbranches, GFP_KERNEL);
+
+	if (!spd->usi_bmap) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	while (bindex < header.usedbranches) {
+		bytesread = __fread(forwardmap, &stopd(sb)->usi_bmap[bindex],
+				    sizeof(struct bmapent), &readpos);
+		if (bytesread < sizeof(struct bmapent)) {
+			err = -EINVAL;
+			goto out_err;
+		}
+		bindex++;
+	}
+
+	mallocsize = sizeof(int) * header.usedbranches;
+	goto out;
+      out_err:
+	if (spd->usi_bmap)
+		KFREE(spd->usi_bmap);
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ * verify_reversemap(struct super_block sb, int rmapindex)
+ *
+ * sb: The unionfs superblock containing all of the current imap info
+ * rmapindex: the index in the usi_reversemaps array that we wish to
+ * verify
+ *
+ * Assumes the reverse maps less than rmapindex are valid.
+ *
+ * returns: 0 if the opperation succeds
+ * 	-EINVAL if the map file does not belong to the forward map
+ *
+ */
+static int verify_reversemap(struct super_block *sb, int rmapindex,
+			     struct unionfs_dentry_info *hidden_root_info)
+{
+	int err = 0, i = 0, bindex = 0, found = 0, bytesread;
+	loff_t readpos = 0;
+	struct file *forwardmap, *reversemap;
+	struct fmaphdr fheader;
+	struct rmaphdr rheader;
+	struct kstatfs st;
+	struct unionfs_sb_info *spd = NULL;
+
+	print_entry_location();
+
+	spd = stopd(sb);
+	BUG_ON(!spd);
+
+	forwardmap = spd->usi_forwardmap;
+	if (!forwardmap) {
+		err = -EINVAL;
+		goto out;
+	}
+	reversemap = spd->usi_reversemaps[rmapindex];
+	if (!reversemap) {
+		err = -EINVAL;
+		goto out;
+	}
+	bytesread = __fread(forwardmap, &fheader, sizeof(struct fmaphdr),
+			    &readpos);
+	if (bytesread < sizeof(struct fmaphdr)) {
+		err = -EINVAL;
+		goto out;
+	}
+	readpos = 0;
+	bytesread = __fread(reversemap, &rheader, sizeof(struct rmaphdr),
+			    &readpos);
+	if (bytesread < sizeof(struct rmaphdr)) {
+		err = -EINVAL;
+		goto out;
+	}
+	if (rheader.magic != REVERSEMAP_MAGIC
+	    || rheader.version != REVERSEMAP_VERSION) {
+		err = -EINVAL;
+		goto out;
+	}
+	if (memcmp(fheader.uuid, rheader.fwduuid, sizeof(fheader.uuid))) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	/* XXX: Ok so here we take the new map and read the fsid from it. Then
+	 * we go through all the branches in the union and see which ones it
+	 * matches with*/
+	for (i = 0; i < spd->usi_num_bmapents && !found; i++) {
+		if (memcmp
+		    (rheader.revuuid, spd->usi_bmap[i].uuid,
+		     sizeof(rheader.revuuid)))
+			continue;
+
+		found = 1;
+		for (bindex = 0; bindex <= hidden_root_info->udi_bend; bindex++) {
+			struct dentry *d;
+			fsid_t fsid;
+			dev_t dev;
+			memset(&st, 0, sizeof(struct kstatfs));
+
+			d = hidden_root_info->udi_dentry[bindex];
+
+			err = d->d_sb->s_op->statfs(d->d_sb, &st);
+			if (err)
+				goto out;
+
+			if (st.f_fsid.val[0] || st.f_fsid.val[1]) {
+				fsid = st.f_fsid;
+			} else {
+
+				dev = d->d_sb->s_dev;
+				fsid.val[0] = MAJOR(dev);
+				fsid.val[1] = MINOR(dev);
+			}
+
+			if (memcmp(&fsid, &rheader.fsid, sizeof(fsid)))
+				continue;
+
+			if (spd->usi_bnum_table[bindex] == -1)
+				spd->usi_bnum_table[bindex] = i;
+			if (spd->usi_map_table[bindex]) {
+				printk(KERN_WARNING
+				       "Two reverse maps share fsid %u%u!\n",
+				       rheader.fsid.val[0],
+				       rheader.fsid.val[1]);
+				err = -EINVAL;
+				goto out;
+			} else {
+				spd->usi_map_table[bindex] = reversemap;
+			}
+		}
+	}
+	if (!found) {
+		printk(KERN_WARNING
+		       "Could not match the reversemap uuid with an entry in the forwardmap table\n");
+		err = -EINVAL;
+	}
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+int init_imap_data(struct super_block *sb,
+		   struct unionfs_dentry_info *hidden_root_info)
+{
+	int i, err = 0, mallocsize = 0;
+	struct unionfs_sb_info *spd;
+
+	print_entry_location();
+
+	spd = stopd(sb);
+
+	spd->usi_forwardmap = NULL;
+	spd->usi_reversemaps = NULL;
+	spd->usi_bnum_table = NULL;
+
+	mallocsize = sizeof(struct file *) * (hidden_root_info->udi_bend + 1);
+	spd->usi_reversemaps = KZALLOC(mallocsize, GFP_KERNEL);
+	if (!spd->usi_reversemaps) {
+		err = -ENOMEM;
+		goto out_error;
+	}
+
+	spd->usi_map_table = KZALLOC(mallocsize, GFP_KERNEL);
+	if (!spd->usi_map_table) {
+		err = -ENOMEM;
+		goto out_error;
+	}
+
+	mallocsize = sizeof(int) * (hidden_root_info->udi_bend + 1);
+	spd->usi_bnum_table = KMALLOC(mallocsize, GFP_KERNEL);
+	if (!spd->usi_bnum_table) {
+		err = -ENOMEM;
+		goto out_error;
+	}
+
+	for (i = 0; i <= hidden_root_info->udi_bend; i++) {
+		spd->usi_bnum_table[i] = -1;
+	}
+
+	if (!err)
+		goto out;
+      out_error:
+
+	if (spd->usi_reversemaps) {
+		KFREE(spd->usi_reversemaps);
+		spd->usi_reversemaps = NULL;
+	}
+
+	if (spd->usi_map_table) {
+		KFREE(spd->usi_map_table);
+		spd->usi_map_table = NULL;
+	}
+
+	if (spd->usi_bnum_table) {
+		KFREE(spd->usi_bnum_table);
+		spd->usi_bnum_table = NULL;
+
+	}
+
+      out:
+	print_exit_status(err);
+	return err;
+
+}
+
+void cleanup_imap_data(struct super_block *sb)
+{
+	int count = 0;
+	struct unionfs_sb_info *spd;
+
+	print_entry_location();
+
+	spd = stopd(sb);
+
+	spd->usi_persistent = 0;
+	count = spd->usi_num_bmapents;
+	while (count - 1 >= 0) {
+		if (spd->usi_reversemaps[count - 1]) {
+			filp_close(spd->usi_reversemaps[count - 1], NULL);
+			spd->usi_reversemaps[count - 1] = NULL;
+		}
+		count--;
+	}
+	if (spd->usi_reversemaps) {
+		KFREE(spd->usi_reversemaps);
+		spd->usi_reversemaps = NULL;
+	}
+
+	if (spd->usi_map_table) {
+		KFREE(spd->usi_map_table);
+		spd->usi_map_table = NULL;
+	}
+
+	if (spd->usi_bnum_table) {
+		KFREE(spd->usi_bnum_table);
+		spd->usi_bnum_table = NULL;
+	}
+	if (spd->usi_forwardmap) {
+		filp_close(spd->usi_forwardmap, NULL);
+		spd->usi_forwardmap = NULL;
+	}
+	print_exit_location();
+}
+
+int parse_imap_option(struct super_block *sb,
+		      struct unionfs_dentry_info *hidden_root_info,
+		      char *options)
+{
+	int count = 0, err = 0;
+	char *name;
+	struct unionfs_sb_info *spd = NULL;
+
+	print_entry_location();
+	spd = stopd(sb);
+	BUG_ON(!spd);
+
+	err = init_imap_data(sb, hidden_root_info);
+	if (err)
+		goto out_error;
+	while ((name = strsep(&options, ":")) != NULL) {
+		if (!*name)
+			continue;
+		if (!spd->usi_forwardmap) {
+			spd->usi_forwardmap = filp_open(name, O_RDWR, 0);
+			if (IS_ERR(spd->usi_forwardmap)) {
+				err = PTR_ERR(spd->usi_forwardmap);
+				spd->usi_forwardmap = NULL;
+				goto out_error;
+			}
+		} else {
+			spd->usi_reversemaps[count] =
+			    filp_open(name, O_RDWR, 0);
+			if (IS_ERR(spd->usi_reversemaps[count])) {
+				err = PTR_ERR(spd->usi_reversemaps[count]);
+				spd->usi_reversemaps[count] = NULL;
+				goto out_error;
+
+			}
+			count++;
+		}
+	}
+	if (count <= 0) {
+		printk(KERN_WARNING "unionfs: no reverse maps specified.\n");
+		err = -EINVAL;
+	}
+	if (err)
+		goto out_error;
+
+	/* Initialize the super block's next_avail field */
+	/* Dave, you can't use 64-bit division here because the i386 doesn't
+	 * support it natively.  Instead you need to punt if the size is
+	 * greater than unsigned long, and then cast it down.  Then you should
+	 * be able to assign to this value, without having these problems. */
+
+	if (spd->usi_forwardmap->f_dentry->d_inode->i_size > ULONG_MAX) {
+		err = -EFBIG;
+		goto out_error;
+	}
+	spd->usi_next_avail =
+	    ((unsigned long)(spd->usi_forwardmap->f_dentry->d_inode->
+			     i_size - (sizeof(struct fmaphdr) +
+				       sizeof(struct bmapent[256])))
+	     / sizeof(struct fmapent));
+
+	if (spd->usi_next_avail < FIRST_VALID_INODE)
+		spd->usi_next_avail = FIRST_VALID_INODE;
+
+	spd->usi_num_bmapents = count;
+	err = verify_forwardmap(sb);
+	if (err)
+		goto out_error;
+	while (count > 0) {
+		err = verify_reversemap(sb, --count, hidden_root_info);
+		if (err)
+			goto out_error;
+	}
+	spd->usi_persistent = 1;
+
+	goto out;
+
+      out_error:
+	spd->usi_num_bmapents = count;
+	cleanup_imap_data(sb);
+
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+ /*
+  * get @ino from @hidden_ino.
+  */
+static int __read_uin(struct unionfs_sb_info *sbi, ino_t hidden_ino, int bindex,
+		      ino_t * ino)
+{
+	int err;
+	struct file *rev;
+	loff_t pos;
+	ssize_t sz;
+	uint64_t ino64;
+	const int elmnt = sizeof(ino64);
+
+	rev = sbi->usi_map_table[bindex];
+	pos = sizeof(struct rmaphdr) + elmnt * hidden_ino;
+	*ino = 0;
+	err = 0;
+	if (pos + elmnt > rev->f_dentry->d_inode->i_size)
+		goto out;
+
+	sz = __fread(rev, &ino64, elmnt, &pos);
+	err = sz;
+	if (err < 0)
+		goto out;
+	err = 0;
+	*ino = -1;
+	if (sz != elmnt || ino64 > *ino)
+		err = -EIO;
+	*ino = ino64;
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ * put unionfs @ino for @hidden_ino on @bindex.
+ */
+static int __write_uin(struct unionfs_sb_info *sbi, ino_t ino, int bindex,
+		       ino_t hidden_ino)
+{
+	struct file *fwd, *rev;
+	struct fmapent ent;
+	loff_t pos;
+	ssize_t sz;
+	int err;
+	uint64_t ino64;
+	const int fwdhdr = sizeof(struct fmaphdr) + sizeof(struct bmapent[256]);
+	const int fwd_elmnt = sizeof(ent);
+	const int rev_elmnt = sizeof(ino64);
+
+	err = -ENOSPC;
+	if (ino < FIRST_VALID_INODE)
+		goto out;
+
+	fwd = sbi->usi_forwardmap;
+	ent.fsnum = sbi->usi_bnum_table[bindex];
+	ent.inode = hidden_ino;
+	pos = fwdhdr + fwd_elmnt * ino;
+	sz = __fwrite(fwd, &ent, fwd_elmnt, &pos);
+	err = sz;
+	if (err < 0)
+		goto out;
+	err = -EIO;
+	if (sz != fwd_elmnt)
+		goto out;
+
+	rev = sbi->usi_map_table[bindex];
+	pos = sizeof(struct rmaphdr) + rev_elmnt * hidden_ino;
+	ino64 = ino;
+	sz = __fwrite(rev, &ino64, rev_elmnt, &pos);
+	err = sz;
+	if (err < 0)
+		goto out;
+	err = 0;
+	if (sz != rev_elmnt)
+		err = -EIO;
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ * read_uin(struct super_block *sb, uint8_t branchnum, ino_t inode_number, int flag, ino_t *uino)
+ * fsnum: branch to reference when getting the inode number
+ * inode_number: lower level inode number use to reference the proper inode.
+ * flag: if set to O_CREAT it will creat the entry if it doesent exist
+ * 		 otherwise it will return the existing one.
+ * returns: the unionfs inode number either created or retrieved based on
+ * 			the information.
+ */
+int read_uin(struct super_block *sb, uint8_t branchnum, ino_t inode_number,
+	     int flag, ino_t * uino)
+{
+	int err = 0;
+	struct unionfs_sb_info *spd;
+
+	print_entry_location();
+
+	spd = stopd(sb);
+	BUG_ON(!spd);
+
+	/* Find appropriate reverse map and then read from the required position */
+	/* get it from the array. */
+	err = __read_uin(spd, inode_number, branchnum, uino);
+	if (err || *uino)
+		goto out;
+
+	err = -EIO;
+	if (!(flag & O_CREAT))
+		goto out;
+
+	/* If we haven't found an entry and we have the O_CREAT flag set we want to
+	 * create a new entry write it out to the file and return its index
+	 */
+	mutex_lock(&sb->s_lock);
+	*uino = spd->usi_next_avail++;
+	err = __write_uin(spd, *uino, branchnum, inode_number);
+	if (err)
+		spd->usi_next_avail--;
+	mutex_unlock(&sb->s_lock);
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+int write_uin(struct super_block *sb, ino_t ino, int bindex, ino_t hidden_ino)
+{
+	int err;
+
+	print_entry_location();
+	err = __write_uin(stopd(sb), ino, bindex, hidden_ino);
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ * get_lin(ino_t inode_number)
+ * inode_number : inode number for the unionfs inode
+ * returns: the lower level inode# and branch#
+ */
+/* entry should use a poiner on the stack. should be staticly allocated one
+ * level up*/
+int get_lin(struct super_block *sb, ino_t inode_number, struct fmapent *entry)
+{
+	struct file *forwardmap;
+	loff_t seek_size;
+	mm_segment_t oldfs;
+	int err = 0, bytesread = 0;
+
+	print_entry_location();
+
+	if (!entry) {
+		entry = ERR_PTR(-ENOMEM);
+		goto out;
+	}
+	forwardmap = stopd(sb)->usi_forwardmap;
+	seek_size =
+	    sizeof(struct fmaphdr) + sizeof(struct bmapent[256]) +
+	    (sizeof(struct fmapent) * inode_number);
+	oldfs = get_fs();
+	set_fs(KERNEL_DS);
+	bytesread = __fread(forwardmap, entry, sizeof(*entry), &seek_size);
+	set_fs(oldfs);
+	if (bytesread != sizeof(*entry))
+		err = -EINVAL;
+
+      out:
+	print_exit_location();
+	return err;
+}
+
+/*
+ * remove_map(struct super_block *sb,int bindex)
+ *
+ * sb: The super block containing all the current imap info
+ * bindex: the index of the branch that is being removed.
+ *
+ * This assumes that end hasen't been decremented yet.
+ *
+ * Returns: This function really can't fail. The only thing
+ * that could possibly happen is that it will oops but that
+ * requires unionfs to be in an inconsistant state which
+ * shoulden't happen.
+ */
+int remove_map(struct super_block *sb, int bindex)
+{
+	int i;
+	struct unionfs_sb_info *spd;
+
+	print_entry_location();
+
+	spd = stopd(sb);
+	BUG_ON(!spd);
+
+	for (i = bindex; i < sbend(sb); i++) {
+		spd->usi_map_table[i] = spd->usi_map_table[i + 1];
+		spd->usi_bnum_table[i] = spd->usi_bnum_table[i + 1];
+	}
+	return 0;
+}
+
+#endif
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/print.c newtree/fs/unionfs/print.c
--- oldtree/fs/unionfs/print.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/print.c	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,439 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: print.c,v 1.77 2006/07/08 17:58:31 ezk Exp $
+ */
+
+/* Print debugging functions */
+
+#include "unionfs.h"
+
+static unsigned int debug_mask = DEFAULT_DEBUG_MASK;
+
+/* get value of debugging variable */
+unsigned int get_debug_mask(void)
+{
+	return debug_mask;
+}
+
+/* set debug level variable and return the previous value */
+int set_debug_mask(int val)
+{
+#ifdef UNIONFS_DEBUG
+	int prev = debug_mask;
+
+	debug_mask = val;
+
+	printk(KERN_INFO UNIONFS_NAME ": debug mask set to %u\n", debug_mask);
+
+	return prev;
+#else /* UNIONFS_DEBUG */
+	printk(KERN_WARNING UNIONFS_NAME ": debugging is not enabled\n");
+	return -ENOTSUPP;
+#endif /* ! UNIONFS_DEBUG */
+}
+
+static inline int should_print(const unsigned int req)
+{
+	return (req & debug_mask);
+}
+
+static void unionfs_print_generic_inode(const char *prefix,
+		const char *prefix2, const struct inode *inode)
+{
+	if (!inode) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PI:%s%s: NULL INODE PASSED!\n", prefix, prefix2);
+		return;
+	}
+
+	if (IS_ERR(inode)) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PI:%s%s: ERROR INODE PASSED: %ld\n", prefix, prefix2,
+		       PTR_ERR(inode));
+		return;
+	}
+
+	printk(KERN_DEBUG UNIONFS_NAME ": PI:%s%s: i_ino=%lu\n",
+			prefix, prefix2, inode->i_ino);
+	printk(KERN_DEBUG UNIONFS_NAME ": PI:%s%s: i_count=%u\n",
+			prefix, prefix2, atomic_read(&inode->i_count));
+	printk(KERN_DEBUG UNIONFS_NAME ": PI:%s%s: i_nlink=%u\n",
+			prefix, prefix2, inode->i_nlink);
+	printk(KERN_DEBUG UNIONFS_NAME ": PI:%s%s: i_mode=%o\n",
+			prefix, prefix2, inode->i_mode);
+	printk(KERN_DEBUG UNIONFS_NAME ": PI:%s%s: i_size=%llu\n",
+			prefix, prefix2, inode->i_size);
+	printk(KERN_DEBUG UNIONFS_NAME ": PI:%s%s: i_op=%p\n",
+			prefix, prefix2, inode->i_op);
+	printk(KERN_DEBUG UNIONFS_NAME ": PI:%s%s: i_sb=%p (%s)\n",
+			prefix, prefix2, inode->i_sb, (inode->i_sb ? sbt(inode->i_sb) : "NullTypeSB"));
+}
+
+void unionfs_print_inode(const unsigned int req, const char *prefix, const struct inode *inode)
+{
+	int bindex;
+
+	if (!should_print(req))
+		return;
+
+	if (!inode) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PI:%s: NULL INODE PASSED!\n", prefix);
+		return;
+	}
+	if (IS_ERR(inode)) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PI:%s: ERROR INODE PASSED: %ld\n", prefix, PTR_ERR(inode));
+		return;
+	}
+
+	unionfs_print_generic_inode(prefix, "", inode);
+
+	if (strcmp("unionfs", sbt(inode->i_sb))) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PI:%s: Not a " UNIONFS_NAME " inode.\n", prefix);
+		return;
+	}
+
+	if (!itopd(inode))
+		return;
+
+	printk(KERN_DEBUG UNIONFS_NAME ": PI:%s: ibstart=%d, ibend=%d\n", prefix, ibstart(inode), ibend(inode));
+
+	if (ibstart(inode) == -1)
+		return;
+
+	for (bindex = ibstart(inode); bindex <= ibend(inode); bindex++) {
+		struct inode *hidden_inode = itohi_index(inode, bindex);
+		char newstr[10];
+		if (!hidden_inode) {
+			printk(KERN_DEBUG UNIONFS_NAME ": PI:%s: HI#%d: NULL\n", prefix, bindex);
+			continue;
+		}
+		snprintf(newstr, 10, ": HI%d", bindex);
+		unionfs_print_generic_inode(prefix, newstr, hidden_inode);
+	}
+}
+
+static void unionfs_print_generic_file(const char *prefix, const char *prefix2,
+				     const struct file *file)
+{
+	printk(KERN_DEBUG UNIONFS_NAME ": PF:%s%s: f_dentry=0x%p\n", prefix, prefix2, file->f_dentry);
+
+	printk(KERN_DEBUG UNIONFS_NAME ": PF:%s%s: name=%s\n", prefix, prefix2, file->f_dentry->d_name.name);
+	if (file->f_dentry->d_inode) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PF:%s%s: f_dentry->d_inode->i_ino=%lu\n", prefix, prefix2, file->f_dentry->d_inode->i_ino);
+		printk(KERN_DEBUG UNIONFS_NAME ": PF:%s%s: f_dentry->d_inode->i_mode=%o\n", prefix, prefix2, file->f_dentry->d_inode->i_mode);
+	}
+	printk(KERN_DEBUG UNIONFS_NAME ": PF:%s%s: f_op=0x%p\n", prefix, prefix2, file->f_op);
+	printk(KERN_DEBUG UNIONFS_NAME ": PF:%s%s: f_mode=0x%x\n", prefix, prefix2, file->f_mode);
+	printk(KERN_DEBUG UNIONFS_NAME ": PF:%s%s: f_pos=0x%llu\n", prefix, prefix2, file->f_pos);
+	printk(KERN_DEBUG UNIONFS_NAME ": PF:%s%s: f_count=%u\n", prefix, prefix2, atomic_read(&file->f_count));
+	printk(KERN_DEBUG UNIONFS_NAME ": PF:%s%s: f_flags=0x%x\n", prefix, prefix2, file->f_flags);
+	printk(KERN_DEBUG UNIONFS_NAME ": PF:%s%s: f_version=%lu\n", prefix, prefix2, file->f_version);
+}
+
+void unionfs_print_file(const unsigned int req, const char *prefix, const struct file *file)
+{
+	struct file *hidden_file;
+
+	if (!should_print(req))
+		return;
+
+	if (!file) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PF:%s: NULL FILE PASSED!\n", prefix);
+		return;
+	}
+
+	unionfs_print_generic_file(prefix, "", file);
+
+	if (strcmp("unionfs", sbt(file->f_dentry->d_sb))) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PF:%s: Not a " UNIONFS_NAME " file.\n", prefix);
+		return;
+	}
+
+	if (ftopd(file)) {
+		int bindex;
+
+		printk(KERN_DEBUG UNIONFS_NAME ": PF:%s: fbstart=%d, fbend=%d\n", prefix, fbstart(file), fbend(file));
+
+		for (bindex = fbstart(file); bindex <= fbend(file); bindex++) {
+			char newstr[10];
+			hidden_file = ftohf_index(file, bindex);
+			if (!hidden_file) {
+				printk(KERN_DEBUG UNIONFS_NAME ": PF:%s: HF#%d is NULL\n", prefix, bindex);
+				continue;
+			}
+			snprintf(newstr, 10, ": HF%d", bindex);
+			unionfs_print_generic_file(prefix, newstr, hidden_file);
+		}
+	}
+}
+
+static char mode_to_type(mode_t mode)
+{
+	if (S_ISDIR(mode))
+		return 'd';
+	if (S_ISLNK(mode))
+		return 'l';
+	if (S_ISCHR(mode))
+		return 'c';
+	if (S_ISBLK(mode))
+		return 'b';
+	if (S_ISREG(mode))
+		return 'f';
+	return '?';
+}
+
+static void unionfs_print_generic_dentry(const char *prefix, const char *prefix2, const
+				 struct dentry *dentry, int check)
+{
+	if (!dentry) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: NULL DENTRY PASSED!\n", prefix, prefix2);
+		return;
+	}
+
+	if (IS_ERR(dentry)) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: ERROR DENTRY (%ld)!\n", prefix, prefix2,
+			    PTR_ERR(dentry));
+		return;
+	}
+
+	printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: dentry = %p\n", prefix, prefix2, dentry);
+	printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: d_count=%d\n", prefix, prefix2, atomic_read(&dentry->d_count));
+	printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: d_flags=%x\n", prefix, prefix2, (int)dentry->d_flags);
+	printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: d_name.name=\"%s\" (len = %d)\n", prefix, prefix2, dentry->d_name.name, dentry->d_name.len);
+	printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: d_sb=%p (%s)\n", prefix, prefix2, dentry->d_sb, sbt(dentry->d_sb));
+	printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: d_inode=%p\n", prefix, prefix2, dentry->d_inode);
+
+	if (dentry->d_inode) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: d_inode->i_ino=%ld (%s)\n", prefix, prefix2,
+			    dentry->d_inode->i_ino,
+			    sbt(dentry->d_inode->i_sb));
+		printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: dentry->d_inode->i_mode: %c%o\n", prefix,
+			    prefix2, mode_to_type(dentry->d_inode->i_mode),
+			    dentry->d_inode->i_mode);
+	}
+
+	printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: d_parent=%p (%s)\n", prefix, prefix2,
+		    dentry->d_parent,
+		    (dentry->d_parent ? sbt(dentry->d_parent->d_sb) : "nil"));
+	printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: d_parent->d_name.name=\"%s\"\n", prefix, prefix2,
+		    dentry->d_parent->d_name.name);
+	printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: d_parent->d_count=%d\n", prefix, prefix2,
+		    atomic_read(&dentry->d_parent->d_count));
+	printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: d_op=%p\n", prefix, prefix2, dentry->d_op);
+	printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: d_fsdata=%p\n", prefix, prefix2,
+		    dentry->d_fsdata);
+	printk(KERN_DEBUG UNIONFS_NAME ": PD:%s%s: hlist_unhashed(d_hash)=%d\n", prefix, prefix2,
+		    hlist_unhashed(&((struct dentry *)dentry)->d_hash));
+
+	/* After we have printed it, we can assert something about it. */
+	if (check)
+		BUG_ON(atomic_read(&dentry->d_count) <= 0);
+}
+
+static void __unionfs_print_dentry(const char *prefix, const struct dentry *dentry,
+			 int check)
+{
+	if (!dentry) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PD:%s: NULL DENTRY PASSED!\n", prefix);
+		return;
+	}
+
+	if (IS_ERR(dentry)) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PD:%s: ERROR DENTRY (%ld)!\n", prefix,
+			    PTR_ERR(dentry));
+		return;
+	}
+
+	unionfs_print_generic_dentry(prefix, "", dentry, check);
+
+	if (strcmp("unionfs", sbt(dentry->d_sb))) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PD:%s: Not a " UNIONFS_NAME " dentry.\n", prefix);
+		return;
+	}
+
+	if (!dtopd(dentry))
+		return;
+
+	printk(KERN_DEBUG UNIONFS_NAME ": PD:%s: dbstart=%d, dbend=%d, dbopaque=%d\n",
+		    prefix, dbstart(dentry), dbend(dentry), dbopaque(dentry));
+
+	if (dbstart(dentry) != -1) {
+		int bindex;
+		char newstr[10];
+		struct dentry *hidden_dentry;
+
+		for (bindex = dbstart(dentry); bindex <= dbend(dentry);
+		     bindex++) {
+			hidden_dentry = dtohd_index(dentry, bindex);
+			if (!hidden_dentry) {
+				printk(KERN_DEBUG UNIONFS_NAME ": PD:%s: HD#%d: NULL\n", prefix, bindex);
+				continue;
+			}
+			snprintf(newstr, 10, ": HD%d", bindex);
+			unionfs_print_generic_dentry(prefix, newstr, hidden_dentry, check);
+		}
+	}
+}
+
+void unionfs_print_dentry(const unsigned int req, const char *prefix, const struct dentry *dentry)
+{
+	if (!should_print(req))
+		return;
+
+	__unionfs_print_dentry(prefix, dentry, 1);
+}
+
+void unionfs_print_dentry_nocheck(const unsigned int req, const char *prefix, const struct dentry *dentry)
+{
+	if (!should_print(req))
+		return;
+
+	__unionfs_print_dentry(prefix, dentry, 0);
+}
+
+void unionfs_checkinode(const unsigned int req, const struct inode *inode, const char *msg)
+{
+	if (!should_print(req))
+		return;
+
+	if (!inode) {
+		printk(KERN_DEBUG UNIONFS_NAME ": unionfs_checkinode - inode is NULL! (%s)\n",
+		       msg);
+		return;
+	}
+
+	if (!itopd(inode)) {
+		printk(KERN_DEBUG UNIONFS_NAME ": unionfs_checkinode(%ld) - no private data (%s)\n",
+			    inode->i_ino, msg);
+		return;
+	}
+
+	if ((itopd(inode)->b_start < 0) || !itohi(inode)) {
+		printk(KERN_DEBUG UNIONFS_NAME
+			    "unionfs_checkinode(%ld) - underlying is NULL! (%s)\n",
+			    inode->i_ino, msg);
+		return;
+	}
+
+	if (!inode->i_sb) {
+		printk(KERN_DEBUG UNIONFS_NAME
+			    ": unionfs_checkinode(%ld) - inode->i_sb is NULL! (%s)\n",
+			    inode->i_ino, msg);
+		return;
+	}
+
+	printk(KERN_DEBUG UNIONFS_NAME ": inode->i_sb->s_type %p\n", inode->i_sb->s_type);
+	if (!inode->i_sb->s_type) {
+		printk(KERN_DEBUG UNIONFS_NAME
+			    ": unionfs_checkinode(%ld) - inode->i_sb->s_type is NULL! (%s)\n",
+			    inode->i_ino, msg);
+		return;
+	}
+
+	printk(KERN_DEBUG UNIONFS_NAME
+		    ": CI: %s: inode->i_count = %d, hidden_inode->i_count = %d, inode = %lu, sb = %s, hidden_sb = %s\n",
+		    msg, atomic_read(&inode->i_count),
+		    itopd(inode)->b_start >=
+		    0 ? atomic_read(&itohi(inode)->i_count) : -1, inode->i_ino,
+		    inode->i_sb->s_type->name,
+		    itopd(inode)->b_start >=
+		    0 ? itohi(inode)->i_sb->s_type->name : "(none)");
+}
+
+void unionfs_print_sb(const unsigned int req, const char *prefix, const struct super_block *sb)
+{
+	struct super_block *hidden_superblock;
+
+	if (!should_print(req))
+		return;
+
+	if (!sb) {
+		printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: NULL SB PASSED!\n", prefix);
+		return;
+	}
+
+	printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: s_blocksize=%lu\n", prefix, sb->s_blocksize);
+	printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: s_blocksize_bits=%u\n", prefix, sb->s_blocksize_bits);
+	printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: s_flags=0x%lx\n", prefix, sb->s_flags);
+	printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: s_magic=0x%lx\n", prefix, sb->s_magic);
+	printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: s_maxbytes=%llu\n", prefix, sb->s_maxbytes);
+	printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: s_count=%d\n", prefix, sb->s_count);
+	printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: s_active=%d\n", prefix, atomic_read(&sb->s_active));
+
+	if (stopd(sb))
+		printk(KERN_DEBUG UNIONFS_NAME ": sbstart=%d, sbend=%d\n", sbstart(sb),
+			    sbend(sb));
+
+	if (stopd(sb)) {
+		int bindex;
+		for (bindex = sbstart(sb); bindex <= sbend(sb); bindex++) {
+			hidden_superblock = stohs_index(sb, bindex);
+			if (!hidden_superblock) {
+				printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: HS#%d is NULL", prefix,
+					    bindex);
+				continue;
+			}
+
+			printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: HS#%d: s_blocksize=%lu\n", prefix, bindex,
+				    hidden_superblock->s_blocksize);
+			printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: HS#%d: s_blocksize_bits=%u\n", prefix, bindex,
+				    hidden_superblock->s_blocksize_bits);
+			printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: HS#%d: s_flags=0x%lx\n", prefix, bindex,
+				    hidden_superblock->s_flags);
+			printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: HS#%d: s_magic=0x%lx\n", prefix, bindex,
+				    hidden_superblock->s_magic);
+			printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: HS#%d: s_maxbytes=%llu\n", prefix, bindex,
+				    hidden_superblock->s_maxbytes);
+			printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: HS#%d: s_count=%d\n", prefix, bindex,
+				    hidden_superblock->s_count);
+			printk(KERN_DEBUG UNIONFS_NAME ": PSB:%s: HS#%d: s_active=%d\n", prefix, bindex,
+				    atomic_read(&hidden_superblock->s_active));
+		}
+	}
+}
+
+int unionfs_print(const unsigned int req, const char *fmt, ...)
+{
+        va_list ap;
+	int r;
+
+	if (!should_print(req))
+		return 0;
+
+	printk(KERN_DEBUG UNIONFS_NAME ": ");
+	va_start(ap, fmt);
+	r = vprintk(fmt, ap);
+	va_end(ap);
+
+	return r;
+}
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/rdstate.c newtree/fs/unionfs/rdstate.c
--- oldtree/fs/unionfs/rdstate.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/rdstate.c	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,330 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: rdstate.c,v 1.33 2006/06/01 03:11:03 jsipek Exp $
+ */
+
+#include "unionfs.h"
+
+/* This file contains the routines for maintaining readdir state. */
+/* There are two structures here, rdstate which is a hash table
+ * of the second structure which is a filldir_node. */
+
+/* This is a kmem_cache_t for filldir nodes, because we allocate a lot of them
+ * and they shouldn't waste memory.  If the node has a small name (as defined
+ * by the dentry structure), then we use an inline name to preserve kmalloc
+ * space. */
+static kmem_cache_t *unionfs_filldir_cachep;
+int init_filldir_cache(void)
+{
+	unionfs_filldir_cachep =
+	    kmem_cache_create("unionfs_filldir", sizeof(struct filldir_node), 0,
+			      SLAB_RECLAIM_ACCOUNT, NULL, NULL);
+
+	if (!unionfs_filldir_cachep)
+		return -ENOMEM;
+
+	return 0;
+}
+
+void destroy_filldir_cache(void)
+{
+	if (!unionfs_filldir_cachep)
+		return;
+	if (kmem_cache_destroy(unionfs_filldir_cachep)) {
+		printk(KERN_ERR
+		       "unionfs_filldir_cache: not all structures were freed\n");
+	}
+	return;
+}
+
+/* This is a tuning parameter that tells us roughly how big to make the
+ * hash table in directory entries per page.  This isn't perfect, but
+ * at least we get a hash table size that shouldn't be too overloaded.
+ * The following averages are based on my home directory.
+ * 14.44693	Overall
+ * 12.29	Single Page Directories
+ * 117.93	Multi-page directories
+ */
+#define DENTPAGE 4096
+#define DENTPERONEPAGE 12
+#define DENTPERPAGE 118
+#define MINHASHSIZE 1
+static int guesstimate_hash_size(struct inode *inode)
+{
+	struct inode *hidden_inode;
+	int bindex;
+	int hashsize = MINHASHSIZE;
+
+	if (itopd(inode)->uii_hashsize > 0)
+		return itopd(inode)->uii_hashsize;
+
+	for (bindex = ibstart(inode); bindex <= ibend(inode); bindex++) {
+		if (!(hidden_inode = itohi_index(inode, bindex)))
+			continue;
+
+		if (hidden_inode->i_size == DENTPAGE) {
+			hashsize += DENTPERONEPAGE;
+		} else {
+			hashsize +=
+			    (hidden_inode->i_size / DENTPAGE) * DENTPERPAGE;
+		}
+	}
+
+	return hashsize;
+}
+
+int init_rdstate(struct file *file)
+{
+	BUG_ON(sizeof(loff_t) != (sizeof(unsigned int) + sizeof(unsigned int)));
+	BUG_ON(ftopd(file)->rdstate != NULL);
+
+	ftopd(file)->rdstate =
+	    alloc_rdstate(file->f_dentry->d_inode, fbstart(file));
+	if (!ftopd(file)->rdstate)
+		return -ENOMEM;
+	return 0;
+}
+
+struct unionfs_dir_state *find_rdstate(struct inode *inode, loff_t fpos)
+{
+	struct unionfs_dir_state *rdstate = NULL;
+	struct list_head *pos;
+
+	print_entry("f_pos: %lld", fpos);
+	spin_lock(&itopd(inode)->uii_rdlock);
+	list_for_each(pos, &itopd(inode)->uii_readdircache) {
+		struct unionfs_dir_state *r =
+		    list_entry(pos, struct unionfs_dir_state, uds_cache);
+		if (fpos == rdstate2offset(r)) {
+			itopd(inode)->uii_rdcount--;
+			list_del(&r->uds_cache);
+			rdstate = r;
+			break;
+		}
+	}
+	spin_unlock(&itopd(inode)->uii_rdlock);
+	print_exit_pointer(rdstate);
+	return rdstate;
+}
+
+struct unionfs_dir_state *alloc_rdstate(struct inode *inode, int bindex)
+{
+	int i = 0;
+	int hashsize;
+	int mallocsize = sizeof(struct unionfs_dir_state);
+	struct unionfs_dir_state *rdstate;
+
+	hashsize = guesstimate_hash_size(inode);
+	mallocsize += hashsize * sizeof(struct list_head);
+	/* Round it up to the next highest power of two. */
+	mallocsize--;
+	mallocsize |= mallocsize >> 1;
+	mallocsize |= mallocsize >> 2;
+	mallocsize |= mallocsize >> 4;
+	mallocsize |= mallocsize >> 8;
+	mallocsize |= mallocsize >> 16;
+	mallocsize++;
+
+	/* This should give us about 500 entries anyway. */
+	if (mallocsize > PAGE_SIZE)
+		mallocsize = PAGE_SIZE;
+
+	hashsize =
+	    (mallocsize -
+	     sizeof(struct unionfs_dir_state)) / sizeof(struct list_head);
+
+	rdstate = KMALLOC(mallocsize, GFP_KERNEL);
+	if (!rdstate)
+		return NULL;
+
+	spin_lock(&itopd(inode)->uii_rdlock);
+	if (itopd(inode)->uii_cookie >= (MAXRDCOOKIE - 1))
+		itopd(inode)->uii_cookie = 1;
+	else
+		itopd(inode)->uii_cookie++;
+
+	rdstate->uds_cookie = itopd(inode)->uii_cookie;
+	spin_unlock(&itopd(inode)->uii_rdlock);
+	rdstate->uds_offset = 1;
+	rdstate->uds_access = jiffies;
+	rdstate->uds_bindex = bindex;
+	rdstate->uds_dirpos = 0;
+	rdstate->uds_hashentries = 0;
+	rdstate->uds_size = hashsize;
+	for (i = 0; i < rdstate->uds_size; i++)
+		INIT_LIST_HEAD(&rdstate->uds_list[i]);
+
+	return rdstate;
+}
+
+static void free_filldir_node(struct filldir_node *node)
+{
+	if (node->namelen >= DNAME_INLINE_LEN_MIN)
+		KFREE(node->name);
+	kmem_cache_free(unionfs_filldir_cachep, node);
+}
+
+void free_rdstate(struct unionfs_dir_state *state)
+{
+	struct filldir_node *tmp;
+	int i;
+
+	for (i = 0; i < state->uds_size; i++) {
+		struct list_head *head = &(state->uds_list[i]);
+		struct list_head *pos, *n;
+
+		/* traverse the list and deallocate space */
+		list_for_each_safe(pos, n, head) {
+			tmp = list_entry(pos, struct filldir_node, file_list);
+			list_del(&tmp->file_list);
+			free_filldir_node(tmp);
+		}
+	}
+
+	KFREE(state);
+}
+
+struct filldir_node *find_filldir_node(struct unionfs_dir_state *rdstate,
+				       const char *name, int namelen)
+{
+	int index;
+	unsigned int hash;
+	struct list_head *head;
+	struct list_head *pos;
+	struct filldir_node *cursor = NULL;
+	int found = 0;
+
+	/* If we print entry, we end up with spurious data. */
+	/* print_entry("name = %*s", namelen, name); */
+	print_entry_location();
+
+	BUG_ON(namelen <= 0);
+
+	hash = full_name_hash(name, namelen);
+	index = hash % rdstate->uds_size;
+
+	head = &(rdstate->uds_list[index]);
+	list_for_each(pos, head) {
+		cursor = list_entry(pos, struct filldir_node, file_list);
+
+		if (cursor->namelen == namelen && cursor->hash == hash
+		    && !strncmp(cursor->name, name, namelen)) {
+			/* a duplicate exists, and hence no need to create entry to the list */
+			found = 1;
+			/* if the duplicate is in this branch, then the file system is corrupted. */
+			if (cursor->bindex == rdstate->uds_bindex) {
+				//buf->error = err = -EIO;
+				dprint(PRINT_DEBUG,
+					    "Possible I/O error unionfs_filldir: a file is duplicated in the same branch %d: %s\n",
+					    rdstate->uds_bindex, cursor->name);
+			}
+			break;
+		}
+	}
+
+	if (!found) {
+		cursor = NULL;
+	}
+	print_exit_pointer(cursor);
+	return cursor;
+}
+
+inline struct filldir_node *alloc_filldir_node(const char *name, int namelen,
+					       unsigned int hash, int bindex)
+{
+	struct filldir_node *newnode;
+
+	newnode =
+	    (struct filldir_node *)kmem_cache_alloc(unionfs_filldir_cachep,
+						    SLAB_KERNEL);
+	if (!newnode)
+		goto out;
+
+      out:
+	return newnode;
+}
+
+int add_filldir_node(struct unionfs_dir_state *rdstate, const char *name,
+		     int namelen, int bindex, int whiteout)
+{
+	struct filldir_node *new;
+	unsigned int hash;
+	int index;
+	int err = 0;
+	struct list_head *head;
+
+	/* We can't print this because we end up Oopsing. */
+	/* print_entry("name = %*s", namelen, name); */
+	print_entry_location();
+
+	BUG_ON(namelen <= 0);
+
+	hash = full_name_hash(name, namelen);
+	index = hash % rdstate->uds_size;
+	head = &(rdstate->uds_list[index]);
+
+	new = alloc_filldir_node(name, namelen, hash, bindex);
+	if (!new) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	INIT_LIST_HEAD(&new->file_list);
+	new->namelen = namelen;
+	new->hash = hash;
+	new->bindex = bindex;
+	new->whiteout = whiteout;
+
+	if (namelen < DNAME_INLINE_LEN_MIN) {
+		new->name = new->iname;
+	} else {
+		new->name = (char *)KMALLOC(namelen + 1, GFP_KERNEL);
+		if (!new->name) {
+			kmem_cache_free(unionfs_filldir_cachep, new);
+			new = NULL;
+			goto out;
+		}
+	}
+
+	memcpy(new->name, name, namelen);
+	new->name[namelen] = '\0';
+
+	rdstate->uds_hashentries++;
+
+	list_add(&(new->file_list), head);
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/rename.c newtree/fs/unionfs/rename.c
--- oldtree/fs/unionfs/rename.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/rename.c	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,941 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: rename.c,v 1.46 2006/07/08 17:58:31 ezk Exp $
+ */
+
+#include "unionfs.h"
+
+static int do_rename(struct inode *old_dir, struct dentry *old_dentry,
+		     struct inode *new_dir, struct dentry *new_dentry,
+		     int bindex, struct dentry **wh_old)
+{
+	int err = 0;
+	struct dentry *hidden_old_dentry;
+	struct dentry *hidden_new_dentry;
+	struct dentry *hidden_old_dir_dentry;
+	struct dentry *hidden_new_dir_dentry;
+	struct dentry *hidden_wh_dentry;
+	struct dentry *hidden_wh_dir_dentry;
+	char *wh_name = NULL;
+
+	print_entry(" bindex=%d", bindex);
+
+	print_dentry("IN: do_rename, old_dentry", old_dentry);
+	print_dentry("IN: do_rename, new_dentry", new_dentry);
+	dprint(PRINT_DEBUG, "do_rename for bindex = %d\n", bindex);
+
+	hidden_new_dentry = dtohd_index(new_dentry, bindex);
+	hidden_old_dentry = dtohd_index(old_dentry, bindex);
+
+	if (!hidden_new_dentry) {
+		hidden_new_dentry =
+		    create_parents(new_dentry->d_parent->d_inode, new_dentry,
+				   bindex);
+		if (IS_ERR(hidden_new_dentry)) {
+			dprint(PRINT_DEBUG,
+				    "error creating directory tree for rename, bindex = %d\n",
+				    bindex);
+			err = PTR_ERR(hidden_new_dentry);
+			goto out;
+		}
+	}
+
+	wh_name = alloc_whname(new_dentry->d_name.name, new_dentry->d_name.len);
+	if (IS_ERR(wh_name)) {
+		err = PTR_ERR(wh_name);
+		goto out;
+	}
+
+	hidden_wh_dentry =
+	    LOOKUP_ONE_LEN(wh_name, hidden_new_dentry->d_parent,
+			   new_dentry->d_name.len + WHLEN);
+	if (IS_ERR(hidden_wh_dentry)) {
+		err = PTR_ERR(hidden_wh_dentry);
+		goto out;
+	}
+
+	if (hidden_wh_dentry->d_inode) {
+		/* get rid of the whiteout that is existing */
+		if (hidden_new_dentry->d_inode) {
+			printk(KERN_WARNING
+			       "Both a whiteout and a dentry exist when doing a rename!\n");
+			err = -EIO;
+
+			DPUT(hidden_wh_dentry);
+			goto out;
+		}
+
+		hidden_wh_dir_dentry = lock_parent(hidden_wh_dentry);
+		if (!(err = is_robranch_super(old_dentry->d_sb, bindex))) {
+			err =
+			    vfs_unlink(hidden_wh_dir_dentry->d_inode,
+				       hidden_wh_dentry);
+		}
+		DPUT(hidden_wh_dentry);
+		unlock_dir(hidden_wh_dir_dentry);
+		if (err)
+			goto out;
+	} else
+		DPUT(hidden_wh_dentry);
+
+	DGET(hidden_old_dentry);
+	hidden_old_dir_dentry = GET_PARENT(hidden_old_dentry);
+	hidden_new_dir_dentry = GET_PARENT(hidden_new_dentry);
+
+	lock_rename(hidden_old_dir_dentry, hidden_new_dir_dentry);
+
+	err = is_robranch_super(old_dentry->d_sb, bindex);
+	if (err)
+		goto out_unlock;
+
+	/* ready to whiteout for old_dentry.
+	   caller will create the actual whiteout,
+	   and must dput(*wh_old) */
+	if (wh_old) {
+		char *whname;
+		whname = alloc_whname(old_dentry->d_name.name,
+				      old_dentry->d_name.len);
+		err = PTR_ERR(whname);
+		if (IS_ERR(whname))
+			goto out_unlock;
+		*wh_old = LOOKUP_ONE_LEN(whname, hidden_old_dir_dentry,
+					 old_dentry->d_name.len + WHLEN);
+		KFREE(whname);
+		err = PTR_ERR(*wh_old);
+		if (IS_ERR(*wh_old)) {
+			*wh_old = NULL;
+			goto out_unlock;
+		}
+	}
+
+	print_dentry("NEWBEF", new_dentry);
+	print_dentry("OLDBEF", old_dentry);
+	err = vfs_rename(hidden_old_dir_dentry->d_inode, hidden_old_dentry,
+			 hidden_new_dir_dentry->d_inode, hidden_new_dentry);
+	print_dentry("NEWAFT", new_dentry);
+	print_dentry("OLDAFT", old_dentry);
+
+      out_unlock:
+	unlock_rename(hidden_old_dir_dentry, hidden_new_dir_dentry);
+
+	DPUT(hidden_old_dir_dentry);
+	DPUT(hidden_new_dir_dentry);
+	DPUT(hidden_old_dentry);
+
+      out:
+	if (!err) {
+		/* Fixup the newdentry. */
+		if (bindex < dbstart(new_dentry))
+			set_dbstart(new_dentry, bindex);
+		else if (bindex > dbend(new_dentry))
+			set_dbend(new_dentry, bindex);
+	}
+
+	KFREE(wh_name);
+
+	print_dentry("OUT: do_rename, old_dentry", old_dentry);
+	print_dentry("OUT: do_rename, new_dentry", new_dentry);
+
+	print_exit_status(err);
+	return err;
+}
+
+static int unionfs_rename_whiteout(struct inode *old_dir,
+				   struct dentry *old_dentry,
+				   struct inode *new_dir,
+				   struct dentry *new_dentry)
+{
+	int err = 0;
+	int bindex, bwh_old;
+	int old_bstart, old_bend;
+	int new_bstart, new_bend;
+	int do_copyup = -1;
+	struct dentry *parent_dentry;
+	int local_err = 0;
+	int eio = 0;
+	int revert = 0;
+	struct dentry *wh_old = NULL;
+
+	print_entry_location();
+
+	old_bstart = dbstart(old_dentry);
+	bwh_old = old_bstart;
+	old_bend = dbend(old_dentry);
+	parent_dentry = old_dentry->d_parent;
+
+	new_bstart = dbstart(new_dentry);
+	new_bend = dbend(new_dentry);
+
+	/* Rename source to destination. */
+	err = do_rename(old_dir, old_dentry, new_dir, new_dentry, old_bstart,
+			&wh_old);
+	if (err) {
+		if (!IS_COPYUP_ERR(err)) {
+			goto out;
+		}
+		do_copyup = old_bstart - 1;
+	} else {
+		revert = 1;
+	}
+
+	/* Unlink all instances of destination that exist to the left of
+	 * bstart of source. On error, revert back, goto out.
+	 */
+	for (bindex = old_bstart - 1; bindex >= new_bstart; bindex--) {
+		struct dentry *unlink_dentry;
+		struct dentry *unlink_dir_dentry;
+
+		unlink_dentry = dtohd_index(new_dentry, bindex);
+		if (!unlink_dentry) {
+			continue;
+		}
+
+		unlink_dir_dentry = lock_parent(unlink_dentry);
+		if (!(err = is_robranch_super(old_dir->i_sb, bindex))) {
+			err =
+			    vfs_unlink(unlink_dir_dentry->d_inode,
+				       unlink_dentry);
+		}
+
+		fist_copy_attr_times(new_dentry->d_parent->d_inode,
+				     unlink_dir_dentry->d_inode);
+		/* propagate number of hard-links */
+		new_dentry->d_parent->d_inode->i_nlink =
+		    get_nlinks(new_dentry->d_parent->d_inode);
+
+		unlock_dir(unlink_dir_dentry);
+		if (!err) {
+			if (bindex != new_bstart) {
+				DPUT(unlink_dentry);
+				set_dtohd_index(new_dentry, bindex, NULL);
+			}
+		} else if (IS_COPYUP_ERR(err)) {
+			do_copyup = bindex - 1;
+		} else if (revert) {
+			DPUT(wh_old);
+			goto revert;
+		}
+	}
+
+	if (do_copyup != -1) {
+		for (bindex = do_copyup; bindex >= 0; bindex--) {
+			/* copyup the file into some left directory, so that you can rename it */
+			err =
+			    copyup_dentry(old_dentry->d_parent->d_inode,
+					  old_dentry, old_bstart, bindex, NULL,
+					  old_dentry->d_inode->i_size);
+			if (!err) {
+				DPUT(wh_old);
+				bwh_old = bindex;
+				err =
+				    do_rename(old_dir, old_dentry, new_dir,
+					      new_dentry, bindex, &wh_old);
+				break;
+			}
+		}
+	}
+
+	/* make it opaque */
+	if (S_ISDIR(old_dentry->d_inode->i_mode)) {
+		err = make_dir_opaque(old_dentry, dbstart(old_dentry));
+		if (err)
+			goto revert;
+	}
+
+	/* Create whiteout for source, only if:
+	 * (1) There is more than one underlying instance of source.
+	 * (2) We did a copy_up
+	 */
+	if ((old_bstart != old_bend) || (do_copyup != -1)) {
+		struct dentry *hidden_parent;
+		BUG_ON(!wh_old || IS_ERR(wh_old) || wh_old->d_inode
+		       || bwh_old < 0);
+		hidden_parent = lock_parent(wh_old);
+		local_err = vfs_create(hidden_parent->d_inode, wh_old, S_IRUGO,
+				       NULL);
+		unlock_dir(hidden_parent);
+		if (!local_err)
+			set_dbopaque(old_dentry, bwh_old);
+		else {
+			/* We can't fix anything now, so we cop-out and use -EIO. */
+			printk
+			    ("<0>We can't create a whiteout for the source in rename!\n");
+			err = -EIO;
+		}
+	}
+
+      out:
+	DPUT(wh_old);
+	print_exit_status(err);
+	return err;
+
+      revert:
+	/* Do revert here. */
+	local_err = unionfs_refresh_hidden_dentry(new_dentry, old_bstart);
+	if (local_err) {
+		printk(KERN_WARNING
+		       "Revert failed in rename: the new refresh failed.\n");
+		eio = -EIO;
+	}
+
+	local_err = unionfs_refresh_hidden_dentry(old_dentry, old_bstart);
+	if (local_err) {
+		printk(KERN_WARNING
+		       "Revert failed in rename: the old refresh failed.\n");
+		eio = -EIO;
+		goto revert_out;
+	}
+
+	if (!dtohd_index(new_dentry, bindex)
+	    || !dtohd_index(new_dentry, bindex)->d_inode) {
+		printk(KERN_WARNING
+		       "Revert failed in rename: the object disappeared from under us!\n");
+		eio = -EIO;
+		goto revert_out;
+	}
+
+	if (dtohd_index(old_dentry, bindex)
+	    && dtohd_index(old_dentry, bindex)->d_inode) {
+		printk(KERN_WARNING
+		       "Revert failed in rename: the object was created underneath us!\n");
+		eio = -EIO;
+		goto revert_out;
+	}
+
+	local_err =
+	    do_rename(new_dir, new_dentry, old_dir, old_dentry, old_bstart,
+		      NULL);
+
+	/* If we can't fix it, then we cop-out with -EIO. */
+	if (local_err) {
+		printk(KERN_WARNING "Revert failed in rename!\n");
+		eio = -EIO;
+	}
+
+	local_err = unionfs_refresh_hidden_dentry(new_dentry, bindex);
+	if (local_err)
+		eio = -EIO;
+	local_err = unionfs_refresh_hidden_dentry(old_dentry, bindex);
+	if (local_err)
+		eio = -EIO;
+
+      revert_out:
+	if (eio)
+		err = eio;
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ * Unfortunately, we cannot simply call things like dbstart() in different
+ * places of the rename code because we move things around. So, we use this
+ * structure to pass the necessary information around to all the places that
+ * need it.
+ */
+struct rename_info {
+	int do_copyup;
+	int do_whiteout;
+	int rename_ok;
+
+	int old_bstart;
+	int old_bend;
+	int new_bstart;
+	int new_bend;
+
+	int isdir;		/* Is the source a directory? */
+	int clobber;		/* Are we clobbering the destination? */
+
+	int bwh_old;		/* where we create the whiteout */
+	struct dentry *wh_old;	/* lookup and set by do_rename() */
+};
+#ifdef UNIONFS_DELETE_ALL
+/*
+ * Rename all occurences of source except for the leftmost destination
+ */
+static int __rename_all(struct inode *old_dir, struct dentry *old_dentry,
+			struct inode *new_dir, struct dentry *new_dentry,
+			fd_set * success_mask, struct rename_info *info)
+{
+	int bindex;
+	int err = 0;
+
+	print_entry_location();
+
+	/* Loop through all the branches from right to left and rename all
+	 * instances of source to destination, except the leftmost destination
+	 */
+	for (bindex = info->old_bend; bindex >= info->old_bstart; bindex--) {
+		/* We don't rename if there is no source. */
+		if (dtohd_index(old_dentry, bindex) == NULL)
+			continue;
+
+		/* we rename the bstart of destination only at the last of
+		 * all operations, so that we don't lose it on error
+		 */
+		if (info->clobber && (bindex == info->new_bstart))
+			continue;
+
+		DPUT(info->wh_old);
+		info->bwh_old = bindex;
+		/* We shouldn't have a handle on this if there is no inode. */
+		err =
+		    do_rename(old_dir, old_dentry, new_dir, new_dentry, bindex,
+			      &info->wh_old);
+		if (!err) {
+			/* For reverting. */
+			FD_SET(bindex, success_mask);
+			/* So we know not to copyup on failures the right */
+			info->rename_ok = bindex;
+		} else if (IS_COPYUP_ERR(err)) {
+			if (info->isdir) {
+				err = -EXDEV;
+				break;
+			}
+
+			/* we need a whiteout... */
+			info->do_whiteout = bindex - 1;
+
+			if (bindex == info->old_bstart)
+				/* ...and a copyup */
+				info->do_copyup = bindex - 1;
+
+			err = 0;	/* reset error */
+		} else
+			break;	/* error is set by do_rename */
+	}
+
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ * Unlink all destinations (if they exist) to the left of the left-most
+ * source
+ */
+static int __rename_all_unlink(struct inode *old_dir, struct dentry *old_dentry,
+			       struct inode *new_dir, struct dentry *new_dentry,
+			       struct rename_info *info)
+{
+	int bindex;
+
+	struct dentry *unlink_dentry;
+	struct dentry *unlink_dir_dentry;
+
+	int err = 0;
+
+	print_entry_location();
+
+	for (bindex = info->old_bstart - 1; bindex > info->new_bstart; bindex--) {
+		unlink_dentry = dtohd_index(new_dentry, bindex);
+		if (!unlink_dentry)
+			continue;
+
+		/* lock, unlink if possible, copyup times, unlock */
+		unlink_dir_dentry = lock_parent(unlink_dentry);
+		if (!(err = is_robranch_super(old_dir->i_sb, bindex)))
+			err =
+			    vfs_unlink(unlink_dir_dentry->d_inode,
+				       unlink_dentry);
+
+		fist_copy_attr_times(new_dentry->d_parent->d_inode,
+				     unlink_dir_dentry->d_inode);
+		new_dentry->d_parent->d_inode->i_nlink =
+		    get_nlinks(new_dentry->d_parent->d_inode);
+
+		unlock_dir(unlink_dir_dentry);
+
+		if (!err) {
+			if (bindex != info->new_bstart) {
+				DPUT(unlink_dentry);
+				set_dtohd_index(new_dentry, bindex, NULL);
+			}
+		} else if (IS_COPYUP_ERR(err)) {
+			if (info->isdir) {
+				err = -EXDEV;
+				break;
+			}
+			info->do_copyup = bindex - 1;
+
+			err = 0;	/* reset error */
+		} else
+			break;	/* err is set by is_ro_branch_super or vfs_unlink */
+	}
+
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ * Try to revert everything we have done in __rename_all and __rename_all_unlink
+ */
+static int __rename_all_revert(struct inode *old_dir, struct dentry *old_dentry,
+			       struct inode *new_dir, struct dentry *new_dentry,
+			       fd_set * success_mask, struct rename_info *info)
+{
+	int bindex;
+
+	int err;
+	int eio = 0;
+
+	print_entry_location();
+
+	for (bindex = info->old_bstart; bindex <= info->old_bend; bindex++) {
+		if (!FD_ISSET(bindex, success_mask))
+			continue;
+
+		err = unionfs_refresh_hidden_dentry(new_dentry, bindex);
+		if (err) {
+			printk(KERN_WARNING "Revert failed in rename: "
+			       "the new refresh failed.\n");
+			eio = -EIO;
+		}
+
+		err = unionfs_refresh_hidden_dentry(old_dentry, bindex);
+		if (err) {
+			printk(KERN_WARNING "Revert failed in rename: "
+			       "the old refresh failed.\n");
+			eio = -EIO;
+			continue;
+		}
+
+		if (!dtohd_index(new_dentry, bindex)
+		    || !dtohd_index(new_dentry, bindex)->d_inode) {
+			printk(KERN_WARNING "Revert failed in rename: "
+			       "the object disappeared from under us!\n");
+			eio = -EIO;
+			continue;
+		}
+
+		if (dtohd_index(old_dentry, bindex)
+		    && dtohd_index(old_dentry, bindex)->d_inode) {
+			printk(KERN_WARNING "Revert failed in rename: "
+			       "the object was created underneath us!\n");
+			eio = -EIO;
+			continue;
+		}
+
+		err =
+		    do_rename(new_dir, new_dentry, old_dir, old_dentry, bindex,
+			      NULL);
+		/* If we can't fix it, then we cop-out with -EIO. */
+		if (err) {
+			printk(KERN_WARNING "Revert failed in rename!\n");
+			eio = -EIO;
+		}
+
+		err = unionfs_refresh_hidden_dentry(new_dentry, bindex);
+		if (err)
+			eio = -EIO;
+		err = unionfs_refresh_hidden_dentry(old_dentry, bindex);
+		if (err)
+			eio = -EIO;
+	}
+
+	print_exit_status(eio);
+	return eio;
+}
+
+/*
+ * Finish off the rename, by either over writing the last destination or
+ * unlinking the last destination to the left of us
+ */
+static int __rename_all_clobber(struct inode *old_dir,
+				struct dentry *old_dentry,
+				struct inode *new_dir,
+				struct dentry *new_dentry,
+				struct rename_info *info)
+{
+	int err = 0;
+
+	print_entry_location();
+
+	if (dtohd_index(old_dentry, info->new_bstart)) {
+		/* rename the last source, knowing we're overwriting something */
+		DPUT(info->wh_old);
+		info->bwh_old = info->new_bstart;
+		err =
+		    do_rename(old_dir, old_dentry, new_dir, new_dentry,
+			      info->new_bstart, &info->wh_old);
+		if (IS_COPYUP_ERR(err)) {
+			if (info->isdir) {
+				err = -EXDEV;
+				goto out;
+			}
+			if (info->rename_ok > info->new_bstart) {
+				if ((info->do_copyup == -1)
+				    || (info->new_bstart - 1 < info->do_copyup))
+					info->do_copyup = info->new_bstart - 1;
+			}
+			if ((info->do_whiteout == -1)
+			    || (info->new_bstart - 1 < info->do_whiteout)) {
+				info->do_whiteout = info->new_bstart - 1;
+			}
+			err = 0;	// reset error
+		}
+	} else if (info->new_bstart < info->old_bstart) {
+		/* the newly renamed file would get hidden, let's unlink the
+		 * file to the left of it */
+		struct dentry *unlink_dentry;
+		struct dentry *unlink_dir_dentry;
+
+		unlink_dentry = dtohd_index(new_dentry, info->new_bstart);
+
+		unlink_dir_dentry = lock_parent(unlink_dentry);
+		if (!(err = is_robranch_super(old_dir->i_sb, info->new_bstart)))
+			err = vfs_unlink(unlink_dir_dentry->d_inode,
+					 unlink_dentry);
+
+		fist_copy_attr_times(new_dentry->d_parent->d_inode,
+				     unlink_dir_dentry->d_inode);
+		new_dentry->d_parent->d_inode->i_nlink =
+		    get_nlinks(new_dentry->d_parent->d_inode);
+
+		unlock_dir(unlink_dir_dentry);
+
+		if (IS_COPYUP_ERR(err)) {
+			if (info->isdir) {
+				err = -EXDEV;
+				goto out;
+			}
+			if ((info->do_copyup == -1)
+			    || (info->new_bstart - 1 < info->do_copyup))
+				info->do_copyup = info->new_bstart - 1;
+
+			err = 0;	// reset error
+		}
+	}
+
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ * The function is nasty, nasty, nasty, but so is rename. :(
+ */
+static int unionfs_rename_all(struct inode *old_dir, struct dentry *old_dentry,
+			      struct inode *new_dir, struct dentry *new_dentry)
+{
+	struct dentry *parent_dentry = NULL;
+	int err = 0;
+	int eio;
+
+	/* These variables control error handling. */
+	fd_set success_mask;
+	char *name = NULL;
+
+	/* unfortunately, we have to resort to this, because dbstart/dbend would
+	   return different things in different place of the rename code */
+	struct rename_info info;
+
+	info.rename_ok = FD_SETSIZE;	/* The last rename that is ok. */
+	info.do_copyup = -1;	/* Where we should start copyup. */
+	info.do_whiteout = -1;	/* Where we should start whiteouts of the source. */
+	info.wh_old = NULL;
+	info.bwh_old = -1;
+
+	print_entry_location();
+
+	parent_dentry = old_dentry->d_parent;
+	name = KMALLOC(old_dentry->d_name.len + 1, GFP_KERNEL);
+	if (!name) {
+		err = -ENOMEM;
+		goto out;
+	}
+	strncpy(name, old_dentry->d_name.name, old_dentry->d_name.len + 1);
+
+	info.new_bstart = dbstart(new_dentry);
+	info.new_bend = dbend(new_dentry);
+
+	info.old_bstart = dbstart(old_dentry);
+	info.old_bend = dbend(old_dentry);
+
+	BUG_ON(info.new_bstart < 0);
+	BUG_ON(info.old_bstart < 0);
+
+	/* The failure mask only can deal with FD_SETSIZE entries. */
+	BUG_ON(info.old_bend > FD_SETSIZE);
+	BUG_ON(info.new_bend > FD_SETSIZE);
+	FD_ZERO(&success_mask);
+
+	/* Life is simpler if the dentry doesn't exist. */
+	info.clobber =
+	    (dtohd_index(new_dentry, info.new_bstart)->d_inode) ? 1 : 0;
+	info.isdir = S_ISDIR(old_dentry->d_inode->i_mode);
+
+	/* rename everything we can */
+	err =
+	    __rename_all(old_dir, old_dentry, new_dir, new_dentry,
+			 &success_mask, &info);
+	if (err)
+		goto revert;
+
+	/* unlink destinations even further left */
+	err =
+	    __rename_all_unlink(old_dir, old_dentry, new_dir, new_dentry,
+				&info);
+	if (err)
+		goto revert;
+
+	if (info.clobber) {
+		/* Now we need to handle the leftmost of the destination. */
+		err =
+		    __rename_all_clobber(old_dir, old_dentry, new_dir,
+					 new_dentry, &info);
+		if (err)
+			goto revert;
+	}
+
+	/* Copy up if necessary */
+	if (info.do_copyup != -1) {
+		int bindex;
+
+		for (bindex = info.do_copyup; bindex >= 0; bindex--) {
+			err =
+			    copyup_dentry(old_dentry->d_parent->d_inode,
+					  old_dentry, info.old_bstart, bindex,
+					  NULL, old_dentry->d_inode->i_size);
+			if (!err) {
+				DPUT(info.wh_old);
+				info.bwh_old = bindex;
+				err =
+				    do_rename(old_dir, old_dentry, new_dir,
+					      new_dentry, bindex, &info.wh_old);
+				break;
+			}
+		}
+	}
+
+	/* make it opaque */
+	if (S_ISDIR(old_dentry->d_inode->i_mode)) {
+		err = make_dir_opaque(old_dentry, dbstart(old_dentry));
+		if (err)
+			goto revert;
+	}
+
+	/* Create a whiteout for the source. */
+	if (info.do_whiteout != -1) {
+		struct dentry *hidden_parent;
+		BUG_ON(info.do_whiteout < 0
+		       || !info.wh_old || IS_ERR(info.wh_old)
+		       || info.wh_old->d_inode || info.bwh_old < 0);
+		hidden_parent = lock_parent(info.wh_old);
+		err = vfs_create(hidden_parent->d_inode, info.wh_old, S_IRUGO,
+				 NULL);
+		unlock_dir(hidden_parent);
+		if (!err)
+			set_dbopaque(old_dentry, info.bwh_old);
+		else {
+			/* We can't fix anything now, so we -EIO. */
+			printk(KERN_WARNING "We can't create a whiteout for the"
+			       "source in rename!\n");
+			err = -EIO;
+			goto out;
+		}
+	}
+
+	/* We are at the point where reverting doesn't happen. */
+	goto out;
+
+      revert:
+	/* something bad happened, try to revert */
+	eio =
+	    __rename_all_revert(old_dir, old_dentry, new_dir, new_dentry,
+				&success_mask, &info);
+	if (eio)
+		err = eio;
+
+      out:
+	DPUT(info.wh_old);
+	KFREE(name);
+	print_exit_status(err);
+	return err;
+}
+#endif
+
+static struct dentry *lookup_whiteout(struct dentry *dentry)
+{
+	char *whname;
+	int bindex = -1, bstart = -1, bend = -1;
+	struct dentry *parent, *hidden_parent, *wh_dentry;
+
+	whname = alloc_whname(dentry->d_name.name, dentry->d_name.len);
+	if (IS_ERR(whname))
+		return (void *)whname;
+
+	parent = GET_PARENT(dentry);
+	lock_dentry(parent);
+	bstart = dbstart(parent);
+	bend = dbend(parent);
+	wh_dentry = ERR_PTR(-ENOENT);
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		hidden_parent = dtohd_index(parent, bindex);
+		if (!hidden_parent)
+			continue;
+		wh_dentry =
+		    LOOKUP_ONE_LEN(whname, hidden_parent,
+				   dentry->d_name.len + WHLEN);
+		if (IS_ERR(wh_dentry))
+			continue;
+		if (wh_dentry->d_inode)
+			break;
+		DPUT(wh_dentry);
+		wh_dentry = ERR_PTR(-ENOENT);
+	}
+	unlock_dentry(parent);
+	DPUT(parent);
+	KFREE(whname);
+	return wh_dentry;
+}
+
+/* We can't copyup a directory, because it may involve huge
+ * numbers of children, etc.  Doing that in the kernel would
+ * be bad, so instead we let the userspace recurse and ask us
+ * to copy up each file separately
+ */
+static int may_rename_dir(struct dentry *dentry)
+{
+	int err, bstart;
+
+	err = check_empty(dentry, NULL);
+	if (err == -ENOTEMPTY) {
+		if (is_robranch(dentry))
+			return -EXDEV;
+	} else if (err)
+		return err;
+
+	bstart = dbstart(dentry);
+	if (dbend(dentry) == bstart || dbopaque(dentry) == bstart)
+		return 0;
+
+	set_dbstart(dentry, bstart + 1);
+	err = check_empty(dentry, NULL);
+	set_dbstart(dentry, bstart);
+	if (err == -ENOTEMPTY)
+		err = -EXDEV;
+	return err;
+}
+
+int unionfs_rename(struct inode *old_dir, struct dentry *old_dentry,
+		   struct inode *new_dir, struct dentry *new_dentry)
+{
+	int err = 0;
+	struct dentry *wh_dentry;
+
+	print_entry_location();
+
+	double_lock_dentry(old_dentry, new_dentry);
+
+	checkinode(old_dir, "unionfs_rename-old_dir");
+	checkinode(new_dir, "unionfs_rename-new_dir");
+	print_dentry("IN: unionfs_rename, old_dentry", old_dentry);
+	print_dentry("IN: unionfs_rename, new_dentry", new_dentry);
+
+	if (!S_ISDIR(old_dentry->d_inode->i_mode))
+		err = unionfs_partial_lookup(old_dentry);
+	else
+		err = may_rename_dir(old_dentry);
+
+	if (err)
+		goto out;
+
+	err = unionfs_partial_lookup(new_dentry);
+	if (err)
+		goto out;
+
+	/*
+	 * if new_dentry is already hidden because of whiteout,
+	 * simply override it even if the whiteouted dir is not empty.
+	 */
+	wh_dentry = lookup_whiteout(new_dentry);
+	if (!IS_ERR(wh_dentry))
+		DPUT(wh_dentry);
+	else if (new_dentry->d_inode) {
+		if (S_ISDIR(old_dentry->d_inode->i_mode) !=
+		    S_ISDIR(new_dentry->d_inode->i_mode)) {
+			err =
+			    S_ISDIR(old_dentry->d_inode->
+				    i_mode) ? -ENOTDIR : -EISDIR;
+			goto out;
+		}
+
+		if (S_ISDIR(new_dentry->d_inode->i_mode)) {
+			struct unionfs_dir_state *namelist;
+			/* check if this unionfs directory is empty or not */
+			err = check_empty(new_dentry, &namelist);
+			if (err)
+				goto out;
+
+			if (!is_robranch(new_dentry))
+				err = delete_whiteouts(new_dentry,
+						dbstart(new_dentry),
+						namelist);
+
+			free_rdstate(namelist);
+
+			if (err)
+				goto out;
+		}
+	}
+#ifdef UNIONFS_DELETE_ALL
+	if (IS_SET(old_dir->i_sb, DELETE_ALL))
+		err = unionfs_rename_all(old_dir, old_dentry, new_dir,
+					 new_dentry);
+	else
+#endif
+		err = unionfs_rename_whiteout(old_dir, old_dentry, new_dir,
+					      new_dentry);
+
+      out:
+	checkinode(new_dir, "post unionfs_rename-new_dir");
+	print_dentry("OUT: unionfs_rename, old_dentry", old_dentry);
+
+	if (err) {
+		/* clear the new_dentry stuff created */
+		d_drop(new_dentry);
+	} else {
+		/* force re-lookup since the dir on ro branch is not renamed,
+		   and hidden dentries still indicate the un-renamed ones. */
+		if (S_ISDIR(old_dentry->d_inode->i_mode))
+			atomic_dec(&dtopd(old_dentry)->udi_generation);
+		print_dentry("OUT: unionfs_rename, new_dentry",
+				  new_dentry);
+	}
+
+	unlock_dentry(new_dentry);
+	unlock_dentry(old_dentry);
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/stale_inode.c newtree/fs/unionfs/stale_inode.c
--- oldtree/fs/unionfs/stale_inode.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/stale_inode.c	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,134 @@
+/*
+ *  Adpated from linux/fs/bad_inode.c
+ *
+ *  Copyright (C) 1997, Stephen Tweedie
+ *
+ *  Provide stub functions for "stale" inodes, a bit friendlier than the
+ *  -EIO that bad_inode.c does.
+ */
+/*
+ *  $Id: stale_inode.c,v 1.13 2006/03/21 09:22:11 jsipek Exp $
+ */
+
+#include <linux/config.h>
+#include <linux/version.h>
+
+#include <linux/fs.h>
+#include <linux/stat.h>
+#include <linux/sched.h>
+
+static struct address_space_operations unionfs_stale_aops;
+
+/* declarations for "sparse */
+extern struct inode_operations stale_inode_ops;
+
+/*
+ * The follow_link operation is special: it must behave as a no-op
+ * so that a stale root inode can at least be unmounted. To do this
+ * we must dput() the base and return the dentry with a dget().
+ */
+static void *stale_follow_link(struct dentry *dent, struct nameidata *nd)
+{
+	int err = vfs_follow_link(nd, ERR_PTR(-ESTALE));
+	return ERR_PTR(err);
+}
+
+static int return_ESTALE(void)
+{
+	return -ESTALE;
+}
+
+#define ESTALE_ERROR ((void *) (return_ESTALE))
+
+static struct file_operations stale_file_ops = {
+	.llseek = ESTALE_ERROR,
+	.read = ESTALE_ERROR,
+	.write = ESTALE_ERROR,
+	.readdir = ESTALE_ERROR,
+	.poll = ESTALE_ERROR,
+	.ioctl = ESTALE_ERROR,
+	.mmap = ESTALE_ERROR,
+	.open = ESTALE_ERROR,
+	.flush = ESTALE_ERROR,
+	.release = ESTALE_ERROR,
+	.fsync = ESTALE_ERROR,
+	.fasync = ESTALE_ERROR,
+	.lock = ESTALE_ERROR,
+};
+
+struct inode_operations stale_inode_ops = {
+	.create = ESTALE_ERROR,
+	.lookup = ESTALE_ERROR,
+	.link = ESTALE_ERROR,
+	.unlink = ESTALE_ERROR,
+	.symlink = ESTALE_ERROR,
+	.mkdir = ESTALE_ERROR,
+	.rmdir = ESTALE_ERROR,
+	.mknod = ESTALE_ERROR,
+	.rename = ESTALE_ERROR,
+	.readlink = ESTALE_ERROR,
+	.follow_link = stale_follow_link,
+	.truncate = ESTALE_ERROR,
+	.permission = ESTALE_ERROR,
+};
+
+/*
+ * When a filesystem is unable to read an inode due to an I/O error in
+ * its read_inode() function, it can call make_stale_inode() to return a
+ * set of stubs which will return ESTALE errors as required.
+ *
+ * We only need to do limited initialisation: all other fields are
+ * preinitialised to zero automatically.
+ */
+
+/**
+ *	make_stale_inode - mark an inode stale due to an I/O error
+ *	@inode: Inode to mark stale
+ *
+ *	When an inode cannot be read due to a media or remote network
+ *	failure this function makes the inode "stale" and causes I/O operations
+ *	on it to fail from this point on.
+ */
+
+void make_stale_inode(struct inode *inode)
+{
+	inode->i_mode = S_IFREG;
+	inode->i_atime = inode->i_mtime = inode->i_ctime = CURRENT_TIME;
+	inode->i_op = &stale_inode_ops;
+	inode->i_fop = &stale_file_ops;
+	inode->i_mapping->a_ops = &unionfs_stale_aops;
+}
+
+/*
+ * This tests whether an inode has been flagged as stale. The test uses
+ * &stale_inode_ops to cover the case of invalidated inodes as well as
+ * those created by make_stale_inode() above.
+ */
+
+/**
+ *	is_stale_inode - is an inode errored
+ *	@inode: inode to test
+ *
+ *	Returns true if the inode in question has been marked as stale.
+ */
+
+int is_stale_inode(struct inode *inode)
+{
+	return (inode->i_op == &stale_inode_ops);
+}
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/subr.c newtree/fs/unionfs/subr.c
--- oldtree/fs/unionfs/subr.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/subr.c	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,253 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: subr.c,v 1.139 2006/06/01 03:11:03 jsipek Exp $
+ */
+
+#include "unionfs.h"
+#include <linux/security.h>
+
+/* Pass an unionfs dentry and an index.  It will try to create a whiteout
+ * for the filename in dentry, and will try in branch 'index'.  On error,
+ * it will proceed to a branch to the left.
+ */
+int create_whiteout(struct dentry *dentry, int start)
+{
+	int bstart, bend, bindex;
+	struct dentry *hidden_dir_dentry;
+	struct dentry *hidden_dentry;
+	struct dentry *hidden_wh_dentry;
+	char *name = NULL;
+	int err = -EINVAL;
+
+	print_entry("start = %d", start);
+
+	verify_locked(dentry);
+
+	print_dentry("IN create_whiteout", dentry);
+	bstart = dbstart(dentry);
+	bend = dbend(dentry);
+
+	/* create dentry's whiteout equivalent */
+	name = alloc_whname(dentry->d_name.name, dentry->d_name.len);
+	if (IS_ERR(name)) {
+		err = PTR_ERR(name);
+		goto out;
+	}
+
+	for (bindex = start; bindex >= 0; bindex--) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+
+		if (!hidden_dentry) {
+			/* if hidden dentry is not present, create the entire
+			 * hidden dentry directory structure and go ahead.
+			 * Since we want to just create whiteout, we only want
+			 * the parent dentry, and hence get rid of this dentry.
+			 */
+			hidden_dentry = create_parents(dentry->d_inode,
+						       dentry, bindex);
+			if (!hidden_dentry || IS_ERR(hidden_dentry)) {
+				dprint(PRINT_DEBUG_WHITEOUT,
+					"create_parents failed for bindex = %d\n",
+					bindex);
+				continue;
+			}
+		}
+		hidden_wh_dentry =
+		    LOOKUP_ONE_LEN(name, hidden_dentry->d_parent,
+				   dentry->d_name.len + WHLEN);
+		if (IS_ERR(hidden_wh_dentry))
+			continue;
+
+		/* The whiteout already exists. This used to be impossible, but
+		 * now is possible because of opaqueness. */
+		if (hidden_wh_dentry->d_inode) {
+			DPUT(hidden_wh_dentry);
+			err = 0;
+			goto out;
+		}
+
+		hidden_dir_dentry = lock_parent(hidden_wh_dentry);
+		if (!(err = is_robranch_super(dentry->d_sb, bindex))) {
+			err =
+			    vfs_create(hidden_dir_dentry->d_inode,
+				       hidden_wh_dentry,
+				       ~current->fs->umask & S_IRWXUGO, NULL);
+
+		}
+		unlock_dir(hidden_dir_dentry);
+		DPUT(hidden_wh_dentry);
+
+		if (!err)
+			break;
+
+		if (!IS_COPYUP_ERR(err))
+			break;
+	}
+
+	/* set dbopaque  so that lookup will not proceed after this branch */
+	if (!err)
+		set_dbopaque(dentry, bindex);
+
+	print_dentry("OUT create_whiteout", dentry);
+      out:
+	KFREE(name);
+	print_exit_status(err);
+	return err;
+}
+
+/* This is a helper function for rename, which ends up with hosed over dentries
+ * when it needs to revert. */
+int unionfs_refresh_hidden_dentry(struct dentry *dentry, int bindex)
+{
+	struct dentry *hidden_dentry;
+	struct dentry *hidden_parent;
+	int err = 0;
+
+	print_entry(" bindex = %d", bindex);
+
+	verify_locked(dentry);
+	lock_dentry(dentry->d_parent);
+	hidden_parent = dtohd_index(dentry->d_parent, bindex);
+	unlock_dentry(dentry->d_parent);
+
+	BUG_ON(!S_ISDIR(hidden_parent->d_inode->i_mode));
+
+	hidden_dentry =
+	    LOOKUP_ONE_LEN(dentry->d_name.name, hidden_parent,
+			   dentry->d_name.len);
+	if (IS_ERR(hidden_dentry)) {
+		err = PTR_ERR(hidden_dentry);
+		goto out;
+	}
+
+	if (dtohd_index(dentry, bindex))
+		DPUT(dtohd_index(dentry, bindex));
+	if (itohi_index(dentry->d_inode, bindex)) {
+		IPUT(itohi_index(dentry->d_inode, bindex));
+		set_itohi_index(dentry->d_inode, bindex, NULL);
+	}
+	if (!hidden_dentry->d_inode) {
+		DPUT(hidden_dentry);
+		set_dtohd_index(dentry, bindex, NULL);
+	} else {
+		set_dtohd_index(dentry, bindex, hidden_dentry);
+		set_itohi_index(dentry->d_inode, bindex,
+				IGRAB(hidden_dentry->d_inode));
+	}
+
+      out:
+	print_exit_status(err);
+	return err;
+}
+
+/* you should call superio_restore() asap */
+void superio_store(struct superio *sio)
+{
+	int err;
+	kernel_cap_t super, drop;
+	struct rlimit *rl;
+
+	drop = cap_t(CAP_TO_MASK(CAP_SYS_RESOURCE)
+		     | CAP_TO_MASK(CAP_SETPCAP)
+		     | CAP_TO_MASK(CAP_SETUID));
+
+	write_lock(&tasklist_lock);	/* task_capability_lock is not exported */
+	err = security_capget(current, &sio->cap.effective,
+			      &sio->cap.inheritable, &sio->cap.permitted);
+	if (err) {
+		write_unlock(&tasklist_lock);
+		printk(KERN_WARNING UNIONFS_NAME
+		       ": %s[%d]: security_capget failed\n",
+		       current->comm, current->pid);
+		return;
+	}
+	super = cap_combine(sio->cap.effective, CAP_FS_MASK);
+	super = cap_drop(super, drop);
+	security_capset_set(current, &super, &sio->cap.inheritable,
+			    &sio->cap.permitted);
+	write_unlock(&tasklist_lock);
+
+	rl = current->signal->rlim + RLIMIT_CORE;
+	task_lock(current->group_leader);	//??
+	sio->rlim_core = rl->rlim_cur;
+	rl->rlim_cur = 0;
+	task_unlock(current->group_leader);
+
+	sio->fsuid = current->fsuid;
+	current->fsuid = 0;
+}
+
+void superio_revert(struct superio *sio)
+{
+	current->fsuid = sio->fsuid;
+	task_lock(current->group_leader);	//??
+	current->signal->rlim[RLIMIT_CORE].rlim_cur = sio->rlim_core;
+	task_unlock(current->group_leader);
+
+	write_lock(&tasklist_lock);	/* task_capability_lock is not exported */
+	security_capset_set(current, &sio->cap.effective,
+			    &sio->cap.inheritable, &sio->cap.permitted);
+	write_unlock(&tasklist_lock);
+}
+
+int make_dir_opaque(struct dentry *dentry, int bindex)
+{
+	int err;
+	struct dentry *hidden_dentry, *diropq;
+	struct inode *hidden_dir;
+
+	hidden_dentry = dtohd_index(dentry, bindex);
+	hidden_dir = hidden_dentry->d_inode;
+	BUG_ON(!S_ISDIR(dentry->d_inode->i_mode)
+	       || !S_ISDIR(hidden_dir->i_mode));
+
+	mutex_lock(&hidden_dir->i_mutex);
+	diropq = LOOKUP_ONE_LEN(UNIONFS_DIR_OPAQUE, hidden_dentry,
+				sizeof(UNIONFS_DIR_OPAQUE) - 1);
+	err = PTR_ERR(diropq);
+	if (IS_ERR(diropq))
+		goto out;
+	err = 0;
+
+	if (!diropq->d_inode)
+		err = vfs_create(hidden_dir, diropq, S_IRUGO, NULL);
+	DPUT(diropq);
+	if (!err)
+		set_dbopaque(dentry, bindex);
+
+      out:
+	mutex_unlock(&hidden_dir->i_mutex);
+	return err;
+}
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/super.c newtree/fs/unionfs/super.c
--- oldtree/fs/unionfs/super.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/super.c	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,826 @@
+/*
+ * Copyright (c) 2003-2005 Erez Zadok
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: super.c,v 1.97 2006/06/30 00:45:13 jsipek Exp $
+ */
+
+#include "unionfs.h"
+
+/* The inode cache is used with alloc_inode for both our inode info and the
+ * vfs inode.  */
+static kmem_cache_t *unionfs_inode_cachep;
+
+static void unionfs_read_inode(struct inode *inode)
+{
+#ifdef UNIONFS_MMAP
+	/* SP: use real address operations */
+	extern struct address_space_operations unionfs_aops;
+#else
+	static struct address_space_operations unionfs_empty_aops;
+#endif
+	int size;
+
+	print_entry_location();
+
+	if (!itopd(inode)) {
+		printk(KERN_ERR
+		       "No kernel memory when allocating inode private data!\n");
+		BUG();
+	}
+
+	memset(itopd(inode), 0, sizeof(struct unionfs_inode_info));
+	itopd(inode)->b_start = -1;
+	itopd(inode)->b_end = -1;
+	atomic_set(&itopd(inode)->uii_generation,
+		   atomic_read(&stopd(inode->i_sb)->usi_generation));
+	itopd(inode)->uii_rdlock = SPIN_LOCK_UNLOCKED;
+	itopd(inode)->uii_rdcount = 1;
+	itopd(inode)->uii_hashsize = -1;
+	INIT_LIST_HEAD(&itopd(inode)->uii_readdircache);
+
+	size = sbmax(inode->i_sb) * sizeof(struct inode *);
+	itohi_ptr(inode) = KZALLOC(size, GFP_KERNEL);
+	if (!itohi_ptr(inode)) {
+		printk(KERN_ERR
+		       "No kernel memory when allocating lower-pointer array!\n");
+		BUG();
+	}
+
+	inode->i_version++;
+	inode->i_op = &unionfs_main_iops;
+	inode->i_fop = &unionfs_main_fops;
+#ifdef UNIONFS_MMAP
+	inode->i_mapping->a_ops = &unionfs_aops;
+#else
+	/* I don't think ->a_ops is ever allowed to be NULL */
+	inode->i_mapping->a_ops = &unionfs_empty_aops;
+	dprint(PRINT_DEBUG, "setting inode 0x%p a_ops to empty (0x%p)\n",
+	       inode, inode->i_mapping->a_ops);
+#endif
+
+	print_exit_location();
+}
+
+static void unionfs_put_inode(struct inode *inode)
+{
+	print_entry_location();
+
+	dprint(PRINT_DEBUG, "%s i_count = %d, i_nlink = %d\n", __FUNCTION__,
+	       atomic_read(&inode->i_count), inode->i_nlink);
+
+	/*
+	 * This is really funky stuff:
+	 * Basically, if i_count == 1, iput will then decrement it and this
+	 * inode will be destroyed.  It is currently holding a reference to the
+	 * hidden inode.  Therefore, it needs to release that reference by
+	 * calling iput on the hidden inode.  iput() _will_ do it for us (by
+	 * calling our clear_inode), but _only_ if i_nlink == 0.  The problem
+	 * is, NFS keeps i_nlink == 1 for silly_rename'd files.  So we must for
+	 * our i_nlink to 0 here to trick iput() into calling our clear_inode.
+	 */
+
+	if (atomic_read(&inode->i_count) == 1)
+		inode->i_nlink = 0;
+
+	print_exit_location();
+}
+
+/*
+ * we now define delete_inode, because there are two VFS paths that may
+ * destroy an inode: one of them calls clear inode before doing everything
+ * else that's needed, and the other is fine.  This way we truncate the inode
+ * size (and its pages) and then clear our own inode, which will do an iput
+ * on our and the lower inode.
+ */
+static void unionfs_delete_inode(struct inode *inode)
+{
+	print_entry_location();
+
+	checkinode(inode, "unionfs_delete_inode IN");
+	inode->i_size = 0;	/* every f/s seems to do that */
+
+#ifdef UNIONFS_MMAP
+	/* SP: if you try to clear_inode() when
+	 * inode->i_data.nrpages != 0, you'll hit a BUG
+	 * this is also what generic_delete_inode does */
+	if (inode->i_data.nrpages)
+		truncate_inode_pages(&inode->i_data, 0);
+#endif
+	clear_inode(inode);
+
+	print_exit_location();
+}
+
+/* final actions when unmounting a file system */
+static void unionfs_put_super(struct super_block *sb)
+{
+	int bindex, bstart, bend;
+	struct unionfs_sb_info *spd;
+
+	print_entry_location();
+
+	if ((spd = stopd(sb))) {
+#ifdef UNIONFS_IMAP
+		/* XXX: Free persistent inode stuff. */
+		cleanup_imap_data(sb);
+#endif
+		bstart = sbstart(sb);
+		bend = sbend(sb);
+		for (bindex = bstart; bindex <= bend; bindex++)
+			mntput(stohiddenmnt_index(sb, bindex));
+
+		/* Make sure we have no leaks of branchget/branchput. */
+		for (bindex = bstart; bindex <= bend; bindex++)
+			BUG_ON(branch_count(sb, bindex) != 0);
+
+		KFREE(spd->usi_data);
+		KFREE(spd);
+		stopd_lhs(sb) = NULL;
+	}
+	dprint(PRINT_DEBUG, "unionfs: released super\n");
+
+	print_exit_location();
+}
+
+static int unionfs_statfs(struct dentry *dentry, struct kstatfs *buf)
+{
+	int err = 0;
+	struct super_block *sb, *hidden_sb;
+	struct kstatfs rsb;
+	int bindex, bindex1, bstart, bend;
+
+	print_entry_location();
+
+	sb = dentry->d_sb;
+
+	memset(buf, 0, sizeof(struct kstatfs));
+	buf->f_type = UNIONFS_SUPER_MAGIC;
+
+	buf->f_frsize = 0;
+	buf->f_namelen = 0;
+
+	bstart = sbstart(sb);
+	bend = sbend(sb);
+
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		int dup = 0;
+
+		hidden_sb = stohs_index(sb, bindex);
+		/* Ignore duplicate super blocks. */
+		for (bindex1 = bstart; bindex1 < bindex; bindex1++) {
+			if (hidden_sb == stohs_index(sb, bindex1)) {
+				dup = 1;
+				break;
+			}
+		}
+		if (dup) {
+			continue;
+		}
+
+		err = vfs_statfs(hidden_sb->s_root, &rsb);
+		dprint(PRINT_DEBUG,
+		       "adding values for bindex:%d bsize:%d blocks:%d bfree:%d bavail:%d\n",
+		       bindex, (int)rsb.f_bsize, (int)rsb.f_blocks,
+		       (int)rsb.f_bfree, (int)rsb.f_bavail);
+
+		if (!buf->f_frsize)
+			buf->f_frsize = rsb.f_frsize;
+		if (!buf->f_namelen) {
+			buf->f_namelen = rsb.f_namelen;
+		} else {
+			if (buf->f_namelen > rsb.f_namelen)
+				buf->f_namelen = rsb.f_namelen;
+		}
+		if (!buf->f_bsize) {
+			buf->f_bsize = rsb.f_bsize;
+		} else {
+			if (buf->f_bsize < rsb.f_bsize) {
+				int shifter = 0;
+				while (buf->f_bsize < rsb.f_bsize) {
+					shifter++;
+					rsb.f_bsize >>= 1;
+				}
+				rsb.f_blocks <<= shifter;
+				rsb.f_bfree <<= shifter;
+				rsb.f_bavail <<= shifter;
+			} else {
+				int shifter = 0;
+				while (buf->f_bsize > rsb.f_bsize) {
+					shifter++;
+					rsb.f_bsize <<= 1;
+				}
+				rsb.f_blocks >>= shifter;
+				rsb.f_bfree >>= shifter;
+				rsb.f_bavail >>= shifter;
+			}
+		}
+		buf->f_blocks += rsb.f_blocks;
+		buf->f_bfree += rsb.f_bfree;
+		buf->f_bavail += rsb.f_bavail;
+		buf->f_files += rsb.f_files;
+		buf->f_ffree += rsb.f_ffree;
+	}
+	buf->f_namelen -= WHLEN;
+
+	memset(&buf->f_fsid, 0, sizeof(__kernel_fsid_t));
+	memset(&buf->f_spare, 0, sizeof(buf->f_spare));
+	print_exit_status(err);
+	return err;
+}
+
+static int do_binary_remount(struct super_block *sb, int *flags, char *data)
+{
+	unsigned long *uldata = (unsigned long *)data;
+	int err;
+
+	uldata++;
+
+	switch (*uldata) {
+	case UNIONFS_IOCTL_DELBRANCH:
+		err = unionfs_ioctl_delbranch(sb, *(uldata + 1));
+		break;
+	default:
+		err = -ENOTTY;
+	}
+
+	return err;
+}
+
+/* We don't support a standard text remount, but we do have a magic remount
+ * for unionctl.  The idea is that you can remove a branch without opening
+ * the union.  Eventually it would be nice to support a full-on remount, so
+ * that you can have all of the directories change at once, but that would
+ * require some pretty complicated matching code. */
+static int unionfs_remount_fs(struct super_block *sb, int *flags, char *data)
+{
+	if (data && *((unsigned long *)data) == UNIONFS_REMOUNT_MAGIC)
+		return do_binary_remount(sb, flags, data);
+	return -ENOSYS;
+}
+
+/*
+ * Called by iput() when the inode reference count reached zero
+ * and the inode is not hashed anywhere.  Used to clear anything
+ * that needs to be, before the inode is completely destroyed and put
+ * on the inode free list.
+ */
+static void unionfs_clear_inode(struct inode *inode)
+{
+	int bindex, bstart, bend;
+	struct inode *hidden_inode;
+	struct list_head *pos, *n;
+	struct unionfs_dir_state *rdstate;
+
+	print_entry_location();
+
+	checkinode(inode, "unionfs_clear_inode IN");
+
+	list_for_each_safe(pos, n, &itopd(inode)->uii_readdircache) {
+		rdstate = list_entry(pos, struct unionfs_dir_state, uds_cache);
+		list_del(&rdstate->uds_cache);
+		free_rdstate(rdstate);
+	}
+
+	/* Decrement a reference to a hidden_inode, which was incremented
+	 * by our read_inode when it was created initially.  */
+	bstart = ibstart(inode);
+	bend = ibend(inode);
+	if (bstart >= 0) {
+		for (bindex = bstart; bindex <= bend; bindex++) {
+			hidden_inode = itohi_index(inode, bindex);
+			if (!hidden_inode)
+				continue;
+			IPUT(hidden_inode);
+		}
+	}
+	// XXX: why this assertion fails?
+	// because it doesn't like us
+	// BUG_ON((inode->i_state & I_DIRTY) != 0);
+	KFREE(itohi_ptr(inode));
+	itohi_ptr(inode) = NULL;
+
+	print_exit_location();
+}
+
+static struct inode *unionfs_alloc_inode(struct super_block *sb)
+{
+	struct unionfs_inode_container *c;
+
+	print_entry_location();
+
+	c = (struct unionfs_inode_container *)
+	    kmem_cache_alloc(unionfs_inode_cachep, SLAB_KERNEL);
+	if (!c) {
+		print_exit_pointer(NULL);
+		return NULL;
+	}
+
+	memset(&c->info, 0, sizeof(c->info));
+
+	c->vfs_inode.i_version = 1;
+	print_exit_pointer(&c->vfs_inode);
+	return &c->vfs_inode;
+}
+
+static void unionfs_destroy_inode(struct inode *inode)
+{
+	print_entry("inode = %p", inode);
+	kmem_cache_free(unionfs_inode_cachep, itopd(inode));
+	print_exit_location();
+}
+
+static void init_once(void *v, kmem_cache_t * cachep, unsigned long flags)
+{
+	struct unionfs_inode_container *c = (struct unionfs_inode_container *)v;
+
+	print_entry_location();
+
+	if ((flags & (SLAB_CTOR_VERIFY | SLAB_CTOR_CONSTRUCTOR)) ==
+	    SLAB_CTOR_CONSTRUCTOR)
+		inode_init_once(&c->vfs_inode);
+
+	print_exit_location();
+}
+
+int init_inode_cache(void)
+{
+	int err = 0;
+
+	print_entry_location();
+
+	unionfs_inode_cachep =
+	    kmem_cache_create("unionfs_inode_cache",
+			      sizeof(struct unionfs_inode_container), 0,
+			      SLAB_RECLAIM_ACCOUNT, init_once, NULL);
+	if (!unionfs_inode_cachep)
+		err = -ENOMEM;
+	print_exit_status(err);
+	return err;
+}
+
+void destroy_inode_cache(void)
+{
+	print_entry_location();
+	if (!unionfs_inode_cachep)
+		goto out;
+	if (kmem_cache_destroy(unionfs_inode_cachep))
+		printk(KERN_ERR
+		       "unionfs_inode_cache: not all structures were freed\n");
+      out:
+	print_exit_location();
+	return;
+}
+
+/* Called when we have a dirty inode, right here we only throw out
+ * parts of our readdir list that are too old.
+ */
+static int unionfs_write_inode(struct inode *inode, int sync)
+{
+	struct list_head *pos, *n;
+	struct unionfs_dir_state *rdstate;
+
+	print_entry_location();
+
+	spin_lock(&itopd(inode)->uii_rdlock);
+	list_for_each_safe(pos, n, &itopd(inode)->uii_readdircache) {
+		rdstate = list_entry(pos, struct unionfs_dir_state, uds_cache);
+		/* We keep this list in LRU order. */
+		if ((rdstate->uds_access + RDCACHE_JIFFIES) > jiffies)
+			break;
+		itopd(inode)->uii_rdcount--;
+		list_del(&rdstate->uds_cache);
+		free_rdstate(rdstate);
+	}
+	spin_unlock(&itopd(inode)->uii_rdlock);
+
+	print_exit_location();
+	return 0;
+}
+
+/*
+ * Used only in nfs, to kill any pending RPC tasks, so that subsequent
+ * code can actually succeed and won't leave tasks that need handling.
+ *
+ * PS. I wonder if this is somehow useful to undo damage that was
+ * left in the kernel after a user level file server (such as amd)
+ * dies.
+ */
+static void unionfs_umount_begin(struct vfsmount *mnt, int flags)
+{
+	struct super_block *sb, *hidden_sb;
+	struct vfsmount *hidden_mnt;
+	int bindex, bstart, bend;
+
+	print_entry_location();
+
+	if (!(flags & MNT_FORCE))
+		/* we are not being MNT_FORCEd, therefore we should emulate old
+		 * behaviour
+		 */
+		goto out;
+
+	sb = mnt->mnt_sb;
+
+	bstart = sbstart(sb);
+	bend = sbend(sb);
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		hidden_mnt = stohiddenmnt_index(sb, bindex);
+		hidden_sb  = stohs_index(sb, bindex);
+
+		if (hidden_mnt && hidden_sb && hidden_sb->s_op &&
+		    hidden_sb->s_op->umount_begin)
+			hidden_sb->s_op->umount_begin(hidden_mnt, flags);
+	}
+
+out:
+	print_exit_location();
+}
+
+static int unionfs_show_options(struct seq_file *m, struct vfsmount *mnt)
+{
+	struct super_block *sb = mnt->mnt_sb;
+	int ret = 0;
+	unsigned long tmp = 0;
+	char *hidden_path;
+	int bindex, bstart, bend;
+	int perms;
+
+	lock_dentry(sb->s_root);
+
+	tmp = __get_free_page(GFP_KERNEL);
+	if (!tmp) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	bindex = bstart = sbstart(sb);
+	bend = sbend(sb);
+
+	seq_printf(m, ",dirs=");
+	for (bindex = bstart; bindex <= bend; bindex++) {
+		hidden_path =
+		    d_path(dtohd_index(sb->s_root, bindex),
+			   stohiddenmnt_index(sb, bindex), (char *)tmp,
+			   PAGE_SIZE);
+		perms = branchperms(sb, bindex);
+		seq_printf(m, "%s=%s", hidden_path,
+			   perms & MAY_WRITE ? "rw" :
+			   perms & MAY_NFSRO ? "nfsro" : "ro");
+		if (bindex != bend) {
+			seq_printf(m, ":");
+		}
+	}
+
+	seq_printf(m, ",debug=%u", get_debug_mask());
+
+#ifdef UNIONFS_DELETE_ALL
+	if (IS_SET(sb, DELETE_ALL))
+		seq_printf(m, ",delete=all");
+	else
+#endif
+		seq_printf(m, ",delete=whiteout");
+      out:
+	if (tmp)
+		free_page(tmp);
+	unlock_dentry(sb->s_root);
+	return ret;
+}
+
+#ifdef CONFIG_EXPORTFS
+/*
+ * export operations.
+ * unionfs cannot handle disconnected dentry, since it has no hidden dentries.
+ */
+/* un-tested 64 bit environment (pointer and inode number) */
+
+#define is_anon(d) ((d)->d_flags & DCACHE_DISCONNECTED)
+extern struct export_operations export_op_default;
+
+static void prepend_path(char **path, const char *name, int len)
+{
+	*path -= len;
+	memcpy(*path, name, len);
+	(*path)--;
+	**path = '/';
+}
+
+struct filldir_arg {
+	int found, called;
+	char *path;
+	ino_t ino, parent_ino;
+};
+
+static int filldir(void *arg, const char *name, int len, loff_t pos, ino_t ino,
+		   unsigned int d_type)
+{
+	struct filldir_arg *a = arg;
+
+	a->called++;
+	if (len == 2 && !strncmp(name, "..", 2)) {
+		a->parent_ino = ino;
+		a->found++;
+	} else if (ino == a->ino) {
+		if (len != 1 || *name != '.')
+			prepend_path(&a->path, name, len);
+		a->found++;
+	}
+	return (a->found == 2) ? 1 : 0;
+}
+
+static struct dentry *get_hidden_parent(struct super_block *hidden_sb,
+					ino_t hidden_parent_ino)
+{
+	__u32 fh[2];
+
+	if (hidden_sb->s_root->d_inode->i_ino == hidden_parent_ino)
+		return DGET(hidden_sb->s_root);
+
+	fh[0] = hidden_parent_ino;
+	fh[1] = 0;
+	return export_op_default.get_dentry(hidden_sb, fh);
+}
+
+static struct dentry *do_get_dentry(struct super_block *sb, ino_t ino,
+				    __u32 gen, struct dentry *hidden_root,
+				    ino_t hidden_ino, ino_t hidden_parent_ino)
+{
+	struct dentry *dentry, *hidden_parent, *parent;
+	char *path, *p;
+	struct filldir_arg arg = {
+		.ino = hidden_ino,
+		.parent_ino = hidden_parent_ino
+	};
+	int open_flags, err, bindex, bend, found;
+	struct file *hidden_file;
+	struct super_block *hidden_sb;
+
+	print_entry("hr%p, hi%lu, hpi%lu",
+		    hidden_root, hidden_ino, hidden_parent_ino);
+
+	dentry = ERR_PTR(-ENOMEM);
+	path = __getname();
+	if (!path)
+		goto out;
+	arg.path = path + PATH_MAX - 1;
+	*arg.path = 0;
+
+	open_flags = O_RDONLY | O_DIRECTORY /* | O_NOATIME */ ;
+	if (force_o_largefile())
+		open_flags |= O_LARGEFILE;
+
+	dentry = ERR_PTR(-ESTALE);
+	unionfs_read_lock(sb);
+	lock_dentry(sb->s_root);
+	bend = dbend(sb->s_root);
+	found = -1;
+	for (bindex = 0; found == -1 && bindex <= bend; bindex++)
+		if (hidden_root == dtohd_index(sb->s_root, bindex))
+			found = bindex;
+	unlock_dentry(sb->s_root);
+	if (found == -1)
+		goto out_unlock;
+
+	bindex = found;
+	hidden_sb = stohs_index(sb, bindex);
+	while (1) {
+		hidden_parent = get_hidden_parent(hidden_sb, hidden_parent_ino);
+		dentry = hidden_parent;
+		if (IS_ERR(hidden_parent))
+			goto out_unlock;
+
+		branchget(sb, bindex);
+		hidden_file = DENTRY_OPEN(DGET(hidden_parent), NULL,
+					  open_flags);
+		if (IS_ERR(hidden_file)) {
+			dentry = (void *)hidden_file;
+			DPUT(hidden_parent);
+			branchput(sb, bindex);
+			goto out_unlock;
+		}
+
+		arg.found = 0;
+		while (arg.found != 2) {
+			arg.called = 0;
+			err = vfs_readdir(hidden_file, filldir, &arg);
+			if (!arg.called || err < 0)
+				break;
+		}
+		fput(hidden_file);
+		branchput(sb, bindex);
+		if (arg.found != 2) {
+			dentry = ERR_PTR(-ESTALE);
+			DPUT(hidden_parent);
+			goto out_unlock;
+		}
+
+		DPUT(hidden_parent);
+		if (hidden_parent_ino == hidden_root->d_inode->i_ino)
+			break;
+		arg.ino = hidden_parent_ino;
+		hidden_parent_ino = arg.parent_ino;
+	}
+	BUG_ON(arg.path < path);
+
+	parent = DGET(sb->s_root);
+	p = strchr(++arg.path, '/');
+	while (p) {
+		mutex_lock(&parent->d_inode->i_mutex);
+		dentry = LOOKUP_ONE_LEN(arg.path, parent, p - arg.path);
+		mutex_unlock(&parent->d_inode->i_mutex);
+		DPUT(parent);
+		if (IS_ERR(dentry))
+			goto out_unlock;
+		if (!dentry->d_inode || !S_ISDIR(dentry->d_inode->i_mode)) {
+			DPUT(dentry);
+			dentry = ERR_PTR(-ESTALE);
+			goto out_unlock;
+		}
+		parent = dentry;
+		arg.path = p + 1;
+		p = strchr(arg.path, '/');
+	}
+	mutex_lock(&parent->d_inode->i_mutex);
+	dentry = LOOKUP_ONE_LEN(arg.path, parent, strlen(arg.path));
+	mutex_unlock(&parent->d_inode->i_mutex);
+	DPUT(parent);
+	if (!IS_ERR(dentry)
+	    && (!dentry->d_inode
+		|| dentry->d_inode->i_ino != ino
+		|| dentry->d_inode->i_generation != gen)) {
+		DPUT(dentry);
+		dentry = ERR_PTR(-ESTALE);
+	}
+
+      out_unlock:
+	unionfs_read_unlock(sb);
+	__putname(path);
+      out:
+	print_exit_pointer(dentry);
+	return dentry;
+}
+
+enum {
+	FhHead = 4, FhHRoot1 = FhHead, FhHRoot2,
+	FhHIno1, FhHIno2, FhHPIno1, FhHPIno2,
+	FhTail
+};
+
+static void do_decode(__u32 * fh, struct dentry **hidden_root,
+		      ino_t * hidden_ino, ino_t * hidden_parent_ino)
+{
+	*hidden_root = (void *)fh[FhHRoot2];
+	*hidden_ino = fh[FhHIno2];
+	*hidden_parent_ino = fh[FhHPIno2];
+#if BITS_PER_LONG == 64
+	*hidden_root |= fh[FhHRoot1] << 32;
+	*hidden_ino |= fh[FhHIno1] << 32;
+	*hidden_parent_ino |= fh[FhHPIno1] << 32;
+#elif BITS_PER_LONG == 32
+	/* ok */
+#else
+#error unknown size
+#endif
+}
+
+static int unionfs_encode_fh(struct dentry *dentry, __u32 * fh, int *max_len,
+			     int connectable)
+{
+	int type, len, bindex;
+	struct super_block *sb;
+	struct dentry *h_root;
+	ino_t h_ino, hp_ino;
+	static int warn;
+
+	print_entry("dentry %p", dentry);
+	BUG_ON(is_anon(dentry) || !dentry->d_inode
+	       || is_anon(dentry->d_parent));
+
+#ifdef UNIONFS_IMAP
+	if (!warn && stopd(dentry->d_sb)->usi_persistent)
+		warn++;
+#endif
+	if (!warn) {
+		printk(KERN_WARNING "Exporting Unionfs without imap"
+		       " option may stop your NFS server or client");
+		warn++;
+	}
+
+	sb = dentry->d_sb;
+	unionfs_read_lock(sb);
+	lock_dentry(dentry);
+
+	len = *max_len;
+	type = export_op_default.encode_fh(dentry, fh, max_len, connectable);
+	if (type == 255 || *max_len > FhHead || len < FhTail) {
+		type = 255;
+		goto out;
+	}
+
+	*max_len = FhTail;
+	bindex = dbstart(dentry);
+	lock_dentry(sb->s_root);
+	h_root = dtohd_index(sb->s_root, bindex);
+	unlock_dentry(sb->s_root);
+	h_ino = itohi_index(dentry->d_inode, bindex)->i_ino;
+	hp_ino = parent_ino(dtohd(dentry));
+	fh[FhHRoot2] = (__u32) h_root;
+	fh[FhHIno2] = h_ino;
+	fh[FhHPIno2] = hp_ino;
+#if BITS_PER_LONG == 64
+	fh[FhHRoot1] = h_root >> 32;
+	fh[FhHIno1] = h_ino >> 32;
+	fh[FhHPIno1] = hp_ino >> 32;
+#endif
+
+      out:
+	unionfs_print(PRINT_MAIN_EXIT, "%d, fh{i%u, g%d, hr%x, hi%u, hpi%u}\n",
+		      type, fh[0], fh[1], fh[FhHRoot2], fh[FhHIno2],
+		      fh[FhHPIno2]);
+	unlock_dentry(dentry);
+	unionfs_read_unlock(sb);
+	return type;
+}
+
+static struct dentry *unionfs_decode_fh(struct super_block *sb, __u32 * fh,
+					int fh_len, int fh_type,
+					int (*acceptable) (void *context,
+							   struct dentry * de),
+					void *context)
+{
+	struct dentry *dentry, *hidden_root;
+	ino_t hidden_ino, hidden_parent_ino;
+
+	print_entry("%d, fh{i%u, g%d, hr%x, hi%u, hpi%u}",
+		    fh_type, fh[0], fh[1], fh[FhHRoot2], fh[FhHIno2],
+		    fh[FhHPIno2]);
+
+	dentry = export_op_default.get_dentry(sb, fh);
+	if (!dentry || IS_ERR(dentry) || (dentry->d_inode && !is_anon(dentry)))
+		return dentry;
+
+	d_drop(dentry);
+	DPUT(dentry);
+	do_decode(fh, &hidden_root, &hidden_ino, &hidden_parent_ino);
+	dentry = do_get_dentry(sb, fh[0], fh[1], hidden_root, hidden_ino,
+			       hidden_parent_ino);
+	if (!IS_ERR(dentry)) {
+		if (acceptable(context, dentry))
+			return dentry;	/* success */
+		DPUT(dentry);
+		dentry = NULL;
+	}
+	return dentry;
+}
+
+struct export_operations unionfs_export_ops = {
+	.decode_fh = unionfs_decode_fh,
+	.encode_fh = unionfs_encode_fh
+};
+#endif
+
+struct super_operations unionfs_sops = {
+	.read_inode = unionfs_read_inode,
+	.put_inode = unionfs_put_inode,
+	.delete_inode = unionfs_delete_inode,
+	.put_super = unionfs_put_super,
+	.statfs = unionfs_statfs,
+	.remount_fs = unionfs_remount_fs,
+	.clear_inode = unionfs_clear_inode,
+	.umount_begin = unionfs_umount_begin,
+	.show_options = unionfs_show_options,
+	.write_inode = unionfs_write_inode,
+	.alloc_inode = unionfs_alloc_inode,
+	.destroy_inode = unionfs_destroy_inode,
+};
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/unionfs.h newtree/fs/unionfs/unionfs.h
--- oldtree/fs/unionfs/unionfs.h	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/unionfs.h	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,739 @@
+#ifndef __UNIONFS_H_
+#define __UNIONFS_H_
+
+#ifdef __KERNEL__
+
+#include <linux/config.h>
+#include <linux/version.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/string.h>
+#include <linux/stat.h>
+#include <linux/errno.h>
+#include <linux/wait.h>
+#include <linux/limits.h>
+#include <linux/random.h>
+#include <linux/poll.h>
+#include <linux/buffer_head.h>
+#include <linux/pagemap.h>
+#include <linux/namei.h>
+#include <linux/module.h>
+#include <linux/mount.h>
+#include <linux/page-flags.h>
+#include <linux/writeback.h>
+#include <linux/page-flags.h>
+#include <linux/statfs.h>
+#include <linux/smp.h>
+#include <linux/smp_lock.h>
+#include <linux/file.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/poll.h>
+#include <linux/list.h>
+#include <linux/init.h>
+#include <linux/xattr.h>
+#include <linux/security.h>
+#include <linux/spinlock.h>
+#include <linux/compat.h>
+
+#include <linux/swap.h>
+
+#include <asm/system.h>
+#include <asm/mman.h>
+#include <linux/seq_file.h>
+#include <linux/dcache.h>
+#include <linux/poll.h>
+
+/* the file system name */
+#define UNIONFS_NAME "unionfs"
+
+/* unionfs file systems superblock magic */
+#define UNIONFS_SUPER_MAGIC 0xf15f083d
+
+/* unionfs root inode number */
+#define UNIONFS_ROOT_INO     1
+
+/* Mount time flags */
+#define MOUNT_FLAG(sb)     (stopd(sb)->usi_mount_flag)
+
+/* number of characters while generating unique temporary file names */
+#define	UNIONFS_TMPNAM_LEN	12
+
+/* Operations vectors defined in specific files. */
+extern struct file_operations unionfs_main_fops;
+extern struct file_operations unionfs_dir_fops;
+extern struct inode_operations unionfs_main_iops;
+extern struct inode_operations unionfs_dir_iops;
+extern struct inode_operations unionfs_symlink_iops;
+extern struct super_operations unionfs_sops;
+extern struct dentry_operations unionfs_dops;
+#ifdef CONFIG_EXPORTFS
+extern struct export_operations unionfs_export_ops;
+#endif
+
+/* How long should an entry be allowed to persist */
+#define RDCACHE_JIFFIES 5*HZ
+
+/* file private data. */
+struct unionfs_file_info {
+	int b_start;
+	int b_end;
+	atomic_t ufi_generation;
+
+	struct unionfs_dir_state *rdstate;
+	struct file **ufi_file;
+};
+
+/* unionfs inode data in memory */
+struct unionfs_inode_info {
+	int b_start;
+	int b_end;
+	atomic_t uii_generation;
+	int uii_stale;
+	/* Stuff for readdir over NFS. */
+	spinlock_t uii_rdlock;
+	struct list_head uii_readdircache;
+	int uii_rdcount;
+	int uii_hashsize;
+	int uii_cookie;
+	/* The hidden inodes */
+	struct inode **uii_inode;
+	/* to keep track of reads/writes for unlinks before closes */
+	atomic_t uii_totalopens;
+};
+
+struct unionfs_inode_container {
+	struct unionfs_inode_info info;
+	struct inode vfs_inode;
+};
+
+/* unionfs dentry data in memory */
+struct unionfs_dentry_info {
+	/* The semaphore is used to lock the dentry as soon as we get into a
+	 * unionfs function from the VFS.  Our lock ordering is that children
+	 * go before their parents. */
+	struct semaphore udi_sem;
+	int udi_bstart;
+	int udi_bend;
+	int udi_bopaque;
+	int udi_bcount;
+	atomic_t udi_generation;
+	struct dentry **udi_dentry;
+};
+
+/* A putmap is used so that older files can still do branchput correctly. */
+struct putmap {
+	atomic_t count;
+	int bend;
+	int map[0];
+};
+
+/* These are the pointers to our various objects. */
+struct unionfs_usi_data {
+	struct super_block *sb;
+	struct vfsmount *hidden_mnt;
+	atomic_t sbcount;
+	int branchperms;
+};
+
+/* unionfs super-block data in memory */
+struct unionfs_sb_info {
+	int b_end;
+
+	atomic_t usi_generation;
+	unsigned long usi_mount_flag;
+	struct rw_semaphore usi_rwsem;
+
+	struct unionfs_usi_data *usi_data;
+
+	/* These map branch numbers for old generation numbers to the new bindex,
+	 * so that branchput will behave properly. */
+	int usi_firstputmap;
+	int usi_lastputmap;
+	struct putmap **usi_putmaps;
+
+#ifdef UNIONFS_IMAP
+	int usi_persistent;
+	/* These will need a lock. */
+	uint64_t usi_next_avail;
+	uint8_t usi_num_bmapents;
+	struct bmapent *usi_bmap;
+	struct file *usi_forwardmap;
+	struct file **usi_reversemaps;
+	struct file **usi_map_table;
+	int *usi_bnum_table;	//This is a table of branches to fsnums.
+#endif				/* UNIONFS_IMAP */
+};
+
+/*
+ * structure for making the linked list of entries by readdir on left branch
+ * to compare with entries on right branch
+ */
+struct filldir_node {
+	struct list_head file_list;	// list for directory entries
+	char *name;		// name entry
+	int hash;		// name hash
+	int namelen;		// name len since name is not 0 terminated
+	int bindex;		// we can check for duplicate whiteouts and files in the same branch in order to return -EIO.
+	int whiteout;		// is this a whiteout entry?
+	char iname[DNAME_INLINE_LEN_MIN];	// Inline name, so we don't need to separately kmalloc small ones
+};
+
+/* Directory hash table. */
+struct unionfs_dir_state {
+	unsigned int uds_cookie;	/* The cookie, which is based off of uii_rdversion */
+	unsigned int uds_offset;	/* The entry we have returned. */
+	int uds_bindex;
+	loff_t uds_dirpos;	/* The offset within the lower level directory. */
+	int uds_size;		/* How big is the hash table? */
+	int uds_hashentries;	/* How many entries have been inserted? */
+	unsigned long uds_access;
+	/* This cache list is used when the inode keeps us around. */
+	struct list_head uds_cache;
+	struct list_head uds_list[0];
+};
+
+/* include miscellaneous macros */
+#include "unionfs_macros.h"
+
+/* include debug macros */
+#include "unionfs_debug.h"
+
+/* include persistent imap code */
+#include "unionfs_imap.h"
+
+/* Cache creation/deletion routines. */
+void destroy_filldir_cache(void);
+int init_filldir_cache(void);
+int init_inode_cache(void);
+void destroy_inode_cache(void);
+int init_dentry_cache(void);
+void destroy_dentry_cache(void);
+
+/* Initialize and free readdir-specific  state. */
+int init_rdstate(struct file *file);
+struct unionfs_dir_state *alloc_rdstate(struct inode *inode, int bindex);
+struct unionfs_dir_state *find_rdstate(struct inode *inode, loff_t fpos);
+void free_rdstate(struct unionfs_dir_state *state);
+int add_filldir_node(struct unionfs_dir_state *rdstate, const char *name,
+		     int namelen, int bindex, int whiteout);
+struct filldir_node *find_filldir_node(struct unionfs_dir_state *rdstate,
+				       const char *name, int namelen);
+
+struct dentry **alloc_new_dentries(int objs);
+struct unionfs_usi_data *alloc_new_data(int objs);
+
+#ifdef FIST_MALLOC_DEBUG
+
+extern void *unionfs_kzalloc(size_t size, gfp_t flags, int line,
+			     const char *file);
+extern void *unionfs_kmalloc(size_t size, gfp_t flags, int line,
+			     const char *file);
+extern void unionfs_kfree(void *ptr, int line, const char *file);
+
+extern struct dentry *unionfs_dget_parent(struct dentry *child, int line,
+					  const char *file);
+extern struct dentry *unionfs_dget(struct dentry *ptr, int line,
+				   const char *file);
+extern void unionfs_dput(struct dentry *ptr, int line, const char *file);
+extern struct inode *unionfs_igrab(struct inode *inode, int line, char *file);
+extern void unionfs_iput(struct inode *inode, int line, char *file);
+extern struct inode *unionfs_iget(struct super_block *sb, unsigned long ino,
+				  int line, char *file);
+extern struct dentry *unionfs_lookup_one_len(const char *name,
+					     struct dentry *parent, int len,
+					     int line, const char *file);
+void record_path_lookup(struct nameidata *nd, int line, const char *file);
+void record_path_release(struct nameidata *nd, int line, const char *file);
+struct file *unionfs_dentry_open(struct dentry *ptr, struct vfsmount *mnt,
+				 int flags, int line, const char *file);
+void record_set(struct dentry *upper, int index, struct dentry *ptr,
+		struct dentry *old, int line, const char *file);
+
+#define KZALLOC(size,flags) unionfs_kzalloc((size),(flags),__LINE__,__FILE__)
+#define KMALLOC(size,flags) unionfs_kmalloc((size),(flags),__LINE__,__FILE__)
+#define KFREE(ptr) unionfs_kfree((ptr),__LINE__,__FILE__)
+#define DGET(d) unionfs_dget((d),__LINE__,__FILE__)
+#define DPUT(d) unionfs_dput((d),__LINE__,__FILE__)
+# define IPUT(a)		unionfs_iput((a),__LINE__,__FILE__)
+# define IGET(a,b)		unionfs_iget((a),(b),__LINE__,__FILE__)
+# define IGRAB(a)		unionfs_igrab((a),__LINE__,__FILE__)
+#define LOOKUP_ONE_LEN(name,parent,len) unionfs_lookup_one_len((name),(parent),(len),__LINE__,__FILE__)
+# define RECORD_PATH_LOOKUP(nd)	record_path_lookup((nd),__LINE__,__FILE__)
+# define RECORD_PATH_RELEASE(nd) record_path_release((nd),__LINE__,__FILE__)
+/* This has the effect of reducing the reference count sooner or later,
+ * if the file is closed.  If it isn't then the mount will be busy and
+ * you can't unmount.
+ */
+# define DENTRY_OPEN(d,m,f) unionfs_dentry_open((d),(m),(f),__LINE__,__FILE__)
+# define GET_PARENT(dentry) unionfs_dget_parent((dentry),__LINE__,__FILE__)
+#else				/* not FIST_MALLOC_DEBUG */
+# define KZALLOC(a,b)		kzalloc((a),(b))
+# define KMALLOC(a,b)		kmalloc((a),(b))
+# define KFREE(a)		kfree((a))
+# define DPUT(a)		dput((a))
+# define DGET(a)		dget((a))
+# define IPUT(a)		iput((a))
+# define IGET(a,b)		iget((a),(b))
+# define IGRAB(a)		igrab((a))
+# define LOOKUP_ONE_LEN(a,b,c)	lookup_one_len((a),(b),(c))
+# define RECORD_PATH_LOOKUP(a)
+# define RECORD_PATH_RELEASE(a)
+# define DENTRY_OPEN(d,m,f)	dentry_open((d),(m),(f))
+# define GET_PARENT(d)		dget_parent(d)
+#endif				/* not FIST_MALLOC_DEBUG */
+
+/* We can only use 32-bits of offset for rdstate --- blech! */
+#define DIREOF (0xfffff)
+#define RDOFFBITS 20		/* This is the number of bits in DIREOF. */
+#define MAXRDCOOKIE (0xfff)
+/* Turn an rdstate into an offset. */
+static inline off_t rdstate2offset(struct unionfs_dir_state *buf)
+{
+	off_t tmp;
+	tmp =
+	    ((buf->uds_cookie & MAXRDCOOKIE) << RDOFFBITS) | (buf->
+							      uds_offset &
+							      DIREOF);
+	return tmp;
+}
+
+#define unionfs_read_lock(sb) down_read(&stopd(sb)->usi_rwsem)
+#define unionfs_read_unlock(sb) up_read(&stopd(sb)->usi_rwsem)
+#define unionfs_write_lock(sb) down_write(&stopd(sb)->usi_rwsem)
+#define unionfs_write_unlock(sb) up_write(&stopd(sb)->usi_rwsem)
+
+/* The double lock function needs to go after the debugmacros, so that
+ * dtopd is defined.  */
+static inline void double_lock_dentry(struct dentry *d1, struct dentry *d2)
+{
+	if (d2 < d1) {
+		struct dentry *tmp = d1;
+		d1 = d2;
+		d2 = tmp;
+	}
+	lock_dentry(d1);
+	lock_dentry(d2);
+}
+
+extern int new_dentry_private_data(struct dentry *dentry);
+void free_dentry_private_data(struct unionfs_dentry_info *udi);
+void update_bstart(struct dentry *dentry);
+#define sbt(sb) ((sb)->s_type->name)
+
+/*
+ * EXTERNALS:
+ */
+/* replicates the directory structure upto given dentry in given branch */
+extern struct dentry *create_parents(struct inode *dir, struct dentry *dentry,
+				     int bindex);
+struct dentry *create_parents_named(struct inode *dir, struct dentry *dentry,
+				    const char *name, int bindex);
+
+/* check if two branches overlap */
+extern int is_branch_overlap(struct dentry *dent1, struct dentry *dent2);
+
+/* partial lookup */
+extern int unionfs_partial_lookup(struct dentry *dentry);
+
+/* Pass an unionfs dentry and an index and it will try to create a whiteout in branch 'index'.
+   On error, it will proceed to a branch to the left */
+extern int create_whiteout(struct dentry *dentry, int start);
+/* copies a file from dbstart to newbindex branch */
+extern int copyup_file(struct inode *dir, struct file *file, int bstart,
+		       int newbindex, loff_t size);
+extern int copyup_named_file(struct inode *dir, struct file *file,
+			     char *name, int bstart, int new_bindex,
+			     loff_t len);
+
+/* copies a dentry from dbstart to newbindex branch */
+extern int copyup_dentry(struct inode *dir, struct dentry *dentry, int bstart,
+			 int new_bindex, struct file **copyup_file, loff_t len);
+extern int copyup_named_dentry(struct inode *dir, struct dentry *dentry,
+			       int bstart, int new_bindex, const char *name,
+			       int namelen, struct file **copyup_file,
+			       loff_t len);
+
+extern int remove_whiteouts(struct dentry *dentry, struct dentry *hidden_dentry,
+			    int bindex);
+
+/* Is this directory empty: 0 if it is empty, -ENOTEMPTY if not. */
+extern int check_empty(struct dentry *dentry,
+		       struct unionfs_dir_state **namelist);
+/* Delete whiteouts from this directory in branch bindex. */
+extern int delete_whiteouts(struct dentry *dentry, int bindex,
+			    struct unionfs_dir_state *namelist);
+
+/* Re-lookup a hidden dentry. */
+extern int unionfs_refresh_hidden_dentry(struct dentry *dentry, int bindex);
+
+extern void unionfs_reinterpose(struct dentry *this_dentry);
+extern struct super_block *unionfs_duplicate_super(struct super_block *sb);
+
+/* Locking functions. */
+extern int unionfs_setlk(struct file *file, int cmd, struct file_lock *fl);
+extern int unionfs_getlk(struct file *file, struct file_lock *fl);
+
+/* Common file operations. */
+extern int unionfs_file_revalidate(struct file *file, int willwrite);
+extern int unionfs_open(struct inode *inode, struct file *file);
+extern int unionfs_file_release(struct inode *inode, struct file *file);
+extern int unionfs_flush(struct file *file, fl_owner_t id);
+extern long unionfs_ioctl(struct file *file, unsigned int cmd,
+			  unsigned long arg);
+
+/* Inode operations */
+extern int unionfs_rename(struct inode *old_dir, struct dentry *old_dentry,
+			  struct inode *new_dir, struct dentry *new_dentry);
+int unionfs_unlink(struct inode *dir, struct dentry *dentry);
+int unionfs_rmdir(struct inode *dir, struct dentry *dentry);
+
+int unionfs_d_revalidate(struct dentry *dentry, struct nameidata *nd);
+
+/* The values for unionfs_interpose's flag. */
+#define INTERPOSE_DEFAULT	0
+#define INTERPOSE_LOOKUP	1
+#define INTERPOSE_REVAL		2
+#define INTERPOSE_REVAL_NEG	3
+#define INTERPOSE_PARTIAL	4
+
+extern int unionfs_interpose(struct dentry *this_dentry, struct super_block *sb,
+			     int flag);
+
+/* Branch management ioctls. */
+int unionfs_ioctl_branchcount(struct file *file, unsigned int cmd,
+			      unsigned long arg);
+int unionfs_ioctl_incgen(struct file *file, unsigned int cmd,
+			 unsigned long arg);
+int unionfs_ioctl_addbranch(struct inode *inode, unsigned int cmd,
+			    unsigned long arg);
+int unionfs_ioctl_delbranch(struct super_block *sb, unsigned long arg);
+int unionfs_ioctl_rdwrbranch(struct inode *inode, unsigned int cmd,
+			     unsigned long arg);
+int unionfs_ioctl_queryfile(struct file *file, unsigned int cmd,
+			    unsigned long arg);
+
+/* Verify that a branch is valid. */
+int check_branch(struct nameidata *nd);
+
+/* Extended attribute functions. */
+extern void *xattr_alloc(size_t size, size_t limit);
+extern void xattr_free(void *ptr, size_t size);
+extern ssize_t vfs_listxattr(struct dentry *d, char *list, size_t size);
+
+extern ssize_t unionfs_getxattr(struct dentry *dentry, const char *name,
+				void *value, size_t size);
+extern int unionfs_removexattr(struct dentry *dentry, const char *name);
+extern ssize_t unionfs_listxattr(struct dentry *dentry, char *list,
+				 size_t size);
+
+int unionfs_setxattr(struct dentry *dentry, const char *name, const void *value,
+		     size_t size, int flags);
+
+/* The root directory is unhashed, but isn't deleted. */
+static inline int d_deleted(struct dentry *d)
+{
+	return d_unhashed(d) && (d != d->d_sb->s_root);
+}
+
+/* returns the sum of the n_link values of all the underlying inodes of the passed inode */
+static inline int get_nlinks(struct inode *inode)
+{
+	int sum_nlinks = 0;
+	int dirs = 0;
+	int bindex;
+	struct inode *hidden_inode;
+
+	if (!S_ISDIR(inode->i_mode))
+		return itohi(inode)->i_nlink;
+
+	for (bindex = ibstart(inode); bindex <= ibend(inode); bindex++) {
+		hidden_inode = itohi_index(inode, bindex);
+		if (!hidden_inode || !S_ISDIR(hidden_inode->i_mode))
+			continue;
+		BUG_ON(hidden_inode->i_nlink < 0);
+
+		/* A deleted directory. */
+		if (hidden_inode->i_nlink == 0)
+			continue;
+		dirs++;
+		/* A broken directory (e.g., squashfs). */
+		if (hidden_inode->i_nlink == 1)
+			sum_nlinks += 2;
+		else
+			sum_nlinks += (hidden_inode->i_nlink - 2);
+	}
+
+	if (!dirs)
+		return 0;
+	return sum_nlinks + 2;
+}
+
+static inline void fist_copy_attr_atime(struct inode *dest,
+					const struct inode *src)
+{
+	dest->i_atime = src->i_atime;
+}
+static inline void fist_copy_attr_times(struct inode *dest,
+					const struct inode *src)
+{
+	dest->i_atime = src->i_atime;
+	dest->i_mtime = src->i_mtime;
+	dest->i_ctime = src->i_ctime;
+}
+static inline void fist_copy_attr_timesizes(struct inode *dest,
+					    const struct inode *src)
+{
+	dest->i_atime = src->i_atime;
+	dest->i_mtime = src->i_mtime;
+	dest->i_ctime = src->i_ctime;
+	dest->i_size = src->i_size;
+	dest->i_blocks = src->i_blocks;
+}
+static inline void fist_copy_attr_all(struct inode *dest,
+				      const struct inode *src)
+{
+	print_entry_location();
+
+	dest->i_mode = src->i_mode;
+	/* we do not need to copy if the file is a deleted file */
+	if (dest->i_nlink > 0)
+		dest->i_nlink = get_nlinks(dest);
+	dest->i_uid = src->i_uid;
+	dest->i_gid = src->i_gid;
+	dest->i_rdev = src->i_rdev;
+	dest->i_atime = src->i_atime;
+	dest->i_mtime = src->i_mtime;
+	dest->i_ctime = src->i_ctime;
+	dest->i_blkbits = src->i_blkbits;
+	dest->i_size = src->i_size;
+	dest->i_blocks = src->i_blocks;
+	dest->i_flags = src->i_flags;
+
+	print_exit_location();
+}
+
+struct dentry *unionfs_lookup_backend(struct dentry *dentry, int lookupmode);
+int is_stale_inode(struct inode *inode);
+void make_stale_inode(struct inode *inode);
+
+#define IS_SET(sb, check_flag) (check_flag & MOUNT_FLAG(sb))
+
+/* unionfs_permission, check if we should bypass error to facilitate copyup */
+#define IS_COPYUP_ERR(err) (err == -EROFS)
+
+/* unionfs_open, check if we need to copyup the file */
+#define OPEN_WRITE_FLAGS (O_WRONLY | O_RDWR | O_APPEND)
+#define IS_WRITE_FLAG(flag) (flag & (OPEN_WRITE_FLAGS))
+
+static inline int branchperms(struct super_block *sb, int index)
+{
+	BUG_ON(index < 0);
+
+	return stopd(sb)->usi_data[index].branchperms;
+}
+static inline int set_branchperms(struct super_block *sb, int index, int perms)
+{
+	BUG_ON(index < 0);
+
+	stopd(sb)->usi_data[index].branchperms = perms;
+
+	return perms;
+}
+
+/* Is this file on a read-only branch? */
+static inline int __is_robranch_super(struct super_block *sb, int index,
+				      char *file, const char *function,
+				      int line)
+{
+	int err = 0;
+
+	print_util_entry_location();
+
+	if (!(branchperms(sb, index) & MAY_WRITE))
+		err = -EROFS;
+
+	print_util_exit_status(err);
+	return err;
+}
+
+/* Is this file on a read-only branch? */
+static inline int __is_robranch_index(struct dentry *dentry, int index,
+				      char *file, const char *function,
+				      int line)
+{
+	int err = 0;
+	int perms;
+
+	print_util_entry_location();
+
+	BUG_ON(index < 0);
+
+	perms = stopd(dentry->d_sb)->usi_data[index].branchperms;
+
+	if ((!(perms & MAY_WRITE))
+	    || (IS_RDONLY(dtohd_index(dentry, index)->d_inode)))
+		err = -EROFS;
+
+	print_util_exit_status(err);
+
+	return err;
+}
+static inline int __is_robranch(struct dentry *dentry, char *file,
+				const char *function, int line)
+{
+	int index;
+	int err;
+
+	print_util_entry_location();
+
+	index = dtopd(dentry)->udi_bstart;
+	BUG_ON(index < 0);
+
+	err = __is_robranch_index(dentry, index, file, function, line);
+
+	print_util_exit_status(err);
+
+	return err;
+}
+
+#define is_robranch(d) __is_robranch(d, __FILE__, __FUNCTION__, __LINE__)
+#define is_robranch_super(s, n) __is_robranch_super(s, n, __FILE__, __FUNCTION__, __LINE__)
+
+/* What do we use for whiteouts. */
+#define WHPFX ".wh."
+#define WHLEN 4
+/* If a directory contains this file, then it is opaque.  We start with the
+ * .wh. flag so that it is blocked by loomkup.
+ */
+#define UNIONFS_DIR_OPAQUE_NAME "__dir_opaque"
+#define UNIONFS_DIR_OPAQUE WHPFX UNIONFS_DIR_OPAQUE_NAME
+
+/* construct whiteout filename */
+static inline char *alloc_whname(const char *name, int len)
+{
+	char *buf;
+
+	buf = KMALLOC(len + WHLEN + 1, GFP_KERNEL);
+	if (!buf)
+		return ERR_PTR(-ENOMEM);
+
+	strcpy(buf, WHPFX);
+	strlcat(buf, name, len + WHLEN + 1);
+
+	return buf;
+}
+
+/* Definitions for various ways to handle errors.
+   Each flag's value is its bit position */
+
+/* 1 = DELETE_ALL, 0 = check for DELETE_WHITEOUT */
+#ifdef UNIONFS_DELETE_ALL
+#define DELETE_ALL		4
+#else
+#define DELETE_ALL 		0
+#endif
+
+#define VALID_MOUNT_FLAGS (DELETE_ALL)
+
+/*
+ * MACROS:
+ */
+
+#ifndef SEEK_SET
+#define SEEK_SET 0
+#endif				/* not SEEK_SET */
+
+#ifndef SEEK_CUR
+#define SEEK_CUR 1
+#endif				/* not SEEK_CUR */
+
+#ifndef SEEK_END
+#define SEEK_END 2
+#endif				/* not SEEK_END */
+
+#ifndef DEFAULT_POLLMASK
+#define DEFAULT_POLLMASK (POLLIN | POLLOUT | POLLRDNORM | POLLWRNORM)
+#endif
+
+/*
+ * EXTERNALS:
+ */
+
+/* JS: These two functions are here because it is kind of daft to copy and paste the
+ * contents of the two functions to 32+ places in unionfs
+ */
+static inline struct dentry *lock_parent(struct dentry *dentry)
+{
+	struct dentry *dir = DGET(dentry->d_parent);
+
+	mutex_lock(&dir->d_inode->i_mutex);
+	return dir;
+}
+
+static inline void unlock_dir(struct dentry *dir)
+{
+	mutex_unlock(&dir->d_inode->i_mutex);
+	DPUT(dir);
+}
+
+struct superio {
+	uid_t fsuid;
+	struct __user_cap_data_struct cap;
+	unsigned long rlim_core;
+};
+extern void superio_store(struct superio *sio);
+extern void superio_revert(struct superio *sio);
+
+extern int make_dir_opaque(struct dentry *dir, int bindex);
+
+#endif				/* __KERNEL__ */
+
+/*
+ * DEFINITIONS FOR USER AND KERNEL CODE:
+ * (Note: ioctl numbers 1--9 are reserved for fistgen, the rest
+ *  are auto-generated automatically based on the user's .fist file.)
+ */
+# define FIST_IOCTL_GET_DEBUG_VALUE	_IOR(0x15, 1, int)
+# define FIST_IOCTL_SET_DEBUG_VALUE	_IOW(0x15, 2, int)
+# define UNIONFS_IOCTL_BRANCH_COUNT	_IOR(0x15, 10, int)
+# define UNIONFS_IOCTL_INCGEN		_IOR(0x15, 11, int)
+# define UNIONFS_IOCTL_ADDBRANCH	_IOW(0x15, 12, int)
+# define UNIONFS_IOCTL_DELBRANCH	_IOW(0x15, 13, int)
+# define UNIONFS_IOCTL_RDWRBRANCH	_IOW(0x15, 14, int)
+# define UNIONFS_IOCTL_QUERYFILE	_IOR(0x15, 15, int)
+
+/* We don't support normal remount, but unionctl uses it. */
+# define UNIONFS_REMOUNT_MAGIC		0x4a5a4380
+
+/* should be at least LAST_USED_UNIONFS_PERMISSION<<1 */
+#define MAY_NFSRO			16
+
+struct unionfs_addbranch_args {
+	unsigned int ab_branch;
+	char *ab_path;
+	unsigned int ab_perms;
+};
+
+struct unionfs_rdwrbranch_args {
+	unsigned int rwb_branch;
+	unsigned int rwb_perms;
+};
+
+#endif				/* not __UNIONFS_H_ */
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/unionfs_debug.h newtree/fs/unionfs/unionfs_debug.h
--- oldtree/fs/unionfs/unionfs_debug.h	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/unionfs_debug.h	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,191 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: unionfs_debug.h,v 1.3 2006/06/01 21:25:18 jsipek Exp $
+ */
+
+#ifndef __UNIONFS_H_
+#error This file should only be included from unionfs.h!
+#endif
+
+#ifdef UNIONFS_DEBUG
+#define DEFAULT_DEBUG_MASK	0
+#else
+#define DEFAULT_DEBUG_MASK	(~0)
+#endif
+
+/* debug print levels */
+#define PRINT_NONE		0x0000
+#define PRINT_MAIN_ENTRY	0x0001
+#define PRINT_MAIN_EXIT		0x0002
+#define PRINT_UTILITY_ENTRY	0x0004
+#define PRINT_UTILITY_EXIT	0x0008
+#define PRINT_MISC_ENTRY	0x0010
+#define PRINT_MISC_EXIT		0x0020
+#define PRINT_DATA_DENTRY	0x0040
+#define PRINT_DATA_FILE		0x0080
+#define PRINT_DATA_INODE	0x0100
+#define PRINT_DATA_SB		0x0200
+#define PRINT_DEBUG		0x0400
+#define __PRINT_DEBUG_XATTR	0x0800
+#define PRINT_DEBUG_XATTR	(PRINT_DEBUG | __PRINT_DEBUG_XATTR)
+#define __PRINT_DEBUG_WHITEOUT	0x1000
+#define PRINT_DEBUG_WHITEOUT	(PRINT_DEBUG | __PRINT_DEBUG_WHITEOUT)
+
+#define PRINT_MAX		(0x2000 - 1)
+#define PRINT_ALL		(~PRINT_NONE)
+
+extern unsigned int get_debug_mask(void);
+extern int set_debug_mask(int val);
+
+/* print inode */
+extern void unionfs_print_inode(const unsigned int req, const char *prefix, const struct inode *inode);
+
+/* check inode */
+extern void unionfs_checkinode(const unsigned int req, const struct inode *inode, const char *msg);
+
+/* prunt file */
+extern void unionfs_print_file(const unsigned int req, const char *prefix, const struct file *file);
+
+/* print dentry */
+extern void unionfs_print_dentry(const unsigned int req, const char *prefix, const struct dentry *dentry);
+
+extern void unionfs_print_dentry_nocheck(const unsigned int req, const char *prefix, const struct dentry *dentry);
+
+/* print superblock */
+extern void unionfs_print_sb(const unsigned int req, const char *prefix, const struct super_block *sb);
+
+/* print message */
+extern int unionfs_print(const unsigned int req, const char *fmt, ...);
+
+/* forced print-debugging functions */
+#define force_print_dentry(prefix, ptr) \
+		unionfs_print_dentry(PRINT_ALL, (prefix), (ptr))
+#define force_print_dentry_nocheck(prefix, ptr) \
+		unionfs_print_dentry_nocheck(PRINT_ALL, (prefix), (ptr))
+#define force_print_file(prefix, ptr) \
+		unionfs_print_file(PRINT_ALL, (prefix), (ptr))
+#define force_print_inode(prefix, ptr) \
+		unionfs_print_inode(PRINT_ALL, (prefix), (ptr))
+#define force_print_sb(prefix, ptr) \
+		unionfs_print_sb(PRINT_ALL, (prefix), (ptr))
+
+#ifdef UNIONFS_DEBUG
+/*
+ * Full-fledged debugging enabled
+ */
+
+#define print_dentry(prefix, ptr) \
+		unionfs_print_dentry(PRINT_DATA_DENTRY, (prefix), (ptr))
+#define print_dentry_nocheck(prefix, ptr) \
+		unionfs_print_dentry_nocheck(PRINT_DATA_DENTRY, (prefix), (ptr))
+#define print_file(prefix, ptr) \
+		unionfs_print_file(PRINT_DATA_FILE, (prefix), (ptr))
+#define print_inode(prefix, ptr) \
+		unionfs_print_inode(PRINT_DATA_INODE, (prefix), (ptr))
+#define print_sb(prefix, ptr) \
+		unionfs_print_sb(PRINT_DATA_SB, (prefix), (ptr))
+#define dprint(req, fmt, args...) \
+		unionfs_print(req, fmt, ## args)
+
+#define checkinode(ptr, msg) \
+		unionfs_checkinode(PRINT_DEBUG, (ptr), (msg))
+
+#define __print_entryexit(req, ee, fmt, args...) \
+		unionfs_print((req), \
+			ee "  %s %s:%d" fmt "\n", \
+			__FUNCTION__, \
+			__FILE__, \
+			__LINE__, \
+			##args)
+
+#define print_entry(fmt, args...) \
+		__print_entryexit(PRINT_MAIN_ENTRY, \
+			"IN: ", " " fmt, ##args)
+
+#define print_entry_location() \
+		__print_entryexit(PRINT_MAIN_ENTRY, \
+			"IN: ", "")
+
+#define print_exit_location() \
+		__print_entryexit(PRINT_MAIN_EXIT, \
+			"OUT:", "")
+
+#define print_exit_status(status) \
+		__print_entryexit(PRINT_MAIN_EXIT, \
+			"OUT:", ", STATUS: %d", status)
+
+static inline void __print_exit_pointer(unsigned int req, void *status)
+{
+	if (IS_ERR(status))
+		__print_entryexit(req, "OUT:", ", STATUS: %ld",
+				PTR_ERR(status));
+	else
+		__print_entryexit(req, "OUT:", ", STATUS: 0x%p",
+				status);
+}
+#define print_exit_pointer(status) \
+		__print_exit_pointer(PRINT_MAIN_EXIT, status)
+
+#define print_util_entry(fmt, args...) \
+		__print_entryexit(PRINT_UTILITY_ENTRY, \
+			"IN: ", " " fmt, ##args)
+
+#define print_util_entry_location() \
+		__print_entryexit(PRINT_UTILITY_ENTRY, \
+			"IN: ", "")
+
+#define print_util_exit_location() \
+		__print_entryexit(PRINT_UTILITY_EXIT, \
+			"OUT:", "")
+
+#define print_util_exit_status(status) \
+		__print_entryexit(PRINT_UTILITY_EXIT, \
+			"OUT:", ", STATUS: %d", status)
+
+#define print_util_exit_pointer(status) \
+		__print_exit_pointer(PRINT_UTILITY_EXIT, status)
+
+#else /* UNIONFS_DEBUG */
+/*
+ * Full-fledged debugging disabled
+ */
+
+#define print_dentry(prefix, ptr)
+#define print_dentry_nocheck(prefix, ptr)
+#define print_file(prefix, ptr)
+#define print_inode(prefix, ptr)
+#define print_sb(prefix, ptr)
+#define dprint(req, fmt, args...)
+
+#define checkinode(ptr, msg)
+
+#define print_entry(args...)
+#define print_entry_location()
+#define print_exit_location()
+#define print_exit_status(status)
+#define print_exit_pointer(status)
+#define print_util_entry(args...)
+#define print_util_entry_location()
+#define print_util_exit_location()
+#define print_util_exit_status(status)
+#define print_util_exit_pointer(status)
+
+#endif /* ! UNIONFS_DEBUG */
+
+
diff -urN oldtree/fs/unionfs/unionfs_imap.h newtree/fs/unionfs/unionfs_imap.h
--- oldtree/fs/unionfs/unionfs_imap.h	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/unionfs_imap.h	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,92 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: unionfs_imap.h,v 1.1 2006/05/30 21:38:45 jsipek Exp $
+ */
+
+#ifndef __UNIONFS_H_
+#error This file should only be included from unionfs.h!
+#endif
+
+#ifdef UNIONFS_IMAP
+
+/*UUID typedef needed later*/
+typedef uint8_t uuid_t[16];
+
+/*
+* Defines,structs,and functions for persistent used by kernel and user
+*/
+#define MAX_MAPS 256
+#define UUID_LEN 16
+#define FORWARDMAP_MAGIC 0x4b1cb38f
+#define REVERSEMAP_MAGIC 0Xfcafad71
+#define FORWARDMAP_VERSION 0x02
+#define REVERSEMAP_VERSION 0x01
+#define FIRST_VALID_INODE 3
+struct fmaphdr {
+	uint32_t magic;
+	uint32_t version;
+	uint8_t usedbranches;
+	uint8_t uuid[UUID_LEN];
+};
+
+struct rmaphdr {
+	uint32_t magic;
+	uint32_t version;
+	uint8_t fwduuid[UUID_LEN];
+	uint8_t revuuid[UUID_LEN];
+	fsid_t fsid;
+};
+struct bmapent {
+	fsid_t fsid;
+	uint8_t uuid[UUID_LEN];
+};
+struct fmapent {
+	uint8_t fsnum;
+	uint64_t inode;
+};
+
+/* Persistant Inode functions */
+extern int read_uin(struct super_block *sb, uint8_t branchnum,
+		    ino_t inode_number, int flag, ino_t * uino);
+extern int write_uin(struct super_block *sb, ino_t ino, int bindex,
+		     ino_t hidden_ino);
+extern int get_lin(struct super_block *sb, ino_t inode_number,
+		   struct fmapent *entry);
+extern int parse_imap_option(struct super_block *sb,
+			     struct unionfs_dentry_info *hidden_root_info,
+			     char *options);
+extern void cleanup_imap_data(struct super_block *sb);
+
+#endif				/*#ifdef UNIONFS_IMAP */
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/unionfs_macros.h newtree/fs/unionfs/unionfs_macros.h
--- oldtree/fs/unionfs/unionfs_macros.h	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/unionfs_macros.h	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,206 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: unionfs_macros.h,v 1.13 2006/06/01 03:11:03 jsipek Exp $
+ */
+
+#ifndef __UNIONFS_H_
+#error This file should only be included from unionfs.h!
+#endif
+
+/* Inode to private data */
+static inline struct unionfs_inode_info *itopd(const struct inode *inode)
+{
+	return
+	    &(container_of(inode, struct unionfs_inode_container, vfs_inode)->
+	      info);
+}
+
+#define itohi_ptr(ino) (itopd(ino)->uii_inode)
+#define ibstart(ino) (itopd(ino)->b_start)
+#define ibend(ino) (itopd(ino)->b_end)
+
+/* Superblock to private data */
+#define stopd(super) ((struct unionfs_sb_info *)(super)->s_fs_info)
+#define stopd_lhs(super) ((super)->s_fs_info)
+#define sbstart(sb) 0
+#define sbend(sb) stopd(sb)->b_end
+#define sbmax(sb) (stopd(sb)->b_end + 1)
+
+/* File to private Data */
+#define ftopd(file) ((struct unionfs_file_info *)((file)->private_data))
+#define ftopd_lhs(file) ((file)->private_data)
+#define ftohf_ptr(file)  (ftopd(file)->ufi_file)
+#define fbstart(file) (ftopd(file)->b_start)
+#define fbend(file) (ftopd(file)->b_end)
+
+/* File to hidden file. */
+static inline struct file *ftohf(struct file *f)
+{
+	return ftopd(f)->ufi_file[fbstart(f)];
+}
+
+static inline struct file *ftohf_index(const struct file *f, int index)
+{
+	return ftopd(f)->ufi_file[index];
+}
+
+static inline void set_ftohf_index(struct file *f, int index, struct file *val)
+{
+	ftopd(f)->ufi_file[index] = val;
+}
+
+static inline void set_ftohf(struct file *f, struct file *val)
+{
+	ftopd(f)->ufi_file[fbstart(f)] = val;
+}
+
+/* Inode to hidden inode. */
+static inline struct inode *itohi(const struct inode *i)
+{
+	return itopd(i)->uii_inode[ibstart(i)];
+}
+
+static inline struct inode *itohi_index(const struct inode *i, int index)
+{
+	return itopd(i)->uii_inode[index];
+}
+
+static inline void set_itohi_index(struct inode *i, int index,
+				   struct inode *val)
+{
+	itopd(i)->uii_inode[index] = val;
+}
+
+static inline void set_itohi(struct inode *i, struct inode *val)
+{
+	itopd(i)->uii_inode[ibstart(i)] = val;
+}
+
+/* Superblock to hidden superblock. */
+static inline struct super_block *stohs(const struct super_block *o)
+{
+	return stopd(o)->usi_data[sbstart(o)].sb;
+}
+
+static inline struct super_block *stohs_index(const struct super_block *o, int index)
+{
+	return stopd(o)->usi_data[index].sb;
+}
+
+static inline void set_stohs_index(struct super_block *o, int index,
+				   struct super_block *val)
+{
+	stopd(o)->usi_data[index].sb = val;
+}
+
+static inline void set_stohs(struct super_block *o, struct super_block *val)
+{
+	stopd(o)->usi_data[sbstart(o)].sb = val;
+}
+
+/* Super to hidden mount. */
+static inline struct vfsmount *stohiddenmnt_index(struct super_block *o,
+						  int index)
+{
+	return stopd(o)->usi_data[index].hidden_mnt;
+}
+
+static inline void set_stohiddenmnt_index(struct super_block *o, int index,
+					  struct vfsmount *val)
+{
+	stopd(o)->usi_data[index].hidden_mnt = val;
+}
+
+/* Branch count macros. */
+static inline int branch_count(struct super_block *o, int index)
+{
+	return atomic_read(&stopd(o)->usi_data[index].sbcount);
+}
+
+static inline void set_branch_count(struct super_block *o, int index, int val)
+{
+	atomic_set(&stopd(o)->usi_data[index].sbcount, val);
+}
+
+static inline void branchget(struct super_block *o, int index)
+{
+	atomic_inc(&stopd(o)->usi_data[index].sbcount);
+}
+
+static inline void branchput(struct super_block *o, int index)
+{
+	atomic_dec(&stopd(o)->usi_data[index].sbcount);
+}
+
+/* Dentry macros */
+static inline struct unionfs_dentry_info *dtopd(const struct dentry *dent)
+{
+	return (struct unionfs_dentry_info *)dent->d_fsdata;
+}
+
+#define dtopd_lhs(dent) ((dent)->d_fsdata)
+#define dtopd_nocheck(dent) dtopd(dent)
+#define dbstart(dent) (dtopd(dent)->udi_bstart)
+#define set_dbstart(dent, val) do { dtopd(dent)->udi_bstart = val; } while(0)
+#define dbend(dent) (dtopd(dent)->udi_bend)
+#define set_dbend(dent, val) do { dtopd(dent)->udi_bend = val; } while(0)
+#define dbopaque(dent) (dtopd(dent)->udi_bopaque)
+#define set_dbopaque(dent, val) do { dtopd(dent)->udi_bopaque = val; } while (0)
+
+static inline void set_dtohd_index(struct dentry *dent, int index,
+				   struct dentry *val)
+{
+	dtopd(dent)->udi_dentry[index] = val;
+}
+
+static inline struct dentry *dtohd_index(const struct dentry *dent, int index)
+{
+	return dtopd(dent)->udi_dentry[index];
+}
+
+static inline struct dentry *dtohd(const struct dentry *dent)
+{
+	return dtopd(dent)->udi_dentry[dbstart(dent)];
+}
+
+#define set_dtohd_index_nocheck(dent, index, val) set_dtohd_index(dent, index, val)
+#define dtohd_index_nocheck(dent, index) dtohd_index(dent, index)
+
+#define dtohd_ptr(dent) (dtopd_nocheck(dent)->udi_dentry)
+
+/* Macros for locking a dentry. */
+#define lock_dentry(d) down(&dtopd(d)->udi_sem)
+#define unlock_dentry(d) up(&dtopd(d)->udi_sem)
+#define verify_locked(d)
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/unlink.c newtree/fs/unionfs/unlink.c
--- oldtree/fs/unionfs/unlink.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/unlink.c	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,377 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: unlink.c,v 1.43 2006/07/08 17:58:31 ezk Exp $
+ */
+
+#include "unionfs.h"
+
+#ifdef UNIONFS_DELETE_ALL
+static int unionfs_unlink_all(struct inode *dir, struct dentry *dentry)
+{
+	struct dentry *hidden_dentry;
+	struct dentry *hidden_dir_dentry;
+	int bstart, bend, bindex;
+	int err = 0;
+	int global_err = 0;
+
+	print_entry_location();
+
+	if ((err = unionfs_partial_lookup(dentry)))
+		goto out;
+
+	bstart = dbstart(dentry);
+	bend = dbend(dentry);
+
+	for (bindex = bend; bindex >= bstart; bindex--) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		if (!hidden_dentry)
+			continue;
+
+		hidden_dir_dentry = lock_parent(hidden_dentry);
+
+		/* avoid destroying the hidden inode if the file is in use */
+		DGET(hidden_dentry);
+		if (!(err = is_robranch_super(dentry->d_sb, bindex)))
+			err = vfs_unlink(hidden_dir_dentry->d_inode,
+					 hidden_dentry);
+		DPUT(hidden_dentry);
+		fist_copy_attr_times(dir, hidden_dir_dentry->d_inode);
+		unlock_dir(hidden_dir_dentry);
+
+		if (err) {
+			/* passup the last error we got */
+			if (!IS_COPYUP_ERR(err))
+				goto out;
+			global_err = err;
+		}
+	}
+
+	/* check if encountered error in the above loop */
+	if (global_err) {
+		/* If we failed in the leftmost branch, then err will be set
+		 * and we should move one over to create the whiteout.
+		 * Otherwise, we should try in the leftmost branch. */
+		if (err) {
+			if (dbstart(dentry) == 0) {
+				goto out;
+			}
+			err = create_whiteout(dentry, dbstart(dentry) - 1);
+		} else {
+			err = create_whiteout(dentry, dbstart(dentry));
+		}
+	} else if (dbopaque(dentry) != -1) {
+		/* There is a hidden lower-priority file with the same name. */
+		err = create_whiteout(dentry, dbopaque(dentry));
+	}
+      out:
+	/* propagate number of hard-links */
+	if (dentry->d_inode->i_nlink != 0) {
+		dentry->d_inode->i_nlink = get_nlinks(dentry->d_inode);
+		if (!err && global_err)
+			dentry->d_inode->i_nlink--;
+	}
+	/* We don't want to leave negative leftover dentries for revalidate. */
+	if (!err && (global_err || dbopaque(dentry) != -1))
+		update_bstart(dentry);
+
+	print_exit_status(err);
+	return err;
+}
+#endif
+static int unionfs_unlink_whiteout(struct inode *dir, struct dentry *dentry)
+{
+	struct dentry *hidden_dentry;
+	struct dentry *hidden_dir_dentry;
+	int bindex;
+	int err = 0;
+
+	print_entry_location();
+
+	if ((err = unionfs_partial_lookup(dentry)))
+		goto out;
+
+	bindex = dbstart(dentry);
+
+	hidden_dentry = dtohd_index(dentry, bindex);
+	if (!hidden_dentry)
+		goto out;
+
+	hidden_dir_dentry = lock_parent(hidden_dentry);
+
+	/* avoid destroying the hidden inode if the file is in use */
+	DGET(hidden_dentry);
+	if (!(err = is_robranch_super(dentry->d_sb, bindex)))
+		err = vfs_unlink(hidden_dir_dentry->d_inode, hidden_dentry);
+	DPUT(hidden_dentry);
+	fist_copy_attr_times(dir, hidden_dir_dentry->d_inode);
+	unlock_dir(hidden_dir_dentry);
+
+	if (err && !IS_COPYUP_ERR(err))
+		goto out;
+
+	if (err) {
+		if (dbstart(dentry) == 0)
+			goto out;
+
+		err = create_whiteout(dentry, dbstart(dentry) - 1);
+	} else if (dbopaque(dentry) != -1) {
+		/* There is a hidden lower-priority file with the same name. */
+		err = create_whiteout(dentry, dbopaque(dentry));
+	} else {
+		err = create_whiteout(dentry, dbstart(dentry));
+	}
+
+      out:
+	if (!err)
+		dentry->d_inode->i_nlink--;
+
+	/* We don't want to leave negative leftover dentries for revalidate. */
+	if (!err && (dbopaque(dentry) != -1))
+		update_bstart(dentry);
+
+	print_exit_status(err);
+	return err;
+
+}
+
+int unionfs_unlink(struct inode *dir, struct dentry *dentry)
+{
+	int err = 0;
+
+	print_entry_location();
+	lock_dentry(dentry);
+	print_dentry("IN unionfs_unlink", dentry);
+
+#ifdef UNIONFS_DELETE_ALL
+	if (IS_SET(dir->i_sb, DELETE_ALL))
+		err = unionfs_unlink_all(dir, dentry);
+	else
+#endif
+		err = unionfs_unlink_whiteout(dir, dentry);
+	/* call d_drop so the system "forgets" about us */
+	if (!err)
+		d_drop(dentry);
+
+	unlock_dentry(dentry);
+	print_exit_status(err);
+	return err;
+}
+
+static int unionfs_rmdir_first(struct inode *dir, struct dentry *dentry,
+			       struct unionfs_dir_state *namelist)
+{
+	int err;
+	struct dentry *hidden_dentry;
+	struct dentry *hidden_dir_dentry = NULL;
+
+	print_entry_location();
+	print_dentry("IN unionfs_rmdir_first: ", dentry);
+
+	/* Here we need to remove whiteout entries. */
+	err = delete_whiteouts(dentry, dbstart(dentry), namelist);
+	if (err) {
+		goto out;
+	}
+
+	hidden_dentry = dtohd(dentry);
+
+	hidden_dir_dentry = lock_parent(hidden_dentry);
+
+	/* avoid destroying the hidden inode if the file is in use */
+	DGET(hidden_dentry);
+	if (!(err = is_robranch(dentry))) {
+		err = vfs_rmdir(hidden_dir_dentry->d_inode, hidden_dentry);
+	}
+	DPUT(hidden_dentry);
+
+	fist_copy_attr_times(dir, hidden_dir_dentry->d_inode);
+	/* propagate number of hard-links */
+	dentry->d_inode->i_nlink = get_nlinks(dentry->d_inode);
+
+      out:
+	if (hidden_dir_dentry) {
+		unlock_dir(hidden_dir_dentry);
+	}
+	print_dentry("OUT unionfs_rmdir_first: ", dentry);
+	print_exit_status(err);
+	return err;
+}
+
+#ifdef UNIONFS_DELETE_ALL
+static int unionfs_rmdir_all(struct inode *dir, struct dentry *dentry,
+			     struct unionfs_dir_state *namelist)
+{
+	struct dentry *hidden_dentry;
+	struct dentry *hidden_dir_dentry;
+	int bstart, bend, bindex;
+	int err = 0;
+	int global_err = 0;
+
+	print_entry_location();
+	print_dentry("IN unionfs_rmdir_all: ", dentry);
+
+	bstart = dbstart(dentry);
+	bend = dbend(dentry);
+
+	for (bindex = bend; bindex >= bstart; bindex--) {
+		hidden_dentry = dtohd_index(dentry, bindex);
+		if (!hidden_dentry)
+			continue;
+
+		hidden_dir_dentry = lock_parent(hidden_dentry);
+		if (S_ISDIR(hidden_dentry->d_inode->i_mode)) {
+			err = delete_whiteouts(dentry, bindex, namelist);
+			if (!err
+			    && !(err =
+				 is_robranch_super(dentry->d_sb, bindex))) {
+				err =
+				    vfs_rmdir(hidden_dir_dentry->d_inode,
+					      hidden_dentry);
+			}
+		} else {
+			err = -EISDIR;
+		}
+
+		fist_copy_attr_times(dir, hidden_dir_dentry->d_inode);
+		unlock_dir(hidden_dir_dentry);
+		if (err) {
+			int local_err =
+			    unionfs_refresh_hidden_dentry(dentry, bindex);
+			if (local_err) {
+				err = local_err;
+				goto out;
+			}
+
+			if (!IS_COPYUP_ERR(err) && err != -ENOTEMPTY
+			    && err != -EISDIR)
+				goto out;
+
+			global_err = err;
+		}
+	}
+
+	/* check if encountered error in the above loop */
+	if (global_err) {
+		/* If we failed in the leftmost branch, then err will be set and we should
+		 * move one over to create the whiteout.  Otherwise, we should try in the
+		 * leftmost branch.
+		 */
+		if (err) {
+			if (dbstart(dentry) == 0) {
+				goto out;
+			}
+			err = create_whiteout(dentry, dbstart(dentry) - 1);
+		} else {
+			err = create_whiteout(dentry, dbstart(dentry));
+		}
+	} else {
+		err = create_whiteout(dentry, dbstart(dentry));
+	}
+
+      out:
+	/* propagate number of hard-links */
+	dentry->d_inode->i_nlink = get_nlinks(dentry->d_inode);
+
+	print_dentry("OUT unionfs_rmdir_all: ", dentry);
+	print_exit_status(err);
+	return err;
+}
+#endif
+int unionfs_rmdir(struct inode *dir, struct dentry *dentry)
+{
+	int err = 0;
+	struct unionfs_dir_state *namelist = NULL;
+
+	print_entry_location();
+	lock_dentry(dentry);
+	print_dentry("IN unionfs_rmdir: ", dentry);
+
+	/* check if this unionfs directory is empty or not */
+	err = check_empty(dentry, &namelist);
+	if (err) {
+#if 0
+		/* vfs_rmdir(our caller) unhashed the dentry.  This will recover
+		 * the Unionfs inode number for the directory itself, but the
+		 * children are already lost.  It seems that tmpfs manages its
+		 * way around this by upping the refcount on everything.
+		 *
+		 * Even if we do this, we still lose the inode numbers of the
+		 * children.  The best way to fix this is to fix the VFS (or
+		 * use persistent inode maps). */
+		if (d_unhashed(dentry))
+			d_rehash(dentry);
+#endif
+		goto out;
+	}
+#ifdef UNIONFS_DELETE_ALL
+	if (IS_SET(dir->i_sb, DELETE_ALL)) {
+		/* delete all. */
+		err = unionfs_rmdir_all(dir, dentry, namelist);
+	} else {		/* Delete the first directory. */
+#endif
+		err = unionfs_rmdir_first(dir, dentry, namelist);
+		/* create whiteout */
+		if (!err) {
+			err = create_whiteout(dentry, dbstart(dentry));
+		} else {
+			int new_err;
+
+			if (dbstart(dentry) == 0)
+				goto out;
+
+			/* exit if the error returned was NOT -EROFS */
+			if (!IS_COPYUP_ERR(err))
+				goto out;
+
+			new_err = create_whiteout(dentry, dbstart(dentry) - 1);
+			if (new_err != -EEXIST)
+				err = new_err;
+		}
+
+#ifdef UNIONFS_DELETE_ALL
+	}
+#endif
+      out:
+	/* call d_drop so the system "forgets" about us */
+	if (!err)
+		d_drop(dentry);
+
+	if (namelist)
+		free_rdstate(namelist);
+
+	unlock_dentry(dentry);
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/fs/unionfs/xattr.c newtree/fs/unionfs/xattr.c
--- oldtree/fs/unionfs/xattr.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/fs/unionfs/xattr.c	2006-07-12 19:01:52.000000000 -0400
@@ -0,0 +1,185 @@
+/*
+ * Copyright (c) 2003-2006 Erez Zadok
+ * Copyright (c) 2003-2006 Charles P. Wright
+ * Copyright (c) 2005-2006 Josef Sipek
+ * Copyright (c) 2005      Arun M. Krishnakumar
+ * Copyright (c) 2005-2006 David P. Quigley
+ * Copyright (c) 2003-2004 Mohammad Nayyer Zubair
+ * Copyright (c) 2003      Puja Gupta
+ * Copyright (c) 2003      Harikesavan Krishnan
+ * Copyright (c) 2003-2006 Stony Brook University
+ * Copyright (c) 2003-2006 The Research Foundation of State University of New York
+ *
+ * For specific licensing information, see the COPYING file distributed with
+ * this package.
+ *
+ * This Copyright notice must be kept intact and distributed with all sources.
+ */
+/*
+ *  $Id: xattr.c,v 1.32 2006/06/01 03:11:03 jsipek Exp $
+ */
+
+#include "unionfs.h"
+
+/* This is lifted from fs/xattr.c */
+void *xattr_alloc(size_t size, size_t limit)
+{
+	void *ptr;
+
+	if (size > limit)
+		return ERR_PTR(-E2BIG);
+
+	if (!size)		/* size request, no buffer is needed */
+		return NULL;
+	else if (size <= PAGE_SIZE)
+		ptr = KMALLOC((unsigned long)size, GFP_KERNEL);
+	else
+		ptr = vmalloc((unsigned long)size);
+	if (!ptr)
+		return ERR_PTR(-ENOMEM);
+	return ptr;
+}
+
+void xattr_free(void *ptr, size_t size)
+{
+	if (!size)		/* size request, no buffer was needed */
+		return;
+	else if (size <= PAGE_SIZE)
+		KFREE(ptr);
+	else
+		vfree(ptr);
+}
+
+/* Function supplied by guy from redhat. In here temporarily till he submits it
+ * to the kernel maling list
+ */
+ssize_t vfs_listxattr(struct dentry *d, char *list, size_t size)
+{
+	ssize_t error;
+
+	error = security_inode_listxattr(d);
+	if (error)
+		return error;
+	error = -EOPNOTSUPP;
+	if (d->d_inode->i_op && d->d_inode->i_op->listxattr) {
+		error = d->d_inode->i_op->listxattr(d, list, size);
+	} else {
+		error = security_inode_listsecurity(d->d_inode, list, size);
+		if (size && error > size)
+			error = -ERANGE;
+	}
+	return error;
+}
+
+/* BKL held by caller.
+ * dentry->d_inode->i_mutex locked
+ * ssize_t (*getxattr) (struct dentry *, const char *, void *, size_t);
+ */
+ssize_t unionfs_getxattr(struct dentry * dentry, const char *name, void *value,
+			 size_t size)
+{
+	struct dentry *hidden_dentry = NULL;
+	int err = -EOPNOTSUPP;
+
+	print_entry_location();
+
+	dprint(PRINT_DEBUG_XATTR, "getxattr: name=\"%s\", value %lu bytes\n",
+			name, size);
+
+	lock_dentry(dentry);
+
+	hidden_dentry = dtohd(dentry);
+
+	err = vfs_getxattr(hidden_dentry, (char*)name, value, size);
+
+	unlock_dentry(dentry);
+	print_exit_status(err);
+	return err;
+}
+
+/* BKL held by caller.
+ * dentry->d_inode->i_mutex locked
+ */
+int
+unionfs_setxattr(struct dentry *dentry, const char *name, const void *value,
+		 size_t size, int flags)
+{
+	struct dentry *hidden_dentry = NULL;
+	int err = -EOPNOTSUPP;
+
+	print_entry_location();
+
+	lock_dentry(dentry);
+	hidden_dentry = dtohd(dentry);
+
+	dprint(PRINT_DEBUG_XATTR, "setxattr: name=\"%s\", value %lu bytes,"
+			"flags=%x\n", name, (unsigned long)size, flags);
+
+	err =
+	    vfs_setxattr(hidden_dentry, (char *)name, (char *)value, size,
+			 flags);
+
+	unlock_dentry(dentry);
+	print_exit_status(err);
+	return err;
+}
+
+/* BKL held by caller.
+ * dentry->d_inode->i_mutex locked
+ */
+int unionfs_removexattr(struct dentry *dentry, const char *name)
+{
+	struct dentry *hidden_dentry = NULL;
+	int err = -EOPNOTSUPP;
+
+	print_entry_location();
+
+	lock_dentry(dentry);
+	hidden_dentry = dtohd(dentry);
+
+	dprint(PRINT_DEBUG_XATTR, "removexattr: name=\"%s\"\n", name);
+
+	err = vfs_removexattr(hidden_dentry, (char*)name);
+
+	unlock_dentry(dentry);
+	print_exit_status(err);
+	return err;
+}
+
+/* BKL held by caller.
+ * dentry->d_inode->i_mutex locked
+ */
+ssize_t unionfs_listxattr(struct dentry * dentry, char *list, size_t size)
+{
+	struct dentry *hidden_dentry = NULL;
+	int err = -EOPNOTSUPP;
+	char *encoded_list = NULL;
+
+	print_entry_location();
+	lock_dentry(dentry);
+
+	hidden_dentry = dtohd(dentry);
+
+	encoded_list = list;
+	err = vfs_listxattr(hidden_dentry, encoded_list, size);
+
+	unlock_dentry(dentry);
+	print_exit_status(err);
+	return err;
+}
+
+/*
+ *
+ * vim:shiftwidth=8
+ * vim:tabstop=8
+ *
+ * For Emacs:
+ * Local variables:
+ * c-basic-offset: 8
+ * c-comment-only-line-offset: 0
+ * c-offsets-alist: ((statement-block-intro . +) (knr-argdecl-intro . 0)
+ *              (substatement-open . 0) (label . 0) (statement-cont . +))
+ * indent-tabs-mode: t
+ * tab-width: 8
+ * End:
+ */
diff -urN oldtree/include/acpi/acpiosxf.h newtree/include/acpi/acpiosxf.h
--- oldtree/include/acpi/acpiosxf.h	2006-07-05 10:06:57.000000000 -0400
+++ newtree/include/acpi/acpiosxf.h	2006-07-12 19:00:43.000000000 -0400
@@ -95,6 +95,10 @@
 acpi_os_table_override(struct acpi_table_header *existing_table,
 		       struct acpi_table_header **new_table);
 
+#ifdef CONFIG_ACPI_CUSTOM_DSDT_INITRD
+extern int acpi_must_unregister_table;
+#endif
+
 /*
  * Spinlock primitives
  */
diff -urN oldtree/include/linux/console_splash.h newtree/include/linux/console_splash.h
--- oldtree/include/linux/console_splash.h	1969-12-31 19:00:00.000000000 -0500
+++ newtree/include/linux/console_splash.h	2006-07-12 19:01:05.000000000 -0400
@@ -0,0 +1,13 @@
+#ifndef _LINUX_CONSOLE_SPLASH_H_
+#define _LINUX_CONSOLE_SPLASH_H_ 1
+
+/* A structure used by the framebuffer splash code (drivers/video/fbsplash.c) */
+struct vc_splash {
+	__u8 bg_color;				/* The color that is to be treated as transparent */
+	__u8 state;				/* Current splash state: 0 = off, 1 = on */
+	__u16 tx, ty;				/* Top left corner coordinates of the text field */
+	__u16 twidth, theight;			/* Width and height of the text field */
+	char* theme;
+};
+
+#endif
diff -urN oldtree/include/linux/console_struct.h newtree/include/linux/console_struct.h
--- oldtree/include/linux/console_struct.h	2006-07-05 10:06:57.000000000 -0400
+++ newtree/include/linux/console_struct.h	2006-07-12 19:01:05.000000000 -0400
@@ -15,6 +15,7 @@
 struct vt_struct;
 
 #define NPAR 16
+#include <linux/console_splash.h>
 
 struct vc_data {
 	unsigned short	vc_num;			/* Console number */
@@ -98,6 +99,8 @@
 	struct vc_data **vc_display_fg;		/* [!] Ptr to var holding fg console for this display */
 	unsigned long	vc_uni_pagedir;
 	unsigned long	*vc_uni_pagedir_loc;  /* [!] Location of uni_pagedir variable for this console */
+
+	struct vc_splash vc_splash;
 	/* additional information is in vt_kern.h */
 };
 
diff -urN oldtree/include/linux/fb.h newtree/include/linux/fb.h
--- oldtree/include/linux/fb.h	2006-07-05 10:06:57.000000000 -0400
+++ newtree/include/linux/fb.h	2006-07-12 19:01:05.000000000 -0400
@@ -9,6 +9,13 @@
 #define FB_MAJOR		29
 #define FB_MAX			32	/* sufficient for now */
 
+struct fb_splash_iowrapper
+{
+	unsigned short vc;		/* Virtual console */
+	unsigned char origin;		/* Point of origin of the request */
+	void *data;
+};
+
 /* ioctls
    0x46 is 'F'								*/
 #define FBIOGET_VSCREENINFO	0x4600
@@ -36,7 +43,15 @@
 #define FBIOGET_HWCINFO         0x4616
 #define FBIOPUT_MODEINFO        0x4617
 #define FBIOGET_DISPINFO        0x4618
-
+#define FBIOSPLASH_SETCFG	_IOWR('F', 0x19, struct fb_splash_iowrapper)
+#define FBIOSPLASH_GETCFG	_IOR('F', 0x1A, struct fb_splash_iowrapper)
+#define FBIOSPLASH_SETSTATE	_IOWR('F', 0x1B, struct fb_splash_iowrapper)
+#define FBIOSPLASH_GETSTATE	_IOR('F', 0x1C, struct fb_splash_iowrapper)
+#define FBIOSPLASH_SETPIC 	_IOWR('F', 0x1D, struct fb_splash_iowrapper)
+
+#define FB_SPLASH_THEME_LEN		128	/* Maximum lenght of a theme name */
+#define FB_SPLASH_IO_ORIG_KERNEL	0	/* Kernel ioctl origin */
+#define FB_SPLASH_IO_ORIG_USER		1 	/* User ioctl origin */
 
 #define FB_TYPE_PACKED_PIXELS		0	/* Packed Pixels	*/
 #define FB_TYPE_PLANES			1	/* Non interleaved planes */
@@ -785,6 +800,9 @@
 #define FBINFO_STATE_SUSPENDED	1
 	u32 state;			/* Hardware state i.e suspend */
 	void *fbcon_par;                /* fbcon use-only private area */
+
+	struct fb_image splash;
+
 	/* From here on everything is device dependent */
 	void *par;	
 };
diff -urN oldtree/include/linux/mm.h newtree/include/linux/mm.h
--- oldtree/include/linux/mm.h	2006-07-05 10:06:57.000000000 -0400
+++ newtree/include/linux/mm.h	2006-07-12 19:03:34.000000000 -0400
@@ -165,6 +165,7 @@
 #define VM_NONLINEAR	0x00800000	/* Is non-linear (remap_file_pages) */
 #define VM_MAPPED_COPY	0x01000000	/* T if mapped copy of data (nommu mmap) */
 #define VM_INSERTPAGE	0x02000000	/* The vma has had "vm_insert_page()" done on it */
+#define VM_SHM          0x00000000      /* ATI Drivers won't work unless you supply this def. */
 
 #ifndef VM_STACK_DEFAULT_FLAGS		/* arch can override this */
 #define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS
diff -urN oldtree/include/linux/mmzone.h newtree/include/linux/mmzone.h
--- oldtree/include/linux/mmzone.h	2006-07-05 10:06:57.000000000 -0400
+++ newtree/include/linux/mmzone.h	2006-07-12 19:02:48.000000000 -0400
@@ -138,7 +138,7 @@
 struct zone {
 	/* Fields commonly accessed by the page allocator */
 	unsigned long		free_pages;
-	unsigned long		pages_min, pages_low, pages_high;
+	unsigned long		pages_min, pages_low, pages_high, pages_lots;
 	/*
 	 * We don't know if the memory that we're going to allocate will be freeable
 	 * or/and it will be released eventually, so to avoid totally wasting several
diff -urN oldtree/include/linux/sched.h newtree/include/linux/sched.h
--- oldtree/include/linux/sched.h	2006-07-05 10:06:57.000000000 -0400
+++ newtree/include/linux/sched.h	2006-07-12 19:01:08.000000000 -0400
@@ -207,6 +207,9 @@
 
 void io_schedule(void);
 long io_schedule_timeout(long timeout);
+#ifdef CONFIG_STAIRCASE
+extern int sched_interactive, sched_compute;
+#endif
 
 extern void cpu_init (void);
 extern void trap_init(void);
@@ -504,6 +507,9 @@
 #define MAX_RT_PRIO		MAX_USER_RT_PRIO
 
 #define MAX_PRIO		(MAX_RT_PRIO + 40)
+#ifdef CONFIG_STAIRCASE
+#define MIN_USER_PRIO          (MAX_PRIO - 1)
+#endif
 
 #define rt_prio(prio)		unlikely((prio) < MAX_RT_PRIO)
 #define rt_task(p)		rt_prio((p)->prio)
@@ -776,14 +782,18 @@
 struct pipe_inode_info;
 struct uts_namespace;
 
+#ifdef CONFIG_INGOSCHED
 enum sleep_type {
 	SLEEP_NORMAL,
 	SLEEP_NONINTERACTIVE,
 	SLEEP_INTERACTIVE,
 	SLEEP_INTERRUPTED,
 };
+#endif
 
+#ifdef CONFIG_INGOSCHED
 struct prio_array;
+#endif
 
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
@@ -802,19 +812,33 @@
 	int load_weight;	/* for niceness load balancing purposes */
 	int prio, static_prio, normal_prio;
 	struct list_head run_list;
+#ifdef CONFIG_INGOSCHED
 	struct prio_array *array;
+#endif
 
 	unsigned short ioprio;
 	unsigned int btrace_seq;
 
+#ifdef CONFIG_INGOSCHED
 	unsigned long sleep_avg;
 	unsigned long long timestamp, last_ran;
+#endif
+#ifdef CONFIG_STAIRCASE
+          unsigned long long timestamp;
+          unsigned long runtime, totalrun, ns_debit, systime;
+          unsigned int bonus;
+          unsigned int slice, time_slice;
+#endif
 	unsigned long long sched_time; /* sched_clock time spent running */
+#ifdef CONFIG_INGOSCHED
 	enum sleep_type sleep_type;
+#endif
 
 	unsigned long policy;
 	cpumask_t cpus_allowed;
+#ifdef CONFIG_INGOSCHED
 	unsigned int time_slice, first_time_slice;
+#endif
 
 #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
 	struct sched_info sched_info;
@@ -1092,6 +1116,10 @@
 #define PF_SPREAD_SLAB	0x02000000	/* Spread some slab caches over cpuset */
 #define PF_MEMPOLICY	0x10000000	/* Non-default NUMA mempolicy */
 #define PF_MUTEX_TESTER	0x20000000	/* Thread belongs to the rt mutex tester */
+#ifdef CONFIG_STAIRCASE
+#define PF_NONSLEEP    0x40000000      /* Waiting on in kernel activity */
+#define PF_FORKED      0x80000000      /* Task just forked another process */
+#endif
 
 /*
  * Only the _current_ task can read/write to tsk->flags, but other
@@ -1227,7 +1255,9 @@
  static inline void kick_process(struct task_struct *tsk) { }
 #endif
 extern void FASTCALL(sched_fork(struct task_struct * p, int clone_flags));
+#ifdef CONFIG_INGOSCHED
 extern void FASTCALL(sched_exit(struct task_struct * p));
+#endif
 
 extern int in_group_p(gid_t);
 extern int in_egroup_p(gid_t);
@@ -1317,6 +1347,8 @@
 extern struct mm_struct *get_task_mm(struct task_struct *task);
 /* Remove the current tasks stale references to the old mm_struct */
 extern void mm_release(struct task_struct *, struct mm_struct *);
+/* Create a new mm for a kernel thread */
+extern int set_new_mm(void);
 
 extern int  copy_thread(int, unsigned long, unsigned long, unsigned long, struct task_struct *, struct pt_regs *);
 extern void flush_thread(void);
diff -urN oldtree/include/linux/squashfs_fs.h newtree/include/linux/squashfs_fs.h
--- oldtree/include/linux/squashfs_fs.h	1969-12-31 19:00:00.000000000 -0500
+++ newtree/include/linux/squashfs_fs.h	2006-07-12 19:01:48.000000000 -0400
@@ -0,0 +1,911 @@
+#ifndef SQUASHFS_FS
+#define SQUASHFS_FS
+
+/*
+ * Squashfs
+ *
+ * Copyright (c) 2002, 2003, 2004, 2005, 2006
+ * Phillip Lougher <phillip@lougher.org.uk>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2,
+ * or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * squashfs_fs.h
+ */
+
+#ifndef CONFIG_SQUASHFS_2_0_COMPATIBILITY
+#define CONFIG_SQUASHFS_2_0_COMPATIBILITY
+#endif
+
+#ifdef	CONFIG_SQUASHFS_VMALLOC
+#define SQUASHFS_ALLOC(a)		vmalloc(a)
+#define SQUASHFS_FREE(a)		vfree(a)
+#else
+#define SQUASHFS_ALLOC(a)		kmalloc(a, GFP_KERNEL)
+#define SQUASHFS_FREE(a)		kfree(a)
+#endif
+#define SQUASHFS_CACHED_FRAGMENTS	CONFIG_SQUASHFS_FRAGMENT_CACHE_SIZE	
+#define SQUASHFS_MAJOR			3
+#define SQUASHFS_MINOR			0
+#define SQUASHFS_MAGIC			0x73717368
+#define SQUASHFS_MAGIC_SWAP		0x68737173
+#define SQUASHFS_START			0
+
+/* size of metadata (inode and directory) blocks */
+#define SQUASHFS_METADATA_SIZE		8192
+#define SQUASHFS_METADATA_LOG		13
+
+/* default size of data blocks */
+#define SQUASHFS_FILE_SIZE		65536
+#define SQUASHFS_FILE_LOG		16
+
+#define SQUASHFS_FILE_MAX_SIZE		65536
+
+/* Max number of uids and gids */
+#define SQUASHFS_UIDS			256
+#define SQUASHFS_GUIDS			255
+
+/* Max length of filename (not 255) */
+#define SQUASHFS_NAME_LEN		256
+
+#define SQUASHFS_INVALID		((long long) 0xffffffffffff)
+#define SQUASHFS_INVALID_FRAG		((unsigned int) 0xffffffff)
+#define SQUASHFS_INVALID_BLK		((long long) -1)
+#define SQUASHFS_USED_BLK		((long long) -2)
+
+/* Filesystem flags */
+#define SQUASHFS_NOI			0
+#define SQUASHFS_NOD			1
+#define SQUASHFS_CHECK			2
+#define SQUASHFS_NOF			3
+#define SQUASHFS_NO_FRAG		4
+#define SQUASHFS_ALWAYS_FRAG		5
+#define SQUASHFS_DUPLICATE		6
+
+#define SQUASHFS_BIT(flag, bit)		((flag >> bit) & 1)
+
+#define SQUASHFS_UNCOMPRESSED_INODES(flags)	SQUASHFS_BIT(flags, \
+						SQUASHFS_NOI)
+
+#define SQUASHFS_UNCOMPRESSED_DATA(flags)	SQUASHFS_BIT(flags, \
+						SQUASHFS_NOD)
+
+#define SQUASHFS_UNCOMPRESSED_FRAGMENTS(flags)	SQUASHFS_BIT(flags, \
+						SQUASHFS_NOF)
+
+#define SQUASHFS_NO_FRAGMENTS(flags)		SQUASHFS_BIT(flags, \
+						SQUASHFS_NO_FRAG)
+
+#define SQUASHFS_ALWAYS_FRAGMENTS(flags)	SQUASHFS_BIT(flags, \
+						SQUASHFS_ALWAYS_FRAG)
+
+#define SQUASHFS_DUPLICATES(flags)		SQUASHFS_BIT(flags, \
+						SQUASHFS_DUPLICATE)
+
+#define SQUASHFS_CHECK_DATA(flags)		SQUASHFS_BIT(flags, \
+						SQUASHFS_CHECK)
+
+#define SQUASHFS_MKFLAGS(noi, nod, check_data, nof, no_frag, always_frag, \
+		duplicate_checking)	(noi | (nod << 1) | (check_data << 2) \
+		| (nof << 3) | (no_frag << 4) | (always_frag << 5) | \
+		(duplicate_checking << 6))
+
+/* Max number of types and file types */
+#define SQUASHFS_DIR_TYPE		1
+#define SQUASHFS_FILE_TYPE		2
+#define SQUASHFS_SYMLINK_TYPE		3
+#define SQUASHFS_BLKDEV_TYPE		4
+#define SQUASHFS_CHRDEV_TYPE		5
+#define SQUASHFS_FIFO_TYPE		6
+#define SQUASHFS_SOCKET_TYPE		7
+#define SQUASHFS_LDIR_TYPE		8
+#define SQUASHFS_LREG_TYPE		9
+
+/* 1.0 filesystem type definitions */
+#define SQUASHFS_TYPES			5
+#define SQUASHFS_IPC_TYPE		0
+
+/* Flag whether block is compressed or uncompressed, bit is set if block is
+ * uncompressed */
+#define SQUASHFS_COMPRESSED_BIT		(1 << 15)
+
+#define SQUASHFS_COMPRESSED_SIZE(B)	(((B) & ~SQUASHFS_COMPRESSED_BIT) ? \
+		(B) & ~SQUASHFS_COMPRESSED_BIT :  SQUASHFS_COMPRESSED_BIT)
+
+#define SQUASHFS_COMPRESSED(B)		(!((B) & SQUASHFS_COMPRESSED_BIT))
+
+#define SQUASHFS_COMPRESSED_BIT_BLOCK		(1 << 24)
+
+#define SQUASHFS_COMPRESSED_SIZE_BLOCK(B)	(((B) & \
+	~SQUASHFS_COMPRESSED_BIT_BLOCK) ? (B) & \
+	~SQUASHFS_COMPRESSED_BIT_BLOCK : SQUASHFS_COMPRESSED_BIT_BLOCK)
+
+#define SQUASHFS_COMPRESSED_BLOCK(B)	(!((B) & SQUASHFS_COMPRESSED_BIT_BLOCK))
+
+/*
+ * Inode number ops.  Inodes consist of a compressed block number, and an
+ * uncompressed  offset within that block
+ */
+#define SQUASHFS_INODE_BLK(a)		((unsigned int) ((a) >> 16))
+
+#define SQUASHFS_INODE_OFFSET(a)	((unsigned int) ((a) & 0xffff))
+
+#define SQUASHFS_MKINODE(A, B)		((squashfs_inode_t)(((squashfs_inode_t) (A)\
+					<< 16) + (B)))
+
+/* Compute 32 bit VFS inode number from squashfs inode number */
+#define SQUASHFS_MK_VFS_INODE(a, b)	((unsigned int) (((a) << 8) + \
+					((b) >> 2) + 1))
+/* XXX */
+
+/* Translate between VFS mode and squashfs mode */
+#define SQUASHFS_MODE(a)		((a) & 0xfff)
+
+/* fragment and fragment table defines */
+#define SQUASHFS_FRAGMENT_BYTES(A)	(A * sizeof(struct squashfs_fragment_entry))
+
+#define SQUASHFS_FRAGMENT_INDEX(A)	(SQUASHFS_FRAGMENT_BYTES(A) / \
+					SQUASHFS_METADATA_SIZE)
+
+#define SQUASHFS_FRAGMENT_INDEX_OFFSET(A)	(SQUASHFS_FRAGMENT_BYTES(A) % \
+						SQUASHFS_METADATA_SIZE)
+
+#define SQUASHFS_FRAGMENT_INDEXES(A)	((SQUASHFS_FRAGMENT_BYTES(A) + \
+					SQUASHFS_METADATA_SIZE - 1) / \
+					SQUASHFS_METADATA_SIZE)
+
+#define SQUASHFS_FRAGMENT_INDEX_BYTES(A)	(SQUASHFS_FRAGMENT_INDEXES(A) *\
+						sizeof(long long))
+
+/* cached data constants for filesystem */
+#define SQUASHFS_CACHED_BLKS		8
+
+#define SQUASHFS_MAX_FILE_SIZE_LOG	64
+
+#define SQUASHFS_MAX_FILE_SIZE		((long long) 1 << \
+					(SQUASHFS_MAX_FILE_SIZE_LOG - 2))
+
+#define SQUASHFS_MARKER_BYTE		0xff
+
+/* meta index cache */
+#define SQUASHFS_META_INDEXES	(SQUASHFS_METADATA_SIZE / sizeof(unsigned int))
+#define SQUASHFS_META_ENTRIES	31
+#define SQUASHFS_META_NUMBER	8
+#define SQUASHFS_SLOTS		4
+
+struct meta_entry {
+	long long		data_block;
+	unsigned int		index_block;
+	unsigned short		offset;
+	unsigned short		pad;
+};
+
+struct meta_index {
+	unsigned int		inode_number;
+	unsigned int		offset;
+	unsigned short		entries;
+	unsigned short		skip;
+	unsigned short		locked;
+	unsigned short		pad;
+	struct meta_entry	meta_entry[SQUASHFS_META_ENTRIES];
+};
+
+
+/*
+ * definitions for structures on disk
+ */
+
+typedef long long		squashfs_block_t;
+typedef long long		squashfs_inode_t;
+
+struct squashfs_super {
+	unsigned int		s_magic;
+	unsigned int		inodes;
+	unsigned int		bytes_used_2;
+	unsigned int		uid_start_2;
+	unsigned int		guid_start_2;
+	unsigned int		inode_table_start_2;
+	unsigned int		directory_table_start_2;
+	unsigned int		s_major:16;
+	unsigned int		s_minor:16;
+	unsigned int		block_size_1:16;
+	unsigned int		block_log:16;
+	unsigned int		flags:8;
+	unsigned int		no_uids:8;
+	unsigned int		no_guids:8;
+	unsigned int		mkfs_time /* time of filesystem creation */;
+	squashfs_inode_t	root_inode;
+	unsigned int		block_size;
+	unsigned int		fragments;
+	unsigned int		fragment_table_start_2;
+	long long		bytes_used;
+	long long		uid_start;
+	long long		guid_start;
+	long long		inode_table_start;
+	long long		directory_table_start;
+	long long		fragment_table_start;
+	long long		unused;
+} __attribute__ ((packed));
+
+struct squashfs_dir_index {
+	unsigned int		index;
+	unsigned int		start_block;
+	unsigned char		size;
+	unsigned char		name[0];
+} __attribute__ ((packed));
+
+#define SQUASHFS_BASE_INODE_HEADER		\
+	unsigned int		inode_type:4;	\
+	unsigned int		mode:12;	\
+	unsigned int		uid:8;		\
+	unsigned int		guid:8;		\
+	unsigned int		mtime;		\
+	unsigned int 		inode_number;
+
+struct squashfs_base_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+} __attribute__ ((packed));
+
+struct squashfs_ipc_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+	unsigned int		nlink;
+} __attribute__ ((packed));
+
+struct squashfs_dev_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+	unsigned int		nlink;
+	unsigned short		rdev;
+} __attribute__ ((packed));
+	
+struct squashfs_symlink_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+	unsigned int		nlink;
+	unsigned short		symlink_size;
+	char			symlink[0];
+} __attribute__ ((packed));
+
+struct squashfs_reg_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+	squashfs_block_t	start_block;
+	unsigned int		fragment;
+	unsigned int		offset;
+	unsigned int		file_size;
+	unsigned short		block_list[0];
+} __attribute__ ((packed));
+
+struct squashfs_lreg_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+	unsigned int		nlink;
+	squashfs_block_t	start_block;
+	unsigned int		fragment;
+	unsigned int		offset;
+	long long		file_size;
+	unsigned short		block_list[0];
+} __attribute__ ((packed));
+
+struct squashfs_dir_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+	unsigned int		nlink;
+	unsigned int		file_size:19;
+	unsigned int		offset:13;
+	unsigned int		start_block;
+	unsigned int		parent_inode;
+} __attribute__  ((packed));
+
+struct squashfs_ldir_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+	unsigned int		nlink;
+	unsigned int		file_size:27;
+	unsigned int		offset:13;
+	unsigned int		start_block;
+	unsigned int		i_count:16;
+	unsigned int		parent_inode;
+	struct squashfs_dir_index	index[0];
+} __attribute__  ((packed));
+
+union squashfs_inode_header {
+	struct squashfs_base_inode_header	base;
+	struct squashfs_dev_inode_header	dev;
+	struct squashfs_symlink_inode_header	symlink;
+	struct squashfs_reg_inode_header	reg;
+	struct squashfs_lreg_inode_header	lreg;
+	struct squashfs_dir_inode_header	dir;
+	struct squashfs_ldir_inode_header	ldir;
+	struct squashfs_ipc_inode_header	ipc;
+};
+	
+struct squashfs_dir_entry {
+	unsigned int		offset:13;
+	unsigned int		type:3;
+	unsigned int		size:8;
+	int			inode_number:16;
+	char			name[0];
+} __attribute__ ((packed));
+
+struct squashfs_dir_header {
+	unsigned int		count:8;
+	unsigned int		start_block;
+	unsigned int		inode_number;
+} __attribute__ ((packed));
+
+struct squashfs_fragment_entry {
+	long long		start_block;
+	unsigned int		size;
+	unsigned int		unused;
+} __attribute__ ((packed));
+
+extern int squashfs_uncompress_block(void *d, int dstlen, void *s, int srclen);
+extern int squashfs_uncompress_init(void);
+extern int squashfs_uncompress_exit(void);
+
+/*
+ * macros to convert each packed bitfield structure from little endian to big
+ * endian and vice versa.  These are needed when creating or using a filesystem
+ * on a machine with different byte ordering to the target architecture.
+ *
+ */
+
+#define SQUASHFS_SWAP_START \
+	int bits;\
+	int b_pos;\
+	unsigned long long val;\
+	unsigned char *s;\
+	unsigned char *d;
+
+#define SQUASHFS_SWAP_SUPER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_super));\
+	SQUASHFS_SWAP((s)->s_magic, d, 0, 32);\
+	SQUASHFS_SWAP((s)->inodes, d, 32, 32);\
+	SQUASHFS_SWAP((s)->bytes_used_2, d, 64, 32);\
+	SQUASHFS_SWAP((s)->uid_start_2, d, 96, 32);\
+	SQUASHFS_SWAP((s)->guid_start_2, d, 128, 32);\
+	SQUASHFS_SWAP((s)->inode_table_start_2, d, 160, 32);\
+	SQUASHFS_SWAP((s)->directory_table_start_2, d, 192, 32);\
+	SQUASHFS_SWAP((s)->s_major, d, 224, 16);\
+	SQUASHFS_SWAP((s)->s_minor, d, 240, 16);\
+	SQUASHFS_SWAP((s)->block_size_1, d, 256, 16);\
+	SQUASHFS_SWAP((s)->block_log, d, 272, 16);\
+	SQUASHFS_SWAP((s)->flags, d, 288, 8);\
+	SQUASHFS_SWAP((s)->no_uids, d, 296, 8);\
+	SQUASHFS_SWAP((s)->no_guids, d, 304, 8);\
+	SQUASHFS_SWAP((s)->mkfs_time, d, 312, 32);\
+	SQUASHFS_SWAP((s)->root_inode, d, 344, 64);\
+	SQUASHFS_SWAP((s)->block_size, d, 408, 32);\
+	SQUASHFS_SWAP((s)->fragments, d, 440, 32);\
+	SQUASHFS_SWAP((s)->fragment_table_start_2, d, 472, 32);\
+	SQUASHFS_SWAP((s)->bytes_used, d, 504, 64);\
+	SQUASHFS_SWAP((s)->uid_start, d, 568, 64);\
+	SQUASHFS_SWAP((s)->guid_start, d, 632, 64);\
+	SQUASHFS_SWAP((s)->inode_table_start, d, 696, 64);\
+	SQUASHFS_SWAP((s)->directory_table_start, d, 760, 64);\
+	SQUASHFS_SWAP((s)->fragment_table_start, d, 824, 64);\
+	SQUASHFS_SWAP((s)->unused, d, 888, 64);\
+}
+
+#define SQUASHFS_SWAP_BASE_INODE_CORE(s, d, n)\
+	SQUASHFS_MEMSET(s, d, n);\
+	SQUASHFS_SWAP((s)->inode_type, d, 0, 4);\
+	SQUASHFS_SWAP((s)->mode, d, 4, 12);\
+	SQUASHFS_SWAP((s)->uid, d, 16, 8);\
+	SQUASHFS_SWAP((s)->guid, d, 24, 8);\
+	SQUASHFS_SWAP((s)->mtime, d, 32, 32);\
+	SQUASHFS_SWAP((s)->inode_number, d, 64, 32);
+
+#define SQUASHFS_SWAP_BASE_INODE_HEADER(s, d, n) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, n)\
+}
+
+#define SQUASHFS_SWAP_IPC_INODE_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, \
+			sizeof(struct squashfs_ipc_inode_header))\
+	SQUASHFS_SWAP((s)->nlink, d, 96, 32);\
+}
+
+#define SQUASHFS_SWAP_DEV_INODE_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, \
+			sizeof(struct squashfs_dev_inode_header)); \
+	SQUASHFS_SWAP((s)->nlink, d, 96, 32);\
+	SQUASHFS_SWAP((s)->rdev, d, 128, 16);\
+}
+
+#define SQUASHFS_SWAP_SYMLINK_INODE_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, \
+			sizeof(struct squashfs_symlink_inode_header));\
+	SQUASHFS_SWAP((s)->nlink, d, 96, 32);\
+	SQUASHFS_SWAP((s)->symlink_size, d, 128, 16);\
+}
+
+#define SQUASHFS_SWAP_REG_INODE_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, \
+			sizeof(struct squashfs_reg_inode_header));\
+	SQUASHFS_SWAP((s)->start_block, d, 96, 64);\
+	SQUASHFS_SWAP((s)->fragment, d, 160, 32);\
+	SQUASHFS_SWAP((s)->offset, d, 192, 32);\
+	SQUASHFS_SWAP((s)->file_size, d, 224, 32);\
+}
+
+#define SQUASHFS_SWAP_LREG_INODE_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, \
+			sizeof(struct squashfs_lreg_inode_header));\
+	SQUASHFS_SWAP((s)->nlink, d, 96, 32);\
+	SQUASHFS_SWAP((s)->start_block, d, 128, 64);\
+	SQUASHFS_SWAP((s)->fragment, d, 192, 32);\
+	SQUASHFS_SWAP((s)->offset, d, 224, 32);\
+	SQUASHFS_SWAP((s)->file_size, d, 256, 64);\
+}
+
+#define SQUASHFS_SWAP_DIR_INODE_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, \
+			sizeof(struct squashfs_dir_inode_header));\
+	SQUASHFS_SWAP((s)->nlink, d, 96, 32);\
+	SQUASHFS_SWAP((s)->file_size, d, 128, 19);\
+	SQUASHFS_SWAP((s)->offset, d, 147, 13);\
+	SQUASHFS_SWAP((s)->start_block, d, 160, 32);\
+	SQUASHFS_SWAP((s)->parent_inode, d, 192, 32);\
+}
+
+#define SQUASHFS_SWAP_LDIR_INODE_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, \
+			sizeof(struct squashfs_ldir_inode_header));\
+	SQUASHFS_SWAP((s)->nlink, d, 96, 32);\
+	SQUASHFS_SWAP((s)->file_size, d, 128, 27);\
+	SQUASHFS_SWAP((s)->offset, d, 155, 13);\
+	SQUASHFS_SWAP((s)->start_block, d, 168, 32);\
+	SQUASHFS_SWAP((s)->i_count, d, 200, 16);\
+	SQUASHFS_SWAP((s)->parent_inode, d, 216, 32);\
+}
+
+#define SQUASHFS_SWAP_DIR_INDEX(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_dir_index));\
+	SQUASHFS_SWAP((s)->index, d, 0, 32);\
+	SQUASHFS_SWAP((s)->start_block, d, 32, 32);\
+	SQUASHFS_SWAP((s)->size, d, 64, 8);\
+}
+
+#define SQUASHFS_SWAP_DIR_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_dir_header));\
+	SQUASHFS_SWAP((s)->count, d, 0, 8);\
+	SQUASHFS_SWAP((s)->start_block, d, 8, 32);\
+	SQUASHFS_SWAP((s)->inode_number, d, 40, 32);\
+}
+
+#define SQUASHFS_SWAP_DIR_ENTRY(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_dir_entry));\
+	SQUASHFS_SWAP((s)->offset, d, 0, 13);\
+	SQUASHFS_SWAP((s)->type, d, 13, 3);\
+	SQUASHFS_SWAP((s)->size, d, 16, 8);\
+	SQUASHFS_SWAP((s)->inode_number, d, 24, 16);\
+}
+
+#define SQUASHFS_SWAP_FRAGMENT_ENTRY(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_fragment_entry));\
+	SQUASHFS_SWAP((s)->start_block, d, 0, 64);\
+	SQUASHFS_SWAP((s)->size, d, 64, 32);\
+}
+
+#define SQUASHFS_SWAP_SHORTS(s, d, n) {\
+	int entry;\
+	int bit_position;\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, n * 2);\
+	for(entry = 0, bit_position = 0; entry < n; entry++, bit_position += \
+			16)\
+		SQUASHFS_SWAP(s[entry], d, bit_position, 16);\
+}
+
+#define SQUASHFS_SWAP_INTS(s, d, n) {\
+	int entry;\
+	int bit_position;\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, n * 4);\
+	for(entry = 0, bit_position = 0; entry < n; entry++, bit_position += \
+			32)\
+		SQUASHFS_SWAP(s[entry], d, bit_position, 32);\
+}
+
+#define SQUASHFS_SWAP_LONG_LONGS(s, d, n) {\
+	int entry;\
+	int bit_position;\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, n * 8);\
+	for(entry = 0, bit_position = 0; entry < n; entry++, bit_position += \
+			64)\
+		SQUASHFS_SWAP(s[entry], d, bit_position, 64);\
+}
+
+#define SQUASHFS_SWAP_DATA(s, d, n, bits) {\
+	int entry;\
+	int bit_position;\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, n * bits / 8);\
+	for(entry = 0, bit_position = 0; entry < n; entry++, bit_position += \
+			bits)\
+		SQUASHFS_SWAP(s[entry], d, bit_position, bits);\
+}
+
+#define SQUASHFS_SWAP_FRAGMENT_INDEXES(s, d, n) SQUASHFS_SWAP_LONG_LONGS(s, d, n)
+
+#ifdef CONFIG_SQUASHFS_1_0_COMPATIBILITY
+
+struct squashfs_base_inode_header_1 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:4; /* index into uid table */
+	unsigned int		guid:4; /* index into guid table */
+} __attribute__ ((packed));
+
+struct squashfs_ipc_inode_header_1 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:4; /* index into uid table */
+	unsigned int		guid:4; /* index into guid table */
+	unsigned int		type:4;
+	unsigned int		offset:4;
+} __attribute__ ((packed));
+
+struct squashfs_dev_inode_header_1 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:4; /* index into uid table */
+	unsigned int		guid:4; /* index into guid table */
+	unsigned short		rdev;
+} __attribute__ ((packed));
+	
+struct squashfs_symlink_inode_header_1 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:4; /* index into uid table */
+	unsigned int		guid:4; /* index into guid table */
+	unsigned short		symlink_size;
+	char			symlink[0];
+} __attribute__ ((packed));
+
+struct squashfs_reg_inode_header_1 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:4; /* index into uid table */
+	unsigned int		guid:4; /* index into guid table */
+	unsigned int		mtime;
+	unsigned int		start_block;
+	unsigned int		file_size:32;
+	unsigned short		block_list[0];
+} __attribute__ ((packed));
+
+struct squashfs_dir_inode_header_1 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:4; /* index into uid table */
+	unsigned int		guid:4; /* index into guid table */
+	unsigned int		file_size:19;
+	unsigned int		offset:13;
+	unsigned int		mtime;
+	unsigned int		start_block:24;
+} __attribute__  ((packed));
+
+#define SQUASHFS_SWAP_BASE_INODE_CORE_1(s, d, n) \
+	SQUASHFS_MEMSET(s, d, n);\
+	SQUASHFS_SWAP((s)->inode_type, d, 0, 4);\
+	SQUASHFS_SWAP((s)->mode, d, 4, 12);\
+	SQUASHFS_SWAP((s)->uid, d, 16, 4);\
+	SQUASHFS_SWAP((s)->guid, d, 20, 4);
+
+#define SQUASHFS_SWAP_BASE_INODE_HEADER_1(s, d, n) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_1(s, d, n)\
+}
+
+#define SQUASHFS_SWAP_IPC_INODE_HEADER_1(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_1(s, d, \
+			sizeof(struct squashfs_ipc_inode_header_1));\
+	SQUASHFS_SWAP((s)->type, d, 24, 4);\
+	SQUASHFS_SWAP((s)->offset, d, 28, 4);\
+}
+
+#define SQUASHFS_SWAP_DEV_INODE_HEADER_1(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_1(s, d, \
+			sizeof(struct squashfs_dev_inode_header_1));\
+	SQUASHFS_SWAP((s)->rdev, d, 24, 16);\
+}
+
+#define SQUASHFS_SWAP_SYMLINK_INODE_HEADER_1(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_1(s, d, \
+			sizeof(struct squashfs_symlink_inode_header_1));\
+	SQUASHFS_SWAP((s)->symlink_size, d, 24, 16);\
+}
+
+#define SQUASHFS_SWAP_REG_INODE_HEADER_1(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_1(s, d, \
+			sizeof(struct squashfs_reg_inode_header_1));\
+	SQUASHFS_SWAP((s)->mtime, d, 24, 32);\
+	SQUASHFS_SWAP((s)->start_block, d, 56, 32);\
+	SQUASHFS_SWAP((s)->file_size, d, 88, 32);\
+}
+
+#define SQUASHFS_SWAP_DIR_INODE_HEADER_1(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_1(s, d, \
+			sizeof(struct squashfs_dir_inode_header_1));\
+	SQUASHFS_SWAP((s)->file_size, d, 24, 19);\
+	SQUASHFS_SWAP((s)->offset, d, 43, 13);\
+	SQUASHFS_SWAP((s)->mtime, d, 56, 32);\
+	SQUASHFS_SWAP((s)->start_block, d, 88, 24);\
+}
+
+#endif
+
+#ifdef CONFIG_SQUASHFS_2_0_COMPATIBILITY
+
+struct squashfs_dir_index_2 {
+	unsigned int		index:27;
+	unsigned int		start_block:29;
+	unsigned char		size;
+	unsigned char		name[0];
+} __attribute__ ((packed));
+
+struct squashfs_base_inode_header_2 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:8; /* index into uid table */
+	unsigned int		guid:8; /* index into guid table */
+} __attribute__ ((packed));
+
+struct squashfs_ipc_inode_header_2 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:8; /* index into uid table */
+	unsigned int		guid:8; /* index into guid table */
+} __attribute__ ((packed));
+
+struct squashfs_dev_inode_header_2 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:8; /* index into uid table */
+	unsigned int		guid:8; /* index into guid table */
+	unsigned short		rdev;
+} __attribute__ ((packed));
+	
+struct squashfs_symlink_inode_header_2 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:8; /* index into uid table */
+	unsigned int		guid:8; /* index into guid table */
+	unsigned short		symlink_size;
+	char			symlink[0];
+} __attribute__ ((packed));
+
+struct squashfs_reg_inode_header_2 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:8; /* index into uid table */
+	unsigned int		guid:8; /* index into guid table */
+	unsigned int		mtime;
+	unsigned int		start_block;
+	unsigned int		fragment;
+	unsigned int		offset;
+	unsigned int		file_size:32;
+	unsigned short		block_list[0];
+} __attribute__ ((packed));
+
+struct squashfs_dir_inode_header_2 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:8; /* index into uid table */
+	unsigned int		guid:8; /* index into guid table */
+	unsigned int		file_size:19;
+	unsigned int		offset:13;
+	unsigned int		mtime;
+	unsigned int		start_block:24;
+} __attribute__  ((packed));
+
+struct squashfs_ldir_inode_header_2 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:8; /* index into uid table */
+	unsigned int		guid:8; /* index into guid table */
+	unsigned int		file_size:27;
+	unsigned int		offset:13;
+	unsigned int		mtime;
+	unsigned int		start_block:24;
+	unsigned int		i_count:16;
+	struct squashfs_dir_index_2	index[0];
+} __attribute__  ((packed));
+
+union squashfs_inode_header_2 {
+	struct squashfs_base_inode_header_2	base;
+	struct squashfs_dev_inode_header_2	dev;
+	struct squashfs_symlink_inode_header_2	symlink;
+	struct squashfs_reg_inode_header_2	reg;
+	struct squashfs_dir_inode_header_2	dir;
+	struct squashfs_ldir_inode_header_2	ldir;
+	struct squashfs_ipc_inode_header_2	ipc;
+};
+	
+struct squashfs_dir_header_2 {
+	unsigned int		count:8;
+	unsigned int		start_block:24;
+} __attribute__ ((packed));
+
+struct squashfs_dir_entry_2 {
+	unsigned int		offset:13;
+	unsigned int		type:3;
+	unsigned int		size:8;
+	char			name[0];
+} __attribute__ ((packed));
+
+struct squashfs_fragment_entry_2 {
+	unsigned int		start_block;
+	unsigned int		size;
+} __attribute__ ((packed));
+
+#define SQUASHFS_SWAP_BASE_INODE_CORE_2(s, d, n)\
+	SQUASHFS_MEMSET(s, d, n);\
+	SQUASHFS_SWAP((s)->inode_type, d, 0, 4);\
+	SQUASHFS_SWAP((s)->mode, d, 4, 12);\
+	SQUASHFS_SWAP((s)->uid, d, 16, 8);\
+	SQUASHFS_SWAP((s)->guid, d, 24, 8);\
+
+#define SQUASHFS_SWAP_BASE_INODE_HEADER_2(s, d, n) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_2(s, d, n)\
+}
+
+#define SQUASHFS_SWAP_IPC_INODE_HEADER_2(s, d) \
+	SQUASHFS_SWAP_BASE_INODE_HEADER_2(s, d, sizeof(struct squashfs_ipc_inode_header_2))
+
+#define SQUASHFS_SWAP_DEV_INODE_HEADER_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_2(s, d, \
+			sizeof(struct squashfs_dev_inode_header_2)); \
+	SQUASHFS_SWAP((s)->rdev, d, 32, 16);\
+}
+
+#define SQUASHFS_SWAP_SYMLINK_INODE_HEADER_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_2(s, d, \
+			sizeof(struct squashfs_symlink_inode_header_2));\
+	SQUASHFS_SWAP((s)->symlink_size, d, 32, 16);\
+}
+
+#define SQUASHFS_SWAP_REG_INODE_HEADER_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_2(s, d, \
+			sizeof(struct squashfs_reg_inode_header_2));\
+	SQUASHFS_SWAP((s)->mtime, d, 32, 32);\
+	SQUASHFS_SWAP((s)->start_block, d, 64, 32);\
+	SQUASHFS_SWAP((s)->fragment, d, 96, 32);\
+	SQUASHFS_SWAP((s)->offset, d, 128, 32);\
+	SQUASHFS_SWAP((s)->file_size, d, 160, 32);\
+}
+
+#define SQUASHFS_SWAP_DIR_INODE_HEADER_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_2(s, d, \
+			sizeof(struct squashfs_dir_inode_header_2));\
+	SQUASHFS_SWAP((s)->file_size, d, 32, 19);\
+	SQUASHFS_SWAP((s)->offset, d, 51, 13);\
+	SQUASHFS_SWAP((s)->mtime, d, 64, 32);\
+	SQUASHFS_SWAP((s)->start_block, d, 96, 24);\
+}
+
+#define SQUASHFS_SWAP_LDIR_INODE_HEADER_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_2(s, d, \
+			sizeof(struct squashfs_ldir_inode_header_2));\
+	SQUASHFS_SWAP((s)->file_size, d, 32, 27);\
+	SQUASHFS_SWAP((s)->offset, d, 59, 13);\
+	SQUASHFS_SWAP((s)->mtime, d, 72, 32);\
+	SQUASHFS_SWAP((s)->start_block, d, 104, 24);\
+	SQUASHFS_SWAP((s)->i_count, d, 128, 16);\
+}
+
+#define SQUASHFS_SWAP_DIR_INDEX_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_dir_index_2));\
+	SQUASHFS_SWAP((s)->index, d, 0, 27);\
+	SQUASHFS_SWAP((s)->start_block, d, 27, 29);\
+	SQUASHFS_SWAP((s)->size, d, 56, 8);\
+}
+#define SQUASHFS_SWAP_DIR_HEADER_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_dir_header_2));\
+	SQUASHFS_SWAP((s)->count, d, 0, 8);\
+	SQUASHFS_SWAP((s)->start_block, d, 8, 24);\
+}
+
+#define SQUASHFS_SWAP_DIR_ENTRY_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_dir_entry_2));\
+	SQUASHFS_SWAP((s)->offset, d, 0, 13);\
+	SQUASHFS_SWAP((s)->type, d, 13, 3);\
+	SQUASHFS_SWAP((s)->size, d, 16, 8);\
+}
+
+#define SQUASHFS_SWAP_FRAGMENT_ENTRY_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_fragment_entry_2));\
+	SQUASHFS_SWAP((s)->start_block, d, 0, 32);\
+	SQUASHFS_SWAP((s)->size, d, 32, 32);\
+}
+
+#define SQUASHFS_SWAP_FRAGMENT_INDEXES_2(s, d, n) SQUASHFS_SWAP_INTS(s, d, n)
+
+/* fragment and fragment table defines */
+#define SQUASHFS_FRAGMENT_BYTES_2(A)	(A * sizeof(struct squashfs_fragment_entry_2))
+
+#define SQUASHFS_FRAGMENT_INDEX_2(A)	(SQUASHFS_FRAGMENT_BYTES_2(A) / \
+					SQUASHFS_METADATA_SIZE)
+
+#define SQUASHFS_FRAGMENT_INDEX_OFFSET_2(A)	(SQUASHFS_FRAGMENT_BYTES_2(A) % \
+						SQUASHFS_METADATA_SIZE)
+
+#define SQUASHFS_FRAGMENT_INDEXES_2(A)	((SQUASHFS_FRAGMENT_BYTES_2(A) + \
+					SQUASHFS_METADATA_SIZE - 1) / \
+					SQUASHFS_METADATA_SIZE)
+
+#define SQUASHFS_FRAGMENT_INDEX_BYTES_2(A)	(SQUASHFS_FRAGMENT_INDEXES_2(A) *\
+						sizeof(int))
+
+#endif
+
+#ifdef __KERNEL__
+
+/*
+ * macros used to swap each structure entry, taking into account
+ * bitfields and different bitfield placing conventions on differing
+ * architectures
+ */
+
+#include <asm/byteorder.h>
+
+#ifdef __BIG_ENDIAN
+	/* convert from little endian to big endian */
+#define SQUASHFS_SWAP(value, p, pos, tbits) _SQUASHFS_SWAP(value, p, pos, \
+		tbits, b_pos)
+#else
+	/* convert from big endian to little endian */ 
+#define SQUASHFS_SWAP(value, p, pos, tbits) _SQUASHFS_SWAP(value, p, pos, \
+		tbits, 64 - tbits - b_pos)
+#endif
+
+#define _SQUASHFS_SWAP(value, p, pos, tbits, SHIFT) {\
+	b_pos = pos % 8;\
+	val = 0;\
+	s = (unsigned char *)p + (pos / 8);\
+	d = ((unsigned char *) &val) + 7;\
+	for(bits = 0; bits < (tbits + b_pos); bits += 8) \
+		*d-- = *s++;\
+	value = (val >> (SHIFT))/* & ((1 << tbits) - 1)*/;\
+}
+
+#define SQUASHFS_MEMSET(s, d, n)	memset(s, 0, n);
+
+#endif
+#endif
diff -urN oldtree/include/linux/squashfs_fs_i.h newtree/include/linux/squashfs_fs_i.h
--- oldtree/include/linux/squashfs_fs_i.h	1969-12-31 19:00:00.000000000 -0500
+++ newtree/include/linux/squashfs_fs_i.h	2006-07-12 19:01:48.000000000 -0400
@@ -0,0 +1,45 @@
+#ifndef SQUASHFS_FS_I
+#define SQUASHFS_FS_I
+/*
+ * Squashfs
+ *
+ * Copyright (c) 2002, 2003, 2004, 2005, 2006
+ * Phillip Lougher <phillip@lougher.org.uk>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2,
+ * or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * squashfs_fs_i.h
+ */
+
+struct squashfs_inode_info {
+	long long	start_block;
+	unsigned int	offset;
+	union {
+		struct {
+			long long	fragment_start_block;
+			unsigned int	fragment_size;
+			unsigned int	fragment_offset;
+			long long	block_list_start;
+		} s1;
+		struct {
+			long long	directory_index_start;
+			unsigned int	directory_index_offset;
+			unsigned int	directory_index_count;
+			unsigned int	parent_inode;
+		} s2;
+	} u;
+	struct inode	vfs_inode;
+};
+#endif
diff -urN oldtree/include/linux/squashfs_fs_sb.h newtree/include/linux/squashfs_fs_sb.h
--- oldtree/include/linux/squashfs_fs_sb.h	1969-12-31 19:00:00.000000000 -0500
+++ newtree/include/linux/squashfs_fs_sb.h	2006-07-12 19:01:48.000000000 -0400
@@ -0,0 +1,74 @@
+#ifndef SQUASHFS_FS_SB
+#define SQUASHFS_FS_SB
+/*
+ * Squashfs
+ *
+ * Copyright (c) 2002, 2003, 2004, 2005, 2006
+ * Phillip Lougher <phillip@lougher.org.uk>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2,
+ * or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * squashfs_fs_sb.h
+ */
+
+#include <linux/squashfs_fs.h>
+
+struct squashfs_cache {
+	long long	block;
+	int		length;
+	long long	next_index;
+	char		*data;
+};
+
+struct squashfs_fragment_cache {
+	long long	block;
+	int		length;
+	unsigned int	locked;
+	char		*data;
+};
+
+struct squashfs_sb_info {
+	struct squashfs_super	sblk;
+	int			devblksize;
+	int			devblksize_log2;
+	int			swap;
+	struct squashfs_cache	*block_cache;
+	struct squashfs_fragment_cache	*fragment;
+	int			next_cache;
+	int			next_fragment;
+	int			next_meta_index;
+	unsigned int		*uid;
+	unsigned int		*guid;
+	long long		*fragment_index;
+	unsigned int		*fragment_index_2;
+	unsigned int		read_size;
+	char			*read_data;
+	char			*read_page;
+	struct semaphore	read_data_mutex;
+	struct semaphore	read_page_mutex;
+	struct semaphore	block_cache_mutex;
+	struct semaphore	fragment_mutex;
+	struct semaphore	meta_index_mutex;
+	wait_queue_head_t	waitq;
+	wait_queue_head_t	fragment_wait_queue;
+	struct meta_index	*meta_index;
+	struct inode		*(*iget)(struct super_block *s,  squashfs_inode_t \
+				inode);
+	long long		(*read_blocklist)(struct inode *inode, int \
+				index, int readahead_blks, char *block_list, \
+				unsigned short **block_p, unsigned int *bsize);
+	int			(*read_fragment_index_table)(struct super_block *s);
+};
+#endif
diff -urN oldtree/include/linux/swap.h newtree/include/linux/swap.h
--- oldtree/include/linux/swap.h	2006-07-05 10:06:57.000000000 -0400
+++ newtree/include/linux/swap.h	2006-07-12 19:02:29.000000000 -0400
@@ -184,7 +184,8 @@
 /* linux/mm/vmscan.c */
 extern unsigned long try_to_free_pages(struct zone **, gfp_t);
 extern unsigned long shrink_all_memory(unsigned long nr_pages);
-extern int vm_swappiness;
+extern int vm_mapped;
+extern int vm_hardmaplimit;
 extern int remove_mapping(struct address_space *mapping, struct page *page);
 extern long vm_total_pages;
 
diff -urN oldtree/include/linux/sysctl.h newtree/include/linux/sysctl.h
--- oldtree/include/linux/sysctl.h	2006-07-05 10:06:57.000000000 -0400
+++ newtree/include/linux/sysctl.h	2006-07-12 19:02:29.000000000 -0400
@@ -153,6 +153,9 @@
 	KERN_MAX_LOCK_DEPTH=74,
 	KERN_NMI_WATCHDOG=75, /* int: enable/disable nmi watchdog */
 	KERN_PANIC_ON_NMI=76, /* int: whether we will panic on an unrecovered */
+        KERN_INTERACTIVE=77,    /* interactive tasks can have cpu bursts */
+        KERN_COMPUTE=78,        /* adjust timeslices for a compute server */
+        KERN_FBSPLASH=79,       /* string: path to fbsplash helper */
 };
 
 
@@ -178,7 +181,7 @@
 	VM_OVERCOMMIT_RATIO=16, /* percent of RAM to allow overcommit in */
 	VM_PAGEBUF=17,		/* struct: Control pagebuf parameters */
 	VM_HUGETLB_PAGES=18,	/* int: Number of available Huge Pages */
-	VM_SWAPPINESS=19,	/* Tendency to steal mapped memory */
+	VM_MAPPED=19,		/* percent mapped min while evicting cache */
 	VM_LOWMEM_RESERVE_RATIO=20,/* reservation ratio for lower memory zones */
 	VM_MIN_FREE_KBYTES=21,	/* Minimum free kilobytes to maintain */
 	VM_MAX_MAP_COUNT=22,	/* int: Maximum number of mmaps/address-space */
@@ -197,6 +200,7 @@
 	VM_SWAP_PREFETCH=35,	/* swap prefetch */
 	VM_READAHEAD_RATIO=36,	/* percent of read-ahead size to thrashing-threshold */
 	VM_READAHEAD_HIT_RATE=37, /* one accessed page legitimizes so many read-ahead pages */
+        VM_HARDMAPLIMIT=38,     /* Make mapped a hard limit */
 };
 
 /* CTL_NET names: */
diff -urN oldtree/include/linux/tty.h newtree/include/linux/tty.h
--- oldtree/include/linux/tty.h	2006-07-05 10:06:57.000000000 -0400
+++ newtree/include/linux/tty.h	2006-07-12 19:00:36.000000000 -0400
@@ -16,6 +16,14 @@
 
 #include <asm/system.h>
 
+/*
+ * CONFIG_NR_TTY_DEVICES is set in the .config
+ * But splashutils doesn't look there, so lets
+ * pretend it isn't defined and define it.
+ */
+#ifndef CONFIG_NR_TTY_DEVICES
+#define CONFIG_NR_TTY_DEVICES 63
+#endif
 
 /*
  * (Note: the *_driver.minor_start values 1, 64, 128, 192 are
diff -urN oldtree/include/linux/types.h newtree/include/linux/types.h
--- oldtree/include/linux/types.h	2006-07-05 10:06:57.000000000 -0400
+++ newtree/include/linux/types.h	2006-07-12 19:01:43.000000000 -0400
@@ -193,4 +193,15 @@
 	char			f_fpack[6];
 };
 
+#ifdef CONFIG_HIDE_FALSE_POSITIVES
+/*
+ *  No parentheses around x = x  because
+ *    int (i=i);
+ *  doesn't compile.
+ */
+# define uninit_var(x) x = x
+#else
+# define uninit_var(x) x
+#endif
+
 #endif /* _LINUX_TYPES_H */
diff -urN oldtree/include/linux/vt.h newtree/include/linux/vt.h
--- oldtree/include/linux/vt.h	2006-07-05 10:06:57.000000000 -0400
+++ newtree/include/linux/vt.h	2006-07-12 19:00:36.000000000 -0400
@@ -2,12 +2,25 @@
 #define _LINUX_VT_H
 
 /*
+ * CONFIG_NR_TTY_DEVICES is set in the .config
+ * But splashutils doesn't look there, so lets
+ * pretend it isn't defined and define it.
+ */
+#ifndef CONFIG_NR_TTY_DEVICES
+#define CONFIG_NR_TTY_DEVICES 63
+#endif
+
+/*
  * These constants are also useful for user-level apps (e.g., VC
  * resizing).
  */
 #define MIN_NR_CONSOLES 1       /* must be at least 1 */
-#define MAX_NR_CONSOLES	63	/* serial lines start at 64 */
-#define MAX_NR_USER_CONSOLES 63	/* must be root to allocate above this */
+/*
+ * NR_TTY_DEVICES:
+ * Value MUST be at least 11 and must never be higher then 63
+ */
+#define MAX_NR_CONSOLES CONFIG_NR_TTY_DEVICES
+#define MAX_NR_USER_CONSOLES CONFIG_NR_TTY_DEVICES
 		/* Note: the ioctl VT_GETSTATE does not work for
 		   consoles 16 and higher (since it returns a short) */
 
diff -urN oldtree/include/video/vesa.h newtree/include/video/vesa.h
--- oldtree/include/video/vesa.h	1969-12-31 19:00:00.000000000 -0500
+++ newtree/include/video/vesa.h	2006-07-12 19:01:08.000000000 -0400
@@ -0,0 +1,150 @@
+#if 0
+#define DPRINTK(fmt, args...)	printk(KERN_DEBUG "%s: " fmt, __FUNCTION__ , \
+						  ## args)
+#else
+#define DPRINTK(fmt, args...)
+#endif
+
+#define p_crtc(arg) ((struct vesafb_crtc_ib*)(arg))
+#define p_vbe(arg)  ((struct vesafb_vbe_ib*)(arg))
+#define p_mode(arg) ((struct vesafb_mode_ib*)(arg))
+
+struct vesafb_task {
+	u8 flags;
+	void *buf;
+	int buf_len;
+	struct vm86_regs regs;
+	struct list_head node;
+	struct completion done;
+};
+
+/* Vesafb task flags and masks */
+#define TF_CALL		0x00
+#define TF_EXIT		0x01
+#define TF_GETVBEIB	0x02
+#define TF_BUF_DI	0x04
+#define TF_BUF_BX	0x08
+#define TF_RETURN_BUF	0x10
+
+/* Macros and functions for manipulating vesafb tasks */
+#define vesafb_create_task(task)				\
+do { 								\
+	task = kmalloc(sizeof(struct vesafb_task), GFP_ATOMIC); \
+	if (task) 						\
+		memset(task, 0, sizeof(struct vesafb_task));	\
+	init_completion(&task->done);				\
+} while (0)
+
+#define vesafb_wait_for_task(task) 	wait_for_completion(&task->done);
+#define vesafb_reset_task(task)		init_completion(&task->done);
+int vesafb_queue_task(struct vesafb_task *task);
+
+/* Functions for controlling the vesafb thread */
+int vesafb_wait_for_thread(void);
+
+#define VBE_CAP_CAN_SWITCH_DAC	0x01
+#define VBE_CAP_VGACOMPAT	0x02
+
+/* This struct is 512 bytes long */
+struct vesafb_vbe_ib {
+	char vbe_signature[4];
+	u16  vbe_version;
+	u32  oem_string_ptr;
+	u32  capabilities;
+	u32  mode_list_ptr;
+	u16  total_memory;
+	u16  oem_software_rev;
+	u32  oem_vendor_name_ptr;
+	u32  oem_product_name_ptr;
+	u32  oem_product_rev_ptr;
+	u8   reserved[222];
+	char oem_data[256];
+} __attribute__ ((packed));
+
+struct vesafb_crtc_ib {
+	u16 horiz_total;
+	u16 horiz_start;
+	u16 horiz_end;
+	u16 vert_total;
+	u16 vert_start;
+	u16 vert_end;
+	u8  flags;
+	u32 pixel_clock;
+	u16 refresh_rate;
+	u8  reserved[40];
+} __attribute__ ((packed));
+
+#define VBE_MODE_VGACOMPAT	0x20
+
+struct vesafb_mode_ib {
+	/* for all VBE revisions */
+	u16 mode_attr;
+	u8  winA_attr;
+	u8  winB_attr;
+	u16 win_granularity;
+	u16 win_size;
+	u16 winA_seg;
+	u16 winB_seg;
+	u32 win_func_ptr;
+	u16 bytes_per_scan_line;
+
+	/* for VBE 1.2+ */
+	u16 x_res;
+	u16 y_res;
+	u8  x_char_size;
+	u8  y_char_size;
+	u8  planes;
+	u8  bits_per_pixel;
+	u8  banks;
+	u8  memory_model;
+	u8  bank_size;
+	u8  image_pages;
+	u8  reserved1;
+
+	/* Direct color fields for direct/6 and YUV/7 memory models. */
+	/* Offsets are bit positions of lsb in the mask. */
+	u8  red_len;
+	u8  red_off;
+	u8  green_len;
+	u8  green_off;
+	u8  blue_len;
+	u8  blue_off;
+	u8  rsvd_len;
+	u8  rsvd_off;
+	u8  direct_color_info;	/* direct color mode attributes */
+
+	/* for VBE 2.0+ */
+	u32 phys_base_ptr;
+	u8  reserved2[6];
+
+	/* for VBE 3.0+ */
+	u16 lin_bytes_per_scan_line;
+	u8  bnk_image_pages;
+	u8  lin_image_pages;
+	u8  lin_red_len;
+	u8  lin_red_off;
+	u8  lin_green_len;
+	u8  lin_green_off;
+	u8  lin_blue_len;
+	u8  lin_blue_off;
+	u8  lin_rsvd_len;
+	u8  lin_rsvd_off;
+	u32 max_pixel_clock;
+	u16 mode_id;
+	u8  depth;
+} __attribute__ ((packed));
+
+struct vesafb_pal_entry {
+	u_char blue, green, red, pad;
+} __attribute__ ((packed));
+
+struct vesafb_par {
+	u8 *vbe_state;
+	int vbe_state_size;
+	atomic_t ref_count;
+	
+	u32 mem_total;
+	int mode_idx;
+	struct vesafb_crtc_ib crtc;
+};
+
diff -urN oldtree/init/Kconfig newtree/init/Kconfig
--- oldtree/init/Kconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/init/Kconfig	2006-07-12 19:02:09.000000000 -0400
@@ -62,6 +62,29 @@
 
 endmenu
 
+menu "No-Sources CPU Scheduler Selection"
+choice
+        prompt "CPU Schedulers:"
+        default STAIRCASE
+ 
+        config INGOSCHED
+          bool "Ingosched CPU Scheduler v2.6.17-mm6"
+          help
+            This is the standard CPU scheduler which is an O(1) dual priority
+            array scheduler with a hybrid interactive design.
+ 
+            Version: 2.6.17-mm6
+ 
+        config STAIRCASE
+          bool "Staircase CPU Scheduler v16 + 2.6.17-mm6"
+          help
+            This scheduler is an O(1) single priority array with a foreground-
+            background interactive design.
+
+            Version: 16 + 2.6.17-mm6 
+endchoice
+endmenu
+
 menu "General setup"
 
 config LOCALVERSION
@@ -450,6 +473,69 @@
 	  option allows the disabling of the VM event counters.
 	  /proc/vmstat will only show page counts.
 
+menuconfig PROCESSOR_SELECT
+	depends X86
+	default y
+	bool "Supported processor vendors" if EMBEDDED
+	help
+	  This lets you choose what x86 vendor support code your kernel
+	  will include.
+
+config CPU_SUP_INTEL
+       default y
+       bool "Support Intel processors" if PROCESSOR_SELECT
+       help
+         This enables extended support for Intel processors
+
+config CPU_SUP_CYRIX
+       default y
+       bool "Support Cyrix processors" if PROCESSOR_SELECT
+       help
+         This enables extended support for Cyrix processors
+
+config CPU_SUP_NSC
+       default y
+       bool "Support NSC processors" if PROCESSOR_SELECT
+       help
+         This enables extended support for NSC processors
+
+config CPU_SUP_AMD
+       default y
+       depends on CPU_SUP_INTEL
+       bool "Support AMD processors" if PROCESSOR_SELECT
+       help
+         This enables extended support for AMD processors
+
+config CPU_SUP_CENTAUR
+       default y
+       bool "Support Centaur processors" if PROCESSOR_SELECT
+       help
+         This enables extended support for Centaur processors
+
+config CPU_SUP_TRANSMETA
+       default y
+       bool "Support Transmeta processors" if PROCESSOR_SELECT
+       help
+         This enables extended support for Transmeta processors
+
+config CPU_SUP_RISE
+       default y
+       bool "Support Rise processors" if PROCESSOR_SELECT
+       help
+         This enables extended support for Rise processors
+
+config CPU_SUP_NEXGEN
+       default y
+       bool "Support NexGen processors" if PROCESSOR_SELECT
+       help
+         This enables extended support for NexGen processors
+
+config CPU_SUP_UMC
+       default y
+       bool "Support UMC processors" if PROCESSOR_SELECT
+       help
+         This enables extended support for UMC processors
+
 endmenu		# General setup
 
 config TINY_SHMEM
diff -urN oldtree/init/main.c newtree/init/main.c
--- oldtree/init/main.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/init/main.c	2006-07-12 19:00:43.000000000 -0400
@@ -609,8 +609,6 @@
 
 	check_bugs();
 
-	acpi_early_init(); /* before LAPIC and SMP init */
-
 	/* Do the rest non-__init'ed, we're now alive */
 	rest_init();
 }
@@ -727,6 +725,14 @@
 	 */
 	child_reaper = current;
 
+ 	/*
+ 	 * Do this before initcalls, because some drivers want to access
+ 	 * firmware files.
+ 	 */
+ 	populate_rootfs();
+ 
+ 	acpi_early_init(); /* before LAPIC and SMP init */
+
 	smp_prepare_cpus(max_cpus);
 
 	do_pre_smp_initcalls();
@@ -736,12 +742,6 @@
 
 	cpuset_init_smp();
 
-	/*
-	 * Do this before initcalls, because some drivers want to access
-	 * firmware files.
-	 */
-	populate_rootfs();
-
 	do_basic_setup();
 
 	/*
diff -urN oldtree/kernel/Kconfig.hz newtree/kernel/Kconfig.hz
--- oldtree/kernel/Kconfig.hz	2006-07-05 10:06:57.000000000 -0400
+++ newtree/kernel/Kconfig.hz	2006-07-12 19:00:51.000000000 -0400
@@ -4,7 +4,7 @@
 
 choice
 	prompt "Timer frequency"
-	default HZ_250
+	default HZ_1000
 	help
 	 Allows the configuration of the timer frequency. It is customary
 	 to have the timer interrupt run at 1000 HZ but 100 HZ may be more
@@ -21,15 +21,26 @@
 	help
 	  100 HZ is a typical choice for servers, SMP and NUMA systems
 	  with lots of processors that may show reduced performance if
-	  too many timer interrupts are occurring.
+	  too many timer interrupts are occurring. Laptops may also show
+	  improved battery life.
 
-	config HZ_250
+	config HZ_250_NODEFAULT
 		bool "250 HZ"
 	help
-	 250 HZ is a good compromise choice allowing server performance
-	 while also showing good interactive responsiveness even
-	 on SMP and NUMA systems.
-
+	 250 HZ is a lousy compromise choice allowing server interactivity
+	 while also showing desktop throughput and no extra power saving on
+	 laptops. Good for when you can't make up your mind.
+
+	 Recommend 100 or 1000 instead.
+
+        config HZ_864
+                bool "864 HZ"
+        help
+         864 HZ is the best value for desktop systems. Most responsive
+         out of all the options. The only reason it is not default is
+         because it may break few drivers. Give it a try if you have
+         a desktop :).
+         
 	config HZ_1000
 		bool "1000 HZ"
 	help
@@ -41,6 +52,7 @@
 config HZ
 	int
 	default 100 if HZ_100
-	default 250 if HZ_250
+	default 250 if HZ_250_NODEFAULT
+        default 864 if HZ_864
 	default 1000 if HZ_1000
 
diff -urN oldtree/kernel/Kconfig.preempt newtree/kernel/Kconfig.preempt
--- oldtree/kernel/Kconfig.preempt	2006-07-05 10:06:57.000000000 -0400
+++ newtree/kernel/Kconfig.preempt	2006-07-12 19:00:51.000000000 -0400
@@ -1,4 +1,3 @@
-
 choice
 	prompt "Preemption Model"
 	default PREEMPT_NONE
diff -urN oldtree/kernel/Makefile newtree/kernel/Makefile
--- oldtree/kernel/Makefile	2006-07-05 10:06:57.000000000 -0400
+++ newtree/kernel/Makefile	2006-07-12 19:00:11.000000000 -0400
@@ -2,7 +2,10 @@
 # Makefile for the linux kernel.
 #
 
-obj-y     = sched.o fork.o exec_domain.o panic.o printk.o profile.o \
+obj-$(CONFIG_INGOSCHED) = sched_ingosched.o
+obj-$(CONFIG_STAIRCASE) = sched_staircase.o
+
+obj-y     += fork.o exec_domain.o panic.o printk.o profile.o \
 	    exit.o itimer.o time.o softirq.o resource.o \
 	    sysctl.o capability.o ptrace.o timer.o user.o \
 	    signal.o sys.o kmod.o workqueue.o pid.o \
diff -urN oldtree/kernel/exit.c newtree/kernel/exit.c
--- oldtree/kernel/exit.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/kernel/exit.c	2006-07-12 19:00:11.000000000 -0400
@@ -165,7 +165,9 @@
 		zap_leader = (leader->exit_signal == -1);
 	}
 
-	sched_exit(p);
+#ifdef CONFIG_INGOSCHED	
+        sched_exit(p);
+#endif
 	write_unlock_irq(&tasklist_lock);
 	proc_flush_task(p);
 	release_thread(p);
@@ -829,9 +831,10 @@
 	     unlikely(tsk->parent->signal->flags & SIGNAL_GROUP_EXIT)))
 		state = EXIT_DEAD;
 	tsk->exit_state = state;
-
+#ifdef CONFIG_INGOSCHED
+        sched_exit(tsk);
+#endif
 	write_unlock_irq(&tasklist_lock);
-
 	list_for_each_safe(_p, _n, &ptrace_dead) {
 		list_del_init(_p);
 		t = list_entry(_p, struct task_struct, ptrace_list);
diff -urN oldtree/kernel/fork.c newtree/kernel/fork.c
--- oldtree/kernel/fork.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/kernel/fork.c	2006-07-12 19:01:08.000000000 -0400
@@ -97,6 +97,7 @@
 
 /* SLAB cache for vm_area_struct structures */
 kmem_cache_t *vm_area_cachep;
+EXPORT_SYMBOL_GPL(vm_area_cachep);
 
 /* SLAB cache for mm_struct structures (tsk->mm) */
 static kmem_cache_t *mm_cachep;
@@ -389,6 +390,40 @@
 EXPORT_SYMBOL_GPL(mmput);
 
 /**
+ * set_new_mm - allocate, init and activate a new mm for a kernel thread
+ */
+int set_new_mm(void)
+{
+	struct mm_struct *mm;
+	struct task_struct *tsk = current;
+	struct mm_struct *active_mm;
+
+	mm = mm_alloc();
+	if (!mm)
+		goto fail_nomem;
+	if (init_new_context(current,mm))
+		goto fail_nocontext;
+
+	task_lock(tsk);
+	tsk->flags |= PF_BORROWED_MM;	
+	active_mm = tsk->active_mm;
+	current->mm = mm;
+	current->active_mm = mm;
+	activate_mm(active_mm, mm);
+	task_unlock(current);
+
+	/* Drop the previous active_mm */
+	mmdrop(active_mm);
+	return 0;
+	
+fail_nocontext:
+	mmdrop(mm);
+fail_nomem:
+	return -EINVAL;
+}
+EXPORT_SYMBOL_GPL(set_new_mm);
+
+/**
  * get_task_mm - acquire a reference to the task's mm
  *
  * Returns %NULL if the task has no mm.  Checks PF_BORROWED_MM (meaning
diff -urN oldtree/kernel/sched.c newtree/kernel/sched.c
--- oldtree/kernel/sched.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/kernel/sched.c	2006-07-12 19:00:11.000000000 -0400
@@ -1,6984 +1,29 @@
-/*
- *  kernel/sched.c
- *
- *  Kernel scheduler and related syscalls
- *
- *  Copyright (C) 1991-2002  Linus Torvalds
- *
- *  1996-12-23  Modified by Dave Grothe to fix bugs in semaphores and
- *		make semaphores SMP safe
- *  1998-11-19	Implemented schedule_timeout() and related stuff
- *		by Andrea Arcangeli
- *  2002-01-04	New ultra-scalable O(1) scheduler by Ingo Molnar:
- *		hybrid priority-list and round-robin design with
- *		an array-switch method of distributing timeslices
- *		and per-CPU runqueues.  Cleanups and useful suggestions
- *		by Davide Libenzi, preemptible kernel bits by Robert Love.
- *  2003-09-03	Interactivity tuning by Con Kolivas.
- *  2004-04-02	Scheduler domains code by Nick Piggin
- */
+// No-Sources Scheduler Selection (NoSched) v0.6
+ - For kernel 2.6.17-mm6
 
-#include <linux/mm.h>
-#include <linux/module.h>
-#include <linux/nmi.h>
-#include <linux/init.h>
-#include <asm/uaccess.h>
-#include <linux/highmem.h>
-#include <linux/smp_lock.h>
-#include <asm/mmu_context.h>
-#include <linux/interrupt.h>
-#include <linux/capability.h>
-#include <linux/completion.h>
-#include <linux/kernel_stat.h>
-#include <linux/debug_locks.h>
-#include <linux/security.h>
-#include <linux/notifier.h>
-#include <linux/profile.h>
-#include <linux/suspend.h>
-#include <linux/vmalloc.h>
-#include <linux/blkdev.h>
-#include <linux/delay.h>
-#include <linux/smp.h>
-#include <linux/threads.h>
-#include <linux/timer.h>
-#include <linux/rcupdate.h>
-#include <linux/cpu.h>
-#include <linux/cpuset.h>
-#include <linux/percpu.h>
-#include <linux/kthread.h>
-#include <linux/seq_file.h>
-#include <linux/sysctl.h>
-#include <linux/syscalls.h>
-#include <linux/times.h>
-#include <linux/acct.h>
-#include <linux/kprobes.h>
-#include <linux/delayacct.h>
-#include <asm/tlb.h>
+// Building scheduler files
+ - kernel/sched_ingosched.c 
+ - kernel/sched_staircase.c 
+
+//Scheduler Versions
+ - Ingosched: 2.6.17-mm6
+ - Staircase: v16 + 2.6.17-mm6
+
+// History of sched.c
+    Kernel scheduler and related syscalls
+  
+    Copyright (C) 1991-2002  Linus Torvalds
+  
+    1996-12-23  Modified by Dave Grothe to fix bugs in semaphores and
+                make semaphores SMP safe
+    1998-11-19  Implemented schedule_timeout() and related stuff
+                by Andrea Arcangeli
+    2002-01-04  New ultra-scalable O(1) scheduler by Ingo Molnar:
+                hybrid priority-list and round-robin design with
+                an array-switch method of distributing timeslices
+                and per-CPU runqueues.  Cleanups and useful suggestions
+                by Davide Libenzi, preemptible kernel bits by Robert Love.
+    2003-09-03  Interactivity tuning by Con Kolivas.
+    2004-04-02  Scheduler domains code by Nick Piggin
+ 
 
-#include <asm/unistd.h>
-
-/*
- * Convert user-nice values [ -20 ... 0 ... 19 ]
- * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],
- * and back.
- */
-#define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 20)
-#define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 20)
-#define TASK_NICE(p)		PRIO_TO_NICE((p)->static_prio)
-
-/*
- * 'User priority' is the nice value converted to something we
- * can work with better when scaling various scheduler parameters,
- * it's a [ 0 ... 39 ] range.
- */
-#define USER_PRIO(p)		((p)-MAX_RT_PRIO)
-#define TASK_USER_PRIO(p)	USER_PRIO((p)->static_prio)
-#define MAX_USER_PRIO		(USER_PRIO(MAX_PRIO))
-
-/*
- * Some helpers for converting nanosecond timing to jiffy resolution
- */
-#define NS_TO_JIFFIES(TIME)	((TIME) / (1000000000 / HZ))
-#define JIFFIES_TO_NS(TIME)	((TIME) * (1000000000 / HZ))
-
-/*
- * These are the 'tuning knobs' of the scheduler:
- *
- * Minimum timeslice is 5 msecs (or 1 jiffy, whichever is larger),
- * default timeslice is 100 msecs, maximum timeslice is 800 msecs.
- * Timeslices get refilled after they expire.
- */
-#define MIN_TIMESLICE		max(5 * HZ / 1000, 1)
-#define DEF_TIMESLICE		(100 * HZ / 1000)
-#define ON_RUNQUEUE_WEIGHT	 30
-#define CHILD_PENALTY		 95
-#define PARENT_PENALTY		100
-#define EXIT_WEIGHT		  3
-#define PRIO_BONUS_RATIO	 25
-#define MAX_BONUS		(MAX_USER_PRIO * PRIO_BONUS_RATIO / 100)
-#define INTERACTIVE_DELTA	  2
-#define MAX_SLEEP_AVG		(DEF_TIMESLICE * MAX_BONUS)
-#define STARVATION_LIMIT	(MAX_SLEEP_AVG)
-#define NS_MAX_SLEEP_AVG	(JIFFIES_TO_NS(MAX_SLEEP_AVG))
-
-/*
- * If a task is 'interactive' then we reinsert it in the active
- * array after it has expired its current timeslice. (it will not
- * continue to run immediately, it will still roundrobin with
- * other interactive tasks.)
- *
- * This part scales the interactivity limit depending on niceness.
- *
- * We scale it linearly, offset by the INTERACTIVE_DELTA delta.
- * Here are a few examples of different nice levels:
- *
- *  TASK_INTERACTIVE(-20): [1,1,1,1,1,1,1,1,1,0,0]
- *  TASK_INTERACTIVE(-10): [1,1,1,1,1,1,1,0,0,0,0]
- *  TASK_INTERACTIVE(  0): [1,1,1,1,0,0,0,0,0,0,0]
- *  TASK_INTERACTIVE( 10): [1,1,0,0,0,0,0,0,0,0,0]
- *  TASK_INTERACTIVE( 19): [0,0,0,0,0,0,0,0,0,0,0]
- *
- * (the X axis represents the possible -5 ... 0 ... +5 dynamic
- *  priority range a task can explore, a value of '1' means the
- *  task is rated interactive.)
- *
- * Ie. nice +19 tasks can never get 'interactive' enough to be
- * reinserted into the active array. And only heavily CPU-hog nice -20
- * tasks will be expired. Default nice 0 tasks are somewhere between,
- * it takes some effort for them to get interactive, but it's not
- * too hard.
- */
-
-#define CURRENT_BONUS(p) \
-	(NS_TO_JIFFIES((p)->sleep_avg) * MAX_BONUS / \
-		MAX_SLEEP_AVG)
-
-#define GRANULARITY	(10 * HZ / 1000 ? : 1)
-
-#ifdef CONFIG_SMP
-#define TIMESLICE_GRANULARITY(p)	(GRANULARITY * \
-		(1 << (((MAX_BONUS - CURRENT_BONUS(p)) ? : 1) - 1)) * \
-			num_online_cpus())
-#else
-#define TIMESLICE_GRANULARITY(p)	(GRANULARITY * \
-		(1 << (((MAX_BONUS - CURRENT_BONUS(p)) ? : 1) - 1)))
-#endif
-
-#define SCALE(v1,v1_max,v2_max) \
-	(v1) * (v2_max) / (v1_max)
-
-#define DELTA(p) \
-	(SCALE(TASK_NICE(p) + 20, 40, MAX_BONUS) - 20 * MAX_BONUS / 40 + \
-		INTERACTIVE_DELTA)
-
-#define TASK_INTERACTIVE(p) \
-	((p)->prio <= (p)->static_prio - DELTA(p))
-
-#define INTERACTIVE_SLEEP(p) \
-	(JIFFIES_TO_NS(MAX_SLEEP_AVG * \
-		(MAX_BONUS / 2 + DELTA((p)) + 1) / MAX_BONUS - 1))
-
-#define TASK_PREEMPTS_CURR(p, rq) \
-	((p)->prio < (rq)->curr->prio)
-
-/*
- * task_timeslice() scales user-nice values [ -20 ... 0 ... 19 ]
- * to time slice values: [800ms ... 100ms ... 5ms]
- *
- * The higher a thread's priority, the bigger timeslices
- * it gets during one round of execution. But even the lowest
- * priority thread gets MIN_TIMESLICE worth of execution time.
- */
-
-#define SCALE_PRIO(x, prio) \
-	max(x * (MAX_PRIO - prio) / (MAX_USER_PRIO / 2), MIN_TIMESLICE)
-
-static unsigned int static_prio_timeslice(int static_prio)
-{
-	if (static_prio < NICE_TO_PRIO(0))
-		return SCALE_PRIO(DEF_TIMESLICE * 4, static_prio);
-	else
-		return SCALE_PRIO(DEF_TIMESLICE, static_prio);
-}
-
-static inline unsigned int task_timeslice(struct task_struct *p)
-{
-	return static_prio_timeslice(p->static_prio);
-}
-
-/*
- * These are the runqueue data structures:
- */
-
-struct prio_array {
-	unsigned int nr_active;
-	DECLARE_BITMAP(bitmap, MAX_PRIO+1); /* include 1 bit for delimiter */
-	struct list_head queue[MAX_PRIO];
-};
-
-/*
- * This is the main, per-CPU runqueue data structure.
- *
- * Locking rule: those places that want to lock multiple runqueues
- * (such as the load balancing or the thread migration code), lock
- * acquire operations must be ordered by ascending &runqueue.
- */
-struct rq {
-	spinlock_t lock;
-
-	/*
-	 * nr_running and cpu_load should be in the same cacheline because
-	 * remote CPUs use both these fields when doing load calculation.
-	 */
-	unsigned long nr_running;
-	unsigned long raw_weighted_load;
-#ifdef CONFIG_SMP
-	unsigned long cpu_load[3];
-#endif
-	unsigned long long nr_switches;
-
-	/*
-	 * This is part of a global counter where only the total sum
-	 * over all CPUs matters. A task can increase this counter on
-	 * one CPU and if it got migrated afterwards it may decrease
-	 * it on another CPU. Always updated under the runqueue lock:
-	 */
-	unsigned long nr_uninterruptible;
-
-	unsigned long expired_timestamp;
-	unsigned long long timestamp_last_tick;
-	struct task_struct *curr, *idle;
-	struct mm_struct *prev_mm;
-	struct prio_array *active, *expired, arrays[2];
-	int best_expired_prio;
-	atomic_t nr_iowait;
-
-#ifdef CONFIG_SMP
-	struct sched_domain *sd;
-
-	/* For active balancing */
-	int active_balance;
-	int push_cpu;
-
-	struct task_struct *migration_thread;
-	struct list_head migration_queue;
-#endif
-
-#ifdef CONFIG_SCHEDSTATS
-	/* latency stats */
-	struct sched_info rq_sched_info;
-
-	/* sys_sched_yield() stats */
-	unsigned long yld_exp_empty;
-	unsigned long yld_act_empty;
-	unsigned long yld_both_empty;
-	unsigned long yld_cnt;
-
-	/* schedule() stats */
-	unsigned long sched_switch;
-	unsigned long sched_cnt;
-	unsigned long sched_goidle;
-
-	/* try_to_wake_up() stats */
-	unsigned long ttwu_cnt;
-	unsigned long ttwu_local;
-#endif
-	struct lock_class_key rq_lock_key;
-};
-
-static DEFINE_PER_CPU(struct rq, runqueues);
-
-/*
- * The domain tree (rq->sd) is protected by RCU's quiescent state transition.
- * See detach_destroy_domains: synchronize_sched for details.
- *
- * The domain tree of any CPU may only be accessed from within
- * preempt-disabled sections.
- */
-#define for_each_domain(cpu, __sd) \
-	for (__sd = rcu_dereference(cpu_rq(cpu)->sd); __sd; __sd = __sd->parent)
-
-#define cpu_rq(cpu)		(&per_cpu(runqueues, (cpu)))
-#define this_rq()		(&__get_cpu_var(runqueues))
-#define task_rq(p)		cpu_rq(task_cpu(p))
-#define cpu_curr(cpu)		(cpu_rq(cpu)->curr)
-
-#ifndef prepare_arch_switch
-# define prepare_arch_switch(next)	do { } while (0)
-#endif
-#ifndef finish_arch_switch
-# define finish_arch_switch(prev)	do { } while (0)
-#endif
-
-#ifndef __ARCH_WANT_UNLOCKED_CTXSW
-static inline int task_running(struct rq *rq, struct task_struct *p)
-{
-	return rq->curr == p;
-}
-
-static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
-{
-}
-
-static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
-{
-#ifdef CONFIG_DEBUG_SPINLOCK
-	/* this is a valid case when another task releases the spinlock */
-	rq->lock.owner = current;
-#endif
-	/*
-	 * If we are tracking spinlock dependencies then we have to
-	 * fix up the runqueue lock - which gets 'carried over' from
-	 * prev into current:
-	 */
-	spin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);
-
-	spin_unlock_irq(&rq->lock);
-}
-
-#else /* __ARCH_WANT_UNLOCKED_CTXSW */
-static inline int task_running(struct rq *rq, struct task_struct *p)
-{
-#ifdef CONFIG_SMP
-	return p->oncpu;
-#else
-	return rq->curr == p;
-#endif
-}
-
-static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
-{
-#ifdef CONFIG_SMP
-	/*
-	 * We can optimise this out completely for !SMP, because the
-	 * SMP rebalancing from interrupt is the only thing that cares
-	 * here.
-	 */
-	next->oncpu = 1;
-#endif
-#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW
-	spin_unlock_irq(&rq->lock);
-#else
-	spin_unlock(&rq->lock);
-#endif
-}
-
-static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
-{
-#ifdef CONFIG_SMP
-	/*
-	 * After ->oncpu is cleared, the task can be moved to a different CPU.
-	 * We must ensure this doesn't happen until the switch is completely
-	 * finished.
-	 */
-	smp_wmb();
-	prev->oncpu = 0;
-#endif
-#ifndef __ARCH_WANT_INTERRUPTS_ON_CTXSW
-	local_irq_enable();
-#endif
-}
-#endif /* __ARCH_WANT_UNLOCKED_CTXSW */
-
-/*
- * __task_rq_lock - lock the runqueue a given task resides on.
- * Must be called interrupts disabled.
- */
-static inline struct rq *__task_rq_lock(struct task_struct *p)
-	__acquires(rq->lock)
-{
-	struct rq *rq;
-
-repeat_lock_task:
-	rq = task_rq(p);
-	spin_lock(&rq->lock);
-	if (unlikely(rq != task_rq(p))) {
-		spin_unlock(&rq->lock);
-		goto repeat_lock_task;
-	}
-	return rq;
-}
-
-/*
- * task_rq_lock - lock the runqueue a given task resides on and disable
- * interrupts.  Note the ordering: we can safely lookup the task_rq without
- * explicitly disabling preemption.
- */
-static struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)
-	__acquires(rq->lock)
-{
-	struct rq *rq;
-
-repeat_lock_task:
-	local_irq_save(*flags);
-	rq = task_rq(p);
-	spin_lock(&rq->lock);
-	if (unlikely(rq != task_rq(p))) {
-		spin_unlock_irqrestore(&rq->lock, *flags);
-		goto repeat_lock_task;
-	}
-	return rq;
-}
-
-static inline void __task_rq_unlock(struct rq *rq)
-	__releases(rq->lock)
-{
-	spin_unlock(&rq->lock);
-}
-
-static inline void task_rq_unlock(struct rq *rq, unsigned long *flags)
-	__releases(rq->lock)
-{
-	spin_unlock_irqrestore(&rq->lock, *flags);
-}
-
-#ifdef CONFIG_SCHEDSTATS
-/*
- * bump this up when changing the output format or the meaning of an existing
- * format, so that tools can adapt (or abort)
- */
-#define SCHEDSTAT_VERSION 12
-
-static int show_schedstat(struct seq_file *seq, void *v)
-{
-	int cpu;
-
-	seq_printf(seq, "version %d\n", SCHEDSTAT_VERSION);
-	seq_printf(seq, "timestamp %lu\n", jiffies);
-	for_each_online_cpu(cpu) {
-		struct rq *rq = cpu_rq(cpu);
-#ifdef CONFIG_SMP
-		struct sched_domain *sd;
-		int dcnt = 0;
-#endif
-
-		/* runqueue-specific stats */
-		seq_printf(seq,
-		    "cpu%d %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu",
-		    cpu, rq->yld_both_empty,
-		    rq->yld_act_empty, rq->yld_exp_empty, rq->yld_cnt,
-		    rq->sched_switch, rq->sched_cnt, rq->sched_goidle,
-		    rq->ttwu_cnt, rq->ttwu_local,
-		    rq->rq_sched_info.cpu_time,
-		    rq->rq_sched_info.run_delay, rq->rq_sched_info.pcnt);
-
-		seq_printf(seq, "\n");
-
-#ifdef CONFIG_SMP
-		/* domain-specific stats */
-		preempt_disable();
-		for_each_domain(cpu, sd) {
-			enum idle_type itype;
-			char mask_str[NR_CPUS];
-
-			cpumask_scnprintf(mask_str, NR_CPUS, sd->span);
-			seq_printf(seq, "domain%d %s", dcnt++, mask_str);
-			for (itype = SCHED_IDLE; itype < MAX_IDLE_TYPES;
-					itype++) {
-				seq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu",
-				    sd->lb_cnt[itype],
-				    sd->lb_balanced[itype],
-				    sd->lb_failed[itype],
-				    sd->lb_imbalance[itype],
-				    sd->lb_gained[itype],
-				    sd->lb_hot_gained[itype],
-				    sd->lb_nobusyq[itype],
-				    sd->lb_nobusyg[itype]);
-			}
-			seq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu\n",
-			    sd->alb_cnt, sd->alb_failed, sd->alb_pushed,
-			    sd->sbe_cnt, sd->sbe_balanced, sd->sbe_pushed,
-			    sd->sbf_cnt, sd->sbf_balanced, sd->sbf_pushed,
-			    sd->ttwu_wake_remote, sd->ttwu_move_affine, sd->ttwu_move_balance);
-		}
-		preempt_enable();
-#endif
-	}
-	return 0;
-}
-
-static int schedstat_open(struct inode *inode, struct file *file)
-{
-	unsigned int size = PAGE_SIZE * (1 + num_online_cpus() / 32);
-	char *buf = kmalloc(size, GFP_KERNEL);
-	struct seq_file *m;
-	int res;
-
-	if (!buf)
-		return -ENOMEM;
-	res = single_open(file, show_schedstat, NULL);
-	if (!res) {
-		m = file->private_data;
-		m->buf = buf;
-		m->size = size;
-	} else
-		kfree(buf);
-	return res;
-}
-
-struct file_operations proc_schedstat_operations = {
-	.open    = schedstat_open,
-	.read    = seq_read,
-	.llseek  = seq_lseek,
-	.release = single_release,
-};
-
-/*
- * Expects runqueue lock to be held for atomicity of update
- */
-static inline void
-rq_sched_info_arrive(struct rq *rq, unsigned long delta_jiffies)
-{
-	if (rq) {
-		rq->rq_sched_info.run_delay += delta_jiffies;
-		rq->rq_sched_info.pcnt++;
-	}
-}
-
-/*
- * Expects runqueue lock to be held for atomicity of update
- */
-static inline void
-rq_sched_info_depart(struct rq *rq, unsigned long delta_jiffies)
-{
-	if (rq)
-		rq->rq_sched_info.cpu_time += delta_jiffies;
-}
-# define schedstat_inc(rq, field)	do { (rq)->field++; } while (0)
-# define schedstat_add(rq, field, amt)	do { (rq)->field += (amt); } while (0)
-#else /* !CONFIG_SCHEDSTATS */
-static inline void
-rq_sched_info_arrive(struct rq *rq, unsigned long delta_jiffies)
-{}
-static inline void
-rq_sched_info_depart(struct rq *rq, unsigned long delta_jiffies)
-{}
-# define schedstat_inc(rq, field)	do { } while (0)
-# define schedstat_add(rq, field, amt)	do { } while (0)
-#endif
-
-/*
- * rq_lock - lock a given runqueue and disable interrupts.
- */
-static inline struct rq *this_rq_lock(void)
-	__acquires(rq->lock)
-{
-	struct rq *rq;
-
-	local_irq_disable();
-	rq = this_rq();
-	spin_lock(&rq->lock);
-
-	return rq;
-}
-
-#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
-/*
- * Called when a process is dequeued from the active array and given
- * the cpu.  We should note that with the exception of interactive
- * tasks, the expired queue will become the active queue after the active
- * queue is empty, without explicitly dequeuing and requeuing tasks in the
- * expired queue.  (Interactive tasks may be requeued directly to the
- * active queue, thus delaying tasks in the expired queue from running;
- * see scheduler_tick()).
- *
- * This function is only called from sched_info_arrive(), rather than
- * dequeue_task(). Even though a task may be queued and dequeued multiple
- * times as it is shuffled about, we're really interested in knowing how
- * long it was from the *first* time it was queued to the time that it
- * finally hit a cpu.
- */
-static inline void sched_info_dequeued(struct task_struct *t)
-{
-	t->sched_info.last_queued = 0;
-}
-
-/*
- * Called when a task finally hits the cpu.  We can now calculate how
- * long it was waiting to run.  We also note when it began so that we
- * can keep stats on how long its timeslice is.
- */
-static void sched_info_arrive(struct task_struct *t)
-{
-	unsigned long now = jiffies, delta_jiffies = 0;
-
-	if (t->sched_info.last_queued)
-		delta_jiffies = now - t->sched_info.last_queued;
-	sched_info_dequeued(t);
-	t->sched_info.run_delay += delta_jiffies;
-	t->sched_info.last_arrival = now;
-	t->sched_info.pcnt++;
-
-	rq_sched_info_arrive(task_rq(t), delta_jiffies);
-}
-
-/*
- * Called when a process is queued into either the active or expired
- * array.  The time is noted and later used to determine how long we
- * had to wait for us to reach the cpu.  Since the expired queue will
- * become the active queue after active queue is empty, without dequeuing
- * and requeuing any tasks, we are interested in queuing to either. It
- * is unusual but not impossible for tasks to be dequeued and immediately
- * requeued in the same or another array: this can happen in sched_yield(),
- * set_user_nice(), and even load_balance() as it moves tasks from runqueue
- * to runqueue.
- *
- * This function is only called from enqueue_task(), but also only updates
- * the timestamp if it is already not set.  It's assumed that
- * sched_info_dequeued() will clear that stamp when appropriate.
- */
-static inline void sched_info_queued(struct task_struct *t)
-{
-	if (unlikely(sched_info_on()))
-		if (!t->sched_info.last_queued)
-			t->sched_info.last_queued = jiffies;
-}
-
-/*
- * Called when a process ceases being the active-running process, either
- * voluntarily or involuntarily.  Now we can calculate how long we ran.
- */
-static inline void sched_info_depart(struct task_struct *t)
-{
-	unsigned long delta_jiffies = jiffies - t->sched_info.last_arrival;
-
-	t->sched_info.cpu_time += delta_jiffies;
-	rq_sched_info_depart(task_rq(t), delta_jiffies);
-}
-
-/*
- * Called when tasks are switched involuntarily due, typically, to expiring
- * their time slice.  (This may also be called when switching to or from
- * the idle task.)  We are only called when prev != next.
- */
-static inline void
-__sched_info_switch(struct task_struct *prev, struct task_struct *next)
-{
-	struct rq *rq = task_rq(prev);
-
-	/*
-	 * prev now departs the cpu.  It's not interesting to record
-	 * stats about how efficient we were at scheduling the idle
-	 * process, however.
-	 */
-	if (prev != rq->idle)
-		sched_info_depart(prev);
-
-	if (next != rq->idle)
-		sched_info_arrive(next);
-}
-static inline void
-sched_info_switch(struct task_struct *prev, struct task_struct *next)
-{
-	if (unlikely(sched_info_on()))
-		__sched_info_switch(prev, next);
-}
-#else
-#define sched_info_queued(t)		do { } while (0)
-#define sched_info_switch(t, next)	do { } while (0)
-#endif /* CONFIG_SCHEDSTATS || CONFIG_TASK_DELAY_ACCT */
-
-/*
- * Adding/removing a task to/from a priority array:
- */
-static void dequeue_task(struct task_struct *p, struct prio_array *array)
-{
-	array->nr_active--;
-	list_del(&p->run_list);
-	if (list_empty(array->queue + p->prio))
-		__clear_bit(p->prio, array->bitmap);
-}
-
-static void enqueue_task(struct task_struct *p, struct prio_array *array)
-{
-	sched_info_queued(p);
-	list_add_tail(&p->run_list, array->queue + p->prio);
-	__set_bit(p->prio, array->bitmap);
-	array->nr_active++;
-	p->array = array;
-}
-
-/*
- * Put task to the end of the run list without the overhead of dequeue
- * followed by enqueue.
- */
-static void requeue_task(struct task_struct *p, struct prio_array *array)
-{
-	list_move_tail(&p->run_list, array->queue + p->prio);
-}
-
-static inline void
-enqueue_task_head(struct task_struct *p, struct prio_array *array)
-{
-	list_add(&p->run_list, array->queue + p->prio);
-	__set_bit(p->prio, array->bitmap);
-	array->nr_active++;
-	p->array = array;
-}
-
-/*
- * __normal_prio - return the priority that is based on the static
- * priority but is modified by bonuses/penalties.
- *
- * We scale the actual sleep average [0 .... MAX_SLEEP_AVG]
- * into the -5 ... 0 ... +5 bonus/penalty range.
- *
- * We use 25% of the full 0...39 priority range so that:
- *
- * 1) nice +19 interactive tasks do not preempt nice 0 CPU hogs.
- * 2) nice -20 CPU hogs do not get preempted by nice 0 tasks.
- *
- * Both properties are important to certain workloads.
- */
-
-static inline int __normal_prio(struct task_struct *p)
-{
-	int bonus, prio;
-
-	bonus = CURRENT_BONUS(p) - MAX_BONUS / 2;
-
-	prio = p->static_prio - bonus;
-	if (prio < MAX_RT_PRIO)
-		prio = MAX_RT_PRIO;
-	if (prio > MAX_PRIO-1)
-		prio = MAX_PRIO-1;
-	return prio;
-}
-
-/*
- * To aid in avoiding the subversion of "niceness" due to uneven distribution
- * of tasks with abnormal "nice" values across CPUs the contribution that
- * each task makes to its run queue's load is weighted according to its
- * scheduling class and "nice" value.  For SCHED_NORMAL tasks this is just a
- * scaled version of the new time slice allocation that they receive on time
- * slice expiry etc.
- */
-
-/*
- * Assume: static_prio_timeslice(NICE_TO_PRIO(0)) == DEF_TIMESLICE
- * If static_prio_timeslice() is ever changed to break this assumption then
- * this code will need modification
- */
-#define TIME_SLICE_NICE_ZERO DEF_TIMESLICE
-#define LOAD_WEIGHT(lp) \
-	(((lp) * SCHED_LOAD_SCALE) / TIME_SLICE_NICE_ZERO)
-#define PRIO_TO_LOAD_WEIGHT(prio) \
-	LOAD_WEIGHT(static_prio_timeslice(prio))
-#define RTPRIO_TO_LOAD_WEIGHT(rp) \
-	(PRIO_TO_LOAD_WEIGHT(MAX_RT_PRIO) + LOAD_WEIGHT(rp))
-
-static void set_load_weight(struct task_struct *p)
-{
-	if (has_rt_policy(p)) {
-#ifdef CONFIG_SMP
-		if (p == task_rq(p)->migration_thread)
-			/*
-			 * The migration thread does the actual balancing.
-			 * Giving its load any weight will skew balancing
-			 * adversely.
-			 */
-			p->load_weight = 0;
-		else
-#endif
-			p->load_weight = RTPRIO_TO_LOAD_WEIGHT(p->rt_priority);
-	} else
-		p->load_weight = PRIO_TO_LOAD_WEIGHT(p->static_prio);
-}
-
-static inline void
-inc_raw_weighted_load(struct rq *rq, const struct task_struct *p)
-{
-	rq->raw_weighted_load += p->load_weight;
-}
-
-static inline void
-dec_raw_weighted_load(struct rq *rq, const struct task_struct *p)
-{
-	rq->raw_weighted_load -= p->load_weight;
-}
-
-static inline void inc_nr_running(struct task_struct *p, struct rq *rq)
-{
-	rq->nr_running++;
-	inc_raw_weighted_load(rq, p);
-}
-
-static inline void dec_nr_running(struct task_struct *p, struct rq *rq)
-{
-	rq->nr_running--;
-	dec_raw_weighted_load(rq, p);
-}
-
-/*
- * Calculate the expected normal priority: i.e. priority
- * without taking RT-inheritance into account. Might be
- * boosted by interactivity modifiers. Changes upon fork,
- * setprio syscalls, and whenever the interactivity
- * estimator recalculates.
- */
-static inline int normal_prio(struct task_struct *p)
-{
-	int prio;
-
-	if (has_rt_policy(p))
-		prio = MAX_RT_PRIO-1 - p->rt_priority;
-	else
-		prio = __normal_prio(p);
-	return prio;
-}
-
-/*
- * Calculate the current priority, i.e. the priority
- * taken into account by the scheduler. This value might
- * be boosted by RT tasks, or might be boosted by
- * interactivity modifiers. Will be RT if the task got
- * RT-boosted. If not then it returns p->normal_prio.
- */
-static int effective_prio(struct task_struct *p)
-{
-	p->normal_prio = normal_prio(p);
-	/*
-	 * If we are RT tasks or we were boosted to RT priority,
-	 * keep the priority unchanged. Otherwise, update priority
-	 * to the normal priority:
-	 */
-	if (!rt_prio(p->prio))
-		return p->normal_prio;
-	return p->prio;
-}
-
-/*
- * __activate_task - move a task to the runqueue.
- */
-static void __activate_task(struct task_struct *p, struct rq *rq)
-{
-	struct prio_array *target = rq->active;
-
-	if (batch_task(p))
-		target = rq->expired;
-	enqueue_task(p, target);
-	inc_nr_running(p, rq);
-}
-
-/*
- * __activate_idle_task - move idle task to the _front_ of runqueue.
- */
-static inline void __activate_idle_task(struct task_struct *p, struct rq *rq)
-{
-	enqueue_task_head(p, rq->active);
-	inc_nr_running(p, rq);
-}
-
-/*
- * Recalculate p->normal_prio and p->prio after having slept,
- * updating the sleep-average too:
- */
-static int recalc_task_prio(struct task_struct *p, unsigned long long now)
-{
-	/* Caller must always ensure 'now >= p->timestamp' */
-	unsigned long sleep_time = now - p->timestamp;
-
-	if (batch_task(p))
-		sleep_time = 0;
-
-	if (likely(sleep_time > 0)) {
-		/*
-		 * This ceiling is set to the lowest priority that would allow
-		 * a task to be reinserted into the active array on timeslice
-		 * completion.
-		 */
-		unsigned long ceiling = INTERACTIVE_SLEEP(p);
-
-		if (p->mm && sleep_time > ceiling && p->sleep_avg < ceiling) {
-			/*
-			 * Prevents user tasks from achieving best priority
-			 * with one single large enough sleep.
-			 */
-			p->sleep_avg = ceiling;
-			/*
-			 * Using INTERACTIVE_SLEEP() as a ceiling places a
-			 * nice(0) task 1ms sleep away from promotion, and
-			 * gives it 700ms to round-robin with no chance of
-			 * being demoted.  This is more than generous, so
-			 * mark this sleep as non-interactive to prevent the
-			 * on-runqueue bonus logic from intervening should
-			 * this task not receive cpu immediately.
-			 */
-			p->sleep_type = SLEEP_NONINTERACTIVE;
-		} else {
-			/*
-			 * Tasks waking from uninterruptible sleep are
-			 * limited in their sleep_avg rise as they
-			 * are likely to be waiting on I/O
-			 */
-			if (p->sleep_type == SLEEP_NONINTERACTIVE && p->mm) {
-				if (p->sleep_avg >= ceiling)
-					sleep_time = 0;
-				else if (p->sleep_avg + sleep_time >=
-					 ceiling) {
-						p->sleep_avg = ceiling;
-						sleep_time = 0;
-				}
-			}
-
-			/*
-			 * This code gives a bonus to interactive tasks.
-			 *
-			 * The boost works by updating the 'average sleep time'
-			 * value here, based on ->timestamp. The more time a
-			 * task spends sleeping, the higher the average gets -
-			 * and the higher the priority boost gets as well.
-			 */
-			p->sleep_avg += sleep_time;
-
-		}
-		if (p->sleep_avg > NS_MAX_SLEEP_AVG)
-			p->sleep_avg = NS_MAX_SLEEP_AVG;
-	}
-
-	return effective_prio(p);
-}
-
-/*
- * activate_task - move a task to the runqueue and do priority recalculation
- *
- * Update all the scheduling statistics stuff. (sleep average
- * calculation, priority modifiers, etc.)
- */
-static void activate_task(struct task_struct *p, struct rq *rq, int local)
-{
-	unsigned long long now;
-
-	now = sched_clock();
-#ifdef CONFIG_SMP
-	if (!local) {
-		/* Compensate for drifting sched_clock */
-		struct rq *this_rq = this_rq();
-		now = (now - this_rq->timestamp_last_tick)
-			+ rq->timestamp_last_tick;
-	}
-#endif
-
-	if (!rt_task(p))
-		p->prio = recalc_task_prio(p, now);
-
-	/*
-	 * This checks to make sure it's not an uninterruptible task
-	 * that is now waking up.
-	 */
-	if (p->sleep_type == SLEEP_NORMAL) {
-		/*
-		 * Tasks which were woken up by interrupts (ie. hw events)
-		 * are most likely of interactive nature. So we give them
-		 * the credit of extending their sleep time to the period
-		 * of time they spend on the runqueue, waiting for execution
-		 * on a CPU, first time around:
-		 */
-		if (in_interrupt())
-			p->sleep_type = SLEEP_INTERRUPTED;
-		else {
-			/*
-			 * Normal first-time wakeups get a credit too for
-			 * on-runqueue time, but it will be weighted down:
-			 */
-			p->sleep_type = SLEEP_INTERACTIVE;
-		}
-	}
-	p->timestamp = now;
-
-	__activate_task(p, rq);
-}
-
-/*
- * deactivate_task - remove a task from the runqueue.
- */
-static void deactivate_task(struct task_struct *p, struct rq *rq)
-{
-	dec_nr_running(p, rq);
-	dequeue_task(p, p->array);
-	p->array = NULL;
-}
-
-/*
- * resched_task - mark a task 'to be rescheduled now'.
- *
- * On UP this means the setting of the need_resched flag, on SMP it
- * might also involve a cross-CPU call to trigger the scheduler on
- * the target CPU.
- */
-#ifdef CONFIG_SMP
-
-#ifndef tsk_is_polling
-#define tsk_is_polling(t) test_tsk_thread_flag(t, TIF_POLLING_NRFLAG)
-#endif
-
-static void resched_task(struct task_struct *p)
-{
-	int cpu;
-
-	assert_spin_locked(&task_rq(p)->lock);
-
-	if (unlikely(test_tsk_thread_flag(p, TIF_NEED_RESCHED)))
-		return;
-
-	set_tsk_thread_flag(p, TIF_NEED_RESCHED);
-
-	cpu = task_cpu(p);
-	if (cpu == smp_processor_id())
-		return;
-
-	/* NEED_RESCHED must be visible before we test polling */
-	smp_mb();
-	if (!tsk_is_polling(p))
-		smp_send_reschedule(cpu);
-}
-#else
-static inline void resched_task(struct task_struct *p)
-{
-	assert_spin_locked(&task_rq(p)->lock);
-	set_tsk_need_resched(p);
-}
-#endif
-
-/**
- * task_curr - is this task currently executing on a CPU?
- * @p: the task in question.
- */
-inline int task_curr(const struct task_struct *p)
-{
-	return cpu_curr(task_cpu(p)) == p;
-}
-
-/* Used instead of source_load when we know the type == 0 */
-unsigned long weighted_cpuload(const int cpu)
-{
-	return cpu_rq(cpu)->raw_weighted_load;
-}
-
-#ifdef CONFIG_SMP
-struct migration_req {
-	struct list_head list;
-
-	struct task_struct *task;
-	int dest_cpu;
-
-	struct completion done;
-};
-
-/*
- * The task's runqueue lock must be held.
- * Returns true if you have to wait for migration thread.
- */
-static int
-migrate_task(struct task_struct *p, int dest_cpu, struct migration_req *req)
-{
-	struct rq *rq = task_rq(p);
-
-	/*
-	 * If the task is not on a runqueue (and not running), then
-	 * it is sufficient to simply update the task's cpu field.
-	 */
-	if (!p->array && !task_running(rq, p)) {
-		set_task_cpu(p, dest_cpu);
-		return 0;
-	}
-
-	init_completion(&req->done);
-	req->task = p;
-	req->dest_cpu = dest_cpu;
-	list_add(&req->list, &rq->migration_queue);
-
-	return 1;
-}
-
-/*
- * wait_task_inactive - wait for a thread to unschedule.
- *
- * The caller must ensure that the task *will* unschedule sometime soon,
- * else this function might spin for a *long* time. This function can't
- * be called with interrupts off, or it may introduce deadlock with
- * smp_call_function() if an IPI is sent by the same process we are
- * waiting to become inactive.
- */
-void wait_task_inactive(struct task_struct *p)
-{
-	unsigned long flags;
-	struct rq *rq;
-	int preempted;
-
-repeat:
-	rq = task_rq_lock(p, &flags);
-	/* Must be off runqueue entirely, not preempted. */
-	if (unlikely(p->array || task_running(rq, p))) {
-		/* If it's preempted, we yield.  It could be a while. */
-		preempted = !task_running(rq, p);
-		task_rq_unlock(rq, &flags);
-		cpu_relax();
-		if (preempted)
-			yield();
-		goto repeat;
-	}
-	task_rq_unlock(rq, &flags);
-}
-
-/***
- * kick_process - kick a running thread to enter/exit the kernel
- * @p: the to-be-kicked thread
- *
- * Cause a process which is running on another CPU to enter
- * kernel-mode, without any delay. (to get signals handled.)
- *
- * NOTE: this function doesnt have to take the runqueue lock,
- * because all it wants to ensure is that the remote task enters
- * the kernel. If the IPI races and the task has been migrated
- * to another CPU then no harm is done and the purpose has been
- * achieved as well.
- */
-void kick_process(struct task_struct *p)
-{
-	int cpu;
-
-	preempt_disable();
-	cpu = task_cpu(p);
-	if ((cpu != smp_processor_id()) && task_curr(p))
-		smp_send_reschedule(cpu);
-	preempt_enable();
-}
-
-/*
- * Return a low guess at the load of a migration-source cpu weighted
- * according to the scheduling class and "nice" value.
- *
- * We want to under-estimate the load of migration sources, to
- * balance conservatively.
- */
-static inline unsigned long source_load(int cpu, int type)
-{
-	struct rq *rq = cpu_rq(cpu);
-
-	if (type == 0)
-		return rq->raw_weighted_load;
-
-	return min(rq->cpu_load[type-1], rq->raw_weighted_load);
-}
-
-/*
- * Return a high guess at the load of a migration-target cpu weighted
- * according to the scheduling class and "nice" value.
- */
-static inline unsigned long target_load(int cpu, int type)
-{
-	struct rq *rq = cpu_rq(cpu);
-
-	if (type == 0)
-		return rq->raw_weighted_load;
-
-	return max(rq->cpu_load[type-1], rq->raw_weighted_load);
-}
-
-/*
- * Return the average load per task on the cpu's run queue
- */
-static inline unsigned long cpu_avg_load_per_task(int cpu)
-{
-	struct rq *rq = cpu_rq(cpu);
-	unsigned long n = rq->nr_running;
-
-	return n ? rq->raw_weighted_load / n : SCHED_LOAD_SCALE;
-}
-
-/*
- * find_idlest_group finds and returns the least busy CPU group within the
- * domain.
- */
-static struct sched_group *
-find_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu)
-{
-	struct sched_group *idlest = NULL, *this = NULL, *group = sd->groups;
-	unsigned long min_load = ULONG_MAX, this_load = 0;
-	int load_idx = sd->forkexec_idx;
-	int imbalance = 100 + (sd->imbalance_pct-100)/2;
-
-	do {
-		unsigned long load, avg_load;
-		int local_group;
-		int i;
-
-		/* Skip over this group if it has no CPUs allowed */
-		if (!cpus_intersects(group->cpumask, p->cpus_allowed))
-			goto nextgroup;
-
-		local_group = cpu_isset(this_cpu, group->cpumask);
-
-		/* Tally up the load of all CPUs in the group */
-		avg_load = 0;
-
-		for_each_cpu_mask(i, group->cpumask) {
-			/* Bias balancing toward cpus of our domain */
-			if (local_group)
-				load = source_load(i, load_idx);
-			else
-				load = target_load(i, load_idx);
-
-			avg_load += load;
-		}
-
-		/* Adjust by relative CPU power of the group */
-		avg_load = (avg_load * SCHED_LOAD_SCALE) / group->cpu_power;
-
-		if (local_group) {
-			this_load = avg_load;
-			this = group;
-		} else if (avg_load < min_load) {
-			min_load = avg_load;
-			idlest = group;
-		}
-nextgroup:
-		group = group->next;
-	} while (group != sd->groups);
-
-	if (!idlest || 100*this_load < imbalance*min_load)
-		return NULL;
-	return idlest;
-}
-
-/*
- * find_idlest_queue - find the idlest runqueue among the cpus in group.
- */
-static int
-find_idlest_cpu(struct sched_group *group, struct task_struct *p, int this_cpu)
-{
-	cpumask_t tmp;
-	unsigned long load, min_load = ULONG_MAX;
-	int idlest = -1;
-	int i;
-
-	/* Traverse only the allowed CPUs */
-	cpus_and(tmp, group->cpumask, p->cpus_allowed);
-
-	for_each_cpu_mask(i, tmp) {
-		load = weighted_cpuload(i);
-
-		if (load < min_load || (load == min_load && i == this_cpu)) {
-			min_load = load;
-			idlest = i;
-		}
-	}
-
-	return idlest;
-}
-
-/*
- * sched_balance_self: balance the current task (running on cpu) in domains
- * that have the 'flag' flag set. In practice, this is SD_BALANCE_FORK and
- * SD_BALANCE_EXEC.
- *
- * Balance, ie. select the least loaded group.
- *
- * Returns the target CPU number, or the same CPU if no balancing is needed.
- *
- * preempt must be disabled.
- */
-static int sched_balance_self(int cpu, int flag)
-{
-	struct task_struct *t = current;
-	struct sched_domain *tmp, *sd = NULL;
-
-	for_each_domain(cpu, tmp) {
- 		/*
- 	 	 * If power savings logic is enabled for a domain, stop there.
- 	 	 */
-		if (tmp->flags & SD_POWERSAVINGS_BALANCE)
-			break;
-		if (tmp->flags & flag)
-			sd = tmp;
-	}
-
-	while (sd) {
-		cpumask_t span;
-		struct sched_group *group;
-		int new_cpu;
-		int weight;
-
-		span = sd->span;
-		group = find_idlest_group(sd, t, cpu);
-		if (!group)
-			goto nextlevel;
-
-		new_cpu = find_idlest_cpu(group, t, cpu);
-		if (new_cpu == -1 || new_cpu == cpu)
-			goto nextlevel;
-
-		/* Now try balancing at a lower domain level */
-		cpu = new_cpu;
-nextlevel:
-		sd = NULL;
-		weight = cpus_weight(span);
-		for_each_domain(cpu, tmp) {
-			if (weight <= cpus_weight(tmp->span))
-				break;
-			if (tmp->flags & flag)
-				sd = tmp;
-		}
-		/* while loop will break here if sd == NULL */
-	}
-
-	return cpu;
-}
-
-#endif /* CONFIG_SMP */
-
-/*
- * wake_idle() will wake a task on an idle cpu if task->cpu is
- * not idle and an idle cpu is available.  The span of cpus to
- * search starts with cpus closest then further out as needed,
- * so we always favor a closer, idle cpu.
- *
- * Returns the CPU we should wake onto.
- */
-#if defined(ARCH_HAS_SCHED_WAKE_IDLE)
-static int wake_idle(int cpu, struct task_struct *p)
-{
-	cpumask_t tmp;
-	struct sched_domain *sd;
-	int i;
-
-	if (idle_cpu(cpu))
-		return cpu;
-
-	for_each_domain(cpu, sd) {
-		if (sd->flags & SD_WAKE_IDLE) {
-			cpus_and(tmp, sd->span, p->cpus_allowed);
-			for_each_cpu_mask(i, tmp) {
-				if (idle_cpu(i))
-					return i;
-			}
-		}
-		else
-			break;
-	}
-	return cpu;
-}
-#else
-static inline int wake_idle(int cpu, struct task_struct *p)
-{
-	return cpu;
-}
-#endif
-
-/***
- * try_to_wake_up - wake up a thread
- * @p: the to-be-woken-up thread
- * @state: the mask of task states that can be woken
- * @sync: do a synchronous wakeup?
- *
- * Put it on the run-queue if it's not already there. The "current"
- * thread is always on the run-queue (except when the actual
- * re-schedule is in progress), and as such you're allowed to do
- * the simpler "current->state = TASK_RUNNING" to mark yourself
- * runnable without the overhead of this.
- *
- * returns failure only if the task is already active.
- */
-static int try_to_wake_up(struct task_struct *p, unsigned int state, int sync)
-{
-	int cpu, this_cpu, success = 0;
-	unsigned long flags;
-	long old_state;
-	struct rq *rq;
-#ifdef CONFIG_SMP
-	struct sched_domain *sd, *this_sd = NULL;
-	unsigned long load, this_load;
-	int new_cpu;
-#endif
-
-	rq = task_rq_lock(p, &flags);
-	old_state = p->state;
-	if (!(old_state & state))
-		goto out;
-
-	if (p->array)
-		goto out_running;
-
-	cpu = task_cpu(p);
-	this_cpu = smp_processor_id();
-
-#ifdef CONFIG_SMP
-	if (unlikely(task_running(rq, p)))
-		goto out_activate;
-
-	new_cpu = cpu;
-
-	schedstat_inc(rq, ttwu_cnt);
-	if (cpu == this_cpu) {
-		schedstat_inc(rq, ttwu_local);
-		goto out_set_cpu;
-	}
-
-	for_each_domain(this_cpu, sd) {
-		if (cpu_isset(cpu, sd->span)) {
-			schedstat_inc(sd, ttwu_wake_remote);
-			this_sd = sd;
-			break;
-		}
-	}
-
-	if (unlikely(!cpu_isset(this_cpu, p->cpus_allowed)))
-		goto out_set_cpu;
-
-	/*
-	 * Check for affine wakeup and passive balancing possibilities.
-	 */
-	if (this_sd) {
-		int idx = this_sd->wake_idx;
-		unsigned int imbalance;
-
-		imbalance = 100 + (this_sd->imbalance_pct - 100) / 2;
-
-		load = source_load(cpu, idx);
-		this_load = target_load(this_cpu, idx);
-
-		new_cpu = this_cpu; /* Wake to this CPU if we can */
-
-		if (this_sd->flags & SD_WAKE_AFFINE) {
-			unsigned long tl = this_load;
-			unsigned long tl_per_task = cpu_avg_load_per_task(this_cpu);
-
-			/*
-			 * If sync wakeup then subtract the (maximum possible)
-			 * effect of the currently running task from the load
-			 * of the current CPU:
-			 */
-			if (sync)
-				tl -= current->load_weight;
-
-			if ((tl <= load &&
-				tl + target_load(cpu, idx) <= tl_per_task) ||
-				100*(tl + p->load_weight) <= imbalance*load) {
-				/*
-				 * This domain has SD_WAKE_AFFINE and
-				 * p is cache cold in this domain, and
-				 * there is no bad imbalance.
-				 */
-				schedstat_inc(this_sd, ttwu_move_affine);
-				goto out_set_cpu;
-			}
-		}
-
-		/*
-		 * Start passive balancing when half the imbalance_pct
-		 * limit is reached.
-		 */
-		if (this_sd->flags & SD_WAKE_BALANCE) {
-			if (imbalance*this_load <= 100*load) {
-				schedstat_inc(this_sd, ttwu_move_balance);
-				goto out_set_cpu;
-			}
-		}
-	}
-
-	new_cpu = cpu; /* Could not wake to this_cpu. Wake to cpu instead */
-out_set_cpu:
-	new_cpu = wake_idle(new_cpu, p);
-	if (new_cpu != cpu) {
-		set_task_cpu(p, new_cpu);
-		task_rq_unlock(rq, &flags);
-		/* might preempt at this point */
-		rq = task_rq_lock(p, &flags);
-		old_state = p->state;
-		if (!(old_state & state))
-			goto out;
-		if (p->array)
-			goto out_running;
-
-		this_cpu = smp_processor_id();
-		cpu = task_cpu(p);
-	}
-
-out_activate:
-#endif /* CONFIG_SMP */
-	if (old_state == TASK_UNINTERRUPTIBLE) {
-		rq->nr_uninterruptible--;
-		/*
-		 * Tasks on involuntary sleep don't earn
-		 * sleep_avg beyond just interactive state.
-		 */
-		p->sleep_type = SLEEP_NONINTERACTIVE;
-	} else
-
-	/*
-	 * Tasks that have marked their sleep as noninteractive get
-	 * woken up with their sleep average not weighted in an
-	 * interactive way.
-	 */
-		if (old_state & TASK_NONINTERACTIVE)
-			p->sleep_type = SLEEP_NONINTERACTIVE;
-
-
-	activate_task(p, rq, cpu == this_cpu);
-	/*
-	 * Sync wakeups (i.e. those types of wakeups where the waker
-	 * has indicated that it will leave the CPU in short order)
-	 * don't trigger a preemption, if the woken up task will run on
-	 * this cpu. (in this case the 'I will reschedule' promise of
-	 * the waker guarantees that the freshly woken up task is going
-	 * to be considered on this CPU.)
-	 */
-	if (!sync || cpu != this_cpu) {
-		if (TASK_PREEMPTS_CURR(p, rq))
-			resched_task(rq->curr);
-	}
-	success = 1;
-
-out_running:
-	p->state = TASK_RUNNING;
-out:
-	task_rq_unlock(rq, &flags);
-
-	return success;
-}
-
-int fastcall wake_up_process(struct task_struct *p)
-{
-	return try_to_wake_up(p, TASK_STOPPED | TASK_TRACED |
-				 TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE, 0);
-}
-EXPORT_SYMBOL(wake_up_process);
-
-int fastcall wake_up_state(struct task_struct *p, unsigned int state)
-{
-	return try_to_wake_up(p, state, 0);
-}
-
-/*
- * Perform scheduler related setup for a newly forked process p.
- * p is forked by current.
- */
-void fastcall sched_fork(struct task_struct *p, int clone_flags)
-{
-	int cpu = get_cpu();
-
-#ifdef CONFIG_SMP
-	cpu = sched_balance_self(cpu, SD_BALANCE_FORK);
-#endif
-	set_task_cpu(p, cpu);
-
-	/*
-	 * We mark the process as running here, but have not actually
-	 * inserted it onto the runqueue yet. This guarantees that
-	 * nobody will actually run it, and a signal or other external
-	 * event cannot wake it up and insert it on the runqueue either.
-	 */
-	p->state = TASK_RUNNING;
-
-	/*
-	 * Make sure we do not leak PI boosting priority to the child:
-	 */
-	p->prio = current->normal_prio;
-
-	INIT_LIST_HEAD(&p->run_list);
-	p->array = NULL;
-#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
-	if (unlikely(sched_info_on()))
-		memset(&p->sched_info, 0, sizeof(p->sched_info));
-#endif
-#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)
-	p->oncpu = 0;
-#endif
-#ifdef CONFIG_PREEMPT
-	/* Want to start with kernel preemption disabled. */
-	task_thread_info(p)->preempt_count = 1;
-#endif
-	/*
-	 * Share the timeslice between parent and child, thus the
-	 * total amount of pending timeslices in the system doesn't change,
-	 * resulting in more scheduling fairness.
-	 */
-	local_irq_disable();
-	p->time_slice = (current->time_slice + 1) >> 1;
-	/*
-	 * The remainder of the first timeslice might be recovered by
-	 * the parent if the child exits early enough.
-	 */
-	p->first_time_slice = 1;
-	current->time_slice >>= 1;
-	p->timestamp = sched_clock();
-	if (unlikely(!current->time_slice)) {
-		/*
-		 * This case is rare, it happens when the parent has only
-		 * a single jiffy left from its timeslice. Taking the
-		 * runqueue lock is not a problem.
-		 */
-		current->time_slice = 1;
-		scheduler_tick();
-	}
-	local_irq_enable();
-	put_cpu();
-}
-
-/*
- * wake_up_new_task - wake up a newly created task for the first time.
- *
- * This function will do some initial scheduler statistics housekeeping
- * that must be done for every newly created context, then puts the task
- * on the runqueue and wakes it.
- */
-void fastcall wake_up_new_task(struct task_struct *p, unsigned long clone_flags)
-{
-	struct rq *rq, *this_rq;
-	unsigned long flags;
-	int this_cpu, cpu;
-
-	rq = task_rq_lock(p, &flags);
-	BUG_ON(p->state != TASK_RUNNING);
-	this_cpu = smp_processor_id();
-	cpu = task_cpu(p);
-
-	/*
-	 * We decrease the sleep average of forking parents
-	 * and children as well, to keep max-interactive tasks
-	 * from forking tasks that are max-interactive. The parent
-	 * (current) is done further down, under its lock.
-	 */
-	p->sleep_avg = JIFFIES_TO_NS(CURRENT_BONUS(p) *
-		CHILD_PENALTY / 100 * MAX_SLEEP_AVG / MAX_BONUS);
-
-	p->prio = effective_prio(p);
-
-	if (likely(cpu == this_cpu)) {
-		if (!(clone_flags & CLONE_VM)) {
-			/*
-			 * The VM isn't cloned, so we're in a good position to
-			 * do child-runs-first in anticipation of an exec. This
-			 * usually avoids a lot of COW overhead.
-			 */
-			if (unlikely(!current->array))
-				__activate_task(p, rq);
-			else {
-				p->prio = current->prio;
-				p->normal_prio = current->normal_prio;
-				list_add_tail(&p->run_list, &current->run_list);
-				p->array = current->array;
-				p->array->nr_active++;
-				inc_nr_running(p, rq);
-			}
-			set_need_resched();
-		} else
-			/* Run child last */
-			__activate_task(p, rq);
-		/*
-		 * We skip the following code due to cpu == this_cpu
-	 	 *
-		 *   task_rq_unlock(rq, &flags);
-		 *   this_rq = task_rq_lock(current, &flags);
-		 */
-		this_rq = rq;
-	} else {
-		this_rq = cpu_rq(this_cpu);
-
-		/*
-		 * Not the local CPU - must adjust timestamp. This should
-		 * get optimised away in the !CONFIG_SMP case.
-		 */
-		p->timestamp = (p->timestamp - this_rq->timestamp_last_tick)
-					+ rq->timestamp_last_tick;
-		__activate_task(p, rq);
-		if (TASK_PREEMPTS_CURR(p, rq))
-			resched_task(rq->curr);
-
-		/*
-		 * Parent and child are on different CPUs, now get the
-		 * parent runqueue to update the parent's ->sleep_avg:
-		 */
-		task_rq_unlock(rq, &flags);
-		this_rq = task_rq_lock(current, &flags);
-	}
-	current->sleep_avg = JIFFIES_TO_NS(CURRENT_BONUS(current) *
-		PARENT_PENALTY / 100 * MAX_SLEEP_AVG / MAX_BONUS);
-	task_rq_unlock(this_rq, &flags);
-}
-
-/*
- * Potentially available exiting-child timeslices are
- * retrieved here - this way the parent does not get
- * penalized for creating too many threads.
- *
- * (this cannot be used to 'generate' timeslices
- * artificially, because any timeslice recovered here
- * was given away by the parent in the first place.)
- */
-void fastcall sched_exit(struct task_struct *p)
-{
-	unsigned long flags;
-	struct rq *rq;
-
-	/*
-	 * If the child was a (relative-) CPU hog then decrease
-	 * the sleep_avg of the parent as well.
-	 */
-	rq = task_rq_lock(p->parent, &flags);
-	if (p->first_time_slice && task_cpu(p) == task_cpu(p->parent)) {
-		p->parent->time_slice += p->time_slice;
-		if (unlikely(p->parent->time_slice > task_timeslice(p)))
-			p->parent->time_slice = task_timeslice(p);
-	}
-	if (p->sleep_avg < p->parent->sleep_avg)
-		p->parent->sleep_avg = p->parent->sleep_avg /
-		(EXIT_WEIGHT + 1) * EXIT_WEIGHT + p->sleep_avg /
-		(EXIT_WEIGHT + 1);
-	task_rq_unlock(rq, &flags);
-}
-
-/**
- * prepare_task_switch - prepare to switch tasks
- * @rq: the runqueue preparing to switch
- * @next: the task we are going to switch to.
- *
- * This is called with the rq lock held and interrupts off. It must
- * be paired with a subsequent finish_task_switch after the context
- * switch.
- *
- * prepare_task_switch sets up locking and calls architecture specific
- * hooks.
- */
-static inline void prepare_task_switch(struct rq *rq, struct task_struct *next)
-{
-	prepare_lock_switch(rq, next);
-	prepare_arch_switch(next);
-}
-
-/**
- * finish_task_switch - clean up after a task-switch
- * @rq: runqueue associated with task-switch
- * @prev: the thread we just switched away from.
- *
- * finish_task_switch must be called after the context switch, paired
- * with a prepare_task_switch call before the context switch.
- * finish_task_switch will reconcile locking set up by prepare_task_switch,
- * and do any other architecture-specific cleanup actions.
- *
- * Note that we may have delayed dropping an mm in context_switch(). If
- * so, we finish that here outside of the runqueue lock.  (Doing it
- * with the lock held can cause deadlocks; see schedule() for
- * details.)
- */
-static inline void finish_task_switch(struct rq *rq, struct task_struct *prev)
-	__releases(rq->lock)
-{
-	struct mm_struct *mm = rq->prev_mm;
-	unsigned long prev_task_flags;
-
-	rq->prev_mm = NULL;
-
-	/*
-	 * A task struct has one reference for the use as "current".
-	 * If a task dies, then it sets EXIT_ZOMBIE in tsk->exit_state and
-	 * calls schedule one last time. The schedule call will never return,
-	 * and the scheduled task must drop that reference.
-	 * The test for EXIT_ZOMBIE must occur while the runqueue locks are
-	 * still held, otherwise prev could be scheduled on another cpu, die
-	 * there before we look at prev->state, and then the reference would
-	 * be dropped twice.
-	 *		Manfred Spraul <manfred@colorfullife.com>
-	 */
-	prev_task_flags = prev->flags;
-	finish_arch_switch(prev);
-	finish_lock_switch(rq, prev);
-	if (mm)
-		mmdrop(mm);
-	if (unlikely(prev_task_flags & PF_DEAD)) {
-		/*
-		 * Remove function-return probe instances associated with this
-		 * task and put them back on the free list.
-	 	 */
-		kprobe_flush_task(prev);
-		put_task_struct(prev);
-	}
-}
-
-/**
- * schedule_tail - first thing a freshly forked thread must call.
- * @prev: the thread we just switched away from.
- */
-asmlinkage void schedule_tail(struct task_struct *prev)
-	__releases(rq->lock)
-{
-	struct rq *rq = this_rq();
-
-	finish_task_switch(rq, prev);
-#ifdef __ARCH_WANT_UNLOCKED_CTXSW
-	/* In this case, finish_task_switch does not reenable preemption */
-	preempt_enable();
-#endif
-	if (current->set_child_tid)
-		put_user(current->pid, current->set_child_tid);
-}
-
-/*
- * context_switch - switch to the new MM and the new
- * thread's register state.
- */
-static inline struct task_struct *
-context_switch(struct rq *rq, struct task_struct *prev,
-	       struct task_struct *next)
-{
-	struct mm_struct *mm = next->mm;
-	struct mm_struct *oldmm = prev->active_mm;
-
-	if (unlikely(!mm)) {
-		next->active_mm = oldmm;
-		atomic_inc(&oldmm->mm_count);
-		enter_lazy_tlb(oldmm, next);
-	} else
-		switch_mm(oldmm, mm, next);
-
-	if (unlikely(!prev->mm)) {
-		prev->active_mm = NULL;
-		WARN_ON(rq->prev_mm);
-		rq->prev_mm = oldmm;
-	}
-	spin_release(&rq->lock.dep_map, 1, _THIS_IP_);
-
-	/* Here we just switch the register state and the stack. */
-	switch_to(prev, next, prev);
-
-	return prev;
-}
-
-/*
- * nr_running, nr_uninterruptible and nr_context_switches:
- *
- * externally visible scheduler statistics: current number of runnable
- * threads, current number of uninterruptible-sleeping threads, total
- * number of context switches performed since bootup.
- */
-unsigned long nr_running(void)
-{
-	unsigned long i, sum = 0;
-
-	for_each_online_cpu(i)
-		sum += cpu_rq(i)->nr_running;
-
-	return sum;
-}
-
-unsigned long nr_uninterruptible(void)
-{
-	unsigned long i, sum = 0;
-
-	for_each_possible_cpu(i)
-		sum += cpu_rq(i)->nr_uninterruptible;
-
-	/*
-	 * Since we read the counters lockless, it might be slightly
-	 * inaccurate. Do not allow it to go below zero though:
-	 */
-	if (unlikely((long)sum < 0))
-		sum = 0;
-
-	return sum;
-}
-
-unsigned long long nr_context_switches(void)
-{
-	int i;
-	unsigned long long sum = 0;
-
-	for_each_possible_cpu(i)
-		sum += cpu_rq(i)->nr_switches;
-
-	return sum;
-}
-
-unsigned long nr_iowait(void)
-{
-	unsigned long i, sum = 0;
-
-	for_each_possible_cpu(i)
-		sum += atomic_read(&cpu_rq(i)->nr_iowait);
-
-	return sum;
-}
-
-unsigned long nr_active(void)
-{
-	unsigned long i, running = 0, uninterruptible = 0;
-
-	for_each_online_cpu(i) {
-		running += cpu_rq(i)->nr_running;
-		uninterruptible += cpu_rq(i)->nr_uninterruptible;
-	}
-
-	if (unlikely((long)uninterruptible < 0))
-		uninterruptible = 0;
-
-	return running + uninterruptible;
-}
-
-#ifdef CONFIG_SMP
-
-/*
- * Is this task likely cache-hot:
- */
-static inline int
-task_hot(struct task_struct *p, unsigned long long now, struct sched_domain *sd)
-{
-	return (long long)(now - p->last_ran) < (long long)sd->cache_hot_time;
-}
-
-/*
- * double_rq_lock - safely lock two runqueues
- *
- * Note this does not disable interrupts like task_rq_lock,
- * you need to do so manually before calling.
- */
-static void double_rq_lock(struct rq *rq1, struct rq *rq2)
-	__acquires(rq1->lock)
-	__acquires(rq2->lock)
-{
-	if (rq1 == rq2) {
-		spin_lock(&rq1->lock);
-		__acquire(rq2->lock);	/* Fake it out ;) */
-	} else {
-		if (rq1 < rq2) {
-			spin_lock(&rq1->lock);
-			spin_lock(&rq2->lock);
-		} else {
-			spin_lock(&rq2->lock);
-			spin_lock(&rq1->lock);
-		}
-	}
-}
-
-/*
- * double_rq_unlock - safely unlock two runqueues
- *
- * Note this does not restore interrupts like task_rq_unlock,
- * you need to do so manually after calling.
- */
-static void double_rq_unlock(struct rq *rq1, struct rq *rq2)
-	__releases(rq1->lock)
-	__releases(rq2->lock)
-{
-	spin_unlock(&rq1->lock);
-	if (rq1 != rq2)
-		spin_unlock(&rq2->lock);
-	else
-		__release(rq2->lock);
-}
-
-/*
- * double_lock_balance - lock the busiest runqueue, this_rq is locked already.
- */
-static void double_lock_balance(struct rq *this_rq, struct rq *busiest)
-	__releases(this_rq->lock)
-	__acquires(busiest->lock)
-	__acquires(this_rq->lock)
-{
-	if (unlikely(!spin_trylock(&busiest->lock))) {
-		if (busiest < this_rq) {
-			spin_unlock(&this_rq->lock);
-			spin_lock(&busiest->lock);
-			spin_lock(&this_rq->lock);
-		} else
-			spin_lock(&busiest->lock);
-	}
-}
-
-/*
- * If dest_cpu is allowed for this process, migrate the task to it.
- * This is accomplished by forcing the cpu_allowed mask to only
- * allow dest_cpu, which will force the cpu onto dest_cpu.  Then
- * the cpu_allowed mask is restored.
- */
-static void sched_migrate_task(struct task_struct *p, int dest_cpu)
-{
-	struct migration_req req;
-	unsigned long flags;
-	struct rq *rq;
-
-	rq = task_rq_lock(p, &flags);
-	if (!cpu_isset(dest_cpu, p->cpus_allowed)
-	    || unlikely(cpu_is_offline(dest_cpu)))
-		goto out;
-
-	/* force the process onto the specified CPU */
-	if (migrate_task(p, dest_cpu, &req)) {
-		/* Need to wait for migration thread (might exit: take ref). */
-		struct task_struct *mt = rq->migration_thread;
-
-		get_task_struct(mt);
-		task_rq_unlock(rq, &flags);
-		wake_up_process(mt);
-		put_task_struct(mt);
-		wait_for_completion(&req.done);
-
-		return;
-	}
-out:
-	task_rq_unlock(rq, &flags);
-}
-
-/*
- * sched_exec - execve() is a valuable balancing opportunity, because at
- * this point the task has the smallest effective memory and cache footprint.
- */
-void sched_exec(void)
-{
-	int new_cpu, this_cpu = get_cpu();
-	new_cpu = sched_balance_self(this_cpu, SD_BALANCE_EXEC);
-	put_cpu();
-	if (new_cpu != this_cpu)
-		sched_migrate_task(current, new_cpu);
-}
-
-/*
- * pull_task - move a task from a remote runqueue to the local runqueue.
- * Both runqueues must be locked.
- */
-static void pull_task(struct rq *src_rq, struct prio_array *src_array,
-		      struct task_struct *p, struct rq *this_rq,
-		      struct prio_array *this_array, int this_cpu)
-{
-	dequeue_task(p, src_array);
-	dec_nr_running(p, src_rq);
-	set_task_cpu(p, this_cpu);
-	inc_nr_running(p, this_rq);
-	enqueue_task(p, this_array);
-	p->timestamp = (p->timestamp - src_rq->timestamp_last_tick)
-				+ this_rq->timestamp_last_tick;
-	/*
-	 * Note that idle threads have a prio of MAX_PRIO, for this test
-	 * to be always true for them.
-	 */
-	if (TASK_PREEMPTS_CURR(p, this_rq))
-		resched_task(this_rq->curr);
-}
-
-/*
- * can_migrate_task - may task p from runqueue rq be migrated to this_cpu?
- */
-static
-int can_migrate_task(struct task_struct *p, struct rq *rq, int this_cpu,
-		     struct sched_domain *sd, enum idle_type idle,
-		     int *all_pinned)
-{
-	/*
-	 * We do not migrate tasks that are:
-	 * 1) running (obviously), or
-	 * 2) cannot be migrated to this CPU due to cpus_allowed, or
-	 * 3) are cache-hot on their current CPU.
-	 */
-	if (!cpu_isset(this_cpu, p->cpus_allowed))
-		return 0;
-	*all_pinned = 0;
-
-	if (task_running(rq, p))
-		return 0;
-
-	/*
-	 * Aggressive migration if:
-	 * 1) task is cache cold, or
-	 * 2) too many balance attempts have failed.
-	 */
-
-	if (sd->nr_balance_failed > sd->cache_nice_tries)
-		return 1;
-
-	if (task_hot(p, rq->timestamp_last_tick, sd))
-		return 0;
-	return 1;
-}
-
-#define rq_best_prio(rq) min((rq)->curr->prio, (rq)->best_expired_prio)
-
-/*
- * move_tasks tries to move up to max_nr_move tasks and max_load_move weighted
- * load from busiest to this_rq, as part of a balancing operation within
- * "domain". Returns the number of tasks moved.
- *
- * Called with both runqueues locked.
- */
-static int move_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,
-		      unsigned long max_nr_move, unsigned long max_load_move,
-		      struct sched_domain *sd, enum idle_type idle,
-		      int *all_pinned)
-{
-	int idx, pulled = 0, pinned = 0, this_best_prio, best_prio,
-	    best_prio_seen, skip_for_load;
-	struct prio_array *array, *dst_array;
-	struct list_head *head, *curr;
-	struct task_struct *tmp;
-	long rem_load_move;
-
-	if (max_nr_move == 0 || max_load_move == 0)
-		goto out;
-
-	rem_load_move = max_load_move;
-	pinned = 1;
-	this_best_prio = rq_best_prio(this_rq);
-	best_prio = rq_best_prio(busiest);
-	/*
-	 * Enable handling of the case where there is more than one task
-	 * with the best priority.   If the current running task is one
-	 * of those with prio==best_prio we know it won't be moved
-	 * and therefore it's safe to override the skip (based on load) of
-	 * any task we find with that prio.
-	 */
-	best_prio_seen = best_prio == busiest->curr->prio;
-
-	/*
-	 * We first consider expired tasks. Those will likely not be
-	 * executed in the near future, and they are most likely to
-	 * be cache-cold, thus switching CPUs has the least effect
-	 * on them.
-	 */
-	if (busiest->expired->nr_active) {
-		array = busiest->expired;
-		dst_array = this_rq->expired;
-	} else {
-		array = busiest->active;
-		dst_array = this_rq->active;
-	}
-
-new_array:
-	/* Start searching at priority 0: */
-	idx = 0;
-skip_bitmap:
-	if (!idx)
-		idx = sched_find_first_bit(array->bitmap);
-	else
-		idx = find_next_bit(array->bitmap, MAX_PRIO, idx);
-	if (idx >= MAX_PRIO) {
-		if (array == busiest->expired && busiest->active->nr_active) {
-			array = busiest->active;
-			dst_array = this_rq->active;
-			goto new_array;
-		}
-		goto out;
-	}
-
-	head = array->queue + idx;
-	curr = head->prev;
-skip_queue:
-	tmp = list_entry(curr, struct task_struct, run_list);
-
-	curr = curr->prev;
-
-	/*
-	 * To help distribute high priority tasks accross CPUs we don't
-	 * skip a task if it will be the highest priority task (i.e. smallest
-	 * prio value) on its new queue regardless of its load weight
-	 */
-	skip_for_load = tmp->load_weight > rem_load_move;
-	if (skip_for_load && idx < this_best_prio)
-		skip_for_load = !best_prio_seen && idx == best_prio;
-	if (skip_for_load ||
-	    !can_migrate_task(tmp, busiest, this_cpu, sd, idle, &pinned)) {
-
-		best_prio_seen |= idx == best_prio;
-		if (curr != head)
-			goto skip_queue;
-		idx++;
-		goto skip_bitmap;
-	}
-
-#ifdef CONFIG_SCHEDSTATS
-	if (task_hot(tmp, busiest->timestamp_last_tick, sd))
-		schedstat_inc(sd, lb_hot_gained[idle]);
-#endif
-
-	pull_task(busiest, array, tmp, this_rq, dst_array, this_cpu);
-	pulled++;
-	rem_load_move -= tmp->load_weight;
-
-	/*
-	 * We only want to steal up to the prescribed number of tasks
-	 * and the prescribed amount of weighted load.
-	 */
-	if (pulled < max_nr_move && rem_load_move > 0) {
-		if (idx < this_best_prio)
-			this_best_prio = idx;
-		if (curr != head)
-			goto skip_queue;
-		idx++;
-		goto skip_bitmap;
-	}
-out:
-	/*
-	 * Right now, this is the only place pull_task() is called,
-	 * so we can safely collect pull_task() stats here rather than
-	 * inside pull_task().
-	 */
-	schedstat_add(sd, lb_gained[idle], pulled);
-
-	if (all_pinned)
-		*all_pinned = pinned;
-	return pulled;
-}
-
-/*
- * find_busiest_group finds and returns the busiest CPU group within the
- * domain. It calculates and returns the amount of weighted load which
- * should be moved to restore balance via the imbalance parameter.
- */
-static struct sched_group *
-find_busiest_group(struct sched_domain *sd, int this_cpu,
-		   unsigned long *imbalance, enum idle_type idle, int *sd_idle)
-{
-	struct sched_group *busiest = NULL, *this = NULL, *group = sd->groups;
-	unsigned long max_load, avg_load, total_load, this_load, total_pwr;
-	unsigned long max_pull;
-	unsigned long busiest_load_per_task, busiest_nr_running;
-	unsigned long this_load_per_task, this_nr_running;
-	int load_idx;
-#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)
-	int power_savings_balance = 1;
-	unsigned long leader_nr_running = 0, min_load_per_task = 0;
-	unsigned long min_nr_running = ULONG_MAX;
-	struct sched_group *group_min = NULL, *group_leader = NULL;
-#endif
-
-	max_load = this_load = total_load = total_pwr = 0;
-	busiest_load_per_task = busiest_nr_running = 0;
-	this_load_per_task = this_nr_running = 0;
-	if (idle == NOT_IDLE)
-		load_idx = sd->busy_idx;
-	else if (idle == NEWLY_IDLE)
-		load_idx = sd->newidle_idx;
-	else
-		load_idx = sd->idle_idx;
-
-	do {
-		unsigned long load, group_capacity;
-		int local_group;
-		int i;
-		unsigned long sum_nr_running, sum_weighted_load;
-
-		local_group = cpu_isset(this_cpu, group->cpumask);
-
-		/* Tally up the load of all CPUs in the group */
-		sum_weighted_load = sum_nr_running = avg_load = 0;
-
-		for_each_cpu_mask(i, group->cpumask) {
-			struct rq *rq = cpu_rq(i);
-
-			if (*sd_idle && !idle_cpu(i))
-				*sd_idle = 0;
-
-			/* Bias balancing toward cpus of our domain */
-			if (local_group)
-				load = target_load(i, load_idx);
-			else
-				load = source_load(i, load_idx);
-
-			avg_load += load;
-			sum_nr_running += rq->nr_running;
-			sum_weighted_load += rq->raw_weighted_load;
-		}
-
-		total_load += avg_load;
-		total_pwr += group->cpu_power;
-
-		/* Adjust by relative CPU power of the group */
-		avg_load = (avg_load * SCHED_LOAD_SCALE) / group->cpu_power;
-
-		group_capacity = group->cpu_power / SCHED_LOAD_SCALE;
-
-		if (local_group) {
-			this_load = avg_load;
-			this = group;
-			this_nr_running = sum_nr_running;
-			this_load_per_task = sum_weighted_load;
-		} else if (avg_load > max_load &&
-			   sum_nr_running > group_capacity) {
-			max_load = avg_load;
-			busiest = group;
-			busiest_nr_running = sum_nr_running;
-			busiest_load_per_task = sum_weighted_load;
-		}
-
-#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)
-		/*
-		 * Busy processors will not participate in power savings
-		 * balance.
-		 */
- 		if (idle == NOT_IDLE || !(sd->flags & SD_POWERSAVINGS_BALANCE))
- 			goto group_next;
-
-		/*
-		 * If the local group is idle or completely loaded
-		 * no need to do power savings balance at this domain
-		 */
-		if (local_group && (this_nr_running >= group_capacity ||
-				    !this_nr_running))
-			power_savings_balance = 0;
-
- 		/*
-		 * If a group is already running at full capacity or idle,
-		 * don't include that group in power savings calculations
- 		 */
- 		if (!power_savings_balance || sum_nr_running >= group_capacity
-		    || !sum_nr_running)
- 			goto group_next;
-
- 		/*
-		 * Calculate the group which has the least non-idle load.
- 		 * This is the group from where we need to pick up the load
- 		 * for saving power
- 		 */
- 		if ((sum_nr_running < min_nr_running) ||
- 		    (sum_nr_running == min_nr_running &&
-		     first_cpu(group->cpumask) <
-		     first_cpu(group_min->cpumask))) {
- 			group_min = group;
- 			min_nr_running = sum_nr_running;
-			min_load_per_task = sum_weighted_load /
-						sum_nr_running;
- 		}
-
- 		/*
-		 * Calculate the group which is almost near its
- 		 * capacity but still has some space to pick up some load
- 		 * from other group and save more power
- 		 */
- 		if (sum_nr_running <= group_capacity - 1) {
- 			if (sum_nr_running > leader_nr_running ||
- 			    (sum_nr_running == leader_nr_running &&
- 			     first_cpu(group->cpumask) >
- 			      first_cpu(group_leader->cpumask))) {
- 				group_leader = group;
- 				leader_nr_running = sum_nr_running;
- 			}
-		}
-group_next:
-#endif
-		group = group->next;
-	} while (group != sd->groups);
-
-	if (!busiest || this_load >= max_load || busiest_nr_running == 0)
-		goto out_balanced;
-
-	avg_load = (SCHED_LOAD_SCALE * total_load) / total_pwr;
-
-	if (this_load >= avg_load ||
-			100*max_load <= sd->imbalance_pct*this_load)
-		goto out_balanced;
-
-	busiest_load_per_task /= busiest_nr_running;
-	/*
-	 * We're trying to get all the cpus to the average_load, so we don't
-	 * want to push ourselves above the average load, nor do we wish to
-	 * reduce the max loaded cpu below the average load, as either of these
-	 * actions would just result in more rebalancing later, and ping-pong
-	 * tasks around. Thus we look for the minimum possible imbalance.
-	 * Negative imbalances (*we* are more loaded than anyone else) will
-	 * be counted as no imbalance for these purposes -- we can't fix that
-	 * by pulling tasks to us.  Be careful of negative numbers as they'll
-	 * appear as very large values with unsigned longs.
-	 */
-	if (max_load <= busiest_load_per_task)
-		goto out_balanced;
-
-	/*
-	 * In the presence of smp nice balancing, certain scenarios can have
-	 * max load less than avg load(as we skip the groups at or below
-	 * its cpu_power, while calculating max_load..)
-	 */
-	if (max_load < avg_load) {
-		*imbalance = 0;
-		goto small_imbalance;
-	}
-
-	/* Don't want to pull so many tasks that a group would go idle */
-	max_pull = min(max_load - avg_load, max_load - busiest_load_per_task);
-
-	/* How much load to actually move to equalise the imbalance */
-	*imbalance = min(max_pull * busiest->cpu_power,
-				(avg_load - this_load) * this->cpu_power)
-			/ SCHED_LOAD_SCALE;
-
-	/*
-	 * if *imbalance is less than the average load per runnable task
-	 * there is no gaurantee that any tasks will be moved so we'll have
-	 * a think about bumping its value to force at least one task to be
-	 * moved
-	 */
-	if (*imbalance < busiest_load_per_task) {
-		unsigned long tmp, pwr_now, pwr_move;
-		unsigned int imbn;
-
-small_imbalance:
-		pwr_move = pwr_now = 0;
-		imbn = 2;
-		if (this_nr_running) {
-			this_load_per_task /= this_nr_running;
-			if (busiest_load_per_task > this_load_per_task)
-				imbn = 1;
-		} else
-			this_load_per_task = SCHED_LOAD_SCALE;
-
-		if (max_load - this_load >= busiest_load_per_task * imbn) {
-			*imbalance = busiest_load_per_task;
-			return busiest;
-		}
-
-		/*
-		 * OK, we don't have enough imbalance to justify moving tasks,
-		 * however we may be able to increase total CPU power used by
-		 * moving them.
-		 */
-
-		pwr_now += busiest->cpu_power *
-			min(busiest_load_per_task, max_load);
-		pwr_now += this->cpu_power *
-			min(this_load_per_task, this_load);
-		pwr_now /= SCHED_LOAD_SCALE;
-
-		/* Amount of load we'd subtract */
-		tmp = busiest_load_per_task*SCHED_LOAD_SCALE/busiest->cpu_power;
-		if (max_load > tmp)
-			pwr_move += busiest->cpu_power *
-				min(busiest_load_per_task, max_load - tmp);
-
-		/* Amount of load we'd add */
-		if (max_load*busiest->cpu_power <
-				busiest_load_per_task*SCHED_LOAD_SCALE)
-			tmp = max_load*busiest->cpu_power/this->cpu_power;
-		else
-			tmp = busiest_load_per_task*SCHED_LOAD_SCALE/this->cpu_power;
-		pwr_move += this->cpu_power*min(this_load_per_task, this_load + tmp);
-		pwr_move /= SCHED_LOAD_SCALE;
-
-		/* Move if we gain throughput */
-		if (pwr_move <= pwr_now)
-			goto out_balanced;
-
-		*imbalance = busiest_load_per_task;
-	}
-
-	return busiest;
-
-out_balanced:
-#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)
-	if (idle == NOT_IDLE || !(sd->flags & SD_POWERSAVINGS_BALANCE))
-		goto ret;
-
-	if (this == group_leader && group_leader != group_min) {
-		*imbalance = min_load_per_task;
-		return group_min;
-	}
-ret:
-#endif
-	*imbalance = 0;
-	return NULL;
-}
-
-/*
- * find_busiest_queue - find the busiest runqueue among the cpus in group.
- */
-static struct rq *
-find_busiest_queue(struct sched_group *group, enum idle_type idle,
-		   unsigned long imbalance)
-{
-	struct rq *busiest = NULL, *rq;
-	unsigned long max_load = 0;
-	int i;
-
-	for_each_cpu_mask(i, group->cpumask) {
-		rq = cpu_rq(i);
-
-		if (rq->nr_running == 1 && rq->raw_weighted_load > imbalance)
-			continue;
-
-		if (rq->raw_weighted_load > max_load) {
-			max_load = rq->raw_weighted_load;
-			busiest = rq;
-		}
-	}
-
-	return busiest;
-}
-
-/*
- * Max backoff if we encounter pinned tasks. Pretty arbitrary value, but
- * so long as it is large enough.
- */
-#define MAX_PINNED_INTERVAL	512
-
-static inline unsigned long minus_1_or_zero(unsigned long n)
-{
-	return n > 0 ? n - 1 : 0;
-}
-
-/*
- * Check this_cpu to ensure it is balanced within domain. Attempt to move
- * tasks if there is an imbalance.
- *
- * Called with this_rq unlocked.
- */
-static int load_balance(int this_cpu, struct rq *this_rq,
-			struct sched_domain *sd, enum idle_type idle)
-{
-	int nr_moved, all_pinned = 0, active_balance = 0, sd_idle = 0;
-	struct sched_group *group;
-	unsigned long imbalance;
-	struct rq *busiest;
-
-	if (idle != NOT_IDLE && sd->flags & SD_SHARE_CPUPOWER &&
-	    !sched_smt_power_savings)
-		sd_idle = 1;
-
-	schedstat_inc(sd, lb_cnt[idle]);
-
-	group = find_busiest_group(sd, this_cpu, &imbalance, idle, &sd_idle);
-	if (!group) {
-		schedstat_inc(sd, lb_nobusyg[idle]);
-		goto out_balanced;
-	}
-
-	busiest = find_busiest_queue(group, idle, imbalance);
-	if (!busiest) {
-		schedstat_inc(sd, lb_nobusyq[idle]);
-		goto out_balanced;
-	}
-
-	BUG_ON(busiest == this_rq);
-
-	schedstat_add(sd, lb_imbalance[idle], imbalance);
-
-	nr_moved = 0;
-	if (busiest->nr_running > 1) {
-		/*
-		 * Attempt to move tasks. If find_busiest_group has found
-		 * an imbalance but busiest->nr_running <= 1, the group is
-		 * still unbalanced. nr_moved simply stays zero, so it is
-		 * correctly treated as an imbalance.
-		 */
-		double_rq_lock(this_rq, busiest);
-		nr_moved = move_tasks(this_rq, this_cpu, busiest,
-				      minus_1_or_zero(busiest->nr_running),
-				      imbalance, sd, idle, &all_pinned);
-		double_rq_unlock(this_rq, busiest);
-
-		/* All tasks on this runqueue were pinned by CPU affinity */
-		if (unlikely(all_pinned))
-			goto out_balanced;
-	}
-
-	if (!nr_moved) {
-		schedstat_inc(sd, lb_failed[idle]);
-		sd->nr_balance_failed++;
-
-		if (unlikely(sd->nr_balance_failed > sd->cache_nice_tries+2)) {
-
-			spin_lock(&busiest->lock);
-
-			/* don't kick the migration_thread, if the curr
-			 * task on busiest cpu can't be moved to this_cpu
-			 */
-			if (!cpu_isset(this_cpu, busiest->curr->cpus_allowed)) {
-				spin_unlock(&busiest->lock);
-				all_pinned = 1;
-				goto out_one_pinned;
-			}
-
-			if (!busiest->active_balance) {
-				busiest->active_balance = 1;
-				busiest->push_cpu = this_cpu;
-				active_balance = 1;
-			}
-			spin_unlock(&busiest->lock);
-			if (active_balance)
-				wake_up_process(busiest->migration_thread);
-
-			/*
-			 * We've kicked active balancing, reset the failure
-			 * counter.
-			 */
-			sd->nr_balance_failed = sd->cache_nice_tries+1;
-		}
-	} else
-		sd->nr_balance_failed = 0;
-
-	if (likely(!active_balance)) {
-		/* We were unbalanced, so reset the balancing interval */
-		sd->balance_interval = sd->min_interval;
-	} else {
-		/*
-		 * If we've begun active balancing, start to back off. This
-		 * case may not be covered by the all_pinned logic if there
-		 * is only 1 task on the busy runqueue (because we don't call
-		 * move_tasks).
-		 */
-		if (sd->balance_interval < sd->max_interval)
-			sd->balance_interval *= 2;
-	}
-
-	if (!nr_moved && !sd_idle && sd->flags & SD_SHARE_CPUPOWER &&
-	    !sched_smt_power_savings)
-		return -1;
-	return nr_moved;
-
-out_balanced:
-	schedstat_inc(sd, lb_balanced[idle]);
-
-	sd->nr_balance_failed = 0;
-
-out_one_pinned:
-	/* tune up the balancing interval */
-	if ((all_pinned && sd->balance_interval < MAX_PINNED_INTERVAL) ||
-			(sd->balance_interval < sd->max_interval))
-		sd->balance_interval *= 2;
-
-	if (!sd_idle && sd->flags & SD_SHARE_CPUPOWER &&
-			!sched_smt_power_savings)
-		return -1;
-	return 0;
-}
-
-/*
- * Check this_cpu to ensure it is balanced within domain. Attempt to move
- * tasks if there is an imbalance.
- *
- * Called from schedule when this_rq is about to become idle (NEWLY_IDLE).
- * this_rq is locked.
- */
-static int
-load_balance_newidle(int this_cpu, struct rq *this_rq, struct sched_domain *sd)
-{
-	struct sched_group *group;
-	struct rq *busiest = NULL;
-	unsigned long imbalance;
-	int nr_moved = 0;
-	int sd_idle = 0;
-
-	if (sd->flags & SD_SHARE_CPUPOWER && !sched_smt_power_savings)
-		sd_idle = 1;
-
-	schedstat_inc(sd, lb_cnt[NEWLY_IDLE]);
-	group = find_busiest_group(sd, this_cpu, &imbalance, NEWLY_IDLE, &sd_idle);
-	if (!group) {
-		schedstat_inc(sd, lb_nobusyg[NEWLY_IDLE]);
-		goto out_balanced;
-	}
-
-	busiest = find_busiest_queue(group, NEWLY_IDLE, imbalance);
-	if (!busiest) {
-		schedstat_inc(sd, lb_nobusyq[NEWLY_IDLE]);
-		goto out_balanced;
-	}
-
-	BUG_ON(busiest == this_rq);
-
-	schedstat_add(sd, lb_imbalance[NEWLY_IDLE], imbalance);
-
-	nr_moved = 0;
-	if (busiest->nr_running > 1) {
-		/* Attempt to move tasks */
-		double_lock_balance(this_rq, busiest);
-		nr_moved = move_tasks(this_rq, this_cpu, busiest,
-					minus_1_or_zero(busiest->nr_running),
-					imbalance, sd, NEWLY_IDLE, NULL);
-		spin_unlock(&busiest->lock);
-	}
-
-	if (!nr_moved) {
-		schedstat_inc(sd, lb_failed[NEWLY_IDLE]);
-		if (!sd_idle && sd->flags & SD_SHARE_CPUPOWER)
-			return -1;
-	} else
-		sd->nr_balance_failed = 0;
-
-	return nr_moved;
-
-out_balanced:
-	schedstat_inc(sd, lb_balanced[NEWLY_IDLE]);
-	if (!sd_idle && sd->flags & SD_SHARE_CPUPOWER &&
-					!sched_smt_power_savings)
-		return -1;
-	sd->nr_balance_failed = 0;
-
-	return 0;
-}
-
-/*
- * idle_balance is called by schedule() if this_cpu is about to become
- * idle. Attempts to pull tasks from other CPUs.
- */
-static void idle_balance(int this_cpu, struct rq *this_rq)
-{
-	struct sched_domain *sd;
-
-	for_each_domain(this_cpu, sd) {
-		if (sd->flags & SD_BALANCE_NEWIDLE) {
-			/* If we've pulled tasks over stop searching: */
-			if (load_balance_newidle(this_cpu, this_rq, sd))
-				break;
-		}
-	}
-}
-
-/*
- * active_load_balance is run by migration threads. It pushes running tasks
- * off the busiest CPU onto idle CPUs. It requires at least 1 task to be
- * running on each physical CPU where possible, and avoids physical /
- * logical imbalances.
- *
- * Called with busiest_rq locked.
- */
-static void active_load_balance(struct rq *busiest_rq, int busiest_cpu)
-{
-	int target_cpu = busiest_rq->push_cpu;
-	struct sched_domain *sd;
-	struct rq *target_rq;
-
-	/* Is there any task to move? */
-	if (busiest_rq->nr_running <= 1)
-		return;
-
-	target_rq = cpu_rq(target_cpu);
-
-	/*
-	 * This condition is "impossible", if it occurs
-	 * we need to fix it.  Originally reported by
-	 * Bjorn Helgaas on a 128-cpu setup.
-	 */
-	BUG_ON(busiest_rq == target_rq);
-
-	/* move a task from busiest_rq to target_rq */
-	double_lock_balance(busiest_rq, target_rq);
-
-	/* Search for an sd spanning us and the target CPU. */
-	for_each_domain(target_cpu, sd) {
-		if ((sd->flags & SD_LOAD_BALANCE) &&
-		    cpu_isset(busiest_cpu, sd->span))
-				break;
-	}
-
-	if (likely(sd)) {
-		schedstat_inc(sd, alb_cnt);
-
-		if (move_tasks(target_rq, target_cpu, busiest_rq, 1,
-			       RTPRIO_TO_LOAD_WEIGHT(100), sd, SCHED_IDLE,
-			       NULL))
-			schedstat_inc(sd, alb_pushed);
-		else
-			schedstat_inc(sd, alb_failed);
-	}
-	spin_unlock(&target_rq->lock);
-}
-
-/*
- * rebalance_tick will get called every timer tick, on every CPU.
- *
- * It checks each scheduling domain to see if it is due to be balanced,
- * and initiates a balancing operation if so.
- *
- * Balancing parameters are set up in arch_init_sched_domains.
- */
-
-/* Don't have all balancing operations going off at once: */
-static inline unsigned long cpu_offset(int cpu)
-{
-	return jiffies + cpu * HZ / NR_CPUS;
-}
-
-static void
-rebalance_tick(int this_cpu, struct rq *this_rq, enum idle_type idle)
-{
-	unsigned long this_load, interval, j = cpu_offset(this_cpu);
-	struct sched_domain *sd;
-	int i, scale;
-
-	this_load = this_rq->raw_weighted_load;
-
-	/* Update our load: */
-	for (i = 0, scale = 1; i < 3; i++, scale <<= 1) {
-		unsigned long old_load, new_load;
-
-		old_load = this_rq->cpu_load[i];
-		new_load = this_load;
-		/*
-		 * Round up the averaging division if load is increasing. This
-		 * prevents us from getting stuck on 9 if the load is 10, for
-		 * example.
-		 */
-		if (new_load > old_load)
-			new_load += scale-1;
-		this_rq->cpu_load[i] = (old_load*(scale-1) + new_load) / scale;
-	}
-
-	for_each_domain(this_cpu, sd) {
-		if (!(sd->flags & SD_LOAD_BALANCE))
-			continue;
-
-		interval = sd->balance_interval;
-		if (idle != SCHED_IDLE)
-			interval *= sd->busy_factor;
-
-		/* scale ms to jiffies */
-		interval = msecs_to_jiffies(interval);
-		if (unlikely(!interval))
-			interval = 1;
-
-		if (j - sd->last_balance >= interval) {
-			if (load_balance(this_cpu, this_rq, sd, idle)) {
-				/*
-				 * We've pulled tasks over so either we're no
-				 * longer idle, or one of our SMT siblings is
-				 * not idle.
-				 */
-				idle = NOT_IDLE;
-			}
-			sd->last_balance += interval;
-		}
-	}
-}
-#else
-/*
- * on UP we do not need to balance between CPUs:
- */
-static inline void rebalance_tick(int cpu, struct rq *rq, enum idle_type idle)
-{
-}
-static inline void idle_balance(int cpu, struct rq *rq)
-{
-}
-#endif
-
-static inline int wake_priority_sleeper(struct rq *rq)
-{
-	int ret = 0;
-
-#ifdef CONFIG_SCHED_SMT
-	spin_lock(&rq->lock);
-	/*
-	 * If an SMT sibling task has been put to sleep for priority
-	 * reasons reschedule the idle task to see if it can now run.
-	 */
-	if (rq->nr_running) {
-		resched_task(rq->idle);
-		ret = 1;
-	}
-	spin_unlock(&rq->lock);
-#endif
-	return ret;
-}
-
-DEFINE_PER_CPU(struct kernel_stat, kstat);
-
-EXPORT_PER_CPU_SYMBOL(kstat);
-
-/*
- * This is called on clock ticks and on context switches.
- * Bank in p->sched_time the ns elapsed since the last tick or switch.
- */
-static inline void
-update_cpu_clock(struct task_struct *p, struct rq *rq, unsigned long long now)
-{
-	p->sched_time += now - max(p->timestamp, rq->timestamp_last_tick);
-}
-
-/*
- * Return current->sched_time plus any more ns on the sched_clock
- * that have not yet been banked.
- */
-unsigned long long current_sched_time(const struct task_struct *p)
-{
-	unsigned long long ns;
-	unsigned long flags;
-
-	local_irq_save(flags);
-	ns = max(p->timestamp, task_rq(p)->timestamp_last_tick);
-	ns = p->sched_time + sched_clock() - ns;
-	local_irq_restore(flags);
-
-	return ns;
-}
-
-/*
- * We place interactive tasks back into the active array, if possible.
- *
- * To guarantee that this does not starve expired tasks we ignore the
- * interactivity of a task if the first expired task had to wait more
- * than a 'reasonable' amount of time. This deadline timeout is
- * load-dependent, as the frequency of array switched decreases with
- * increasing number of running tasks. We also ignore the interactivity
- * if a better static_prio task has expired:
- */
-static inline int expired_starving(struct rq *rq)
-{
-	if (rq->curr->static_prio > rq->best_expired_prio)
-		return 1;
-	if (!STARVATION_LIMIT || !rq->expired_timestamp)
-		return 0;
-	if (jiffies - rq->expired_timestamp > STARVATION_LIMIT * rq->nr_running)
-		return 1;
-	return 0;
-}
-
-/*
- * Account user cpu time to a process.
- * @p: the process that the cpu time gets accounted to
- * @hardirq_offset: the offset to subtract from hardirq_count()
- * @cputime: the cpu time spent in user space since the last update
- */
-void account_user_time(struct task_struct *p, cputime_t cputime)
-{
-	struct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;
-	cputime64_t tmp;
-
-	p->utime = cputime_add(p->utime, cputime);
-
-	/* Add user time to cpustat. */
-	tmp = cputime_to_cputime64(cputime);
-	if (TASK_NICE(p) > 0)
-		cpustat->nice = cputime64_add(cpustat->nice, tmp);
-	else
-		cpustat->user = cputime64_add(cpustat->user, tmp);
-}
-
-/*
- * Account system cpu time to a process.
- * @p: the process that the cpu time gets accounted to
- * @hardirq_offset: the offset to subtract from hardirq_count()
- * @cputime: the cpu time spent in kernel space since the last update
- */
-void account_system_time(struct task_struct *p, int hardirq_offset,
-			 cputime_t cputime)
-{
-	struct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;
-	struct rq *rq = this_rq();
-	cputime64_t tmp;
-
-	p->stime = cputime_add(p->stime, cputime);
-
-	/* Add system time to cpustat. */
-	tmp = cputime_to_cputime64(cputime);
-	if (hardirq_count() - hardirq_offset)
-		cpustat->irq = cputime64_add(cpustat->irq, tmp);
-	else if (softirq_count())
-		cpustat->softirq = cputime64_add(cpustat->softirq, tmp);
-	else if (p != rq->idle)
-		cpustat->system = cputime64_add(cpustat->system, tmp);
-	else if (atomic_read(&rq->nr_iowait) > 0)
-		cpustat->iowait = cputime64_add(cpustat->iowait, tmp);
-	else
-		cpustat->idle = cputime64_add(cpustat->idle, tmp);
-	/* Account for system time used */
-	acct_update_integrals(p);
-}
-
-/*
- * Account for involuntary wait time.
- * @p: the process from which the cpu time has been stolen
- * @steal: the cpu time spent in involuntary wait
- */
-void account_steal_time(struct task_struct *p, cputime_t steal)
-{
-	struct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;
-	cputime64_t tmp = cputime_to_cputime64(steal);
-	struct rq *rq = this_rq();
-
-	if (p == rq->idle) {
-		p->stime = cputime_add(p->stime, steal);
-		if (atomic_read(&rq->nr_iowait) > 0)
-			cpustat->iowait = cputime64_add(cpustat->iowait, tmp);
-		else
-			cpustat->idle = cputime64_add(cpustat->idle, tmp);
-	} else
-		cpustat->steal = cputime64_add(cpustat->steal, tmp);
-}
-
-/*
- * This function gets called by the timer code, with HZ frequency.
- * We call it with interrupts disabled.
- *
- * It also gets called by the fork code, when changing the parent's
- * timeslices.
- */
-void scheduler_tick(void)
-{
-	unsigned long long now = sched_clock();
-	struct task_struct *p = current;
-	int cpu = smp_processor_id();
-	struct rq *rq = cpu_rq(cpu);
-
-	update_cpu_clock(p, rq, now);
-
-	rq->timestamp_last_tick = now;
-
-	if (p == rq->idle) {
-		if (wake_priority_sleeper(rq))
-			goto out;
-		rebalance_tick(cpu, rq, SCHED_IDLE);
-		return;
-	}
-
-	/* Task might have expired already, but not scheduled off yet */
-	if (p->array != rq->active) {
-		set_tsk_need_resched(p);
-		goto out;
-	}
-	spin_lock(&rq->lock);
-	/*
-	 * The task was running during this tick - update the
-	 * time slice counter. Note: we do not update a thread's
-	 * priority until it either goes to sleep or uses up its
-	 * timeslice. This makes it possible for interactive tasks
-	 * to use up their timeslices at their highest priority levels.
-	 */
-	if (rt_task(p)) {
-		/*
-		 * RR tasks need a special form of timeslice management.
-		 * FIFO tasks have no timeslices.
-		 */
-		if ((p->policy == SCHED_RR) && !--p->time_slice) {
-			p->time_slice = task_timeslice(p);
-			p->first_time_slice = 0;
-			set_tsk_need_resched(p);
-
-			/* put it at the end of the queue: */
-			requeue_task(p, rq->active);
-		}
-		goto out_unlock;
-	}
-	if (!--p->time_slice) {
-		dequeue_task(p, rq->active);
-		set_tsk_need_resched(p);
-		p->prio = effective_prio(p);
-		p->time_slice = task_timeslice(p);
-		p->first_time_slice = 0;
-
-		if (!rq->expired_timestamp)
-			rq->expired_timestamp = jiffies;
-		if (!TASK_INTERACTIVE(p) || expired_starving(rq)) {
-			enqueue_task(p, rq->expired);
-			if (p->static_prio < rq->best_expired_prio)
-				rq->best_expired_prio = p->static_prio;
-		} else
-			enqueue_task(p, rq->active);
-	} else {
-		/*
-		 * Prevent a too long timeslice allowing a task to monopolize
-		 * the CPU. We do this by splitting up the timeslice into
-		 * smaller pieces.
-		 *
-		 * Note: this does not mean the task's timeslices expire or
-		 * get lost in any way, they just might be preempted by
-		 * another task of equal priority. (one with higher
-		 * priority would have preempted this task already.) We
-		 * requeue this task to the end of the list on this priority
-		 * level, which is in essence a round-robin of tasks with
-		 * equal priority.
-		 *
-		 * This only applies to tasks in the interactive
-		 * delta range with at least TIMESLICE_GRANULARITY to requeue.
-		 */
-		if (TASK_INTERACTIVE(p) && !((task_timeslice(p) -
-			p->time_slice) % TIMESLICE_GRANULARITY(p)) &&
-			(p->time_slice >= TIMESLICE_GRANULARITY(p)) &&
-			(p->array == rq->active)) {
-
-			requeue_task(p, rq->active);
-			set_tsk_need_resched(p);
-		}
-	}
-out_unlock:
-	spin_unlock(&rq->lock);
-out:
-	rebalance_tick(cpu, rq, NOT_IDLE);
-}
-
-#ifdef CONFIG_SCHED_SMT
-static inline void wakeup_busy_runqueue(struct rq *rq)
-{
-	/* If an SMT runqueue is sleeping due to priority reasons wake it up */
-	if (rq->curr == rq->idle && rq->nr_running)
-		resched_task(rq->idle);
-}
-
-/*
- * Called with interrupt disabled and this_rq's runqueue locked.
- */
-static void wake_sleeping_dependent(int this_cpu)
-{
-	struct sched_domain *tmp, *sd = NULL;
-	int i;
-
-	for_each_domain(this_cpu, tmp) {
-		if (tmp->flags & SD_SHARE_CPUPOWER) {
-			sd = tmp;
-			break;
-		}
-	}
-
-	if (!sd)
-		return;
-
-	for_each_cpu_mask(i, sd->span) {
-		struct rq *smt_rq = cpu_rq(i);
-
-		if (i == this_cpu)
-			continue;
-		if (unlikely(!spin_trylock(&smt_rq->lock)))
-			continue;
-
-		wakeup_busy_runqueue(smt_rq);
-		spin_unlock(&smt_rq->lock);
-	}
-}
-
-/*
- * number of 'lost' timeslices this task wont be able to fully
- * utilize, if another task runs on a sibling. This models the
- * slowdown effect of other tasks running on siblings:
- */
-static inline unsigned long
-smt_slice(struct task_struct *p, struct sched_domain *sd)
-{
-	return p->time_slice * (100 - sd->per_cpu_gain) / 100;
-}
-
-/*
- * To minimise lock contention and not have to drop this_rq's runlock we only
- * trylock the sibling runqueues and bypass those runqueues if we fail to
- * acquire their lock. As we only trylock the normal locking order does not
- * need to be obeyed.
- */
-static int
-dependent_sleeper(int this_cpu, struct rq *this_rq, struct task_struct *p)
-{
-	struct sched_domain *tmp, *sd = NULL;
-	int ret = 0, i;
-
-	/* kernel/rt threads do not participate in dependent sleeping */
-	if (!p->mm || rt_task(p))
-		return 0;
-
-	for_each_domain(this_cpu, tmp) {
-		if (tmp->flags & SD_SHARE_CPUPOWER) {
-			sd = tmp;
-			break;
-		}
-	}
-
-	if (!sd)
-		return 0;
-
-	for_each_cpu_mask(i, sd->span) {
-		struct task_struct *smt_curr;
-		struct rq *smt_rq;
-
-		if (i == this_cpu)
-			continue;
-
-		smt_rq = cpu_rq(i);
-		if (unlikely(!spin_trylock(&smt_rq->lock)))
-			continue;
-
-		smt_curr = smt_rq->curr;
-
-		if (!smt_curr->mm)
-			goto unlock;
-
-		/*
-		 * If a user task with lower static priority than the
-		 * running task on the SMT sibling is trying to schedule,
-		 * delay it till there is proportionately less timeslice
-		 * left of the sibling task to prevent a lower priority
-		 * task from using an unfair proportion of the
-		 * physical cpu's resources. -ck
-		 */
-		if (rt_task(smt_curr)) {
-			/*
-			 * With real time tasks we run non-rt tasks only
-			 * per_cpu_gain% of the time.
-			 */
-			if ((jiffies % DEF_TIMESLICE) >
-				(sd->per_cpu_gain * DEF_TIMESLICE / 100))
-					ret = 1;
-		} else {
-			if (smt_curr->static_prio < p->static_prio &&
-				!TASK_PREEMPTS_CURR(p, smt_rq) &&
-				smt_slice(smt_curr, sd) > task_timeslice(p))
-					ret = 1;
-		}
-unlock:
-		spin_unlock(&smt_rq->lock);
-	}
-	return ret;
-}
-#else
-static inline void wake_sleeping_dependent(int this_cpu)
-{
-}
-static inline int
-dependent_sleeper(int this_cpu, struct rq *this_rq, struct task_struct *p)
-{
-	return 0;
-}
-#endif
-
-#if defined(CONFIG_PREEMPT) && defined(CONFIG_DEBUG_PREEMPT)
-
-void fastcall add_preempt_count(int val)
-{
-	/*
-	 * Underflow?
-	 */
-	if (DEBUG_LOCKS_WARN_ON((preempt_count() < 0)))
-		return;
-	preempt_count() += val;
-	/*
-	 * Spinlock count overflowing soon?
-	 */
-	DEBUG_LOCKS_WARN_ON((preempt_count() & PREEMPT_MASK) >= PREEMPT_MASK-10);
-}
-EXPORT_SYMBOL(add_preempt_count);
-
-void fastcall sub_preempt_count(int val)
-{
-	/*
-	 * Underflow?
-	 */
-	if (DEBUG_LOCKS_WARN_ON(val > preempt_count()))
-		return;
-	/*
-	 * Is the spinlock portion underflowing?
-	 */
-	if (DEBUG_LOCKS_WARN_ON((val < PREEMPT_MASK) &&
-			!(preempt_count() & PREEMPT_MASK)))
-		return;
-
-	preempt_count() -= val;
-}
-EXPORT_SYMBOL(sub_preempt_count);
-
-#endif
-
-static inline int interactive_sleep(enum sleep_type sleep_type)
-{
-	return (sleep_type == SLEEP_INTERACTIVE ||
-		sleep_type == SLEEP_INTERRUPTED);
-}
-
-/*
- * schedule() is the main scheduler function.
- */
-asmlinkage void __sched schedule(void)
-{
-	struct task_struct *prev, *next;
-	struct prio_array *array;
-	struct list_head *queue;
-	unsigned long long now;
-	unsigned long run_time;
-	int cpu, idx, new_prio;
-	long *switch_count;
-	struct rq *rq;
-
-	/*
-	 * Test if we are atomic.  Since do_exit() needs to call into
-	 * schedule() atomically, we ignore that path for now.
-	 * Otherwise, whine if we are scheduling when we should not be.
-	 */
-	if (unlikely(in_atomic() && !current->exit_state)) {
-		printk(KERN_ERR "BUG: scheduling while atomic: "
-			"%s/0x%08x/%d\n",
-			current->comm, preempt_count(), current->pid);
-		dump_stack();
-	}
-	profile_hit(SCHED_PROFILING, __builtin_return_address(0));
-
-need_resched:
-	preempt_disable();
-	prev = current;
-	release_kernel_lock(prev);
-need_resched_nonpreemptible:
-	rq = this_rq();
-
-	/*
-	 * The idle thread is not allowed to schedule!
-	 * Remove this check after it has been exercised a bit.
-	 */
-	if (unlikely(prev == rq->idle) && prev->state != TASK_RUNNING) {
-		printk(KERN_ERR "bad: scheduling from the idle thread!\n");
-		dump_stack();
-	}
-
-	schedstat_inc(rq, sched_cnt);
-	now = sched_clock();
-	if (likely((long long)(now - prev->timestamp) < NS_MAX_SLEEP_AVG)) {
-		run_time = now - prev->timestamp;
-		if (unlikely((long long)(now - prev->timestamp) < 0))
-			run_time = 0;
-	} else
-		run_time = NS_MAX_SLEEP_AVG;
-
-	/*
-	 * Tasks charged proportionately less run_time at high sleep_avg to
-	 * delay them losing their interactive status
-	 */
-	run_time /= (CURRENT_BONUS(prev) ? : 1);
-
-	spin_lock_irq(&rq->lock);
-
-	if (unlikely(prev->flags & PF_DEAD))
-		prev->state = EXIT_DEAD;
-
-	switch_count = &prev->nivcsw;
-	if (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {
-		switch_count = &prev->nvcsw;
-		if (unlikely((prev->state & TASK_INTERRUPTIBLE) &&
-				unlikely(signal_pending(prev))))
-			prev->state = TASK_RUNNING;
-		else {
-			if (prev->state == TASK_UNINTERRUPTIBLE)
-				rq->nr_uninterruptible++;
-			deactivate_task(prev, rq);
-		}
-	}
-
-	cpu = smp_processor_id();
-	if (unlikely(!rq->nr_running)) {
-		idle_balance(cpu, rq);
-		if (!rq->nr_running) {
-			next = rq->idle;
-			rq->expired_timestamp = 0;
-			wake_sleeping_dependent(cpu);
-			goto switch_tasks;
-		}
-	}
-
-	array = rq->active;
-	if (unlikely(!array->nr_active)) {
-		/*
-		 * Switch the active and expired arrays.
-		 */
-		schedstat_inc(rq, sched_switch);
-		rq->active = rq->expired;
-		rq->expired = array;
-		array = rq->active;
-		rq->expired_timestamp = 0;
-		rq->best_expired_prio = MAX_PRIO;
-	}
-
-	idx = sched_find_first_bit(array->bitmap);
-	queue = array->queue + idx;
-	next = list_entry(queue->next, struct task_struct, run_list);
-
-	if (!rt_task(next) && interactive_sleep(next->sleep_type)) {
-		unsigned long long delta = now - next->timestamp;
-		if (unlikely((long long)(now - next->timestamp) < 0))
-			delta = 0;
-
-		if (next->sleep_type == SLEEP_INTERACTIVE)
-			delta = delta * (ON_RUNQUEUE_WEIGHT * 128 / 100) / 128;
-
-		array = next->array;
-		new_prio = recalc_task_prio(next, next->timestamp + delta);
-
-		if (unlikely(next->prio != new_prio)) {
-			dequeue_task(next, array);
-			next->prio = new_prio;
-			enqueue_task(next, array);
-		}
-	}
-	next->sleep_type = SLEEP_NORMAL;
-	if (dependent_sleeper(cpu, rq, next))
-		next = rq->idle;
-switch_tasks:
-	if (next == rq->idle)
-		schedstat_inc(rq, sched_goidle);
-	prefetch(next);
-	prefetch_stack(next);
-	clear_tsk_need_resched(prev);
-	rcu_qsctr_inc(task_cpu(prev));
-
-	update_cpu_clock(prev, rq, now);
-
-	prev->sleep_avg -= run_time;
-	if ((long)prev->sleep_avg <= 0)
-		prev->sleep_avg = 0;
-	prev->timestamp = prev->last_ran = now;
-
-	sched_info_switch(prev, next);
-	if (likely(prev != next)) {
-		next->timestamp = now;
-		rq->nr_switches++;
-		rq->curr = next;
-		++*switch_count;
-
-		prepare_task_switch(rq, next);
-		prev = context_switch(rq, prev, next);
-		barrier();
-		/*
-		 * this_rq must be evaluated again because prev may have moved
-		 * CPUs since it called schedule(), thus the 'rq' on its stack
-		 * frame will be invalid.
-		 */
-		finish_task_switch(this_rq(), prev);
-	} else
-		spin_unlock_irq(&rq->lock);
-
-	prev = current;
-	if (unlikely(reacquire_kernel_lock(prev) < 0))
-		goto need_resched_nonpreemptible;
-	preempt_enable_no_resched();
-	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
-		goto need_resched;
-}
-EXPORT_SYMBOL(schedule);
-
-#ifdef CONFIG_PREEMPT
-/*
- * this is the entry point to schedule() from in-kernel preemption
- * off of preempt_enable.  Kernel preemptions off return from interrupt
- * occur there and call schedule directly.
- */
-asmlinkage void __sched preempt_schedule(void)
-{
-	struct thread_info *ti = current_thread_info();
-#ifdef CONFIG_PREEMPT_BKL
-	struct task_struct *task = current;
-	int saved_lock_depth;
-#endif
-	/*
-	 * If there is a non-zero preempt_count or interrupts are disabled,
-	 * we do not want to preempt the current task.  Just return..
-	 */
-	if (unlikely(ti->preempt_count || irqs_disabled()))
-		return;
-
-need_resched:
-	add_preempt_count(PREEMPT_ACTIVE);
-	/*
-	 * We keep the big kernel semaphore locked, but we
-	 * clear ->lock_depth so that schedule() doesnt
-	 * auto-release the semaphore:
-	 */
-#ifdef CONFIG_PREEMPT_BKL
-	saved_lock_depth = task->lock_depth;
-	task->lock_depth = -1;
-#endif
-	schedule();
-#ifdef CONFIG_PREEMPT_BKL
-	task->lock_depth = saved_lock_depth;
-#endif
-	sub_preempt_count(PREEMPT_ACTIVE);
-
-	/* we could miss a preemption opportunity between schedule and now */
-	barrier();
-	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
-		goto need_resched;
-}
-EXPORT_SYMBOL(preempt_schedule);
-
-/*
- * this is the entry point to schedule() from kernel preemption
- * off of irq context.
- * Note, that this is called and return with irqs disabled. This will
- * protect us against recursive calling from irq.
- */
-asmlinkage void __sched preempt_schedule_irq(void)
-{
-	struct thread_info *ti = current_thread_info();
-#ifdef CONFIG_PREEMPT_BKL
-	struct task_struct *task = current;
-	int saved_lock_depth;
-#endif
-	/* Catch callers which need to be fixed */
-	BUG_ON(ti->preempt_count || !irqs_disabled());
-
-need_resched:
-	add_preempt_count(PREEMPT_ACTIVE);
-	/*
-	 * We keep the big kernel semaphore locked, but we
-	 * clear ->lock_depth so that schedule() doesnt
-	 * auto-release the semaphore:
-	 */
-#ifdef CONFIG_PREEMPT_BKL
-	saved_lock_depth = task->lock_depth;
-	task->lock_depth = -1;
-#endif
-	local_irq_enable();
-	schedule();
-	local_irq_disable();
-#ifdef CONFIG_PREEMPT_BKL
-	task->lock_depth = saved_lock_depth;
-#endif
-	sub_preempt_count(PREEMPT_ACTIVE);
-
-	/* we could miss a preemption opportunity between schedule and now */
-	barrier();
-	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
-		goto need_resched;
-}
-
-#endif /* CONFIG_PREEMPT */
-
-int default_wake_function(wait_queue_t *curr, unsigned mode, int sync,
-			  void *key)
-{
-	return try_to_wake_up(curr->private, mode, sync);
-}
-EXPORT_SYMBOL(default_wake_function);
-
-/*
- * The core wakeup function.  Non-exclusive wakeups (nr_exclusive == 0) just
- * wake everything up.  If it's an exclusive wakeup (nr_exclusive == small +ve
- * number) then we wake all the non-exclusive tasks and one exclusive task.
- *
- * There are circumstances in which we can try to wake a task which has already
- * started to run but is not in state TASK_RUNNING.  try_to_wake_up() returns
- * zero in this (rare) case, and we handle it by continuing to scan the queue.
- */
-static void __wake_up_common(wait_queue_head_t *q, unsigned int mode,
-			     int nr_exclusive, int sync, void *key)
-{
-	struct list_head *tmp, *next;
-
-	list_for_each_safe(tmp, next, &q->task_list) {
-		wait_queue_t *curr = list_entry(tmp, wait_queue_t, task_list);
-		unsigned flags = curr->flags;
-
-		if (curr->func(curr, mode, sync, key) &&
-				(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
-			break;
-	}
-}
-
-/**
- * __wake_up - wake up threads blocked on a waitqueue.
- * @q: the waitqueue
- * @mode: which threads
- * @nr_exclusive: how many wake-one or wake-many threads to wake up
- * @key: is directly passed to the wakeup function
- */
-void fastcall __wake_up(wait_queue_head_t *q, unsigned int mode,
-			int nr_exclusive, void *key)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&q->lock, flags);
-	__wake_up_common(q, mode, nr_exclusive, 0, key);
-	spin_unlock_irqrestore(&q->lock, flags);
-}
-EXPORT_SYMBOL(__wake_up);
-
-/*
- * Same as __wake_up but called with the spinlock in wait_queue_head_t held.
- */
-void fastcall __wake_up_locked(wait_queue_head_t *q, unsigned int mode)
-{
-	__wake_up_common(q, mode, 1, 0, NULL);
-}
-
-/**
- * __wake_up_sync - wake up threads blocked on a waitqueue.
- * @q: the waitqueue
- * @mode: which threads
- * @nr_exclusive: how many wake-one or wake-many threads to wake up
- *
- * The sync wakeup differs that the waker knows that it will schedule
- * away soon, so while the target thread will be woken up, it will not
- * be migrated to another CPU - ie. the two threads are 'synchronized'
- * with each other. This can prevent needless bouncing between CPUs.
- *
- * On UP it can prevent extra preemption.
- */
-void fastcall
-__wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
-{
-	unsigned long flags;
-	int sync = 1;
-
-	if (unlikely(!q))
-		return;
-
-	if (unlikely(!nr_exclusive))
-		sync = 0;
-
-	spin_lock_irqsave(&q->lock, flags);
-	__wake_up_common(q, mode, nr_exclusive, sync, NULL);
-	spin_unlock_irqrestore(&q->lock, flags);
-}
-EXPORT_SYMBOL_GPL(__wake_up_sync);	/* For internal use only */
-
-void fastcall complete(struct completion *x)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&x->wait.lock, flags);
-	x->done++;
-	__wake_up_common(&x->wait, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE,
-			 1, 0, NULL);
-	spin_unlock_irqrestore(&x->wait.lock, flags);
-}
-EXPORT_SYMBOL(complete);
-
-void fastcall complete_all(struct completion *x)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&x->wait.lock, flags);
-	x->done += UINT_MAX/2;
-	__wake_up_common(&x->wait, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE,
-			 0, 0, NULL);
-	spin_unlock_irqrestore(&x->wait.lock, flags);
-}
-EXPORT_SYMBOL(complete_all);
-
-void fastcall __sched wait_for_completion(struct completion *x)
-{
-	might_sleep();
-
-	spin_lock_irq(&x->wait.lock);
-	if (!x->done) {
-		DECLARE_WAITQUEUE(wait, current);
-
-		wait.flags |= WQ_FLAG_EXCLUSIVE;
-		__add_wait_queue_tail(&x->wait, &wait);
-		do {
-			__set_current_state(TASK_UNINTERRUPTIBLE);
-			spin_unlock_irq(&x->wait.lock);
-			schedule();
-			spin_lock_irq(&x->wait.lock);
-		} while (!x->done);
-		__remove_wait_queue(&x->wait, &wait);
-	}
-	x->done--;
-	spin_unlock_irq(&x->wait.lock);
-}
-EXPORT_SYMBOL(wait_for_completion);
-
-unsigned long fastcall __sched
-wait_for_completion_timeout(struct completion *x, unsigned long timeout)
-{
-	might_sleep();
-
-	spin_lock_irq(&x->wait.lock);
-	if (!x->done) {
-		DECLARE_WAITQUEUE(wait, current);
-
-		wait.flags |= WQ_FLAG_EXCLUSIVE;
-		__add_wait_queue_tail(&x->wait, &wait);
-		do {
-			__set_current_state(TASK_UNINTERRUPTIBLE);
-			spin_unlock_irq(&x->wait.lock);
-			timeout = schedule_timeout(timeout);
-			spin_lock_irq(&x->wait.lock);
-			if (!timeout) {
-				__remove_wait_queue(&x->wait, &wait);
-				goto out;
-			}
-		} while (!x->done);
-		__remove_wait_queue(&x->wait, &wait);
-	}
-	x->done--;
-out:
-	spin_unlock_irq(&x->wait.lock);
-	return timeout;
-}
-EXPORT_SYMBOL(wait_for_completion_timeout);
-
-int fastcall __sched wait_for_completion_interruptible(struct completion *x)
-{
-	int ret = 0;
-
-	might_sleep();
-
-	spin_lock_irq(&x->wait.lock);
-	if (!x->done) {
-		DECLARE_WAITQUEUE(wait, current);
-
-		wait.flags |= WQ_FLAG_EXCLUSIVE;
-		__add_wait_queue_tail(&x->wait, &wait);
-		do {
-			if (signal_pending(current)) {
-				ret = -ERESTARTSYS;
-				__remove_wait_queue(&x->wait, &wait);
-				goto out;
-			}
-			__set_current_state(TASK_INTERRUPTIBLE);
-			spin_unlock_irq(&x->wait.lock);
-			schedule();
-			spin_lock_irq(&x->wait.lock);
-		} while (!x->done);
-		__remove_wait_queue(&x->wait, &wait);
-	}
-	x->done--;
-out:
-	spin_unlock_irq(&x->wait.lock);
-
-	return ret;
-}
-EXPORT_SYMBOL(wait_for_completion_interruptible);
-
-unsigned long fastcall __sched
-wait_for_completion_interruptible_timeout(struct completion *x,
-					  unsigned long timeout)
-{
-	might_sleep();
-
-	spin_lock_irq(&x->wait.lock);
-	if (!x->done) {
-		DECLARE_WAITQUEUE(wait, current);
-
-		wait.flags |= WQ_FLAG_EXCLUSIVE;
-		__add_wait_queue_tail(&x->wait, &wait);
-		do {
-			if (signal_pending(current)) {
-				timeout = -ERESTARTSYS;
-				__remove_wait_queue(&x->wait, &wait);
-				goto out;
-			}
-			__set_current_state(TASK_INTERRUPTIBLE);
-			spin_unlock_irq(&x->wait.lock);
-			timeout = schedule_timeout(timeout);
-			spin_lock_irq(&x->wait.lock);
-			if (!timeout) {
-				__remove_wait_queue(&x->wait, &wait);
-				goto out;
-			}
-		} while (!x->done);
-		__remove_wait_queue(&x->wait, &wait);
-	}
-	x->done--;
-out:
-	spin_unlock_irq(&x->wait.lock);
-	return timeout;
-}
-EXPORT_SYMBOL(wait_for_completion_interruptible_timeout);
-
-
-#define	SLEEP_ON_VAR					\
-	unsigned long flags;				\
-	wait_queue_t wait;				\
-	init_waitqueue_entry(&wait, current);
-
-#define SLEEP_ON_HEAD					\
-	spin_lock_irqsave(&q->lock,flags);		\
-	__add_wait_queue(q, &wait);			\
-	spin_unlock(&q->lock);
-
-#define	SLEEP_ON_TAIL					\
-	spin_lock_irq(&q->lock);			\
-	__remove_wait_queue(q, &wait);			\
-	spin_unlock_irqrestore(&q->lock, flags);
-
-void fastcall __sched interruptible_sleep_on(wait_queue_head_t *q)
-{
-	SLEEP_ON_VAR
-
-	current->state = TASK_INTERRUPTIBLE;
-
-	SLEEP_ON_HEAD
-	schedule();
-	SLEEP_ON_TAIL
-}
-EXPORT_SYMBOL(interruptible_sleep_on);
-
-long fastcall __sched
-interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
-{
-	SLEEP_ON_VAR
-
-	current->state = TASK_INTERRUPTIBLE;
-
-	SLEEP_ON_HEAD
-	timeout = schedule_timeout(timeout);
-	SLEEP_ON_TAIL
-
-	return timeout;
-}
-EXPORT_SYMBOL(interruptible_sleep_on_timeout);
-
-void fastcall __sched sleep_on(wait_queue_head_t *q)
-{
-	SLEEP_ON_VAR
-
-	current->state = TASK_UNINTERRUPTIBLE;
-
-	SLEEP_ON_HEAD
-	schedule();
-	SLEEP_ON_TAIL
-}
-EXPORT_SYMBOL(sleep_on);
-
-long fastcall __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)
-{
-	SLEEP_ON_VAR
-
-	current->state = TASK_UNINTERRUPTIBLE;
-
-	SLEEP_ON_HEAD
-	timeout = schedule_timeout(timeout);
-	SLEEP_ON_TAIL
-
-	return timeout;
-}
-
-EXPORT_SYMBOL(sleep_on_timeout);
-
-#ifdef CONFIG_RT_MUTEXES
-
-/*
- * rt_mutex_setprio - set the current priority of a task
- * @p: task
- * @prio: prio value (kernel-internal form)
- *
- * This function changes the 'effective' priority of a task. It does
- * not touch ->normal_prio like __setscheduler().
- *
- * Used by the rt_mutex code to implement priority inheritance logic.
- */
-void rt_mutex_setprio(struct task_struct *p, int prio)
-{
-	struct prio_array *array;
-	unsigned long flags;
-	struct rq *rq;
-	int oldprio;
-
-	BUG_ON(prio < 0 || prio > MAX_PRIO);
-
-	rq = task_rq_lock(p, &flags);
-
-	oldprio = p->prio;
-	array = p->array;
-	if (array)
-		dequeue_task(p, array);
-	p->prio = prio;
-
-	if (array) {
-		/*
-		 * If changing to an RT priority then queue it
-		 * in the active array!
-		 */
-		if (rt_task(p))
-			array = rq->active;
-		enqueue_task(p, array);
-		/*
-		 * Reschedule if we are currently running on this runqueue and
-		 * our priority decreased, or if we are not currently running on
-		 * this runqueue and our priority is higher than the current's
-		 */
-		if (task_running(rq, p)) {
-			if (p->prio > oldprio)
-				resched_task(rq->curr);
-		} else if (TASK_PREEMPTS_CURR(p, rq))
-			resched_task(rq->curr);
-	}
-	task_rq_unlock(rq, &flags);
-}
-
-#endif
-
-void set_user_nice(struct task_struct *p, long nice)
-{
-	struct prio_array *array;
-	int old_prio, delta;
-	unsigned long flags;
-	struct rq *rq;
-
-	if (TASK_NICE(p) == nice || nice < -20 || nice > 19)
-		return;
-	/*
-	 * We have to be careful, if called from sys_setpriority(),
-	 * the task might be in the middle of scheduling on another CPU.
-	 */
-	rq = task_rq_lock(p, &flags);
-	/*
-	 * The RT priorities are set via sched_setscheduler(), but we still
-	 * allow the 'normal' nice value to be set - but as expected
-	 * it wont have any effect on scheduling until the task is
-	 * not SCHED_NORMAL/SCHED_BATCH:
-	 */
-	if (has_rt_policy(p)) {
-		p->static_prio = NICE_TO_PRIO(nice);
-		goto out_unlock;
-	}
-	array = p->array;
-	if (array) {
-		dequeue_task(p, array);
-		dec_raw_weighted_load(rq, p);
-	}
-
-	p->static_prio = NICE_TO_PRIO(nice);
-	set_load_weight(p);
-	old_prio = p->prio;
-	p->prio = effective_prio(p);
-	delta = p->prio - old_prio;
-
-	if (array) {
-		enqueue_task(p, array);
-		inc_raw_weighted_load(rq, p);
-		/*
-		 * If the task increased its priority or is running and
-		 * lowered its priority, then reschedule its CPU:
-		 */
-		if (delta < 0 || (delta > 0 && task_running(rq, p)))
-			resched_task(rq->curr);
-	}
-out_unlock:
-	task_rq_unlock(rq, &flags);
-}
-EXPORT_SYMBOL(set_user_nice);
-
-/*
- * can_nice - check if a task can reduce its nice value
- * @p: task
- * @nice: nice value
- */
-int can_nice(const struct task_struct *p, const int nice)
-{
-	/* convert nice value [19,-20] to rlimit style value [1,40] */
-	int nice_rlim = 20 - nice;
-
-	return (nice_rlim <= p->signal->rlim[RLIMIT_NICE].rlim_cur ||
-		capable(CAP_SYS_NICE));
-}
-
-#ifdef __ARCH_WANT_SYS_NICE
-
-/*
- * sys_nice - change the priority of the current process.
- * @increment: priority increment
- *
- * sys_setpriority is a more generic, but much slower function that
- * does similar things.
- */
-asmlinkage long sys_nice(int increment)
-{
-	long nice, retval;
-
-	/*
-	 * Setpriority might change our priority at the same moment.
-	 * We don't have to worry. Conceptually one call occurs first
-	 * and we have a single winner.
-	 */
-	if (increment < -40)
-		increment = -40;
-	if (increment > 40)
-		increment = 40;
-
-	nice = PRIO_TO_NICE(current->static_prio) + increment;
-	if (nice < -20)
-		nice = -20;
-	if (nice > 19)
-		nice = 19;
-
-	if (increment < 0 && !can_nice(current, nice))
-		return -EPERM;
-
-	retval = security_task_setnice(current, nice);
-	if (retval)
-		return retval;
-
-	set_user_nice(current, nice);
-	return 0;
-}
-
-#endif
-
-/**
- * task_prio - return the priority value of a given task.
- * @p: the task in question.
- *
- * This is the priority value as seen by users in /proc.
- * RT tasks are offset by -200. Normal tasks are centered
- * around 0, value goes from -16 to +15.
- */
-int task_prio(const struct task_struct *p)
-{
-	return p->prio - MAX_RT_PRIO;
-}
-
-/**
- * task_nice - return the nice value of a given task.
- * @p: the task in question.
- */
-int task_nice(const struct task_struct *p)
-{
-	return TASK_NICE(p);
-}
-EXPORT_SYMBOL_GPL(task_nice);
-
-/**
- * idle_cpu - is a given cpu idle currently?
- * @cpu: the processor in question.
- */
-int idle_cpu(int cpu)
-{
-	return cpu_curr(cpu) == cpu_rq(cpu)->idle;
-}
-
-/**
- * idle_task - return the idle task for a given cpu.
- * @cpu: the processor in question.
- */
-struct task_struct *idle_task(int cpu)
-{
-	return cpu_rq(cpu)->idle;
-}
-
-/**
- * find_process_by_pid - find a process with a matching PID value.
- * @pid: the pid in question.
- */
-static inline struct task_struct *find_process_by_pid(pid_t pid)
-{
-	return pid ? find_task_by_pid(pid) : current;
-}
-
-/* Actually do priority change: must hold rq lock. */
-static void __setscheduler(struct task_struct *p, int policy, int prio)
-{
-	BUG_ON(p->array);
-
-	p->policy = policy;
-	p->rt_priority = prio;
-	p->normal_prio = normal_prio(p);
-	/* we are holding p->pi_lock already */
-	p->prio = rt_mutex_getprio(p);
-	/*
-	 * SCHED_BATCH tasks are treated as perpetual CPU hogs:
-	 */
-	if (policy == SCHED_BATCH)
-		p->sleep_avg = 0;
-	set_load_weight(p);
-}
-
-/**
- * sched_setscheduler - change the scheduling policy and/or RT priority of
- * a thread.
- * @p: the task in question.
- * @policy: new policy.
- * @param: structure containing the new RT priority.
- */
-int sched_setscheduler(struct task_struct *p, int policy,
-		       struct sched_param *param)
-{
-	int retval, oldprio, oldpolicy = -1;
-	struct prio_array *array;
-	unsigned long flags;
-	struct rq *rq;
-
-	/* may grab non-irq protected spin_locks */
-	BUG_ON(in_interrupt());
-recheck:
-	/* double check policy once rq lock held */
-	if (policy < 0)
-		policy = oldpolicy = p->policy;
-	else if (policy != SCHED_FIFO && policy != SCHED_RR &&
-			policy != SCHED_NORMAL && policy != SCHED_BATCH)
-		return -EINVAL;
-	/*
-	 * Valid priorities for SCHED_FIFO and SCHED_RR are
-	 * 1..MAX_USER_RT_PRIO-1, valid priority for SCHED_NORMAL and
-	 * SCHED_BATCH is 0.
-	 */
-	if (param->sched_priority < 0 ||
-	    (p->mm && param->sched_priority > MAX_USER_RT_PRIO-1) ||
-	    (!p->mm && param->sched_priority > MAX_RT_PRIO-1))
-		return -EINVAL;
-	if ((policy == SCHED_NORMAL || policy == SCHED_BATCH)
-					!= (param->sched_priority == 0))
-		return -EINVAL;
-
-	/*
-	 * Allow unprivileged RT tasks to decrease priority:
-	 */
-	if (!capable(CAP_SYS_NICE)) {
-		/*
-		 * can't change policy, except between SCHED_NORMAL
-		 * and SCHED_BATCH:
-		 */
-		if (((policy != SCHED_NORMAL && p->policy != SCHED_BATCH) &&
-			(policy != SCHED_BATCH && p->policy != SCHED_NORMAL)) &&
-				!p->signal->rlim[RLIMIT_RTPRIO].rlim_cur)
-			return -EPERM;
-		/* can't increase priority */
-		if ((policy != SCHED_NORMAL && policy != SCHED_BATCH) &&
-		    param->sched_priority > p->rt_priority &&
-		    param->sched_priority >
-				p->signal->rlim[RLIMIT_RTPRIO].rlim_cur)
-			return -EPERM;
-		/* can't change other user's priorities */
-		if ((current->euid != p->euid) &&
-		    (current->euid != p->uid))
-			return -EPERM;
-	}
-
-	retval = security_task_setscheduler(p, policy, param);
-	if (retval)
-		return retval;
-	/*
-	 * make sure no PI-waiters arrive (or leave) while we are
-	 * changing the priority of the task:
-	 */
-	spin_lock_irqsave(&p->pi_lock, flags);
-	/*
-	 * To be able to change p->policy safely, the apropriate
-	 * runqueue lock must be held.
-	 */
-	rq = __task_rq_lock(p);
-	/* recheck policy now with rq lock held */
-	if (unlikely(oldpolicy != -1 && oldpolicy != p->policy)) {
-		policy = oldpolicy = -1;
-		__task_rq_unlock(rq);
-		spin_unlock_irqrestore(&p->pi_lock, flags);
-		goto recheck;
-	}
-	array = p->array;
-	if (array)
-		deactivate_task(p, rq);
-	oldprio = p->prio;
-	__setscheduler(p, policy, param->sched_priority);
-	if (array) {
-		__activate_task(p, rq);
-		/*
-		 * Reschedule if we are currently running on this runqueue and
-		 * our priority decreased, or if we are not currently running on
-		 * this runqueue and our priority is higher than the current's
-		 */
-		if (task_running(rq, p)) {
-			if (p->prio > oldprio)
-				resched_task(rq->curr);
-		} else if (TASK_PREEMPTS_CURR(p, rq))
-			resched_task(rq->curr);
-	}
-	__task_rq_unlock(rq);
-	spin_unlock_irqrestore(&p->pi_lock, flags);
-
-	rt_mutex_adjust_pi(p);
-
-	return 0;
-}
-EXPORT_SYMBOL_GPL(sched_setscheduler);
-
-static int
-do_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)
-{
-	struct sched_param lparam;
-	struct task_struct *p;
-	int retval;
-
-	if (!param || pid < 0)
-		return -EINVAL;
-	if (copy_from_user(&lparam, param, sizeof(struct sched_param)))
-		return -EFAULT;
-	read_lock_irq(&tasklist_lock);
-	p = find_process_by_pid(pid);
-	if (!p) {
-		read_unlock_irq(&tasklist_lock);
-		return -ESRCH;
-	}
-	get_task_struct(p);
-	read_unlock_irq(&tasklist_lock);
-	retval = sched_setscheduler(p, policy, &lparam);
-	put_task_struct(p);
-
-	return retval;
-}
-
-/**
- * sys_sched_setscheduler - set/change the scheduler policy and RT priority
- * @pid: the pid in question.
- * @policy: new policy.
- * @param: structure containing the new RT priority.
- */
-asmlinkage long sys_sched_setscheduler(pid_t pid, int policy,
-				       struct sched_param __user *param)
-{
-	/* negative values for policy are not valid */
-	if (policy < 0)
-		return -EINVAL;
-
-	return do_sched_setscheduler(pid, policy, param);
-}
-
-/**
- * sys_sched_setparam - set/change the RT priority of a thread
- * @pid: the pid in question.
- * @param: structure containing the new RT priority.
- */
-asmlinkage long sys_sched_setparam(pid_t pid, struct sched_param __user *param)
-{
-	return do_sched_setscheduler(pid, -1, param);
-}
-
-/**
- * sys_sched_getscheduler - get the policy (scheduling class) of a thread
- * @pid: the pid in question.
- */
-asmlinkage long sys_sched_getscheduler(pid_t pid)
-{
-	struct task_struct *p;
-	int retval = -EINVAL;
-
-	if (pid < 0)
-		goto out_nounlock;
-
-	retval = -ESRCH;
-	read_lock(&tasklist_lock);
-	p = find_process_by_pid(pid);
-	if (p) {
-		retval = security_task_getscheduler(p);
-		if (!retval)
-			retval = p->policy;
-	}
-	read_unlock(&tasklist_lock);
-
-out_nounlock:
-	return retval;
-}
-
-/**
- * sys_sched_getscheduler - get the RT priority of a thread
- * @pid: the pid in question.
- * @param: structure containing the RT priority.
- */
-asmlinkage long sys_sched_getparam(pid_t pid, struct sched_param __user *param)
-{
-	struct sched_param lp;
-	struct task_struct *p;
-	int retval = -EINVAL;
-
-	if (!param || pid < 0)
-		goto out_nounlock;
-
-	read_lock(&tasklist_lock);
-	p = find_process_by_pid(pid);
-	retval = -ESRCH;
-	if (!p)
-		goto out_unlock;
-
-	retval = security_task_getscheduler(p);
-	if (retval)
-		goto out_unlock;
-
-	lp.sched_priority = p->rt_priority;
-	read_unlock(&tasklist_lock);
-
-	/*
-	 * This one might sleep, we cannot do it with a spinlock held ...
-	 */
-	retval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;
-
-out_nounlock:
-	return retval;
-
-out_unlock:
-	read_unlock(&tasklist_lock);
-	return retval;
-}
-
-long sched_setaffinity(pid_t pid, cpumask_t new_mask)
-{
-	cpumask_t cpus_allowed;
-	struct task_struct *p;
-	int retval;
-
-	lock_cpu_hotplug();
-	read_lock(&tasklist_lock);
-
-	p = find_process_by_pid(pid);
-	if (!p) {
-		read_unlock(&tasklist_lock);
-		unlock_cpu_hotplug();
-		return -ESRCH;
-	}
-
-	/*
-	 * It is not safe to call set_cpus_allowed with the
-	 * tasklist_lock held.  We will bump the task_struct's
-	 * usage count and then drop tasklist_lock.
-	 */
-	get_task_struct(p);
-	read_unlock(&tasklist_lock);
-
-	retval = -EPERM;
-	if ((current->euid != p->euid) && (current->euid != p->uid) &&
-			!capable(CAP_SYS_NICE))
-		goto out_unlock;
-
-	retval = security_task_setscheduler(p, 0, NULL);
-	if (retval)
-		goto out_unlock;
-
-	cpus_allowed = cpuset_cpus_allowed(p);
-	cpus_and(new_mask, new_mask, cpus_allowed);
-	retval = set_cpus_allowed(p, new_mask);
-
-out_unlock:
-	put_task_struct(p);
-	unlock_cpu_hotplug();
-	return retval;
-}
-
-static int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,
-			     cpumask_t *new_mask)
-{
-	if (len < sizeof(cpumask_t)) {
-		memset(new_mask, 0, sizeof(cpumask_t));
-	} else if (len > sizeof(cpumask_t)) {
-		len = sizeof(cpumask_t);
-	}
-	return copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;
-}
-
-/**
- * sys_sched_setaffinity - set the cpu affinity of a process
- * @pid: pid of the process
- * @len: length in bytes of the bitmask pointed to by user_mask_ptr
- * @user_mask_ptr: user-space pointer to the new cpu mask
- */
-asmlinkage long sys_sched_setaffinity(pid_t pid, unsigned int len,
-				      unsigned long __user *user_mask_ptr)
-{
-	cpumask_t new_mask;
-	int retval;
-
-	retval = get_user_cpu_mask(user_mask_ptr, len, &new_mask);
-	if (retval)
-		return retval;
-
-	return sched_setaffinity(pid, new_mask);
-}
-
-/*
- * Represents all cpu's present in the system
- * In systems capable of hotplug, this map could dynamically grow
- * as new cpu's are detected in the system via any platform specific
- * method, such as ACPI for e.g.
- */
-
-cpumask_t cpu_present_map __read_mostly;
-EXPORT_SYMBOL(cpu_present_map);
-
-#ifndef CONFIG_SMP
-cpumask_t cpu_online_map __read_mostly = CPU_MASK_ALL;
-cpumask_t cpu_possible_map __read_mostly = CPU_MASK_ALL;
-#endif
-
-long sched_getaffinity(pid_t pid, cpumask_t *mask)
-{
-	struct task_struct *p;
-	int retval;
-
-	lock_cpu_hotplug();
-	read_lock(&tasklist_lock);
-
-	retval = -ESRCH;
-	p = find_process_by_pid(pid);
-	if (!p)
-		goto out_unlock;
-
-	retval = security_task_getscheduler(p);
-	if (retval)
-		goto out_unlock;
-
-	cpus_and(*mask, p->cpus_allowed, cpu_online_map);
-
-out_unlock:
-	read_unlock(&tasklist_lock);
-	unlock_cpu_hotplug();
-	if (retval)
-		return retval;
-
-	return 0;
-}
-
-/**
- * sys_sched_getaffinity - get the cpu affinity of a process
- * @pid: pid of the process
- * @len: length in bytes of the bitmask pointed to by user_mask_ptr
- * @user_mask_ptr: user-space pointer to hold the current cpu mask
- */
-asmlinkage long sys_sched_getaffinity(pid_t pid, unsigned int len,
-				      unsigned long __user *user_mask_ptr)
-{
-	int ret;
-	cpumask_t mask;
-
-	if (len < sizeof(cpumask_t))
-		return -EINVAL;
-
-	ret = sched_getaffinity(pid, &mask);
-	if (ret < 0)
-		return ret;
-
-	if (copy_to_user(user_mask_ptr, &mask, sizeof(cpumask_t)))
-		return -EFAULT;
-
-	return sizeof(cpumask_t);
-}
-
-/**
- * sys_sched_yield - yield the current processor to other threads.
- *
- * this function yields the current CPU by moving the calling thread
- * to the expired array. If there are no other threads running on this
- * CPU then this function will return.
- */
-asmlinkage long sys_sched_yield(void)
-{
-	struct rq *rq = this_rq_lock();
-	struct prio_array *array = current->array, *target = rq->expired;
-
-	schedstat_inc(rq, yld_cnt);
-	/*
-	 * We implement yielding by moving the task into the expired
-	 * queue.
-	 *
-	 * (special rule: RT tasks will just roundrobin in the active
-	 *  array.)
-	 */
-	if (rt_task(current))
-		target = rq->active;
-
-	if (array->nr_active == 1) {
-		schedstat_inc(rq, yld_act_empty);
-		if (!rq->expired->nr_active)
-			schedstat_inc(rq, yld_both_empty);
-	} else if (!rq->expired->nr_active)
-		schedstat_inc(rq, yld_exp_empty);
-
-	if (array != target) {
-		dequeue_task(current, array);
-		enqueue_task(current, target);
-	} else
-		/*
-		 * requeue_task is cheaper so perform that if possible.
-		 */
-		requeue_task(current, array);
-
-	/*
-	 * Since we are going to call schedule() anyway, there's
-	 * no need to preempt or enable interrupts:
-	 */
-	__release(rq->lock);
-	spin_release(&rq->lock.dep_map, 1, _THIS_IP_);
-	_raw_spin_unlock(&rq->lock);
-	preempt_enable_no_resched();
-
-	schedule();
-
-	return 0;
-}
-
-static inline int __resched_legal(void)
-{
-	if (unlikely(preempt_count()))
-		return 0;
-	if (unlikely(system_state != SYSTEM_RUNNING))
-		return 0;
-	return 1;
-}
-
-static void __cond_resched(void)
-{
-#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP
-	__might_sleep(__FILE__, __LINE__);
-#endif
-	/*
-	 * The BKS might be reacquired before we have dropped
-	 * PREEMPT_ACTIVE, which could trigger a second
-	 * cond_resched() call.
-	 */
-	do {
-		add_preempt_count(PREEMPT_ACTIVE);
-		schedule();
-		sub_preempt_count(PREEMPT_ACTIVE);
-	} while (need_resched());
-}
-
-int __sched cond_resched(void)
-{
-	if (need_resched() && __resched_legal()) {
-		__cond_resched();
-		return 1;
-	}
-	return 0;
-}
-EXPORT_SYMBOL(cond_resched);
-
-/*
- * cond_resched_lock() - if a reschedule is pending, drop the given lock,
- * call schedule, and on return reacquire the lock.
- *
- * This works OK both with and without CONFIG_PREEMPT.  We do strange low-level
- * operations here to prevent schedule() from being called twice (once via
- * spin_unlock(), once by hand).
- */
-int cond_resched_lock(spinlock_t *lock)
-{
-	int ret = 0;
-
-	if (need_lockbreak(lock)) {
-		spin_unlock(lock);
-		cpu_relax();
-		ret = 1;
-		spin_lock(lock);
-	}
-	if (need_resched() && __resched_legal()) {
-		spin_release(&lock->dep_map, 1, _THIS_IP_);
-		_raw_spin_unlock(lock);
-		preempt_enable_no_resched();
-		__cond_resched();
-		ret = 1;
-		spin_lock(lock);
-	}
-	return ret;
-}
-EXPORT_SYMBOL(cond_resched_lock);
-
-int __sched cond_resched_softirq(void)
-{
-	BUG_ON(!in_softirq());
-
-	if (need_resched() && __resched_legal()) {
-		raw_local_irq_disable();
-		_local_bh_enable();
-		raw_local_irq_enable();
-		__cond_resched();
-		local_bh_disable();
-		return 1;
-	}
-	return 0;
-}
-EXPORT_SYMBOL(cond_resched_softirq);
-
-/**
- * yield - yield the current processor to other threads.
- *
- * this is a shortcut for kernel-space yielding - it marks the
- * thread runnable and calls sys_sched_yield().
- */
-void __sched yield(void)
-{
-	set_current_state(TASK_RUNNING);
-	sys_sched_yield();
-}
-EXPORT_SYMBOL(yield);
-
-/*
- * This task is about to go to sleep on IO.  Increment rq->nr_iowait so
- * that process accounting knows that this is a task in IO wait state.
- *
- * But don't do that if it is a deliberate, throttling IO wait (this task
- * has set its backing_dev_info: the queue against which it should throttle)
- */
-void __sched io_schedule(void)
-{
-	struct rq *rq = &__raw_get_cpu_var(runqueues);
-
-	delayacct_blkio_start();
-	atomic_inc(&rq->nr_iowait);
-	schedule();
-	atomic_dec(&rq->nr_iowait);
-	delayacct_blkio_end();
-}
-EXPORT_SYMBOL(io_schedule);
-
-long __sched io_schedule_timeout(long timeout)
-{
-	struct rq *rq = &__raw_get_cpu_var(runqueues);
-	long ret;
-
-	delayacct_blkio_start();
-	atomic_inc(&rq->nr_iowait);
-	ret = schedule_timeout(timeout);
-	atomic_dec(&rq->nr_iowait);
-	delayacct_blkio_end();
-	return ret;
-}
-
-/**
- * sys_sched_get_priority_max - return maximum RT priority.
- * @policy: scheduling class.
- *
- * this syscall returns the maximum rt_priority that can be used
- * by a given scheduling class.
- */
-asmlinkage long sys_sched_get_priority_max(int policy)
-{
-	int ret = -EINVAL;
-
-	switch (policy) {
-	case SCHED_FIFO:
-	case SCHED_RR:
-		ret = MAX_USER_RT_PRIO-1;
-		break;
-	case SCHED_NORMAL:
-	case SCHED_BATCH:
-		ret = 0;
-		break;
-	}
-	return ret;
-}
-
-/**
- * sys_sched_get_priority_min - return minimum RT priority.
- * @policy: scheduling class.
- *
- * this syscall returns the minimum rt_priority that can be used
- * by a given scheduling class.
- */
-asmlinkage long sys_sched_get_priority_min(int policy)
-{
-	int ret = -EINVAL;
-
-	switch (policy) {
-	case SCHED_FIFO:
-	case SCHED_RR:
-		ret = 1;
-		break;
-	case SCHED_NORMAL:
-	case SCHED_BATCH:
-		ret = 0;
-	}
-	return ret;
-}
-
-/**
- * sys_sched_rr_get_interval - return the default timeslice of a process.
- * @pid: pid of the process.
- * @interval: userspace pointer to the timeslice value.
- *
- * this syscall writes the default timeslice value of a given process
- * into the user-space timespec buffer. A value of '0' means infinity.
- */
-asmlinkage
-long sys_sched_rr_get_interval(pid_t pid, struct timespec __user *interval)
-{
-	struct task_struct *p;
-	int retval = -EINVAL;
-	struct timespec t;
-
-	if (pid < 0)
-		goto out_nounlock;
-
-	retval = -ESRCH;
-	read_lock(&tasklist_lock);
-	p = find_process_by_pid(pid);
-	if (!p)
-		goto out_unlock;
-
-	retval = security_task_getscheduler(p);
-	if (retval)
-		goto out_unlock;
-
-	jiffies_to_timespec(p->policy == SCHED_FIFO ?
-				0 : task_timeslice(p), &t);
-	read_unlock(&tasklist_lock);
-	retval = copy_to_user(interval, &t, sizeof(t)) ? -EFAULT : 0;
-out_nounlock:
-	return retval;
-out_unlock:
-	read_unlock(&tasklist_lock);
-	return retval;
-}
-
-static inline struct task_struct *eldest_child(struct task_struct *p)
-{
-	if (list_empty(&p->children))
-		return NULL;
-	return list_entry(p->children.next,struct task_struct,sibling);
-}
-
-static inline struct task_struct *older_sibling(struct task_struct *p)
-{
-	if (p->sibling.prev==&p->parent->children)
-		return NULL;
-	return list_entry(p->sibling.prev,struct task_struct,sibling);
-}
-
-static inline struct task_struct *younger_sibling(struct task_struct *p)
-{
-	if (p->sibling.next==&p->parent->children)
-		return NULL;
-	return list_entry(p->sibling.next,struct task_struct,sibling);
-}
-
-static const char stat_nam[] = "RSDTtZX";
-
-static void show_task(struct task_struct *p)
-{
-	struct task_struct *relative;
-	unsigned long free = 0;
-	unsigned state;
-
-	state = p->state ? __ffs(p->state) + 1 : 0;
-	printk("%-13.13s %c", p->comm,
-		state < sizeof(stat_nam) - 1 ? stat_nam[state] : '?');
-#if (BITS_PER_LONG == 32)
-	if (state == TASK_RUNNING)
-		printk(" running ");
-	else
-		printk(" %08lX ", thread_saved_pc(p));
-#else
-	if (state == TASK_RUNNING)
-		printk("  running task   ");
-	else
-		printk(" %016lx ", thread_saved_pc(p));
-#endif
-#ifdef CONFIG_DEBUG_STACK_USAGE
-	{
-		unsigned long *n = end_of_stack(p);
-		while (!*n)
-			n++;
-		free = (unsigned long)n - (unsigned long)end_of_stack(p);
-	}
-#endif
-	printk("%5lu %5d %6d ", free, p->pid, p->parent->pid);
-	if ((relative = eldest_child(p)))
-		printk("%5d ", relative->pid);
-	else
-		printk("      ");
-	if ((relative = younger_sibling(p)))
-		printk("%7d", relative->pid);
-	else
-		printk("       ");
-	if ((relative = older_sibling(p)))
-		printk(" %5d", relative->pid);
-	else
-		printk("      ");
-	if (!p->mm)
-		printk(" (L-TLB)\n");
-	else
-		printk(" (NOTLB)\n");
-
-	if (state != TASK_RUNNING)
-		show_stack(p, NULL);
-}
-
-void show_state(void)
-{
-	struct task_struct *g, *p;
-
-#if (BITS_PER_LONG == 32)
-	printk("\n"
-	       "                                               sibling\n");
-	printk("  task             PC      pid father child younger older\n");
-#else
-	printk("\n"
-	       "                                                       sibling\n");
-	printk("  task                 PC          pid father child younger older\n");
-#endif
-	read_lock(&tasklist_lock);
-	do_each_thread(g, p) {
-		/*
-		 * reset the NMI-timeout, listing all files on a slow
-		 * console might take alot of time:
-		 */
-		touch_nmi_watchdog();
-		show_task(p);
-	} while_each_thread(g, p);
-
-	read_unlock(&tasklist_lock);
-	debug_show_all_locks();
-}
-
-/**
- * init_idle - set up an idle thread for a given CPU
- * @idle: task in question
- * @cpu: cpu the idle task belongs to
- *
- * NOTE: this function does not set the idle thread's NEED_RESCHED
- * flag, to make booting more robust.
- */
-void __devinit init_idle(struct task_struct *idle, int cpu)
-{
-	struct rq *rq = cpu_rq(cpu);
-	unsigned long flags;
-
-	idle->timestamp = sched_clock();
-	idle->sleep_avg = 0;
-	idle->array = NULL;
-	idle->prio = idle->normal_prio = MAX_PRIO;
-	idle->state = TASK_RUNNING;
-	idle->cpus_allowed = cpumask_of_cpu(cpu);
-	set_task_cpu(idle, cpu);
-
-	spin_lock_irqsave(&rq->lock, flags);
-	rq->curr = rq->idle = idle;
-#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)
-	idle->oncpu = 1;
-#endif
-	spin_unlock_irqrestore(&rq->lock, flags);
-
-	/* Set the preempt count _outside_ the spinlocks! */
-#if defined(CONFIG_PREEMPT) && !defined(CONFIG_PREEMPT_BKL)
-	task_thread_info(idle)->preempt_count = (idle->lock_depth >= 0);
-#else
-	task_thread_info(idle)->preempt_count = 0;
-#endif
-}
-
-/*
- * In a system that switches off the HZ timer nohz_cpu_mask
- * indicates which cpus entered this state. This is used
- * in the rcu update to wait only for active cpus. For system
- * which do not switch off the HZ timer nohz_cpu_mask should
- * always be CPU_MASK_NONE.
- */
-cpumask_t nohz_cpu_mask = CPU_MASK_NONE;
-
-#ifdef CONFIG_SMP
-/*
- * This is how migration works:
- *
- * 1) we queue a struct migration_req structure in the source CPU's
- *    runqueue and wake up that CPU's migration thread.
- * 2) we down() the locked semaphore => thread blocks.
- * 3) migration thread wakes up (implicitly it forces the migrated
- *    thread off the CPU)
- * 4) it gets the migration request and checks whether the migrated
- *    task is still in the wrong runqueue.
- * 5) if it's in the wrong runqueue then the migration thread removes
- *    it and puts it into the right queue.
- * 6) migration thread up()s the semaphore.
- * 7) we wake up and the migration is done.
- */
-
-/*
- * Change a given task's CPU affinity. Migrate the thread to a
- * proper CPU and schedule it away if the CPU it's executing on
- * is removed from the allowed bitmask.
- *
- * NOTE: the caller must have a valid reference to the task, the
- * task must not exit() & deallocate itself prematurely.  The
- * call is not atomic; no spinlocks may be held.
- */
-int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
-{
-	struct migration_req req;
-	unsigned long flags;
-	struct rq *rq;
-	int ret = 0;
-
-	rq = task_rq_lock(p, &flags);
-	if (!cpus_intersects(new_mask, cpu_online_map)) {
-		ret = -EINVAL;
-		goto out;
-	}
-
-	p->cpus_allowed = new_mask;
-	/* Can the task run on the task's current CPU? If so, we're done */
-	if (cpu_isset(task_cpu(p), new_mask))
-		goto out;
-
-	if (migrate_task(p, any_online_cpu(new_mask), &req)) {
-		/* Need help from migration thread: drop lock and wait. */
-		task_rq_unlock(rq, &flags);
-		wake_up_process(rq->migration_thread);
-		wait_for_completion(&req.done);
-		tlb_migrate_finish(p->mm);
-		return 0;
-	}
-out:
-	task_rq_unlock(rq, &flags);
-
-	return ret;
-}
-EXPORT_SYMBOL_GPL(set_cpus_allowed);
-
-/*
- * Move (not current) task off this cpu, onto dest cpu.  We're doing
- * this because either it can't run here any more (set_cpus_allowed()
- * away from this CPU, or CPU going down), or because we're
- * attempting to rebalance this task on exec (sched_exec).
- *
- * So we race with normal scheduler movements, but that's OK, as long
- * as the task is no longer on this CPU.
- *
- * Returns non-zero if task was successfully migrated.
- */
-static int __migrate_task(struct task_struct *p, int src_cpu, int dest_cpu)
-{
-	struct rq *rq_dest, *rq_src;
-	int ret = 0;
-
-	if (unlikely(cpu_is_offline(dest_cpu)))
-		return ret;
-
-	rq_src = cpu_rq(src_cpu);
-	rq_dest = cpu_rq(dest_cpu);
-
-	double_rq_lock(rq_src, rq_dest);
-	/* Already moved. */
-	if (task_cpu(p) != src_cpu)
-		goto out;
-	/* Affinity changed (again). */
-	if (!cpu_isset(dest_cpu, p->cpus_allowed))
-		goto out;
-
-	set_task_cpu(p, dest_cpu);
-	if (p->array) {
-		/*
-		 * Sync timestamp with rq_dest's before activating.
-		 * The same thing could be achieved by doing this step
-		 * afterwards, and pretending it was a local activate.
-		 * This way is cleaner and logically correct.
-		 */
-		p->timestamp = p->timestamp - rq_src->timestamp_last_tick
-				+ rq_dest->timestamp_last_tick;
-		deactivate_task(p, rq_src);
-		__activate_task(p, rq_dest);
-		if (TASK_PREEMPTS_CURR(p, rq_dest))
-			resched_task(rq_dest->curr);
-	}
-	ret = 1;
-out:
-	double_rq_unlock(rq_src, rq_dest);
-	return ret;
-}
-
-/*
- * migration_thread - this is a highprio system thread that performs
- * thread migration by bumping thread off CPU then 'pushing' onto
- * another runqueue.
- */
-static int migration_thread(void *data)
-{
-	int cpu = (long)data;
-	struct rq *rq;
-
-	rq = cpu_rq(cpu);
-	BUG_ON(rq->migration_thread != current);
-
-	set_current_state(TASK_INTERRUPTIBLE);
-	while (!kthread_should_stop()) {
-		struct migration_req *req;
-		struct list_head *head;
-
-		try_to_freeze();
-
-		spin_lock_irq(&rq->lock);
-
-		if (cpu_is_offline(cpu)) {
-			spin_unlock_irq(&rq->lock);
-			goto wait_to_die;
-		}
-
-		if (rq->active_balance) {
-			active_load_balance(rq, cpu);
-			rq->active_balance = 0;
-		}
-
-		head = &rq->migration_queue;
-
-		if (list_empty(head)) {
-			spin_unlock_irq(&rq->lock);
-			schedule();
-			set_current_state(TASK_INTERRUPTIBLE);
-			continue;
-		}
-		req = list_entry(head->next, struct migration_req, list);
-		list_del_init(head->next);
-
-		spin_unlock(&rq->lock);
-		__migrate_task(req->task, cpu, req->dest_cpu);
-		local_irq_enable();
-
-		complete(&req->done);
-	}
-	__set_current_state(TASK_RUNNING);
-	return 0;
-
-wait_to_die:
-	/* Wait for kthread_stop */
-	set_current_state(TASK_INTERRUPTIBLE);
-	while (!kthread_should_stop()) {
-		schedule();
-		set_current_state(TASK_INTERRUPTIBLE);
-	}
-	__set_current_state(TASK_RUNNING);
-	return 0;
-}
-
-#ifdef CONFIG_HOTPLUG_CPU
-/* Figure out where task on dead CPU should go, use force if neccessary. */
-static void move_task_off_dead_cpu(int dead_cpu, struct task_struct *p)
-{
-	unsigned long flags;
-	cpumask_t mask;
-	struct rq *rq;
-	int dest_cpu;
-
-restart:
-	/* On same node? */
-	mask = node_to_cpumask(cpu_to_node(dead_cpu));
-	cpus_and(mask, mask, p->cpus_allowed);
-	dest_cpu = any_online_cpu(mask);
-
-	/* On any allowed CPU? */
-	if (dest_cpu == NR_CPUS)
-		dest_cpu = any_online_cpu(p->cpus_allowed);
-
-	/* No more Mr. Nice Guy. */
-	if (dest_cpu == NR_CPUS) {
-		rq = task_rq_lock(p, &flags);
-		cpus_setall(p->cpus_allowed);
-		dest_cpu = any_online_cpu(p->cpus_allowed);
-		task_rq_unlock(rq, &flags);
-
-		/*
-		 * Don't tell them about moving exiting tasks or
-		 * kernel threads (both mm NULL), since they never
-		 * leave kernel.
-		 */
-		if (p->mm && printk_ratelimit())
-			printk(KERN_INFO "process %d (%s) no "
-			       "longer affine to cpu%d\n",
-			       p->pid, p->comm, dead_cpu);
-	}
-	if (!__migrate_task(p, dead_cpu, dest_cpu))
-		goto restart;
-}
-
-/*
- * While a dead CPU has no uninterruptible tasks queued at this point,
- * it might still have a nonzero ->nr_uninterruptible counter, because
- * for performance reasons the counter is not stricly tracking tasks to
- * their home CPUs. So we just add the counter to another CPU's counter,
- * to keep the global sum constant after CPU-down:
- */
-static void migrate_nr_uninterruptible(struct rq *rq_src)
-{
-	struct rq *rq_dest = cpu_rq(any_online_cpu(CPU_MASK_ALL));
-	unsigned long flags;
-
-	local_irq_save(flags);
-	double_rq_lock(rq_src, rq_dest);
-	rq_dest->nr_uninterruptible += rq_src->nr_uninterruptible;
-	rq_src->nr_uninterruptible = 0;
-	double_rq_unlock(rq_src, rq_dest);
-	local_irq_restore(flags);
-}
-
-/* Run through task list and migrate tasks from the dead cpu. */
-static void migrate_live_tasks(int src_cpu)
-{
-	struct task_struct *p, *t;
-
-	write_lock_irq(&tasklist_lock);
-
-	do_each_thread(t, p) {
-		if (p == current)
-			continue;
-
-		if (task_cpu(p) == src_cpu)
-			move_task_off_dead_cpu(src_cpu, p);
-	} while_each_thread(t, p);
-
-	write_unlock_irq(&tasklist_lock);
-}
-
-/* Schedules idle task to be the next runnable task on current CPU.
- * It does so by boosting its priority to highest possible and adding it to
- * the _front_ of the runqueue. Used by CPU offline code.
- */
-void sched_idle_next(void)
-{
-	int this_cpu = smp_processor_id();
-	struct rq *rq = cpu_rq(this_cpu);
-	struct task_struct *p = rq->idle;
-	unsigned long flags;
-
-	/* cpu has to be offline */
-	BUG_ON(cpu_online(this_cpu));
-
-	/*
-	 * Strictly not necessary since rest of the CPUs are stopped by now
-	 * and interrupts disabled on the current cpu.
-	 */
-	spin_lock_irqsave(&rq->lock, flags);
-
-	__setscheduler(p, SCHED_FIFO, MAX_RT_PRIO-1);
-
-	/* Add idle task to the _front_ of its priority queue: */
-	__activate_idle_task(p, rq);
-
-	spin_unlock_irqrestore(&rq->lock, flags);
-}
-
-/*
- * Ensures that the idle task is using init_mm right before its cpu goes
- * offline.
- */
-void idle_task_exit(void)
-{
-	struct mm_struct *mm = current->active_mm;
-
-	BUG_ON(cpu_online(smp_processor_id()));
-
-	if (mm != &init_mm)
-		switch_mm(mm, &init_mm, current);
-	mmdrop(mm);
-}
-
-static void migrate_dead(unsigned int dead_cpu, struct task_struct *p)
-{
-	struct rq *rq = cpu_rq(dead_cpu);
-
-	/* Must be exiting, otherwise would be on tasklist. */
-	BUG_ON(p->exit_state != EXIT_ZOMBIE && p->exit_state != EXIT_DEAD);
-
-	/* Cannot have done final schedule yet: would have vanished. */
-	BUG_ON(p->flags & PF_DEAD);
-
-	get_task_struct(p);
-
-	/*
-	 * Drop lock around migration; if someone else moves it,
-	 * that's OK.  No task can be added to this CPU, so iteration is
-	 * fine.
-	 */
-	spin_unlock_irq(&rq->lock);
-	move_task_off_dead_cpu(dead_cpu, p);
-	spin_lock_irq(&rq->lock);
-
-	put_task_struct(p);
-}
-
-/* release_task() removes task from tasklist, so we won't find dead tasks. */
-static void migrate_dead_tasks(unsigned int dead_cpu)
-{
-	struct rq *rq = cpu_rq(dead_cpu);
-	unsigned int arr, i;
-
-	for (arr = 0; arr < 2; arr++) {
-		for (i = 0; i < MAX_PRIO; i++) {
-			struct list_head *list = &rq->arrays[arr].queue[i];
-
-			while (!list_empty(list))
-				migrate_dead(dead_cpu, list_entry(list->next,
-					     struct task_struct, run_list));
-		}
-	}
-}
-#endif /* CONFIG_HOTPLUG_CPU */
-
-#if defined(CONFIG_DEBUG_KERNEL) && defined(CONFIG_SYSCTL)
-static struct ctl_table sd_ctl_dir[] = {
-	{1, "sched_domain", NULL, 0, 0755, NULL, },
-	{0,},
-};
-
-static struct ctl_table sd_ctl_root[] = {
-	{1, "kernel", NULL, 0, 0755, sd_ctl_dir, },
-	{0,},
-};
-
-static struct ctl_table *sd_alloc_ctl_entry(int n)
-{
-	struct ctl_table *entry =
-		kmalloc(n * sizeof(struct ctl_table), GFP_KERNEL);
-	BUG_ON(!entry);
-	memset(entry, 0, n * sizeof(struct ctl_table));
-	return entry;
-}
-
-static void set_table_entry(struct ctl_table *entry, int ctl_name,
-			const char *procname, void *data, int maxlen,
-			mode_t mode, proc_handler *proc_handler)
-{
-	entry->ctl_name = ctl_name;
-	entry->procname = procname;
-	entry->data = data;
-	entry->maxlen = maxlen;
-	entry->mode = mode;
-	entry->proc_handler = proc_handler;
-}
-
-static struct ctl_table *
-sd_alloc_ctl_domain_table(struct sched_domain *sd)
-{
-	struct ctl_table *table;
-	table = sd_alloc_ctl_entry(14);
-
-	set_table_entry(&table[0], 1, "min_interval", &sd->min_interval,
-		sizeof(long), 0644, proc_doulongvec_minmax);
-	set_table_entry(&table[1], 2, "max_interval", &sd->max_interval,
-		sizeof(long), 0644, proc_doulongvec_minmax);
-	set_table_entry(&table[2], 3, "busy_idx", &sd->busy_idx,
-		sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[3], 4, "idle_idx", &sd->idle_idx,
-		sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[4], 5, "newidle_idx", &sd->newidle_idx,
-		sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[5], 6, "wake_idx", &sd->wake_idx,
-		sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[6], 7, "forkexec_idx", &sd->forkexec_idx,
-		sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[7], 8, "busy_factor", &sd->busy_factor,
-		sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[8], 9, "imbalance_pct", &sd->imbalance_pct,
-		sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[9], 10, "cache_hot_time", &sd->cache_hot_time,
-		sizeof(long long), 0644, proc_doulongvec_minmax);
-	set_table_entry(&table[10], 11, "cache_nice_tries", &sd->cache_nice_tries,
-		sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[11], 12, "per_cpu_gain", &sd->per_cpu_gain,
-		sizeof(int), 0644, proc_dointvec_minmax);
-	set_table_entry(&table[12], 13, "flags", &sd->flags,
-		sizeof(int), 0644, proc_dointvec_minmax);
-	return table;
-}
-
-static ctl_table *sd_alloc_ctl_cpu_table(int cpu)
-{
-	struct sched_domain *sd;
-	int domain_num = 0, i;
-	struct ctl_table *entry, *table;
-	char buf[32];
-	for_each_domain(cpu, sd)
-		domain_num++;
-	entry = table = sd_alloc_ctl_entry(domain_num + 1);
-
-	i = 0;
-	for_each_domain(cpu, sd) {
-		snprintf(buf, 32, "domain%d", i);
-		entry->ctl_name = i + 1;
-		entry->procname = kstrdup(buf, GFP_KERNEL);
-		entry->mode = 0755;
-		entry->child = sd_alloc_ctl_domain_table(sd);
-		entry++;
-		i++;
-	}
-	return table;
-}
-
-static struct ctl_table_header *sd_sysctl_header;
-static void init_sched_domain_sysctl(void)
-{
-	int i, cpu_num = num_online_cpus();
-	char buf[32];
-	struct ctl_table *entry = sd_alloc_ctl_entry(cpu_num + 1);
-
-	sd_ctl_dir[0].child = entry;
-
-	for (i = 0; i < cpu_num; i++, entry++) {
-		snprintf(buf, 32, "cpu%d", i);
-		entry->ctl_name = i + 1;
-		entry->procname = kstrdup(buf, GFP_KERNEL);
-		entry->mode = 0755;
-		entry->child = sd_alloc_ctl_cpu_table(i);
-	}
-	sd_sysctl_header = register_sysctl_table(sd_ctl_root, 0);
-}
-#else
-static void init_sched_domain_sysctl(void)
-{
-}
-#endif
-
-/*
- * migration_call - callback that gets triggered when a CPU is added.
- * Here we can start up the necessary migration thread for the new CPU.
- */
-static int __cpuinit
-migration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)
-{
-	struct task_struct *p;
-	int cpu = (long)hcpu;
-	unsigned long flags;
-	struct rq *rq;
-
-	switch (action) {
-	case CPU_UP_PREPARE:
-		p = kthread_create(migration_thread, hcpu, "migration/%d",cpu);
-		if (IS_ERR(p))
-			return NOTIFY_BAD;
-		p->flags |= PF_NOFREEZE;
-		kthread_bind(p, cpu);
-		/* Must be high prio: stop_machine expects to yield to it. */
-		rq = task_rq_lock(p, &flags);
-		__setscheduler(p, SCHED_FIFO, MAX_RT_PRIO-1);
-		task_rq_unlock(rq, &flags);
-		cpu_rq(cpu)->migration_thread = p;
-		break;
-
-	case CPU_ONLINE:
-		/* Strictly unneccessary, as first user will wake it. */
-		wake_up_process(cpu_rq(cpu)->migration_thread);
-		break;
-
-#ifdef CONFIG_HOTPLUG_CPU
-	case CPU_UP_CANCELED:
-		if (!cpu_rq(cpu)->migration_thread)
-			break;
-		/* Unbind it from offline cpu so it can run.  Fall thru. */
-		kthread_bind(cpu_rq(cpu)->migration_thread,
-			     any_online_cpu(cpu_online_map));
-		kthread_stop(cpu_rq(cpu)->migration_thread);
-		cpu_rq(cpu)->migration_thread = NULL;
-		break;
-
-	case CPU_DEAD:
-		migrate_live_tasks(cpu);
-		rq = cpu_rq(cpu);
-		kthread_stop(rq->migration_thread);
-		rq->migration_thread = NULL;
-		/* Idle task back to normal (off runqueue, low prio) */
-		rq = task_rq_lock(rq->idle, &flags);
-		deactivate_task(rq->idle, rq);
-		rq->idle->static_prio = MAX_PRIO;
-		__setscheduler(rq->idle, SCHED_NORMAL, 0);
-		migrate_dead_tasks(cpu);
-		task_rq_unlock(rq, &flags);
-		migrate_nr_uninterruptible(rq);
-		BUG_ON(rq->nr_running != 0);
-
-		/* No need to migrate the tasks: it was best-effort if
-		 * they didn't do lock_cpu_hotplug().  Just wake up
-		 * the requestors. */
-		spin_lock_irq(&rq->lock);
-		while (!list_empty(&rq->migration_queue)) {
-			struct migration_req *req;
-
-			req = list_entry(rq->migration_queue.next,
-					 struct migration_req, list);
-			list_del_init(&req->list);
-			complete(&req->done);
-		}
-		spin_unlock_irq(&rq->lock);
-		break;
-#endif
-	}
-	return NOTIFY_OK;
-}
-
-/* Register at highest priority so that task migration (migrate_all_tasks)
- * happens before everything else.
- */
-static struct notifier_block __cpuinitdata migration_notifier = {
-	.notifier_call = migration_call,
-	.priority = 10
-};
-
-int __init migration_init(void)
-{
-	void *cpu = (void *)(long)smp_processor_id();
-
-	/* Start one for the boot CPU: */
-	migration_call(&migration_notifier, CPU_UP_PREPARE, cpu);
-	migration_call(&migration_notifier, CPU_ONLINE, cpu);
-	register_cpu_notifier(&migration_notifier);
-
-	return 0;
-}
-#endif
-
-#ifdef CONFIG_SMP
-#undef SCHED_DOMAIN_DEBUG
-#ifdef SCHED_DOMAIN_DEBUG
-static void sched_domain_debug(struct sched_domain *sd, int cpu)
-{
-	int level = 0;
-
-	if (!sd) {
-		printk(KERN_DEBUG "CPU%d attaching NULL sched-domain.\n", cpu);
-		return;
-	}
-
-	printk(KERN_DEBUG "CPU%d attaching sched-domain:\n", cpu);
-
-	do {
-		int i;
-		char str[NR_CPUS];
-		struct sched_group *group = sd->groups;
-		cpumask_t groupmask;
-
-		cpumask_scnprintf(str, NR_CPUS, sd->span);
-		cpus_clear(groupmask);
-
-		printk(KERN_DEBUG);
-		for (i = 0; i < level + 1; i++)
-			printk(" ");
-		printk("domain %d: ", level);
-
-		if (!(sd->flags & SD_LOAD_BALANCE)) {
-			printk("does not load-balance\n");
-			if (sd->parent)
-				printk(KERN_ERR "ERROR: !SD_LOAD_BALANCE domain has parent");
-			break;
-		}
-
-		printk("span %s\n", str);
-
-		if (!cpu_isset(cpu, sd->span))
-			printk(KERN_ERR "ERROR: domain->span does not contain CPU%d\n", cpu);
-		if (!cpu_isset(cpu, group->cpumask))
-			printk(KERN_ERR "ERROR: domain->groups does not contain CPU%d\n", cpu);
-
-		printk(KERN_DEBUG);
-		for (i = 0; i < level + 2; i++)
-			printk(" ");
-		printk("groups:");
-		do {
-			if (!group) {
-				printk("\n");
-				printk(KERN_ERR "ERROR: group is NULL\n");
-				break;
-			}
-
-			if (!group->cpu_power) {
-				printk("\n");
-				printk(KERN_ERR "ERROR: domain->cpu_power not set\n");
-			}
-
-			if (!cpus_weight(group->cpumask)) {
-				printk("\n");
-				printk(KERN_ERR "ERROR: empty group\n");
-			}
-
-			if (cpus_intersects(groupmask, group->cpumask)) {
-				printk("\n");
-				printk(KERN_ERR "ERROR: repeated CPUs\n");
-			}
-
-			cpus_or(groupmask, groupmask, group->cpumask);
-
-			cpumask_scnprintf(str, NR_CPUS, group->cpumask);
-			printk(" %s", str);
-
-			group = group->next;
-		} while (group != sd->groups);
-		printk("\n");
-
-		if (!cpus_equal(sd->span, groupmask))
-			printk(KERN_ERR "ERROR: groups don't span domain->span\n");
-
-		level++;
-		sd = sd->parent;
-
-		if (sd) {
-			if (!cpus_subset(groupmask, sd->span))
-				printk(KERN_ERR "ERROR: parent span is not a superset of domain->span\n");
-		}
-
-	} while (sd);
-}
-#else
-# define sched_domain_debug(sd, cpu) do { } while (0)
-#endif
-
-static int sd_degenerate(struct sched_domain *sd)
-{
-	if (cpus_weight(sd->span) == 1)
-		return 1;
-
-	/* Following flags need at least 2 groups */
-	if (sd->flags & (SD_LOAD_BALANCE |
-			 SD_BALANCE_NEWIDLE |
-			 SD_BALANCE_FORK |
-			 SD_BALANCE_EXEC)) {
-		if (sd->groups != sd->groups->next)
-			return 0;
-	}
-
-	/* Following flags don't use groups */
-	if (sd->flags & (SD_WAKE_IDLE |
-			 SD_WAKE_AFFINE |
-			 SD_WAKE_BALANCE))
-		return 0;
-
-	return 1;
-}
-
-static int
-sd_parent_degenerate(struct sched_domain *sd, struct sched_domain *parent)
-{
-	unsigned long cflags = sd->flags, pflags = parent->flags;
-
-	if (sd_degenerate(parent))
-		return 1;
-
-	if (!cpus_equal(sd->span, parent->span))
-		return 0;
-
-	/* Does parent contain flags not in child? */
-	/* WAKE_BALANCE is a subset of WAKE_AFFINE */
-	if (cflags & SD_WAKE_AFFINE)
-		pflags &= ~SD_WAKE_BALANCE;
-	/* Flags needing groups don't count if only 1 group in parent */
-	if (parent->groups == parent->groups->next) {
-		pflags &= ~(SD_LOAD_BALANCE |
-				SD_BALANCE_NEWIDLE |
-				SD_BALANCE_FORK |
-				SD_BALANCE_EXEC);
-	}
-	if (~cflags & pflags)
-		return 0;
-
-	return 1;
-}
-
-/*
- * Attach the domain 'sd' to 'cpu' as its base domain.  Callers must
- * hold the hotplug lock.
- */
-static void cpu_attach_domain(struct sched_domain *sd, int cpu)
-{
-	struct rq *rq = cpu_rq(cpu);
-	struct sched_domain *tmp;
-
-	/* Remove the sched domains which do not contribute to scheduling. */
-	for (tmp = sd; tmp; tmp = tmp->parent) {
-		struct sched_domain *parent = tmp->parent;
-		if (!parent)
-			break;
-		if (sd_parent_degenerate(tmp, parent))
-			tmp->parent = parent->parent;
-	}
-
-	if (sd && sd_degenerate(sd))
-		sd = sd->parent;
-
-	sched_domain_debug(sd, cpu);
-
-	rcu_assign_pointer(rq->sd, sd);
-}
-
-/* cpus with isolated domains */
-static cpumask_t __devinitdata cpu_isolated_map = CPU_MASK_NONE;
-
-/* Setup the mask of cpus configured for isolated domains */
-static int __init isolated_cpu_setup(char *str)
-{
-	int ints[NR_CPUS], i;
-
-	str = get_options(str, ARRAY_SIZE(ints), ints);
-	cpus_clear(cpu_isolated_map);
-	for (i = 1; i <= ints[0]; i++)
-		if (ints[i] < NR_CPUS)
-			cpu_set(ints[i], cpu_isolated_map);
-	return 1;
-}
-
-__setup ("isolcpus=", isolated_cpu_setup);
-
-/*
- * init_sched_build_groups takes an array of groups, the cpumask we wish
- * to span, and a pointer to a function which identifies what group a CPU
- * belongs to. The return value of group_fn must be a valid index into the
- * groups[] array, and must be >= 0 and < NR_CPUS (due to the fact that we
- * keep track of groups covered with a cpumask_t).
- *
- * init_sched_build_groups will build a circular linked list of the groups
- * covered by the given span, and will set each group's ->cpumask correctly,
- * and ->cpu_power to 0.
- */
-static void init_sched_build_groups(struct sched_group groups[], cpumask_t span,
-				    int (*group_fn)(int cpu))
-{
-	struct sched_group *first = NULL, *last = NULL;
-	cpumask_t covered = CPU_MASK_NONE;
-	int i;
-
-	for_each_cpu_mask(i, span) {
-		int group = group_fn(i);
-		struct sched_group *sg = &groups[group];
-		int j;
-
-		if (cpu_isset(i, covered))
-			continue;
-
-		sg->cpumask = CPU_MASK_NONE;
-		sg->cpu_power = 0;
-
-		for_each_cpu_mask(j, span) {
-			if (group_fn(j) != group)
-				continue;
-
-			cpu_set(j, covered);
-			cpu_set(j, sg->cpumask);
-		}
-		if (!first)
-			first = sg;
-		if (last)
-			last->next = sg;
-		last = sg;
-	}
-	last->next = first;
-}
-
-#define SD_NODES_PER_DOMAIN 16
-
-/*
- * Self-tuning task migration cost measurement between source and target CPUs.
- *
- * This is done by measuring the cost of manipulating buffers of varying
- * sizes. For a given buffer-size here are the steps that are taken:
- *
- * 1) the source CPU reads+dirties a shared buffer
- * 2) the target CPU reads+dirties the same shared buffer
- *
- * We measure how long they take, in the following 4 scenarios:
- *
- *  - source: CPU1, target: CPU2 | cost1
- *  - source: CPU2, target: CPU1 | cost2
- *  - source: CPU1, target: CPU1 | cost3
- *  - source: CPU2, target: CPU2 | cost4
- *
- * We then calculate the cost3+cost4-cost1-cost2 difference - this is
- * the cost of migration.
- *
- * We then start off from a small buffer-size and iterate up to larger
- * buffer sizes, in 5% steps - measuring each buffer-size separately, and
- * doing a maximum search for the cost. (The maximum cost for a migration
- * normally occurs when the working set size is around the effective cache
- * size.)
- */
-#define SEARCH_SCOPE		2
-#define MIN_CACHE_SIZE		(64*1024U)
-#define DEFAULT_CACHE_SIZE	(5*1024*1024U)
-#define ITERATIONS		1
-#define SIZE_THRESH		130
-#define COST_THRESH		130
-
-/*
- * The migration cost is a function of 'domain distance'. Domain
- * distance is the number of steps a CPU has to iterate down its
- * domain tree to share a domain with the other CPU. The farther
- * two CPUs are from each other, the larger the distance gets.
- *
- * Note that we use the distance only to cache measurement results,
- * the distance value is not used numerically otherwise. When two
- * CPUs have the same distance it is assumed that the migration
- * cost is the same. (this is a simplification but quite practical)
- */
-#define MAX_DOMAIN_DISTANCE 32
-
-static unsigned long long migration_cost[MAX_DOMAIN_DISTANCE] =
-		{ [ 0 ... MAX_DOMAIN_DISTANCE-1 ] =
-/*
- * Architectures may override the migration cost and thus avoid
- * boot-time calibration. Unit is nanoseconds. Mostly useful for
- * virtualized hardware:
- */
-#ifdef CONFIG_DEFAULT_MIGRATION_COST
-			CONFIG_DEFAULT_MIGRATION_COST
-#else
-			-1LL
-#endif
-};
-
-/*
- * Allow override of migration cost - in units of microseconds.
- * E.g. migration_cost=1000,2000,3000 will set up a level-1 cost
- * of 1 msec, level-2 cost of 2 msecs and level3 cost of 3 msecs:
- */
-static int __init migration_cost_setup(char *str)
-{
-	int ints[MAX_DOMAIN_DISTANCE+1], i;
-
-	str = get_options(str, ARRAY_SIZE(ints), ints);
-
-	printk("#ints: %d\n", ints[0]);
-	for (i = 1; i <= ints[0]; i++) {
-		migration_cost[i-1] = (unsigned long long)ints[i]*1000;
-		printk("migration_cost[%d]: %Ld\n", i-1, migration_cost[i-1]);
-	}
-	return 1;
-}
-
-__setup ("migration_cost=", migration_cost_setup);
-
-/*
- * Global multiplier (divisor) for migration-cutoff values,
- * in percentiles. E.g. use a value of 150 to get 1.5 times
- * longer cache-hot cutoff times.
- *
- * (We scale it from 100 to 128 to long long handling easier.)
- */
-
-#define MIGRATION_FACTOR_SCALE 128
-
-static unsigned int migration_factor = MIGRATION_FACTOR_SCALE;
-
-static int __init setup_migration_factor(char *str)
-{
-	get_option(&str, &migration_factor);
-	migration_factor = migration_factor * MIGRATION_FACTOR_SCALE / 100;
-	return 1;
-}
-
-__setup("migration_factor=", setup_migration_factor);
-
-/*
- * Estimated distance of two CPUs, measured via the number of domains
- * we have to pass for the two CPUs to be in the same span:
- */
-static unsigned long domain_distance(int cpu1, int cpu2)
-{
-	unsigned long distance = 0;
-	struct sched_domain *sd;
-
-	for_each_domain(cpu1, sd) {
-		WARN_ON(!cpu_isset(cpu1, sd->span));
-		if (cpu_isset(cpu2, sd->span))
-			return distance;
-		distance++;
-	}
-	if (distance >= MAX_DOMAIN_DISTANCE) {
-		WARN_ON(1);
-		distance = MAX_DOMAIN_DISTANCE-1;
-	}
-
-	return distance;
-}
-
-static unsigned int migration_debug;
-
-static int __init setup_migration_debug(char *str)
-{
-	get_option(&str, &migration_debug);
-	return 1;
-}
-
-__setup("migration_debug=", setup_migration_debug);
-
-/*
- * Maximum cache-size that the scheduler should try to measure.
- * Architectures with larger caches should tune this up during
- * bootup. Gets used in the domain-setup code (i.e. during SMP
- * bootup).
- */
-unsigned int max_cache_size;
-
-static int __init setup_max_cache_size(char *str)
-{
-	get_option(&str, &max_cache_size);
-	return 1;
-}
-
-__setup("max_cache_size=", setup_max_cache_size);
-
-/*
- * Dirty a big buffer in a hard-to-predict (for the L2 cache) way. This
- * is the operation that is timed, so we try to generate unpredictable
- * cachemisses that still end up filling the L2 cache:
- */
-static void touch_cache(void *__cache, unsigned long __size)
-{
-	unsigned long size = __size/sizeof(long), chunk1 = size/3,
-			chunk2 = 2*size/3;
-	unsigned long *cache = __cache;
-	int i;
-
-	for (i = 0; i < size/6; i += 8) {
-		switch (i % 6) {
-			case 0: cache[i]++;
-			case 1: cache[size-1-i]++;
-			case 2: cache[chunk1-i]++;
-			case 3: cache[chunk1+i]++;
-			case 4: cache[chunk2-i]++;
-			case 5: cache[chunk2+i]++;
-		}
-	}
-}
-
-/*
- * Measure the cache-cost of one task migration. Returns in units of nsec.
- */
-static unsigned long long
-measure_one(void *cache, unsigned long size, int source, int target)
-{
-	cpumask_t mask, saved_mask;
-	unsigned long long t0, t1, t2, t3, cost;
-
-	saved_mask = current->cpus_allowed;
-
-	/*
-	 * Flush source caches to RAM and invalidate them:
-	 */
-	sched_cacheflush();
-
-	/*
-	 * Migrate to the source CPU:
-	 */
-	mask = cpumask_of_cpu(source);
-	set_cpus_allowed(current, mask);
-	WARN_ON(smp_processor_id() != source);
-
-	/*
-	 * Dirty the working set:
-	 */
-	t0 = sched_clock();
-	touch_cache(cache, size);
-	t1 = sched_clock();
-
-	/*
-	 * Migrate to the target CPU, dirty the L2 cache and access
-	 * the shared buffer. (which represents the working set
-	 * of a migrated task.)
-	 */
-	mask = cpumask_of_cpu(target);
-	set_cpus_allowed(current, mask);
-	WARN_ON(smp_processor_id() != target);
-
-	t2 = sched_clock();
-	touch_cache(cache, size);
-	t3 = sched_clock();
-
-	cost = t1-t0 + t3-t2;
-
-	if (migration_debug >= 2)
-		printk("[%d->%d]: %8Ld %8Ld %8Ld => %10Ld.\n",
-			source, target, t1-t0, t1-t0, t3-t2, cost);
-	/*
-	 * Flush target caches to RAM and invalidate them:
-	 */
-	sched_cacheflush();
-
-	set_cpus_allowed(current, saved_mask);
-
-	return cost;
-}
-
-/*
- * Measure a series of task migrations and return the average
- * result. Since this code runs early during bootup the system
- * is 'undisturbed' and the average latency makes sense.
- *
- * The algorithm in essence auto-detects the relevant cache-size,
- * so it will properly detect different cachesizes for different
- * cache-hierarchies, depending on how the CPUs are connected.
- *
- * Architectures can prime the upper limit of the search range via
- * max_cache_size, otherwise the search range defaults to 20MB...64K.
- */
-static unsigned long long
-measure_cost(int cpu1, int cpu2, void *cache, unsigned int size)
-{
-	unsigned long long cost1, cost2;
-	int i;
-
-	/*
-	 * Measure the migration cost of 'size' bytes, over an
-	 * average of 10 runs:
-	 *
-	 * (We perturb the cache size by a small (0..4k)
-	 *  value to compensate size/alignment related artifacts.
-	 *  We also subtract the cost of the operation done on
-	 *  the same CPU.)
-	 */
-	cost1 = 0;
-
-	/*
-	 * dry run, to make sure we start off cache-cold on cpu1,
-	 * and to get any vmalloc pagefaults in advance:
-	 */
-	measure_one(cache, size, cpu1, cpu2);
-	for (i = 0; i < ITERATIONS; i++)
-		cost1 += measure_one(cache, size - i*1024, cpu1, cpu2);
-
-	measure_one(cache, size, cpu2, cpu1);
-	for (i = 0; i < ITERATIONS; i++)
-		cost1 += measure_one(cache, size - i*1024, cpu2, cpu1);
-
-	/*
-	 * (We measure the non-migrating [cached] cost on both
-	 *  cpu1 and cpu2, to handle CPUs with different speeds)
-	 */
-	cost2 = 0;
-
-	measure_one(cache, size, cpu1, cpu1);
-	for (i = 0; i < ITERATIONS; i++)
-		cost2 += measure_one(cache, size - i*1024, cpu1, cpu1);
-
-	measure_one(cache, size, cpu2, cpu2);
-	for (i = 0; i < ITERATIONS; i++)
-		cost2 += measure_one(cache, size - i*1024, cpu2, cpu2);
-
-	/*
-	 * Get the per-iteration migration cost:
-	 */
-	do_div(cost1, 2*ITERATIONS);
-	do_div(cost2, 2*ITERATIONS);
-
-	return cost1 - cost2;
-}
-
-static unsigned long long measure_migration_cost(int cpu1, int cpu2)
-{
-	unsigned long long max_cost = 0, fluct = 0, avg_fluct = 0;
-	unsigned int max_size, size, size_found = 0;
-	long long cost = 0, prev_cost;
-	void *cache;
-
-	/*
-	 * Search from max_cache_size*5 down to 64K - the real relevant
-	 * cachesize has to lie somewhere inbetween.
-	 */
-	if (max_cache_size) {
-		max_size = max(max_cache_size * SEARCH_SCOPE, MIN_CACHE_SIZE);
-		size = max(max_cache_size / SEARCH_SCOPE, MIN_CACHE_SIZE);
-	} else {
-		/*
-		 * Since we have no estimation about the relevant
-		 * search range
-		 */
-		max_size = DEFAULT_CACHE_SIZE * SEARCH_SCOPE;
-		size = MIN_CACHE_SIZE;
-	}
-
-	if (!cpu_online(cpu1) || !cpu_online(cpu2)) {
-		printk("cpu %d and %d not both online!\n", cpu1, cpu2);
-		return 0;
-	}
-
-	/*
-	 * Allocate the working set:
-	 */
-	cache = vmalloc(max_size);
-	if (!cache) {
-		printk("could not vmalloc %d bytes for cache!\n", 2*max_size);
-		return 1000000; /* return 1 msec on very small boxen */
-	}
-
-	while (size <= max_size) {
-		prev_cost = cost;
-		cost = measure_cost(cpu1, cpu2, cache, size);
-
-		/*
-		 * Update the max:
-		 */
-		if (cost > 0) {
-			if (max_cost < cost) {
-				max_cost = cost;
-				size_found = size;
-			}
-		}
-		/*
-		 * Calculate average fluctuation, we use this to prevent
-		 * noise from triggering an early break out of the loop:
-		 */
-		fluct = abs(cost - prev_cost);
-		avg_fluct = (avg_fluct + fluct)/2;
-
-		if (migration_debug)
-			printk("-> [%d][%d][%7d] %3ld.%ld [%3ld.%ld] (%ld): (%8Ld %8Ld)\n",
-				cpu1, cpu2, size,
-				(long)cost / 1000000,
-				((long)cost / 100000) % 10,
-				(long)max_cost / 1000000,
-				((long)max_cost / 100000) % 10,
-				domain_distance(cpu1, cpu2),
-				cost, avg_fluct);
-
-		/*
-		 * If we iterated at least 20% past the previous maximum,
-		 * and the cost has dropped by more than 20% already,
-		 * (taking fluctuations into account) then we assume to
-		 * have found the maximum and break out of the loop early:
-		 */
-		if (size_found && (size*100 > size_found*SIZE_THRESH))
-			if (cost+avg_fluct <= 0 ||
-				max_cost*100 > (cost+avg_fluct)*COST_THRESH) {
-
-				if (migration_debug)
-					printk("-> found max.\n");
-				break;
-			}
-		/*
-		 * Increase the cachesize in 10% steps:
-		 */
-		size = size * 10 / 9;
-	}
-
-	if (migration_debug)
-		printk("[%d][%d] working set size found: %d, cost: %Ld\n",
-			cpu1, cpu2, size_found, max_cost);
-
-	vfree(cache);
-
-	/*
-	 * A task is considered 'cache cold' if at least 2 times
-	 * the worst-case cost of migration has passed.
-	 *
-	 * (this limit is only listened to if the load-balancing
-	 * situation is 'nice' - if there is a large imbalance we
-	 * ignore it for the sake of CPU utilization and
-	 * processing fairness.)
-	 */
-	return 2 * max_cost * migration_factor / MIGRATION_FACTOR_SCALE;
-}
-
-static void calibrate_migration_costs(const cpumask_t *cpu_map)
-{
-	int cpu1 = -1, cpu2 = -1, cpu, orig_cpu = raw_smp_processor_id();
-	unsigned long j0, j1, distance, max_distance = 0;
-	struct sched_domain *sd;
-
-	j0 = jiffies;
-
-	/*
-	 * First pass - calculate the cacheflush times:
-	 */
-	for_each_cpu_mask(cpu1, *cpu_map) {
-		for_each_cpu_mask(cpu2, *cpu_map) {
-			if (cpu1 == cpu2)
-				continue;
-			distance = domain_distance(cpu1, cpu2);
-			max_distance = max(max_distance, distance);
-			/*
-			 * No result cached yet?
-			 */
-			if (migration_cost[distance] == -1LL)
-				migration_cost[distance] =
-					measure_migration_cost(cpu1, cpu2);
-		}
-	}
-	/*
-	 * Second pass - update the sched domain hierarchy with
-	 * the new cache-hot-time estimations:
-	 */
-	for_each_cpu_mask(cpu, *cpu_map) {
-		distance = 0;
-		for_each_domain(cpu, sd) {
-			sd->cache_hot_time = migration_cost[distance];
-			distance++;
-		}
-	}
-	/*
-	 * Print the matrix:
-	 */
-	if (migration_debug)
-		printk("migration: max_cache_size: %d, cpu: %d MHz:\n",
-			max_cache_size,
-#ifdef CONFIG_X86
-			cpu_khz/1000
-#else
-			-1
-#endif
-		);
-	if (system_state == SYSTEM_BOOTING) {
-		printk("migration_cost=");
-		for (distance = 0; distance <= max_distance; distance++) {
-			if (distance)
-				printk(",");
-			printk("%ld", (long)migration_cost[distance] / 1000);
-		}
-		printk("\n");
-	}
-	j1 = jiffies;
-	if (migration_debug)
-		printk("migration: %ld seconds\n", (j1-j0)/HZ);
-
-	/*
-	 * Move back to the original CPU. NUMA-Q gets confused
-	 * if we migrate to another quad during bootup.
-	 */
-	if (raw_smp_processor_id() != orig_cpu) {
-		cpumask_t mask = cpumask_of_cpu(orig_cpu),
-			saved_mask = current->cpus_allowed;
-
-		set_cpus_allowed(current, mask);
-		set_cpus_allowed(current, saved_mask);
-	}
-}
-
-#ifdef CONFIG_NUMA
-
-/**
- * find_next_best_node - find the next node to include in a sched_domain
- * @node: node whose sched_domain we're building
- * @used_nodes: nodes already in the sched_domain
- *
- * Find the next node to include in a given scheduling domain.  Simply
- * finds the closest node not already in the @used_nodes map.
- *
- * Should use nodemask_t.
- */
-static int find_next_best_node(int node, unsigned long *used_nodes)
-{
-	int i, n, val, min_val, best_node = 0;
-
-	min_val = INT_MAX;
-
-	for (i = 0; i < MAX_NUMNODES; i++) {
-		/* Start at @node */
-		n = (node + i) % MAX_NUMNODES;
-
-		if (!nr_cpus_node(n))
-			continue;
-
-		/* Skip already used nodes */
-		if (test_bit(n, used_nodes))
-			continue;
-
-		/* Simple min distance search */
-		val = node_distance(node, n);
-
-		if (val < min_val) {
-			min_val = val;
-			best_node = n;
-		}
-	}
-
-	set_bit(best_node, used_nodes);
-	return best_node;
-}
-
-/**
- * sched_domain_node_span - get a cpumask for a node's sched_domain
- * @node: node whose cpumask we're constructing
- * @size: number of nodes to include in this span
- *
- * Given a node, construct a good cpumask for its sched_domain to span.  It
- * should be one that prevents unnecessary balancing, but also spreads tasks
- * out optimally.
- */
-static cpumask_t sched_domain_node_span(int node)
-{
-	DECLARE_BITMAP(used_nodes, MAX_NUMNODES);
-	cpumask_t span, nodemask;
-	int i;
-
-	cpus_clear(span);
-	bitmap_zero(used_nodes, MAX_NUMNODES);
-
-	nodemask = node_to_cpumask(node);
-	cpus_or(span, span, nodemask);
-	set_bit(node, used_nodes);
-
-	for (i = 1; i < SD_NODES_PER_DOMAIN; i++) {
-		int next_node = find_next_best_node(node, used_nodes);
-
-		nodemask = node_to_cpumask(next_node);
-		cpus_or(span, span, nodemask);
-	}
-
-	return span;
-}
-#endif
-
-int sched_smt_power_savings = 0, sched_mc_power_savings = 0;
-
-/*
- * SMT sched-domains:
- */
-#ifdef CONFIG_SCHED_SMT
-static DEFINE_PER_CPU(struct sched_domain, cpu_domains);
-static struct sched_group sched_group_cpus[NR_CPUS];
-
-static int cpu_to_cpu_group(int cpu)
-{
-	return cpu;
-}
-#endif
-
-/*
- * multi-core sched-domains:
- */
-#ifdef CONFIG_SCHED_MC
-static DEFINE_PER_CPU(struct sched_domain, core_domains);
-static struct sched_group *sched_group_core_bycpu[NR_CPUS];
-#endif
-
-#if defined(CONFIG_SCHED_MC) && defined(CONFIG_SCHED_SMT)
-static int cpu_to_core_group(int cpu)
-{
-	return first_cpu(cpu_sibling_map[cpu]);
-}
-#elif defined(CONFIG_SCHED_MC)
-static int cpu_to_core_group(int cpu)
-{
-	return cpu;
-}
-#endif
-
-static DEFINE_PER_CPU(struct sched_domain, phys_domains);
-static struct sched_group *sched_group_phys_bycpu[NR_CPUS];
-
-static int cpu_to_phys_group(int cpu)
-{
-#ifdef CONFIG_SCHED_MC
-	cpumask_t mask = cpu_coregroup_map(cpu);
-	return first_cpu(mask);
-#elif defined(CONFIG_SCHED_SMT)
-	return first_cpu(cpu_sibling_map[cpu]);
-#else
-	return cpu;
-#endif
-}
-
-#ifdef CONFIG_NUMA
-/*
- * The init_sched_build_groups can't handle what we want to do with node
- * groups, so roll our own. Now each node has its own list of groups which
- * gets dynamically allocated.
- */
-static DEFINE_PER_CPU(struct sched_domain, node_domains);
-static struct sched_group **sched_group_nodes_bycpu[NR_CPUS];
-
-static DEFINE_PER_CPU(struct sched_domain, allnodes_domains);
-static struct sched_group *sched_group_allnodes_bycpu[NR_CPUS];
-
-static int cpu_to_allnodes_group(int cpu)
-{
-	return cpu_to_node(cpu);
-}
-static void init_numa_sched_groups_power(struct sched_group *group_head)
-{
-	struct sched_group *sg = group_head;
-	int j;
-
-	if (!sg)
-		return;
-next_sg:
-	for_each_cpu_mask(j, sg->cpumask) {
-		struct sched_domain *sd;
-
-		sd = &per_cpu(phys_domains, j);
-		if (j != first_cpu(sd->groups->cpumask)) {
-			/*
-			 * Only add "power" once for each
-			 * physical package.
-			 */
-			continue;
-		}
-
-		sg->cpu_power += sd->groups->cpu_power;
-	}
-	sg = sg->next;
-	if (sg != group_head)
-		goto next_sg;
-}
-#endif
-
-/* Free memory allocated for various sched_group structures */
-static void free_sched_groups(const cpumask_t *cpu_map)
-{
-	int cpu;
-#ifdef CONFIG_NUMA
-	int i;
-
-	for_each_cpu_mask(cpu, *cpu_map) {
-		struct sched_group *sched_group_allnodes
-			= sched_group_allnodes_bycpu[cpu];
-		struct sched_group **sched_group_nodes
-			= sched_group_nodes_bycpu[cpu];
-
-		if (sched_group_allnodes) {
-			kfree(sched_group_allnodes);
-			sched_group_allnodes_bycpu[cpu] = NULL;
-		}
-
-		if (!sched_group_nodes)
-			continue;
-
-		for (i = 0; i < MAX_NUMNODES; i++) {
-			cpumask_t nodemask = node_to_cpumask(i);
-			struct sched_group *oldsg, *sg = sched_group_nodes[i];
-
-			cpus_and(nodemask, nodemask, *cpu_map);
-			if (cpus_empty(nodemask))
-				continue;
-
-			if (sg == NULL)
-				continue;
-			sg = sg->next;
-next_sg:
-			oldsg = sg;
-			sg = sg->next;
-			kfree(oldsg);
-			if (oldsg != sched_group_nodes[i])
-				goto next_sg;
-		}
-		kfree(sched_group_nodes);
-		sched_group_nodes_bycpu[cpu] = NULL;
-	}
-#endif
-	for_each_cpu_mask(cpu, *cpu_map) {
-		if (sched_group_phys_bycpu[cpu]) {
-			kfree(sched_group_phys_bycpu[cpu]);
-			sched_group_phys_bycpu[cpu] = NULL;
-		}
-#ifdef CONFIG_SCHED_MC
-		if (sched_group_core_bycpu[cpu]) {
-			kfree(sched_group_core_bycpu[cpu]);
-			sched_group_core_bycpu[cpu] = NULL;
-		}
-#endif
-	}
-}
-
-/*
- * Build sched domains for a given set of cpus and attach the sched domains
- * to the individual cpus
- */
-static int build_sched_domains(const cpumask_t *cpu_map)
-{
-	int i;
-	struct sched_group *sched_group_phys = NULL;
-#ifdef CONFIG_SCHED_MC
-	struct sched_group *sched_group_core = NULL;
-#endif
-#ifdef CONFIG_NUMA
-	struct sched_group **sched_group_nodes = NULL;
-	struct sched_group *sched_group_allnodes = NULL;
-
-	/*
-	 * Allocate the per-node list of sched groups
-	 */
-	sched_group_nodes = kzalloc(sizeof(struct sched_group*)*MAX_NUMNODES,
-					   GFP_KERNEL);
-	if (!sched_group_nodes) {
-		printk(KERN_WARNING "Can not alloc sched group node list\n");
-		return -ENOMEM;
-	}
-	sched_group_nodes_bycpu[first_cpu(*cpu_map)] = sched_group_nodes;
-#endif
-
-	/*
-	 * Set up domains for cpus specified by the cpu_map.
-	 */
-	for_each_cpu_mask(i, *cpu_map) {
-		int group;
-		struct sched_domain *sd = NULL, *p;
-		cpumask_t nodemask = node_to_cpumask(cpu_to_node(i));
-
-		cpus_and(nodemask, nodemask, *cpu_map);
-
-#ifdef CONFIG_NUMA
-		if (cpus_weight(*cpu_map)
-				> SD_NODES_PER_DOMAIN*cpus_weight(nodemask)) {
-			if (!sched_group_allnodes) {
-				sched_group_allnodes
-					= kmalloc(sizeof(struct sched_group)
-							* MAX_NUMNODES,
-						  GFP_KERNEL);
-				if (!sched_group_allnodes) {
-					printk(KERN_WARNING
-					"Can not alloc allnodes sched group\n");
-					goto error;
-				}
-				sched_group_allnodes_bycpu[i]
-						= sched_group_allnodes;
-			}
-			sd = &per_cpu(allnodes_domains, i);
-			*sd = SD_ALLNODES_INIT;
-			sd->span = *cpu_map;
-			group = cpu_to_allnodes_group(i);
-			sd->groups = &sched_group_allnodes[group];
-			p = sd;
-		} else
-			p = NULL;
-
-		sd = &per_cpu(node_domains, i);
-		*sd = SD_NODE_INIT;
-		sd->span = sched_domain_node_span(cpu_to_node(i));
-		sd->parent = p;
-		cpus_and(sd->span, sd->span, *cpu_map);
-#endif
-
-		if (!sched_group_phys) {
-			sched_group_phys
-				= kmalloc(sizeof(struct sched_group) * NR_CPUS,
-					  GFP_KERNEL);
-			if (!sched_group_phys) {
-				printk (KERN_WARNING "Can not alloc phys sched"
-						     "group\n");
-				goto error;
-			}
-			sched_group_phys_bycpu[i] = sched_group_phys;
-		}
-
-		p = sd;
-		sd = &per_cpu(phys_domains, i);
-		group = cpu_to_phys_group(i);
-		*sd = SD_CPU_INIT;
-		sd->span = nodemask;
-		sd->parent = p;
-		sd->groups = &sched_group_phys[group];
-
-#ifdef CONFIG_SCHED_MC
-		if (!sched_group_core) {
-			sched_group_core
-				= kmalloc(sizeof(struct sched_group) * NR_CPUS,
-					  GFP_KERNEL);
-			if (!sched_group_core) {
-				printk (KERN_WARNING "Can not alloc core sched"
-						     "group\n");
-				goto error;
-			}
-			sched_group_core_bycpu[i] = sched_group_core;
-		}
-
-		p = sd;
-		sd = &per_cpu(core_domains, i);
-		group = cpu_to_core_group(i);
-		*sd = SD_MC_INIT;
-		sd->span = cpu_coregroup_map(i);
-		cpus_and(sd->span, sd->span, *cpu_map);
-		sd->parent = p;
-		sd->groups = &sched_group_core[group];
-#endif
-
-#ifdef CONFIG_SCHED_SMT
-		p = sd;
-		sd = &per_cpu(cpu_domains, i);
-		group = cpu_to_cpu_group(i);
-		*sd = SD_SIBLING_INIT;
-		sd->span = cpu_sibling_map[i];
-		cpus_and(sd->span, sd->span, *cpu_map);
-		sd->parent = p;
-		sd->groups = &sched_group_cpus[group];
-#endif
-	}
-
-#ifdef CONFIG_SCHED_SMT
-	/* Set up CPU (sibling) groups */
-	for_each_cpu_mask(i, *cpu_map) {
-		cpumask_t this_sibling_map = cpu_sibling_map[i];
-		cpus_and(this_sibling_map, this_sibling_map, *cpu_map);
-		if (i != first_cpu(this_sibling_map))
-			continue;
-
-		init_sched_build_groups(sched_group_cpus, this_sibling_map,
-						&cpu_to_cpu_group);
-	}
-#endif
-
-#ifdef CONFIG_SCHED_MC
-	/* Set up multi-core groups */
-	for_each_cpu_mask(i, *cpu_map) {
-		cpumask_t this_core_map = cpu_coregroup_map(i);
-		cpus_and(this_core_map, this_core_map, *cpu_map);
-		if (i != first_cpu(this_core_map))
-			continue;
-		init_sched_build_groups(sched_group_core, this_core_map,
-					&cpu_to_core_group);
-	}
-#endif
-
-
-	/* Set up physical groups */
-	for (i = 0; i < MAX_NUMNODES; i++) {
-		cpumask_t nodemask = node_to_cpumask(i);
-
-		cpus_and(nodemask, nodemask, *cpu_map);
-		if (cpus_empty(nodemask))
-			continue;
-
-		init_sched_build_groups(sched_group_phys, nodemask,
-						&cpu_to_phys_group);
-	}
-
-#ifdef CONFIG_NUMA
-	/* Set up node groups */
-	if (sched_group_allnodes)
-		init_sched_build_groups(sched_group_allnodes, *cpu_map,
-					&cpu_to_allnodes_group);
-
-	for (i = 0; i < MAX_NUMNODES; i++) {
-		/* Set up node groups */
-		struct sched_group *sg, *prev;
-		cpumask_t nodemask = node_to_cpumask(i);
-		cpumask_t domainspan;
-		cpumask_t covered = CPU_MASK_NONE;
-		int j;
-
-		cpus_and(nodemask, nodemask, *cpu_map);
-		if (cpus_empty(nodemask)) {
-			sched_group_nodes[i] = NULL;
-			continue;
-		}
-
-		domainspan = sched_domain_node_span(i);
-		cpus_and(domainspan, domainspan, *cpu_map);
-
-		sg = kmalloc_node(sizeof(struct sched_group), GFP_KERNEL, i);
-		if (!sg) {
-			printk(KERN_WARNING "Can not alloc domain group for "
-				"node %d\n", i);
-			goto error;
-		}
-		sched_group_nodes[i] = sg;
-		for_each_cpu_mask(j, nodemask) {
-			struct sched_domain *sd;
-			sd = &per_cpu(node_domains, j);
-			sd->groups = sg;
-		}
-		sg->cpu_power = 0;
-		sg->cpumask = nodemask;
-		sg->next = sg;
-		cpus_or(covered, covered, nodemask);
-		prev = sg;
-
-		for (j = 0; j < MAX_NUMNODES; j++) {
-			cpumask_t tmp, notcovered;
-			int n = (i + j) % MAX_NUMNODES;
-
-			cpus_complement(notcovered, covered);
-			cpus_and(tmp, notcovered, *cpu_map);
-			cpus_and(tmp, tmp, domainspan);
-			if (cpus_empty(tmp))
-				break;
-
-			nodemask = node_to_cpumask(n);
-			cpus_and(tmp, tmp, nodemask);
-			if (cpus_empty(tmp))
-				continue;
-
-			sg = kmalloc_node(sizeof(struct sched_group),
-					  GFP_KERNEL, i);
-			if (!sg) {
-				printk(KERN_WARNING
-				"Can not alloc domain group for node %d\n", j);
-				goto error;
-			}
-			sg->cpu_power = 0;
-			sg->cpumask = tmp;
-			sg->next = prev->next;
-			cpus_or(covered, covered, tmp);
-			prev->next = sg;
-			prev = sg;
-		}
-	}
-#endif
-
-	/* Calculate CPU power for physical packages and nodes */
-#ifdef CONFIG_SCHED_SMT
-	for_each_cpu_mask(i, *cpu_map) {
-		struct sched_domain *sd;
-		sd = &per_cpu(cpu_domains, i);
-		sd->groups->cpu_power = SCHED_LOAD_SCALE;
-	}
-#endif
-#ifdef CONFIG_SCHED_MC
-	for_each_cpu_mask(i, *cpu_map) {
-		int power;
-		struct sched_domain *sd;
-		sd = &per_cpu(core_domains, i);
-		if (sched_smt_power_savings)
-			power = SCHED_LOAD_SCALE * cpus_weight(sd->groups->cpumask);
-		else
-			power = SCHED_LOAD_SCALE + (cpus_weight(sd->groups->cpumask)-1)
-					    * SCHED_LOAD_SCALE / 10;
-		sd->groups->cpu_power = power;
-	}
-#endif
-
-	for_each_cpu_mask(i, *cpu_map) {
-		struct sched_domain *sd;
-#ifdef CONFIG_SCHED_MC
-		sd = &per_cpu(phys_domains, i);
-		if (i != first_cpu(sd->groups->cpumask))
-			continue;
-
-		sd->groups->cpu_power = 0;
-		if (sched_mc_power_savings || sched_smt_power_savings) {
-			int j;
-
- 			for_each_cpu_mask(j, sd->groups->cpumask) {
-				struct sched_domain *sd1;
- 				sd1 = &per_cpu(core_domains, j);
- 				/*
- 			 	 * for each core we will add once
- 				 * to the group in physical domain
- 			 	 */
-  	 			if (j != first_cpu(sd1->groups->cpumask))
- 					continue;
-
- 				if (sched_smt_power_savings)
-   					sd->groups->cpu_power += sd1->groups->cpu_power;
- 				else
-   					sd->groups->cpu_power += SCHED_LOAD_SCALE;
-   			}
- 		} else
- 			/*
- 			 * This has to be < 2 * SCHED_LOAD_SCALE
- 			 * Lets keep it SCHED_LOAD_SCALE, so that
- 			 * while calculating NUMA group's cpu_power
- 			 * we can simply do
- 			 *  numa_group->cpu_power += phys_group->cpu_power;
- 			 *
- 			 * See "only add power once for each physical pkg"
- 			 * comment below
- 			 */
- 			sd->groups->cpu_power = SCHED_LOAD_SCALE;
-#else
-		int power;
-		sd = &per_cpu(phys_domains, i);
-		if (sched_smt_power_savings)
-			power = SCHED_LOAD_SCALE * cpus_weight(sd->groups->cpumask);
-		else
-			power = SCHED_LOAD_SCALE;
-		sd->groups->cpu_power = power;
-#endif
-	}
-
-#ifdef CONFIG_NUMA
-	for (i = 0; i < MAX_NUMNODES; i++)
-		init_numa_sched_groups_power(sched_group_nodes[i]);
-
-	init_numa_sched_groups_power(sched_group_allnodes);
-#endif
-
-	/* Attach the domains */
-	for_each_cpu_mask(i, *cpu_map) {
-		struct sched_domain *sd;
-#ifdef CONFIG_SCHED_SMT
-		sd = &per_cpu(cpu_domains, i);
-#elif defined(CONFIG_SCHED_MC)
-		sd = &per_cpu(core_domains, i);
-#else
-		sd = &per_cpu(phys_domains, i);
-#endif
-		cpu_attach_domain(sd, i);
-	}
-	/*
-	 * Tune cache-hot values:
-	 */
-	calibrate_migration_costs(cpu_map);
-
-	return 0;
-
-error:
-	free_sched_groups(cpu_map);
-	return -ENOMEM;
-}
-/*
- * Set up scheduler domains and groups.  Callers must hold the hotplug lock.
- */
-static int arch_init_sched_domains(const cpumask_t *cpu_map)
-{
-	cpumask_t cpu_default_map;
-	int err;
-
-	/*
-	 * Setup mask for cpus without special case scheduling requirements.
-	 * For now this just excludes isolated cpus, but could be used to
-	 * exclude other special cases in the future.
-	 */
-	cpus_andnot(cpu_default_map, *cpu_map, cpu_isolated_map);
-
-	err = build_sched_domains(&cpu_default_map);
-
-	return err;
-}
-
-static void arch_destroy_sched_domains(const cpumask_t *cpu_map)
-{
-	free_sched_groups(cpu_map);
-}
-
-/*
- * Detach sched domains from a group of cpus specified in cpu_map
- * These cpus will now be attached to the NULL domain
- */
-static void detach_destroy_domains(const cpumask_t *cpu_map)
-{
-	int i;
-
-	for_each_cpu_mask(i, *cpu_map)
-		cpu_attach_domain(NULL, i);
-	synchronize_sched();
-	arch_destroy_sched_domains(cpu_map);
-}
-
-/*
- * Partition sched domains as specified by the cpumasks below.
- * This attaches all cpus from the cpumasks to the NULL domain,
- * waits for a RCU quiescent period, recalculates sched
- * domain information and then attaches them back to the
- * correct sched domains
- * Call with hotplug lock held
- */
-int partition_sched_domains(cpumask_t *partition1, cpumask_t *partition2)
-{
-	cpumask_t change_map;
-	int err = 0;
-
-	cpus_and(*partition1, *partition1, cpu_online_map);
-	cpus_and(*partition2, *partition2, cpu_online_map);
-	cpus_or(change_map, *partition1, *partition2);
-
-	/* Detach sched domains from all of the affected cpus */
-	detach_destroy_domains(&change_map);
-	if (!cpus_empty(*partition1))
-		err = build_sched_domains(partition1);
-	if (!err && !cpus_empty(*partition2))
-		err = build_sched_domains(partition2);
-
-	return err;
-}
-
-#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)
-int arch_reinit_sched_domains(void)
-{
-	int err;
-
-	lock_cpu_hotplug();
-	detach_destroy_domains(&cpu_online_map);
-	err = arch_init_sched_domains(&cpu_online_map);
-	unlock_cpu_hotplug();
-
-	return err;
-}
-
-static ssize_t sched_power_savings_store(const char *buf, size_t count, int smt)
-{
-	int ret;
-
-	if (buf[0] != '0' && buf[0] != '1')
-		return -EINVAL;
-
-	if (smt)
-		sched_smt_power_savings = (buf[0] == '1');
-	else
-		sched_mc_power_savings = (buf[0] == '1');
-
-	ret = arch_reinit_sched_domains();
-
-	return ret ? ret : count;
-}
-
-int sched_create_sysfs_power_savings_entries(struct sysdev_class *cls)
-{
-	int err = 0;
-
-#ifdef CONFIG_SCHED_SMT
-	if (smt_capable())
-		err = sysfs_create_file(&cls->kset.kobj,
-					&attr_sched_smt_power_savings.attr);
-#endif
-#ifdef CONFIG_SCHED_MC
-	if (!err && mc_capable())
-		err = sysfs_create_file(&cls->kset.kobj,
-					&attr_sched_mc_power_savings.attr);
-#endif
-	return err;
-}
-#endif
-
-#ifdef CONFIG_SCHED_MC
-static ssize_t sched_mc_power_savings_show(struct sys_device *dev, char *page)
-{
-	return sprintf(page, "%u\n", sched_mc_power_savings);
-}
-static ssize_t sched_mc_power_savings_store(struct sys_device *dev,
-					    const char *buf, size_t count)
-{
-	return sched_power_savings_store(buf, count, 0);
-}
-SYSDEV_ATTR(sched_mc_power_savings, 0644, sched_mc_power_savings_show,
-	    sched_mc_power_savings_store);
-#endif
-
-#ifdef CONFIG_SCHED_SMT
-static ssize_t sched_smt_power_savings_show(struct sys_device *dev, char *page)
-{
-	return sprintf(page, "%u\n", sched_smt_power_savings);
-}
-static ssize_t sched_smt_power_savings_store(struct sys_device *dev,
-					     const char *buf, size_t count)
-{
-	return sched_power_savings_store(buf, count, 1);
-}
-SYSDEV_ATTR(sched_smt_power_savings, 0644, sched_smt_power_savings_show,
-	    sched_smt_power_savings_store);
-#endif
-
-
-#ifdef CONFIG_HOTPLUG_CPU
-/*
- * Force a reinitialization of the sched domains hierarchy.  The domains
- * and groups cannot be updated in place without racing with the balancing
- * code, so we temporarily attach all running cpus to the NULL domain
- * which will prevent rebalancing while the sched domains are recalculated.
- */
-static int update_sched_domains(struct notifier_block *nfb,
-				unsigned long action, void *hcpu)
-{
-	switch (action) {
-	case CPU_UP_PREPARE:
-	case CPU_DOWN_PREPARE:
-		detach_destroy_domains(&cpu_online_map);
-		return NOTIFY_OK;
-
-	case CPU_UP_CANCELED:
-	case CPU_DOWN_FAILED:
-	case CPU_ONLINE:
-	case CPU_DEAD:
-		/*
-		 * Fall through and re-initialise the domains.
-		 */
-		break;
-	default:
-		return NOTIFY_DONE;
-	}
-
-	/* The hotplug lock is already held by cpu_up/cpu_down */
-	arch_init_sched_domains(&cpu_online_map);
-
-	return NOTIFY_OK;
-}
-#endif
-
-void __init sched_init_smp(void)
-{
-	lock_cpu_hotplug();
-	arch_init_sched_domains(&cpu_online_map);
-	unlock_cpu_hotplug();
-	/* XXX: Theoretical race here - CPU may be hotplugged now */
-	hotcpu_notifier(update_sched_domains, 0);
-	init_sched_domain_sysctl();
-}
-#else
-void __init sched_init_smp(void)
-{
-}
-#endif /* CONFIG_SMP */
-
-int in_sched_functions(unsigned long addr)
-{
-	/* Linker adds these: start and end of __sched functions */
-	extern char __sched_text_start[], __sched_text_end[];
-
-	return in_lock_functions(addr) ||
-		(addr >= (unsigned long)__sched_text_start
-		&& addr < (unsigned long)__sched_text_end);
-}
-
-void __init sched_init(void)
-{
-	int i, j, k;
-
-	for_each_possible_cpu(i) {
-		struct prio_array *array;
-		struct rq *rq;
-
-		rq = cpu_rq(i);
-		spin_lock_init(&rq->lock);
-		lockdep_set_class(&rq->lock, &rq->rq_lock_key);
-		rq->nr_running = 0;
-		rq->active = rq->arrays;
-		rq->expired = rq->arrays + 1;
-		rq->best_expired_prio = MAX_PRIO;
-
-#ifdef CONFIG_SMP
-		rq->sd = NULL;
-		for (j = 1; j < 3; j++)
-			rq->cpu_load[j] = 0;
-		rq->active_balance = 0;
-		rq->push_cpu = 0;
-		rq->migration_thread = NULL;
-		INIT_LIST_HEAD(&rq->migration_queue);
-#endif
-		atomic_set(&rq->nr_iowait, 0);
-
-		for (j = 0; j < 2; j++) {
-			array = rq->arrays + j;
-			for (k = 0; k < MAX_PRIO; k++) {
-				INIT_LIST_HEAD(array->queue + k);
-				__clear_bit(k, array->bitmap);
-			}
-			// delimiter for bitsearch
-			__set_bit(MAX_PRIO, array->bitmap);
-		}
-	}
-
-	set_load_weight(&init_task);
-	/*
-	 * The boot idle thread does lazy MMU switching as well:
-	 */
-	atomic_inc(&init_mm.mm_count);
-	enter_lazy_tlb(&init_mm, current);
-
-	/*
-	 * Make us the idle thread. Technically, schedule() should not be
-	 * called from this thread, however somewhere below it might be,
-	 * but because we are the idle thread, we just pick up running again
-	 * when this runqueue becomes "idle".
-	 */
-	init_idle(current, smp_processor_id());
-}
-
-#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP
-void __might_sleep(char *file, int line)
-{
-#ifdef in_atomic
-	static unsigned long prev_jiffy;	/* ratelimiting */
-
-	if ((in_atomic() || irqs_disabled()) &&
-	    system_state == SYSTEM_RUNNING && !oops_in_progress) {
-		if (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)
-			return;
-		prev_jiffy = jiffies;
-		printk(KERN_ERR "BUG: sleeping function called from invalid"
-				" context at %s:%d\n", file, line);
-		printk("in_atomic():%d, irqs_disabled():%d\n",
-			in_atomic(), irqs_disabled());
-		dump_stack();
-	}
-#endif
-}
-EXPORT_SYMBOL(__might_sleep);
-#endif
-
-#ifdef CONFIG_MAGIC_SYSRQ
-void normalize_rt_tasks(void)
-{
-	struct prio_array *array;
-	struct task_struct *p;
-	unsigned long flags;
-	struct rq *rq;
-
-	read_lock_irq(&tasklist_lock);
-	for_each_process(p) {
-		if (!rt_task(p))
-			continue;
-
-		spin_lock_irqsave(&p->pi_lock, flags);
-		rq = __task_rq_lock(p);
-
-		array = p->array;
-		if (array)
-			deactivate_task(p, task_rq(p));
-		__setscheduler(p, SCHED_NORMAL, 0);
-		if (array) {
-			__activate_task(p, task_rq(p));
-			resched_task(rq->curr);
-		}
-
-		__task_rq_unlock(rq);
-		spin_unlock_irqrestore(&p->pi_lock, flags);
-	}
-	read_unlock_irq(&tasklist_lock);
-}
-
-#endif /* CONFIG_MAGIC_SYSRQ */
-
-#ifdef CONFIG_IA64
-/*
- * These functions are only useful for the IA64 MCA handling.
- *
- * They can only be called when the whole system has been
- * stopped - every CPU needs to be quiescent, and no scheduling
- * activity can take place. Using them for anything else would
- * be a serious bug, and as a result, they aren't even visible
- * under any other configuration.
- */
-
-/**
- * curr_task - return the current task for a given cpu.
- * @cpu: the processor in question.
- *
- * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!
- */
-struct task_struct *curr_task(int cpu)
-{
-	return cpu_curr(cpu);
-}
-
-/**
- * set_curr_task - set the current task for a given cpu.
- * @cpu: the processor in question.
- * @p: the task pointer to set.
- *
- * Description: This function must only be used when non-maskable interrupts
- * are serviced on a separate stack.  It allows the architecture to switch the
- * notion of the current task on a cpu in a non-blocking manner.  This function
- * must be called with all CPU's synchronized, and interrupts disabled, the
- * and caller must save the original value of the current task (see
- * curr_task() above) and restore that value before reenabling interrupts and
- * re-starting the system.
- *
- * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!
- */
-void set_curr_task(int cpu, struct task_struct *p)
-{
-	cpu_curr(cpu) = p;
-}
-
-#endif
diff -urN oldtree/kernel/sched_ingosched.c newtree/kernel/sched_ingosched.c
--- oldtree/kernel/sched_ingosched.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/kernel/sched_ingosched.c	2006-07-12 19:00:11.000000000 -0400
@@ -0,0 +1,6973 @@
+/*
+ *  kernel/sched_ingosched.c
+ *  ( Ingosched CPU Scheduler v2.6.17-mm6 )
+ */
+
+/* Ingosched Scheduler Code Begin: */
+
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/nmi.h>
+#include <linux/init.h>
+#include <asm/uaccess.h>
+#include <linux/highmem.h>
+#include <linux/smp_lock.h>
+#include <asm/mmu_context.h>
+#include <linux/interrupt.h>
+#include <linux/capability.h>
+#include <linux/completion.h>
+#include <linux/kernel_stat.h>
+#include <linux/debug_locks.h>
+#include <linux/security.h>
+#include <linux/notifier.h>
+#include <linux/profile.h>
+#include <linux/suspend.h>
+#include <linux/vmalloc.h>
+#include <linux/blkdev.h>
+#include <linux/delay.h>
+#include <linux/smp.h>
+#include <linux/threads.h>
+#include <linux/timer.h>
+#include <linux/rcupdate.h>
+#include <linux/cpu.h>
+#include <linux/cpuset.h>
+#include <linux/percpu.h>
+#include <linux/kthread.h>
+#include <linux/seq_file.h>
+#include <linux/sysctl.h>
+#include <linux/syscalls.h>
+#include <linux/times.h>
+#include <linux/acct.h>
+#include <linux/kprobes.h>
+#include <linux/delayacct.h>
+#include <asm/tlb.h>
+
+#include <asm/unistd.h>
+
+/*
+ * Convert user-nice values [ -20 ... 0 ... 19 ]
+ * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],
+ * and back.
+ */
+#define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 20)
+#define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 20)
+#define TASK_NICE(p)		PRIO_TO_NICE((p)->static_prio)
+
+/*
+ * 'User priority' is the nice value converted to something we
+ * can work with better when scaling various scheduler parameters,
+ * it's a [ 0 ... 39 ] range.
+ */
+#define USER_PRIO(p)		((p)-MAX_RT_PRIO)
+#define TASK_USER_PRIO(p)	USER_PRIO((p)->static_prio)
+#define MAX_USER_PRIO		(USER_PRIO(MAX_PRIO))
+
+/*
+ * Some helpers for converting nanosecond timing to jiffy resolution
+ */
+#define NS_TO_JIFFIES(TIME)	((TIME) / (1000000000 / HZ))
+#define JIFFIES_TO_NS(TIME)	((TIME) * (1000000000 / HZ))
+
+/*
+ * These are the 'tuning knobs' of the scheduler:
+ *
+ * Minimum timeslice is 5 msecs (or 1 jiffy, whichever is larger),
+ * default timeslice is 100 msecs, maximum timeslice is 800 msecs.
+ * Timeslices get refilled after they expire.
+ */
+#define MIN_TIMESLICE		max(5 * HZ / 1000, 1)
+#define DEF_TIMESLICE		(100 * HZ / 1000)
+#define ON_RUNQUEUE_WEIGHT	 30
+#define CHILD_PENALTY		 95
+#define PARENT_PENALTY		100
+#define EXIT_WEIGHT		  3
+#define PRIO_BONUS_RATIO	 25
+#define MAX_BONUS		(MAX_USER_PRIO * PRIO_BONUS_RATIO / 100)
+#define INTERACTIVE_DELTA	  2
+#define MAX_SLEEP_AVG		(DEF_TIMESLICE * MAX_BONUS)
+#define STARVATION_LIMIT	(MAX_SLEEP_AVG)
+#define NS_MAX_SLEEP_AVG	(JIFFIES_TO_NS(MAX_SLEEP_AVG))
+
+/*
+ * If a task is 'interactive' then we reinsert it in the active
+ * array after it has expired its current timeslice. (it will not
+ * continue to run immediately, it will still roundrobin with
+ * other interactive tasks.)
+ *
+ * This part scales the interactivity limit depending on niceness.
+ *
+ * We scale it linearly, offset by the INTERACTIVE_DELTA delta.
+ * Here are a few examples of different nice levels:
+ *
+ *  TASK_INTERACTIVE(-20): [1,1,1,1,1,1,1,1,1,0,0]
+ *  TASK_INTERACTIVE(-10): [1,1,1,1,1,1,1,0,0,0,0]
+ *  TASK_INTERACTIVE(  0): [1,1,1,1,0,0,0,0,0,0,0]
+ *  TASK_INTERACTIVE( 10): [1,1,0,0,0,0,0,0,0,0,0]
+ *  TASK_INTERACTIVE( 19): [0,0,0,0,0,0,0,0,0,0,0]
+ *
+ * (the X axis represents the possible -5 ... 0 ... +5 dynamic
+ *  priority range a task can explore, a value of '1' means the
+ *  task is rated interactive.)
+ *
+ * Ie. nice +19 tasks can never get 'interactive' enough to be
+ * reinserted into the active array. And only heavily CPU-hog nice -20
+ * tasks will be expired. Default nice 0 tasks are somewhere between,
+ * it takes some effort for them to get interactive, but it's not
+ * too hard.
+ */
+
+#define CURRENT_BONUS(p) \
+	(NS_TO_JIFFIES((p)->sleep_avg) * MAX_BONUS / \
+		MAX_SLEEP_AVG)
+
+#define GRANULARITY	(10 * HZ / 1000 ? : 1)
+
+#ifdef CONFIG_SMP
+#define TIMESLICE_GRANULARITY(p)	(GRANULARITY * \
+		(1 << (((MAX_BONUS - CURRENT_BONUS(p)) ? : 1) - 1)) * \
+			num_online_cpus())
+#else
+#define TIMESLICE_GRANULARITY(p)	(GRANULARITY * \
+		(1 << (((MAX_BONUS - CURRENT_BONUS(p)) ? : 1) - 1)))
+#endif
+
+#define SCALE(v1,v1_max,v2_max) \
+	(v1) * (v2_max) / (v1_max)
+
+#define DELTA(p) \
+	(SCALE(TASK_NICE(p) + 20, 40, MAX_BONUS) - 20 * MAX_BONUS / 40 + \
+		INTERACTIVE_DELTA)
+
+#define TASK_INTERACTIVE(p) \
+	((p)->prio <= (p)->static_prio - DELTA(p))
+
+#define INTERACTIVE_SLEEP(p) \
+	(JIFFIES_TO_NS(MAX_SLEEP_AVG * \
+		(MAX_BONUS / 2 + DELTA((p)) + 1) / MAX_BONUS - 1))
+
+#define TASK_PREEMPTS_CURR(p, rq) \
+	((p)->prio < (rq)->curr->prio)
+
+/*
+ * task_timeslice() scales user-nice values [ -20 ... 0 ... 19 ]
+ * to time slice values: [800ms ... 100ms ... 5ms]
+ *
+ * The higher a thread's priority, the bigger timeslices
+ * it gets during one round of execution. But even the lowest
+ * priority thread gets MIN_TIMESLICE worth of execution time.
+ */
+
+#define SCALE_PRIO(x, prio) \
+	max(x * (MAX_PRIO - prio) / (MAX_USER_PRIO / 2), MIN_TIMESLICE)
+
+static unsigned int static_prio_timeslice(int static_prio)
+{
+	if (static_prio < NICE_TO_PRIO(0))
+		return SCALE_PRIO(DEF_TIMESLICE * 4, static_prio);
+	else
+		return SCALE_PRIO(DEF_TIMESLICE, static_prio);
+}
+
+static inline unsigned int task_timeslice(struct task_struct *p)
+{
+	return static_prio_timeslice(p->static_prio);
+}
+
+/*
+ * These are the runqueue data structures:
+ */
+
+struct prio_array {
+	unsigned int nr_active;
+	DECLARE_BITMAP(bitmap, MAX_PRIO+1); /* include 1 bit for delimiter */
+	struct list_head queue[MAX_PRIO];
+};
+
+/*
+ * This is the main, per-CPU runqueue data structure.
+ *
+ * Locking rule: those places that want to lock multiple runqueues
+ * (such as the load balancing or the thread migration code), lock
+ * acquire operations must be ordered by ascending &runqueue.
+ */
+struct rq {
+	spinlock_t lock;
+
+	/*
+	 * nr_running and cpu_load should be in the same cacheline because
+	 * remote CPUs use both these fields when doing load calculation.
+	 */
+	unsigned long nr_running;
+	unsigned long raw_weighted_load;
+#ifdef CONFIG_SMP
+	unsigned long cpu_load[3];
+#endif
+	unsigned long long nr_switches;
+
+	/*
+	 * This is part of a global counter where only the total sum
+	 * over all CPUs matters. A task can increase this counter on
+	 * one CPU and if it got migrated afterwards it may decrease
+	 * it on another CPU. Always updated under the runqueue lock:
+	 */
+	unsigned long nr_uninterruptible;
+
+	unsigned long expired_timestamp;
+	unsigned long long timestamp_last_tick;
+	struct task_struct *curr, *idle;
+	struct mm_struct *prev_mm;
+	struct prio_array *active, *expired, arrays[2];
+	int best_expired_prio;
+	atomic_t nr_iowait;
+
+#ifdef CONFIG_SMP
+	struct sched_domain *sd;
+
+	/* For active balancing */
+	int active_balance;
+	int push_cpu;
+
+	struct task_struct *migration_thread;
+	struct list_head migration_queue;
+#endif
+
+#ifdef CONFIG_SCHEDSTATS
+	/* latency stats */
+	struct sched_info rq_sched_info;
+
+	/* sys_sched_yield() stats */
+	unsigned long yld_exp_empty;
+	unsigned long yld_act_empty;
+	unsigned long yld_both_empty;
+	unsigned long yld_cnt;
+
+	/* schedule() stats */
+	unsigned long sched_switch;
+	unsigned long sched_cnt;
+	unsigned long sched_goidle;
+
+	/* try_to_wake_up() stats */
+	unsigned long ttwu_cnt;
+	unsigned long ttwu_local;
+#endif
+	struct lock_class_key rq_lock_key;
+};
+
+static DEFINE_PER_CPU(struct rq, runqueues);
+
+/*
+ * The domain tree (rq->sd) is protected by RCU's quiescent state transition.
+ * See detach_destroy_domains: synchronize_sched for details.
+ *
+ * The domain tree of any CPU may only be accessed from within
+ * preempt-disabled sections.
+ */
+#define for_each_domain(cpu, __sd) \
+	for (__sd = rcu_dereference(cpu_rq(cpu)->sd); __sd; __sd = __sd->parent)
+
+#define cpu_rq(cpu)		(&per_cpu(runqueues, (cpu)))
+#define this_rq()		(&__get_cpu_var(runqueues))
+#define task_rq(p)		cpu_rq(task_cpu(p))
+#define cpu_curr(cpu)		(cpu_rq(cpu)->curr)
+
+#ifndef prepare_arch_switch
+# define prepare_arch_switch(next)	do { } while (0)
+#endif
+#ifndef finish_arch_switch
+# define finish_arch_switch(prev)	do { } while (0)
+#endif
+
+#ifndef __ARCH_WANT_UNLOCKED_CTXSW
+static inline int task_running(struct rq *rq, struct task_struct *p)
+{
+	return rq->curr == p;
+}
+
+static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
+{
+}
+
+static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
+{
+#ifdef CONFIG_DEBUG_SPINLOCK
+	/* this is a valid case when another task releases the spinlock */
+	rq->lock.owner = current;
+#endif
+	/*
+	 * If we are tracking spinlock dependencies then we have to
+	 * fix up the runqueue lock - which gets 'carried over' from
+	 * prev into current:
+	 */
+	spin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);
+
+	spin_unlock_irq(&rq->lock);
+}
+
+#else /* __ARCH_WANT_UNLOCKED_CTXSW */
+static inline int task_running(struct rq *rq, struct task_struct *p)
+{
+#ifdef CONFIG_SMP
+	return p->oncpu;
+#else
+	return rq->curr == p;
+#endif
+}
+
+static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
+{
+#ifdef CONFIG_SMP
+	/*
+	 * We can optimise this out completely for !SMP, because the
+	 * SMP rebalancing from interrupt is the only thing that cares
+	 * here.
+	 */
+	next->oncpu = 1;
+#endif
+#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW
+	spin_unlock_irq(&rq->lock);
+#else
+	spin_unlock(&rq->lock);
+#endif
+}
+
+static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
+{
+#ifdef CONFIG_SMP
+	/*
+	 * After ->oncpu is cleared, the task can be moved to a different CPU.
+	 * We must ensure this doesn't happen until the switch is completely
+	 * finished.
+	 */
+	smp_wmb();
+	prev->oncpu = 0;
+#endif
+#ifndef __ARCH_WANT_INTERRUPTS_ON_CTXSW
+	local_irq_enable();
+#endif
+}
+#endif /* __ARCH_WANT_UNLOCKED_CTXSW */
+
+/*
+ * __task_rq_lock - lock the runqueue a given task resides on.
+ * Must be called interrupts disabled.
+ */
+static inline struct rq *__task_rq_lock(struct task_struct *p)
+	__acquires(rq->lock)
+{
+	struct rq *rq;
+
+repeat_lock_task:
+	rq = task_rq(p);
+	spin_lock(&rq->lock);
+	if (unlikely(rq != task_rq(p))) {
+		spin_unlock(&rq->lock);
+		goto repeat_lock_task;
+	}
+	return rq;
+}
+
+/*
+ * task_rq_lock - lock the runqueue a given task resides on and disable
+ * interrupts.  Note the ordering: we can safely lookup the task_rq without
+ * explicitly disabling preemption.
+ */
+static struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)
+	__acquires(rq->lock)
+{
+	struct rq *rq;
+
+repeat_lock_task:
+	local_irq_save(*flags);
+	rq = task_rq(p);
+	spin_lock(&rq->lock);
+	if (unlikely(rq != task_rq(p))) {
+		spin_unlock_irqrestore(&rq->lock, *flags);
+		goto repeat_lock_task;
+	}
+	return rq;
+}
+
+static inline void __task_rq_unlock(struct rq *rq)
+	__releases(rq->lock)
+{
+	spin_unlock(&rq->lock);
+}
+
+static inline void task_rq_unlock(struct rq *rq, unsigned long *flags)
+	__releases(rq->lock)
+{
+	spin_unlock_irqrestore(&rq->lock, *flags);
+}
+
+#ifdef CONFIG_SCHEDSTATS
+/*
+ * bump this up when changing the output format or the meaning of an existing
+ * format, so that tools can adapt (or abort)
+ */
+#define SCHEDSTAT_VERSION 12
+
+static int show_schedstat(struct seq_file *seq, void *v)
+{
+	int cpu;
+
+	seq_printf(seq, "version %d\n", SCHEDSTAT_VERSION);
+	seq_printf(seq, "timestamp %lu\n", jiffies);
+	for_each_online_cpu(cpu) {
+		struct rq *rq = cpu_rq(cpu);
+#ifdef CONFIG_SMP
+		struct sched_domain *sd;
+		int dcnt = 0;
+#endif
+
+		/* runqueue-specific stats */
+		seq_printf(seq,
+		    "cpu%d %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu",
+		    cpu, rq->yld_both_empty,
+		    rq->yld_act_empty, rq->yld_exp_empty, rq->yld_cnt,
+		    rq->sched_switch, rq->sched_cnt, rq->sched_goidle,
+		    rq->ttwu_cnt, rq->ttwu_local,
+		    rq->rq_sched_info.cpu_time,
+		    rq->rq_sched_info.run_delay, rq->rq_sched_info.pcnt);
+
+		seq_printf(seq, "\n");
+
+#ifdef CONFIG_SMP
+		/* domain-specific stats */
+		preempt_disable();
+		for_each_domain(cpu, sd) {
+			enum idle_type itype;
+			char mask_str[NR_CPUS];
+
+			cpumask_scnprintf(mask_str, NR_CPUS, sd->span);
+			seq_printf(seq, "domain%d %s", dcnt++, mask_str);
+			for (itype = SCHED_IDLE; itype < MAX_IDLE_TYPES;
+					itype++) {
+				seq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu",
+				    sd->lb_cnt[itype],
+				    sd->lb_balanced[itype],
+				    sd->lb_failed[itype],
+				    sd->lb_imbalance[itype],
+				    sd->lb_gained[itype],
+				    sd->lb_hot_gained[itype],
+				    sd->lb_nobusyq[itype],
+				    sd->lb_nobusyg[itype]);
+			}
+			seq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu\n",
+			    sd->alb_cnt, sd->alb_failed, sd->alb_pushed,
+			    sd->sbe_cnt, sd->sbe_balanced, sd->sbe_pushed,
+			    sd->sbf_cnt, sd->sbf_balanced, sd->sbf_pushed,
+			    sd->ttwu_wake_remote, sd->ttwu_move_affine, sd->ttwu_move_balance);
+		}
+		preempt_enable();
+#endif
+	}
+	return 0;
+}
+
+static int schedstat_open(struct inode *inode, struct file *file)
+{
+	unsigned int size = PAGE_SIZE * (1 + num_online_cpus() / 32);
+	char *buf = kmalloc(size, GFP_KERNEL);
+	struct seq_file *m;
+	int res;
+
+	if (!buf)
+		return -ENOMEM;
+	res = single_open(file, show_schedstat, NULL);
+	if (!res) {
+		m = file->private_data;
+		m->buf = buf;
+		m->size = size;
+	} else
+		kfree(buf);
+	return res;
+}
+
+struct file_operations proc_schedstat_operations = {
+	.open    = schedstat_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+};
+
+/*
+ * Expects runqueue lock to be held for atomicity of update
+ */
+static inline void
+rq_sched_info_arrive(struct rq *rq, unsigned long delta_jiffies)
+{
+	if (rq) {
+		rq->rq_sched_info.run_delay += delta_jiffies;
+		rq->rq_sched_info.pcnt++;
+	}
+}
+
+/*
+ * Expects runqueue lock to be held for atomicity of update
+ */
+static inline void
+rq_sched_info_depart(struct rq *rq, unsigned long delta_jiffies)
+{
+	if (rq)
+		rq->rq_sched_info.cpu_time += delta_jiffies;
+}
+# define schedstat_inc(rq, field)	do { (rq)->field++; } while (0)
+# define schedstat_add(rq, field, amt)	do { (rq)->field += (amt); } while (0)
+#else /* !CONFIG_SCHEDSTATS */
+static inline void
+rq_sched_info_arrive(struct rq *rq, unsigned long delta_jiffies)
+{}
+static inline void
+rq_sched_info_depart(struct rq *rq, unsigned long delta_jiffies)
+{}
+# define schedstat_inc(rq, field)	do { } while (0)
+# define schedstat_add(rq, field, amt)	do { } while (0)
+#endif
+
+/*
+ * rq_lock - lock a given runqueue and disable interrupts.
+ */
+static inline struct rq *this_rq_lock(void)
+	__acquires(rq->lock)
+{
+	struct rq *rq;
+
+	local_irq_disable();
+	rq = this_rq();
+	spin_lock(&rq->lock);
+
+	return rq;
+}
+
+#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
+/*
+ * Called when a process is dequeued from the active array and given
+ * the cpu.  We should note that with the exception of interactive
+ * tasks, the expired queue will become the active queue after the active
+ * queue is empty, without explicitly dequeuing and requeuing tasks in the
+ * expired queue.  (Interactive tasks may be requeued directly to the
+ * active queue, thus delaying tasks in the expired queue from running;
+ * see scheduler_tick()).
+ *
+ * This function is only called from sched_info_arrive(), rather than
+ * dequeue_task(). Even though a task may be queued and dequeued multiple
+ * times as it is shuffled about, we're really interested in knowing how
+ * long it was from the *first* time it was queued to the time that it
+ * finally hit a cpu.
+ */
+static inline void sched_info_dequeued(struct task_struct *t)
+{
+	t->sched_info.last_queued = 0;
+}
+
+/*
+ * Called when a task finally hits the cpu.  We can now calculate how
+ * long it was waiting to run.  We also note when it began so that we
+ * can keep stats on how long its timeslice is.
+ */
+static void sched_info_arrive(struct task_struct *t)
+{
+	unsigned long now = jiffies, delta_jiffies = 0;
+
+	if (t->sched_info.last_queued)
+		delta_jiffies = now - t->sched_info.last_queued;
+	sched_info_dequeued(t);
+	t->sched_info.run_delay += delta_jiffies;
+	t->sched_info.last_arrival = now;
+	t->sched_info.pcnt++;
+
+	rq_sched_info_arrive(task_rq(t), delta_jiffies);
+}
+
+/*
+ * Called when a process is queued into either the active or expired
+ * array.  The time is noted and later used to determine how long we
+ * had to wait for us to reach the cpu.  Since the expired queue will
+ * become the active queue after active queue is empty, without dequeuing
+ * and requeuing any tasks, we are interested in queuing to either. It
+ * is unusual but not impossible for tasks to be dequeued and immediately
+ * requeued in the same or another array: this can happen in sched_yield(),
+ * set_user_nice(), and even load_balance() as it moves tasks from runqueue
+ * to runqueue.
+ *
+ * This function is only called from enqueue_task(), but also only updates
+ * the timestamp if it is already not set.  It's assumed that
+ * sched_info_dequeued() will clear that stamp when appropriate.
+ */
+static inline void sched_info_queued(struct task_struct *t)
+{
+	if (unlikely(sched_info_on()))
+		if (!t->sched_info.last_queued)
+			t->sched_info.last_queued = jiffies;
+}
+
+/*
+ * Called when a process ceases being the active-running process, either
+ * voluntarily or involuntarily.  Now we can calculate how long we ran.
+ */
+static inline void sched_info_depart(struct task_struct *t)
+{
+	unsigned long delta_jiffies = jiffies - t->sched_info.last_arrival;
+
+	t->sched_info.cpu_time += delta_jiffies;
+	rq_sched_info_depart(task_rq(t), delta_jiffies);
+}
+
+/*
+ * Called when tasks are switched involuntarily due, typically, to expiring
+ * their time slice.  (This may also be called when switching to or from
+ * the idle task.)  We are only called when prev != next.
+ */
+static inline void
+__sched_info_switch(struct task_struct *prev, struct task_struct *next)
+{
+	struct rq *rq = task_rq(prev);
+
+	/*
+	 * prev now departs the cpu.  It's not interesting to record
+	 * stats about how efficient we were at scheduling the idle
+	 * process, however.
+	 */
+	if (prev != rq->idle)
+		sched_info_depart(prev);
+
+	if (next != rq->idle)
+		sched_info_arrive(next);
+}
+static inline void
+sched_info_switch(struct task_struct *prev, struct task_struct *next)
+{
+	if (unlikely(sched_info_on()))
+		__sched_info_switch(prev, next);
+}
+#else
+#define sched_info_queued(t)		do { } while (0)
+#define sched_info_switch(t, next)	do { } while (0)
+#endif /* CONFIG_SCHEDSTATS || CONFIG_TASK_DELAY_ACCT */
+
+/*
+ * Adding/removing a task to/from a priority array:
+ */
+static void dequeue_task(struct task_struct *p, struct prio_array *array)
+{
+	array->nr_active--;
+	list_del(&p->run_list);
+	if (list_empty(array->queue + p->prio))
+		__clear_bit(p->prio, array->bitmap);
+}
+
+static void enqueue_task(struct task_struct *p, struct prio_array *array)
+{
+	sched_info_queued(p);
+	list_add_tail(&p->run_list, array->queue + p->prio);
+	__set_bit(p->prio, array->bitmap);
+	array->nr_active++;
+	p->array = array;
+}
+
+/*
+ * Put task to the end of the run list without the overhead of dequeue
+ * followed by enqueue.
+ */
+static void requeue_task(struct task_struct *p, struct prio_array *array)
+{
+	list_move_tail(&p->run_list, array->queue + p->prio);
+}
+
+static inline void
+enqueue_task_head(struct task_struct *p, struct prio_array *array)
+{
+	list_add(&p->run_list, array->queue + p->prio);
+	__set_bit(p->prio, array->bitmap);
+	array->nr_active++;
+	p->array = array;
+}
+
+/*
+ * __normal_prio - return the priority that is based on the static
+ * priority but is modified by bonuses/penalties.
+ *
+ * We scale the actual sleep average [0 .... MAX_SLEEP_AVG]
+ * into the -5 ... 0 ... +5 bonus/penalty range.
+ *
+ * We use 25% of the full 0...39 priority range so that:
+ *
+ * 1) nice +19 interactive tasks do not preempt nice 0 CPU hogs.
+ * 2) nice -20 CPU hogs do not get preempted by nice 0 tasks.
+ *
+ * Both properties are important to certain workloads.
+ */
+
+static inline int __normal_prio(struct task_struct *p)
+{
+	int bonus, prio;
+
+	bonus = CURRENT_BONUS(p) - MAX_BONUS / 2;
+
+	prio = p->static_prio - bonus;
+	if (prio < MAX_RT_PRIO)
+		prio = MAX_RT_PRIO;
+	if (prio > MAX_PRIO-1)
+		prio = MAX_PRIO-1;
+	return prio;
+}
+
+/*
+ * To aid in avoiding the subversion of "niceness" due to uneven distribution
+ * of tasks with abnormal "nice" values across CPUs the contribution that
+ * each task makes to its run queue's load is weighted according to its
+ * scheduling class and "nice" value.  For SCHED_NORMAL tasks this is just a
+ * scaled version of the new time slice allocation that they receive on time
+ * slice expiry etc.
+ */
+
+/*
+ * Assume: static_prio_timeslice(NICE_TO_PRIO(0)) == DEF_TIMESLICE
+ * If static_prio_timeslice() is ever changed to break this assumption then
+ * this code will need modification
+ */
+#define TIME_SLICE_NICE_ZERO DEF_TIMESLICE
+#define LOAD_WEIGHT(lp) \
+	(((lp) * SCHED_LOAD_SCALE) / TIME_SLICE_NICE_ZERO)
+#define PRIO_TO_LOAD_WEIGHT(prio) \
+	LOAD_WEIGHT(static_prio_timeslice(prio))
+#define RTPRIO_TO_LOAD_WEIGHT(rp) \
+	(PRIO_TO_LOAD_WEIGHT(MAX_RT_PRIO) + LOAD_WEIGHT(rp))
+
+static void set_load_weight(struct task_struct *p)
+{
+	if (has_rt_policy(p)) {
+#ifdef CONFIG_SMP
+		if (p == task_rq(p)->migration_thread)
+			/*
+			 * The migration thread does the actual balancing.
+			 * Giving its load any weight will skew balancing
+			 * adversely.
+			 */
+			p->load_weight = 0;
+		else
+#endif
+			p->load_weight = RTPRIO_TO_LOAD_WEIGHT(p->rt_priority);
+	} else
+		p->load_weight = PRIO_TO_LOAD_WEIGHT(p->static_prio);
+}
+
+static inline void
+inc_raw_weighted_load(struct rq *rq, const struct task_struct *p)
+{
+	rq->raw_weighted_load += p->load_weight;
+}
+
+static inline void
+dec_raw_weighted_load(struct rq *rq, const struct task_struct *p)
+{
+	rq->raw_weighted_load -= p->load_weight;
+}
+
+static inline void inc_nr_running(struct task_struct *p, struct rq *rq)
+{
+	rq->nr_running++;
+	inc_raw_weighted_load(rq, p);
+}
+
+static inline void dec_nr_running(struct task_struct *p, struct rq *rq)
+{
+	rq->nr_running--;
+	dec_raw_weighted_load(rq, p);
+}
+
+/*
+ * Calculate the expected normal priority: i.e. priority
+ * without taking RT-inheritance into account. Might be
+ * boosted by interactivity modifiers. Changes upon fork,
+ * setprio syscalls, and whenever the interactivity
+ * estimator recalculates.
+ */
+static inline int normal_prio(struct task_struct *p)
+{
+	int prio;
+
+	if (has_rt_policy(p))
+		prio = MAX_RT_PRIO-1 - p->rt_priority;
+	else
+		prio = __normal_prio(p);
+	return prio;
+}
+
+/*
+ * Calculate the current priority, i.e. the priority
+ * taken into account by the scheduler. This value might
+ * be boosted by RT tasks, or might be boosted by
+ * interactivity modifiers. Will be RT if the task got
+ * RT-boosted. If not then it returns p->normal_prio.
+ */
+static int effective_prio(struct task_struct *p)
+{
+	p->normal_prio = normal_prio(p);
+	/*
+	 * If we are RT tasks or we were boosted to RT priority,
+	 * keep the priority unchanged. Otherwise, update priority
+	 * to the normal priority:
+	 */
+	if (!rt_prio(p->prio))
+		return p->normal_prio;
+	return p->prio;
+}
+
+/*
+ * __activate_task - move a task to the runqueue.
+ */
+static void __activate_task(struct task_struct *p, struct rq *rq)
+{
+	struct prio_array *target = rq->active;
+
+	if (batch_task(p))
+		target = rq->expired;
+	enqueue_task(p, target);
+	inc_nr_running(p, rq);
+}
+
+/*
+ * __activate_idle_task - move idle task to the _front_ of runqueue.
+ */
+static inline void __activate_idle_task(struct task_struct *p, struct rq *rq)
+{
+	enqueue_task_head(p, rq->active);
+	inc_nr_running(p, rq);
+}
+
+/*
+ * Recalculate p->normal_prio and p->prio after having slept,
+ * updating the sleep-average too:
+ */
+static int recalc_task_prio(struct task_struct *p, unsigned long long now)
+{
+	/* Caller must always ensure 'now >= p->timestamp' */
+	unsigned long sleep_time = now - p->timestamp;
+
+	if (batch_task(p))
+		sleep_time = 0;
+
+	if (likely(sleep_time > 0)) {
+		/*
+		 * This ceiling is set to the lowest priority that would allow
+		 * a task to be reinserted into the active array on timeslice
+		 * completion.
+		 */
+		unsigned long ceiling = INTERACTIVE_SLEEP(p);
+
+		if (p->mm && sleep_time > ceiling && p->sleep_avg < ceiling) {
+			/*
+			 * Prevents user tasks from achieving best priority
+			 * with one single large enough sleep.
+			 */
+			p->sleep_avg = ceiling;
+			/*
+			 * Using INTERACTIVE_SLEEP() as a ceiling places a
+			 * nice(0) task 1ms sleep away from promotion, and
+			 * gives it 700ms to round-robin with no chance of
+			 * being demoted.  This is more than generous, so
+			 * mark this sleep as non-interactive to prevent the
+			 * on-runqueue bonus logic from intervening should
+			 * this task not receive cpu immediately.
+			 */
+			p->sleep_type = SLEEP_NONINTERACTIVE;
+		} else {
+			/*
+			 * Tasks waking from uninterruptible sleep are
+			 * limited in their sleep_avg rise as they
+			 * are likely to be waiting on I/O
+			 */
+			if (p->sleep_type == SLEEP_NONINTERACTIVE && p->mm) {
+				if (p->sleep_avg >= ceiling)
+					sleep_time = 0;
+				else if (p->sleep_avg + sleep_time >=
+					 ceiling) {
+						p->sleep_avg = ceiling;
+						sleep_time = 0;
+				}
+			}
+
+			/*
+			 * This code gives a bonus to interactive tasks.
+			 *
+			 * The boost works by updating the 'average sleep time'
+			 * value here, based on ->timestamp. The more time a
+			 * task spends sleeping, the higher the average gets -
+			 * and the higher the priority boost gets as well.
+			 */
+			p->sleep_avg += sleep_time;
+
+		}
+		if (p->sleep_avg > NS_MAX_SLEEP_AVG)
+			p->sleep_avg = NS_MAX_SLEEP_AVG;
+	}
+
+	return effective_prio(p);
+}
+
+/*
+ * activate_task - move a task to the runqueue and do priority recalculation
+ *
+ * Update all the scheduling statistics stuff. (sleep average
+ * calculation, priority modifiers, etc.)
+ */
+static void activate_task(struct task_struct *p, struct rq *rq, int local)
+{
+	unsigned long long now;
+
+	now = sched_clock();
+#ifdef CONFIG_SMP
+	if (!local) {
+		/* Compensate for drifting sched_clock */
+		struct rq *this_rq = this_rq();
+		now = (now - this_rq->timestamp_last_tick)
+			+ rq->timestamp_last_tick;
+	}
+#endif
+
+	if (!rt_task(p))
+		p->prio = recalc_task_prio(p, now);
+
+	/*
+	 * This checks to make sure it's not an uninterruptible task
+	 * that is now waking up.
+	 */
+	if (p->sleep_type == SLEEP_NORMAL) {
+		/*
+		 * Tasks which were woken up by interrupts (ie. hw events)
+		 * are most likely of interactive nature. So we give them
+		 * the credit of extending their sleep time to the period
+		 * of time they spend on the runqueue, waiting for execution
+		 * on a CPU, first time around:
+		 */
+		if (in_interrupt())
+			p->sleep_type = SLEEP_INTERRUPTED;
+		else {
+			/*
+			 * Normal first-time wakeups get a credit too for
+			 * on-runqueue time, but it will be weighted down:
+			 */
+			p->sleep_type = SLEEP_INTERACTIVE;
+		}
+	}
+	p->timestamp = now;
+
+	__activate_task(p, rq);
+}
+
+/*
+ * deactivate_task - remove a task from the runqueue.
+ */
+static void deactivate_task(struct task_struct *p, struct rq *rq)
+{
+	dec_nr_running(p, rq);
+	dequeue_task(p, p->array);
+	p->array = NULL;
+}
+
+/*
+ * resched_task - mark a task 'to be rescheduled now'.
+ *
+ * On UP this means the setting of the need_resched flag, on SMP it
+ * might also involve a cross-CPU call to trigger the scheduler on
+ * the target CPU.
+ */
+#ifdef CONFIG_SMP
+
+#ifndef tsk_is_polling
+#define tsk_is_polling(t) test_tsk_thread_flag(t, TIF_POLLING_NRFLAG)
+#endif
+
+static void resched_task(struct task_struct *p)
+{
+	int cpu;
+
+	assert_spin_locked(&task_rq(p)->lock);
+
+	if (unlikely(test_tsk_thread_flag(p, TIF_NEED_RESCHED)))
+		return;
+
+	set_tsk_thread_flag(p, TIF_NEED_RESCHED);
+
+	cpu = task_cpu(p);
+	if (cpu == smp_processor_id())
+		return;
+
+	/* NEED_RESCHED must be visible before we test polling */
+	smp_mb();
+	if (!tsk_is_polling(p))
+		smp_send_reschedule(cpu);
+}
+#else
+static inline void resched_task(struct task_struct *p)
+{
+	assert_spin_locked(&task_rq(p)->lock);
+	set_tsk_need_resched(p);
+}
+#endif
+
+/**
+ * task_curr - is this task currently executing on a CPU?
+ * @p: the task in question.
+ */
+inline int task_curr(const struct task_struct *p)
+{
+	return cpu_curr(task_cpu(p)) == p;
+}
+
+/* Used instead of source_load when we know the type == 0 */
+unsigned long weighted_cpuload(const int cpu)
+{
+	return cpu_rq(cpu)->raw_weighted_load;
+}
+
+#ifdef CONFIG_SMP
+struct migration_req {
+	struct list_head list;
+
+	struct task_struct *task;
+	int dest_cpu;
+
+	struct completion done;
+};
+
+/*
+ * The task's runqueue lock must be held.
+ * Returns true if you have to wait for migration thread.
+ */
+static int
+migrate_task(struct task_struct *p, int dest_cpu, struct migration_req *req)
+{
+	struct rq *rq = task_rq(p);
+
+	/*
+	 * If the task is not on a runqueue (and not running), then
+	 * it is sufficient to simply update the task's cpu field.
+	 */
+	if (!p->array && !task_running(rq, p)) {
+		set_task_cpu(p, dest_cpu);
+		return 0;
+	}
+
+	init_completion(&req->done);
+	req->task = p;
+	req->dest_cpu = dest_cpu;
+	list_add(&req->list, &rq->migration_queue);
+
+	return 1;
+}
+
+/*
+ * wait_task_inactive - wait for a thread to unschedule.
+ *
+ * The caller must ensure that the task *will* unschedule sometime soon,
+ * else this function might spin for a *long* time. This function can't
+ * be called with interrupts off, or it may introduce deadlock with
+ * smp_call_function() if an IPI is sent by the same process we are
+ * waiting to become inactive.
+ */
+void wait_task_inactive(struct task_struct *p)
+{
+	unsigned long flags;
+	struct rq *rq;
+	int preempted;
+
+repeat:
+	rq = task_rq_lock(p, &flags);
+	/* Must be off runqueue entirely, not preempted. */
+	if (unlikely(p->array || task_running(rq, p))) {
+		/* If it's preempted, we yield.  It could be a while. */
+		preempted = !task_running(rq, p);
+		task_rq_unlock(rq, &flags);
+		cpu_relax();
+		if (preempted)
+			yield();
+		goto repeat;
+	}
+	task_rq_unlock(rq, &flags);
+}
+
+/***
+ * kick_process - kick a running thread to enter/exit the kernel
+ * @p: the to-be-kicked thread
+ *
+ * Cause a process which is running on another CPU to enter
+ * kernel-mode, without any delay. (to get signals handled.)
+ *
+ * NOTE: this function doesnt have to take the runqueue lock,
+ * because all it wants to ensure is that the remote task enters
+ * the kernel. If the IPI races and the task has been migrated
+ * to another CPU then no harm is done and the purpose has been
+ * achieved as well.
+ */
+void kick_process(struct task_struct *p)
+{
+	int cpu;
+
+	preempt_disable();
+	cpu = task_cpu(p);
+	if ((cpu != smp_processor_id()) && task_curr(p))
+		smp_send_reschedule(cpu);
+	preempt_enable();
+}
+
+/*
+ * Return a low guess at the load of a migration-source cpu weighted
+ * according to the scheduling class and "nice" value.
+ *
+ * We want to under-estimate the load of migration sources, to
+ * balance conservatively.
+ */
+static inline unsigned long source_load(int cpu, int type)
+{
+	struct rq *rq = cpu_rq(cpu);
+
+	if (type == 0)
+		return rq->raw_weighted_load;
+
+	return min(rq->cpu_load[type-1], rq->raw_weighted_load);
+}
+
+/*
+ * Return a high guess at the load of a migration-target cpu weighted
+ * according to the scheduling class and "nice" value.
+ */
+static inline unsigned long target_load(int cpu, int type)
+{
+	struct rq *rq = cpu_rq(cpu);
+
+	if (type == 0)
+		return rq->raw_weighted_load;
+
+	return max(rq->cpu_load[type-1], rq->raw_weighted_load);
+}
+
+/*
+ * Return the average load per task on the cpu's run queue
+ */
+static inline unsigned long cpu_avg_load_per_task(int cpu)
+{
+	struct rq *rq = cpu_rq(cpu);
+	unsigned long n = rq->nr_running;
+
+	return n ? rq->raw_weighted_load / n : SCHED_LOAD_SCALE;
+}
+
+/*
+ * find_idlest_group finds and returns the least busy CPU group within the
+ * domain.
+ */
+static struct sched_group *
+find_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu)
+{
+	struct sched_group *idlest = NULL, *this = NULL, *group = sd->groups;
+	unsigned long min_load = ULONG_MAX, this_load = 0;
+	int load_idx = sd->forkexec_idx;
+	int imbalance = 100 + (sd->imbalance_pct-100)/2;
+
+	do {
+		unsigned long load, avg_load;
+		int local_group;
+		int i;
+
+		/* Skip over this group if it has no CPUs allowed */
+		if (!cpus_intersects(group->cpumask, p->cpus_allowed))
+			goto nextgroup;
+
+		local_group = cpu_isset(this_cpu, group->cpumask);
+
+		/* Tally up the load of all CPUs in the group */
+		avg_load = 0;
+
+		for_each_cpu_mask(i, group->cpumask) {
+			/* Bias balancing toward cpus of our domain */
+			if (local_group)
+				load = source_load(i, load_idx);
+			else
+				load = target_load(i, load_idx);
+
+			avg_load += load;
+		}
+
+		/* Adjust by relative CPU power of the group */
+		avg_load = (avg_load * SCHED_LOAD_SCALE) / group->cpu_power;
+
+		if (local_group) {
+			this_load = avg_load;
+			this = group;
+		} else if (avg_load < min_load) {
+			min_load = avg_load;
+			idlest = group;
+		}
+nextgroup:
+		group = group->next;
+	} while (group != sd->groups);
+
+	if (!idlest || 100*this_load < imbalance*min_load)
+		return NULL;
+	return idlest;
+}
+
+/*
+ * find_idlest_queue - find the idlest runqueue among the cpus in group.
+ */
+static int
+find_idlest_cpu(struct sched_group *group, struct task_struct *p, int this_cpu)
+{
+	cpumask_t tmp;
+	unsigned long load, min_load = ULONG_MAX;
+	int idlest = -1;
+	int i;
+
+	/* Traverse only the allowed CPUs */
+	cpus_and(tmp, group->cpumask, p->cpus_allowed);
+
+	for_each_cpu_mask(i, tmp) {
+		load = weighted_cpuload(i);
+
+		if (load < min_load || (load == min_load && i == this_cpu)) {
+			min_load = load;
+			idlest = i;
+		}
+	}
+
+	return idlest;
+}
+
+/*
+ * sched_balance_self: balance the current task (running on cpu) in domains
+ * that have the 'flag' flag set. In practice, this is SD_BALANCE_FORK and
+ * SD_BALANCE_EXEC.
+ *
+ * Balance, ie. select the least loaded group.
+ *
+ * Returns the target CPU number, or the same CPU if no balancing is needed.
+ *
+ * preempt must be disabled.
+ */
+static int sched_balance_self(int cpu, int flag)
+{
+	struct task_struct *t = current;
+	struct sched_domain *tmp, *sd = NULL;
+
+	for_each_domain(cpu, tmp) {
+ 		/*
+ 	 	 * If power savings logic is enabled for a domain, stop there.
+ 	 	 */
+		if (tmp->flags & SD_POWERSAVINGS_BALANCE)
+			break;
+		if (tmp->flags & flag)
+			sd = tmp;
+	}
+
+	while (sd) {
+		cpumask_t span;
+		struct sched_group *group;
+		int new_cpu;
+		int weight;
+
+		span = sd->span;
+		group = find_idlest_group(sd, t, cpu);
+		if (!group)
+			goto nextlevel;
+
+		new_cpu = find_idlest_cpu(group, t, cpu);
+		if (new_cpu == -1 || new_cpu == cpu)
+			goto nextlevel;
+
+		/* Now try balancing at a lower domain level */
+		cpu = new_cpu;
+nextlevel:
+		sd = NULL;
+		weight = cpus_weight(span);
+		for_each_domain(cpu, tmp) {
+			if (weight <= cpus_weight(tmp->span))
+				break;
+			if (tmp->flags & flag)
+				sd = tmp;
+		}
+		/* while loop will break here if sd == NULL */
+	}
+
+	return cpu;
+}
+
+#endif /* CONFIG_SMP */
+
+/*
+ * wake_idle() will wake a task on an idle cpu if task->cpu is
+ * not idle and an idle cpu is available.  The span of cpus to
+ * search starts with cpus closest then further out as needed,
+ * so we always favor a closer, idle cpu.
+ *
+ * Returns the CPU we should wake onto.
+ */
+#if defined(ARCH_HAS_SCHED_WAKE_IDLE)
+static int wake_idle(int cpu, struct task_struct *p)
+{
+	cpumask_t tmp;
+	struct sched_domain *sd;
+	int i;
+
+	if (idle_cpu(cpu))
+		return cpu;
+
+	for_each_domain(cpu, sd) {
+		if (sd->flags & SD_WAKE_IDLE) {
+			cpus_and(tmp, sd->span, p->cpus_allowed);
+			for_each_cpu_mask(i, tmp) {
+				if (idle_cpu(i))
+					return i;
+			}
+		}
+		else
+			break;
+	}
+	return cpu;
+}
+#else
+static inline int wake_idle(int cpu, struct task_struct *p)
+{
+	return cpu;
+}
+#endif
+
+/***
+ * try_to_wake_up - wake up a thread
+ * @p: the to-be-woken-up thread
+ * @state: the mask of task states that can be woken
+ * @sync: do a synchronous wakeup?
+ *
+ * Put it on the run-queue if it's not already there. The "current"
+ * thread is always on the run-queue (except when the actual
+ * re-schedule is in progress), and as such you're allowed to do
+ * the simpler "current->state = TASK_RUNNING" to mark yourself
+ * runnable without the overhead of this.
+ *
+ * returns failure only if the task is already active.
+ */
+static int try_to_wake_up(struct task_struct *p, unsigned int state, int sync)
+{
+	int cpu, this_cpu, success = 0;
+	unsigned long flags;
+	long old_state;
+	struct rq *rq;
+#ifdef CONFIG_SMP
+	struct sched_domain *sd, *this_sd = NULL;
+	unsigned long load, this_load;
+	int new_cpu;
+#endif
+
+	rq = task_rq_lock(p, &flags);
+	old_state = p->state;
+	if (!(old_state & state))
+		goto out;
+
+	if (p->array)
+		goto out_running;
+
+	cpu = task_cpu(p);
+	this_cpu = smp_processor_id();
+
+#ifdef CONFIG_SMP
+	if (unlikely(task_running(rq, p)))
+		goto out_activate;
+
+	new_cpu = cpu;
+
+	schedstat_inc(rq, ttwu_cnt);
+	if (cpu == this_cpu) {
+		schedstat_inc(rq, ttwu_local);
+		goto out_set_cpu;
+	}
+
+	for_each_domain(this_cpu, sd) {
+		if (cpu_isset(cpu, sd->span)) {
+			schedstat_inc(sd, ttwu_wake_remote);
+			this_sd = sd;
+			break;
+		}
+	}
+
+	if (unlikely(!cpu_isset(this_cpu, p->cpus_allowed)))
+		goto out_set_cpu;
+
+	/*
+	 * Check for affine wakeup and passive balancing possibilities.
+	 */
+	if (this_sd) {
+		int idx = this_sd->wake_idx;
+		unsigned int imbalance;
+
+		imbalance = 100 + (this_sd->imbalance_pct - 100) / 2;
+
+		load = source_load(cpu, idx);
+		this_load = target_load(this_cpu, idx);
+
+		new_cpu = this_cpu; /* Wake to this CPU if we can */
+
+		if (this_sd->flags & SD_WAKE_AFFINE) {
+			unsigned long tl = this_load;
+			unsigned long tl_per_task = cpu_avg_load_per_task(this_cpu);
+
+			/*
+			 * If sync wakeup then subtract the (maximum possible)
+			 * effect of the currently running task from the load
+			 * of the current CPU:
+			 */
+			if (sync)
+				tl -= current->load_weight;
+
+			if ((tl <= load &&
+				tl + target_load(cpu, idx) <= tl_per_task) ||
+				100*(tl + p->load_weight) <= imbalance*load) {
+				/*
+				 * This domain has SD_WAKE_AFFINE and
+				 * p is cache cold in this domain, and
+				 * there is no bad imbalance.
+				 */
+				schedstat_inc(this_sd, ttwu_move_affine);
+				goto out_set_cpu;
+			}
+		}
+
+		/*
+		 * Start passive balancing when half the imbalance_pct
+		 * limit is reached.
+		 */
+		if (this_sd->flags & SD_WAKE_BALANCE) {
+			if (imbalance*this_load <= 100*load) {
+				schedstat_inc(this_sd, ttwu_move_balance);
+				goto out_set_cpu;
+			}
+		}
+	}
+
+	new_cpu = cpu; /* Could not wake to this_cpu. Wake to cpu instead */
+out_set_cpu:
+	new_cpu = wake_idle(new_cpu, p);
+	if (new_cpu != cpu) {
+		set_task_cpu(p, new_cpu);
+		task_rq_unlock(rq, &flags);
+		/* might preempt at this point */
+		rq = task_rq_lock(p, &flags);
+		old_state = p->state;
+		if (!(old_state & state))
+			goto out;
+		if (p->array)
+			goto out_running;
+
+		this_cpu = smp_processor_id();
+		cpu = task_cpu(p);
+	}
+
+out_activate:
+#endif /* CONFIG_SMP */
+	if (old_state == TASK_UNINTERRUPTIBLE) {
+		rq->nr_uninterruptible--;
+		/*
+		 * Tasks on involuntary sleep don't earn
+		 * sleep_avg beyond just interactive state.
+		 */
+		p->sleep_type = SLEEP_NONINTERACTIVE;
+	} else
+
+	/*
+	 * Tasks that have marked their sleep as noninteractive get
+	 * woken up with their sleep average not weighted in an
+	 * interactive way.
+	 */
+		if (old_state & TASK_NONINTERACTIVE)
+			p->sleep_type = SLEEP_NONINTERACTIVE;
+
+
+	activate_task(p, rq, cpu == this_cpu);
+	/*
+	 * Sync wakeups (i.e. those types of wakeups where the waker
+	 * has indicated that it will leave the CPU in short order)
+	 * don't trigger a preemption, if the woken up task will run on
+	 * this cpu. (in this case the 'I will reschedule' promise of
+	 * the waker guarantees that the freshly woken up task is going
+	 * to be considered on this CPU.)
+	 */
+	if (!sync || cpu != this_cpu) {
+		if (TASK_PREEMPTS_CURR(p, rq))
+			resched_task(rq->curr);
+	}
+	success = 1;
+
+out_running:
+	p->state = TASK_RUNNING;
+out:
+	task_rq_unlock(rq, &flags);
+
+	return success;
+}
+
+int fastcall wake_up_process(struct task_struct *p)
+{
+	return try_to_wake_up(p, TASK_STOPPED | TASK_TRACED |
+				 TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE, 0);
+}
+EXPORT_SYMBOL(wake_up_process);
+
+int fastcall wake_up_state(struct task_struct *p, unsigned int state)
+{
+	return try_to_wake_up(p, state, 0);
+}
+
+/*
+ * Perform scheduler related setup for a newly forked process p.
+ * p is forked by current.
+ */
+void fastcall sched_fork(struct task_struct *p, int clone_flags)
+{
+	int cpu = get_cpu();
+
+#ifdef CONFIG_SMP
+	cpu = sched_balance_self(cpu, SD_BALANCE_FORK);
+#endif
+	set_task_cpu(p, cpu);
+
+	/*
+	 * We mark the process as running here, but have not actually
+	 * inserted it onto the runqueue yet. This guarantees that
+	 * nobody will actually run it, and a signal or other external
+	 * event cannot wake it up and insert it on the runqueue either.
+	 */
+	p->state = TASK_RUNNING;
+
+	/*
+	 * Make sure we do not leak PI boosting priority to the child:
+	 */
+	p->prio = current->normal_prio;
+
+	INIT_LIST_HEAD(&p->run_list);
+	p->array = NULL;
+#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
+	if (unlikely(sched_info_on()))
+		memset(&p->sched_info, 0, sizeof(p->sched_info));
+#endif
+#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)
+	p->oncpu = 0;
+#endif
+#ifdef CONFIG_PREEMPT
+	/* Want to start with kernel preemption disabled. */
+	task_thread_info(p)->preempt_count = 1;
+#endif
+	/*
+	 * Share the timeslice between parent and child, thus the
+	 * total amount of pending timeslices in the system doesn't change,
+	 * resulting in more scheduling fairness.
+	 */
+	local_irq_disable();
+	p->time_slice = (current->time_slice + 1) >> 1;
+	/*
+	 * The remainder of the first timeslice might be recovered by
+	 * the parent if the child exits early enough.
+	 */
+	p->first_time_slice = 1;
+	current->time_slice >>= 1;
+	p->timestamp = sched_clock();
+	if (unlikely(!current->time_slice)) {
+		/*
+		 * This case is rare, it happens when the parent has only
+		 * a single jiffy left from its timeslice. Taking the
+		 * runqueue lock is not a problem.
+		 */
+		current->time_slice = 1;
+		scheduler_tick();
+	}
+	local_irq_enable();
+	put_cpu();
+}
+
+/*
+ * wake_up_new_task - wake up a newly created task for the first time.
+ *
+ * This function will do some initial scheduler statistics housekeeping
+ * that must be done for every newly created context, then puts the task
+ * on the runqueue and wakes it.
+ */
+void fastcall wake_up_new_task(struct task_struct *p, unsigned long clone_flags)
+{
+	struct rq *rq, *this_rq;
+	unsigned long flags;
+	int this_cpu, cpu;
+
+	rq = task_rq_lock(p, &flags);
+	BUG_ON(p->state != TASK_RUNNING);
+	this_cpu = smp_processor_id();
+	cpu = task_cpu(p);
+
+	/*
+	 * We decrease the sleep average of forking parents
+	 * and children as well, to keep max-interactive tasks
+	 * from forking tasks that are max-interactive. The parent
+	 * (current) is done further down, under its lock.
+	 */
+	p->sleep_avg = JIFFIES_TO_NS(CURRENT_BONUS(p) *
+		CHILD_PENALTY / 100 * MAX_SLEEP_AVG / MAX_BONUS);
+
+	p->prio = effective_prio(p);
+
+	if (likely(cpu == this_cpu)) {
+		if (!(clone_flags & CLONE_VM)) {
+			/*
+			 * The VM isn't cloned, so we're in a good position to
+			 * do child-runs-first in anticipation of an exec. This
+			 * usually avoids a lot of COW overhead.
+			 */
+			if (unlikely(!current->array))
+				__activate_task(p, rq);
+			else {
+				p->prio = current->prio;
+				p->normal_prio = current->normal_prio;
+				list_add_tail(&p->run_list, &current->run_list);
+				p->array = current->array;
+				p->array->nr_active++;
+				inc_nr_running(p, rq);
+			}
+			set_need_resched();
+		} else
+			/* Run child last */
+			__activate_task(p, rq);
+		/*
+		 * We skip the following code due to cpu == this_cpu
+	 	 *
+		 *   task_rq_unlock(rq, &flags);
+		 *   this_rq = task_rq_lock(current, &flags);
+		 */
+		this_rq = rq;
+	} else {
+		this_rq = cpu_rq(this_cpu);
+
+		/*
+		 * Not the local CPU - must adjust timestamp. This should
+		 * get optimised away in the !CONFIG_SMP case.
+		 */
+		p->timestamp = (p->timestamp - this_rq->timestamp_last_tick)
+					+ rq->timestamp_last_tick;
+		__activate_task(p, rq);
+		if (TASK_PREEMPTS_CURR(p, rq))
+			resched_task(rq->curr);
+
+		/*
+		 * Parent and child are on different CPUs, now get the
+		 * parent runqueue to update the parent's ->sleep_avg:
+		 */
+		task_rq_unlock(rq, &flags);
+		this_rq = task_rq_lock(current, &flags);
+	}
+	current->sleep_avg = JIFFIES_TO_NS(CURRENT_BONUS(current) *
+		PARENT_PENALTY / 100 * MAX_SLEEP_AVG / MAX_BONUS);
+	task_rq_unlock(this_rq, &flags);
+}
+
+/*
+ * Potentially available exiting-child timeslices are
+ * retrieved here - this way the parent does not get
+ * penalized for creating too many threads.
+ *
+ * (this cannot be used to 'generate' timeslices
+ * artificially, because any timeslice recovered here
+ * was given away by the parent in the first place.)
+ */
+void fastcall sched_exit(struct task_struct *p)
+{
+	unsigned long flags;
+	struct rq *rq;
+
+	/*
+	 * If the child was a (relative-) CPU hog then decrease
+	 * the sleep_avg of the parent as well.
+	 */
+	rq = task_rq_lock(p->parent, &flags);
+	if (p->first_time_slice && task_cpu(p) == task_cpu(p->parent)) {
+		p->parent->time_slice += p->time_slice;
+		if (unlikely(p->parent->time_slice > task_timeslice(p)))
+			p->parent->time_slice = task_timeslice(p);
+	}
+	if (p->sleep_avg < p->parent->sleep_avg)
+		p->parent->sleep_avg = p->parent->sleep_avg /
+		(EXIT_WEIGHT + 1) * EXIT_WEIGHT + p->sleep_avg /
+		(EXIT_WEIGHT + 1);
+	task_rq_unlock(rq, &flags);
+}
+
+/**
+ * prepare_task_switch - prepare to switch tasks
+ * @rq: the runqueue preparing to switch
+ * @next: the task we are going to switch to.
+ *
+ * This is called with the rq lock held and interrupts off. It must
+ * be paired with a subsequent finish_task_switch after the context
+ * switch.
+ *
+ * prepare_task_switch sets up locking and calls architecture specific
+ * hooks.
+ */
+static inline void prepare_task_switch(struct rq *rq, struct task_struct *next)
+{
+	prepare_lock_switch(rq, next);
+	prepare_arch_switch(next);
+}
+
+/**
+ * finish_task_switch - clean up after a task-switch
+ * @rq: runqueue associated with task-switch
+ * @prev: the thread we just switched away from.
+ *
+ * finish_task_switch must be called after the context switch, paired
+ * with a prepare_task_switch call before the context switch.
+ * finish_task_switch will reconcile locking set up by prepare_task_switch,
+ * and do any other architecture-specific cleanup actions.
+ *
+ * Note that we may have delayed dropping an mm in context_switch(). If
+ * so, we finish that here outside of the runqueue lock.  (Doing it
+ * with the lock held can cause deadlocks; see schedule() for
+ * details.)
+ */
+static inline void finish_task_switch(struct rq *rq, struct task_struct *prev)
+	__releases(rq->lock)
+{
+	struct mm_struct *mm = rq->prev_mm;
+	unsigned long prev_task_flags;
+
+	rq->prev_mm = NULL;
+
+	/*
+	 * A task struct has one reference for the use as "current".
+	 * If a task dies, then it sets EXIT_ZOMBIE in tsk->exit_state and
+	 * calls schedule one last time. The schedule call will never return,
+	 * and the scheduled task must drop that reference.
+	 * The test for EXIT_ZOMBIE must occur while the runqueue locks are
+	 * still held, otherwise prev could be scheduled on another cpu, die
+	 * there before we look at prev->state, and then the reference would
+	 * be dropped twice.
+	 *		Manfred Spraul <manfred@colorfullife.com>
+	 */
+	prev_task_flags = prev->flags;
+	finish_arch_switch(prev);
+	finish_lock_switch(rq, prev);
+	if (mm)
+		mmdrop(mm);
+	if (unlikely(prev_task_flags & PF_DEAD)) {
+		/*
+		 * Remove function-return probe instances associated with this
+		 * task and put them back on the free list.
+	 	 */
+		kprobe_flush_task(prev);
+		put_task_struct(prev);
+	}
+}
+
+/**
+ * schedule_tail - first thing a freshly forked thread must call.
+ * @prev: the thread we just switched away from.
+ */
+asmlinkage void schedule_tail(struct task_struct *prev)
+	__releases(rq->lock)
+{
+	struct rq *rq = this_rq();
+
+	finish_task_switch(rq, prev);
+#ifdef __ARCH_WANT_UNLOCKED_CTXSW
+	/* In this case, finish_task_switch does not reenable preemption */
+	preempt_enable();
+#endif
+	if (current->set_child_tid)
+		put_user(current->pid, current->set_child_tid);
+}
+
+/*
+ * context_switch - switch to the new MM and the new
+ * thread's register state.
+ */
+static inline struct task_struct *
+context_switch(struct rq *rq, struct task_struct *prev,
+	       struct task_struct *next)
+{
+	struct mm_struct *mm = next->mm;
+	struct mm_struct *oldmm = prev->active_mm;
+
+	if (unlikely(!mm)) {
+		next->active_mm = oldmm;
+		atomic_inc(&oldmm->mm_count);
+		enter_lazy_tlb(oldmm, next);
+	} else
+		switch_mm(oldmm, mm, next);
+
+	if (unlikely(!prev->mm)) {
+		prev->active_mm = NULL;
+		WARN_ON(rq->prev_mm);
+		rq->prev_mm = oldmm;
+	}
+	spin_release(&rq->lock.dep_map, 1, _THIS_IP_);
+
+	/* Here we just switch the register state and the stack. */
+	switch_to(prev, next, prev);
+
+	return prev;
+}
+
+/*
+ * nr_running, nr_uninterruptible and nr_context_switches:
+ *
+ * externally visible scheduler statistics: current number of runnable
+ * threads, current number of uninterruptible-sleeping threads, total
+ * number of context switches performed since bootup.
+ */
+unsigned long nr_running(void)
+{
+	unsigned long i, sum = 0;
+
+	for_each_online_cpu(i)
+		sum += cpu_rq(i)->nr_running;
+
+	return sum;
+}
+
+unsigned long nr_uninterruptible(void)
+{
+	unsigned long i, sum = 0;
+
+	for_each_possible_cpu(i)
+		sum += cpu_rq(i)->nr_uninterruptible;
+
+	/*
+	 * Since we read the counters lockless, it might be slightly
+	 * inaccurate. Do not allow it to go below zero though:
+	 */
+	if (unlikely((long)sum < 0))
+		sum = 0;
+
+	return sum;
+}
+
+unsigned long long nr_context_switches(void)
+{
+	int i;
+	unsigned long long sum = 0;
+
+	for_each_possible_cpu(i)
+		sum += cpu_rq(i)->nr_switches;
+
+	return sum;
+}
+
+unsigned long nr_iowait(void)
+{
+	unsigned long i, sum = 0;
+
+	for_each_possible_cpu(i)
+		sum += atomic_read(&cpu_rq(i)->nr_iowait);
+
+	return sum;
+}
+
+unsigned long nr_active(void)
+{
+	unsigned long i, running = 0, uninterruptible = 0;
+
+	for_each_online_cpu(i) {
+		running += cpu_rq(i)->nr_running;
+		uninterruptible += cpu_rq(i)->nr_uninterruptible;
+	}
+
+	if (unlikely((long)uninterruptible < 0))
+		uninterruptible = 0;
+
+	return running + uninterruptible;
+}
+
+#ifdef CONFIG_SMP
+
+/*
+ * Is this task likely cache-hot:
+ */
+static inline int
+task_hot(struct task_struct *p, unsigned long long now, struct sched_domain *sd)
+{
+	return (long long)(now - p->last_ran) < (long long)sd->cache_hot_time;
+}
+
+/*
+ * double_rq_lock - safely lock two runqueues
+ *
+ * Note this does not disable interrupts like task_rq_lock,
+ * you need to do so manually before calling.
+ */
+static void double_rq_lock(struct rq *rq1, struct rq *rq2)
+	__acquires(rq1->lock)
+	__acquires(rq2->lock)
+{
+	if (rq1 == rq2) {
+		spin_lock(&rq1->lock);
+		__acquire(rq2->lock);	/* Fake it out ;) */
+	} else {
+		if (rq1 < rq2) {
+			spin_lock(&rq1->lock);
+			spin_lock(&rq2->lock);
+		} else {
+			spin_lock(&rq2->lock);
+			spin_lock(&rq1->lock);
+		}
+	}
+}
+
+/*
+ * double_rq_unlock - safely unlock two runqueues
+ *
+ * Note this does not restore interrupts like task_rq_unlock,
+ * you need to do so manually after calling.
+ */
+static void double_rq_unlock(struct rq *rq1, struct rq *rq2)
+	__releases(rq1->lock)
+	__releases(rq2->lock)
+{
+	spin_unlock(&rq1->lock);
+	if (rq1 != rq2)
+		spin_unlock(&rq2->lock);
+	else
+		__release(rq2->lock);
+}
+
+/*
+ * double_lock_balance - lock the busiest runqueue, this_rq is locked already.
+ */
+static void double_lock_balance(struct rq *this_rq, struct rq *busiest)
+	__releases(this_rq->lock)
+	__acquires(busiest->lock)
+	__acquires(this_rq->lock)
+{
+	if (unlikely(!spin_trylock(&busiest->lock))) {
+		if (busiest < this_rq) {
+			spin_unlock(&this_rq->lock);
+			spin_lock(&busiest->lock);
+			spin_lock(&this_rq->lock);
+		} else
+			spin_lock(&busiest->lock);
+	}
+}
+
+/*
+ * If dest_cpu is allowed for this process, migrate the task to it.
+ * This is accomplished by forcing the cpu_allowed mask to only
+ * allow dest_cpu, which will force the cpu onto dest_cpu.  Then
+ * the cpu_allowed mask is restored.
+ */
+static void sched_migrate_task(struct task_struct *p, int dest_cpu)
+{
+	struct migration_req req;
+	unsigned long flags;
+	struct rq *rq;
+
+	rq = task_rq_lock(p, &flags);
+	if (!cpu_isset(dest_cpu, p->cpus_allowed)
+	    || unlikely(cpu_is_offline(dest_cpu)))
+		goto out;
+
+	/* force the process onto the specified CPU */
+	if (migrate_task(p, dest_cpu, &req)) {
+		/* Need to wait for migration thread (might exit: take ref). */
+		struct task_struct *mt = rq->migration_thread;
+
+		get_task_struct(mt);
+		task_rq_unlock(rq, &flags);
+		wake_up_process(mt);
+		put_task_struct(mt);
+		wait_for_completion(&req.done);
+
+		return;
+	}
+out:
+	task_rq_unlock(rq, &flags);
+}
+
+/*
+ * sched_exec - execve() is a valuable balancing opportunity, because at
+ * this point the task has the smallest effective memory and cache footprint.
+ */
+void sched_exec(void)
+{
+	int new_cpu, this_cpu = get_cpu();
+	new_cpu = sched_balance_self(this_cpu, SD_BALANCE_EXEC);
+	put_cpu();
+	if (new_cpu != this_cpu)
+		sched_migrate_task(current, new_cpu);
+}
+
+/*
+ * pull_task - move a task from a remote runqueue to the local runqueue.
+ * Both runqueues must be locked.
+ */
+static void pull_task(struct rq *src_rq, struct prio_array *src_array,
+		      struct task_struct *p, struct rq *this_rq,
+		      struct prio_array *this_array, int this_cpu)
+{
+	dequeue_task(p, src_array);
+	dec_nr_running(p, src_rq);
+	set_task_cpu(p, this_cpu);
+	inc_nr_running(p, this_rq);
+	enqueue_task(p, this_array);
+	p->timestamp = (p->timestamp - src_rq->timestamp_last_tick)
+				+ this_rq->timestamp_last_tick;
+	/*
+	 * Note that idle threads have a prio of MAX_PRIO, for this test
+	 * to be always true for them.
+	 */
+	if (TASK_PREEMPTS_CURR(p, this_rq))
+		resched_task(this_rq->curr);
+}
+
+/*
+ * can_migrate_task - may task p from runqueue rq be migrated to this_cpu?
+ */
+static
+int can_migrate_task(struct task_struct *p, struct rq *rq, int this_cpu,
+		     struct sched_domain *sd, enum idle_type idle,
+		     int *all_pinned)
+{
+	/*
+	 * We do not migrate tasks that are:
+	 * 1) running (obviously), or
+	 * 2) cannot be migrated to this CPU due to cpus_allowed, or
+	 * 3) are cache-hot on their current CPU.
+	 */
+	if (!cpu_isset(this_cpu, p->cpus_allowed))
+		return 0;
+	*all_pinned = 0;
+
+	if (task_running(rq, p))
+		return 0;
+
+	/*
+	 * Aggressive migration if:
+	 * 1) task is cache cold, or
+	 * 2) too many balance attempts have failed.
+	 */
+
+	if (sd->nr_balance_failed > sd->cache_nice_tries)
+		return 1;
+
+	if (task_hot(p, rq->timestamp_last_tick, sd))
+		return 0;
+	return 1;
+}
+
+#define rq_best_prio(rq) min((rq)->curr->prio, (rq)->best_expired_prio)
+
+/*
+ * move_tasks tries to move up to max_nr_move tasks and max_load_move weighted
+ * load from busiest to this_rq, as part of a balancing operation within
+ * "domain". Returns the number of tasks moved.
+ *
+ * Called with both runqueues locked.
+ */
+static int move_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,
+		      unsigned long max_nr_move, unsigned long max_load_move,
+		      struct sched_domain *sd, enum idle_type idle,
+		      int *all_pinned)
+{
+	int idx, pulled = 0, pinned = 0, this_best_prio, best_prio,
+	    best_prio_seen, skip_for_load;
+	struct prio_array *array, *dst_array;
+	struct list_head *head, *curr;
+	struct task_struct *tmp;
+	long rem_load_move;
+
+	if (max_nr_move == 0 || max_load_move == 0)
+		goto out;
+
+	rem_load_move = max_load_move;
+	pinned = 1;
+	this_best_prio = rq_best_prio(this_rq);
+	best_prio = rq_best_prio(busiest);
+	/*
+	 * Enable handling of the case where there is more than one task
+	 * with the best priority.   If the current running task is one
+	 * of those with prio==best_prio we know it won't be moved
+	 * and therefore it's safe to override the skip (based on load) of
+	 * any task we find with that prio.
+	 */
+	best_prio_seen = best_prio == busiest->curr->prio;
+
+	/*
+	 * We first consider expired tasks. Those will likely not be
+	 * executed in the near future, and they are most likely to
+	 * be cache-cold, thus switching CPUs has the least effect
+	 * on them.
+	 */
+	if (busiest->expired->nr_active) {
+		array = busiest->expired;
+		dst_array = this_rq->expired;
+	} else {
+		array = busiest->active;
+		dst_array = this_rq->active;
+	}
+
+new_array:
+	/* Start searching at priority 0: */
+	idx = 0;
+skip_bitmap:
+	if (!idx)
+		idx = sched_find_first_bit(array->bitmap);
+	else
+		idx = find_next_bit(array->bitmap, MAX_PRIO, idx);
+	if (idx >= MAX_PRIO) {
+		if (array == busiest->expired && busiest->active->nr_active) {
+			array = busiest->active;
+			dst_array = this_rq->active;
+			goto new_array;
+		}
+		goto out;
+	}
+
+	head = array->queue + idx;
+	curr = head->prev;
+skip_queue:
+	tmp = list_entry(curr, struct task_struct, run_list);
+
+	curr = curr->prev;
+
+	/*
+	 * To help distribute high priority tasks accross CPUs we don't
+	 * skip a task if it will be the highest priority task (i.e. smallest
+	 * prio value) on its new queue regardless of its load weight
+	 */
+	skip_for_load = tmp->load_weight > rem_load_move;
+	if (skip_for_load && idx < this_best_prio)
+		skip_for_load = !best_prio_seen && idx == best_prio;
+	if (skip_for_load ||
+	    !can_migrate_task(tmp, busiest, this_cpu, sd, idle, &pinned)) {
+
+		best_prio_seen |= idx == best_prio;
+		if (curr != head)
+			goto skip_queue;
+		idx++;
+		goto skip_bitmap;
+	}
+
+#ifdef CONFIG_SCHEDSTATS
+	if (task_hot(tmp, busiest->timestamp_last_tick, sd))
+		schedstat_inc(sd, lb_hot_gained[idle]);
+#endif
+
+	pull_task(busiest, array, tmp, this_rq, dst_array, this_cpu);
+	pulled++;
+	rem_load_move -= tmp->load_weight;
+
+	/*
+	 * We only want to steal up to the prescribed number of tasks
+	 * and the prescribed amount of weighted load.
+	 */
+	if (pulled < max_nr_move && rem_load_move > 0) {
+		if (idx < this_best_prio)
+			this_best_prio = idx;
+		if (curr != head)
+			goto skip_queue;
+		idx++;
+		goto skip_bitmap;
+	}
+out:
+	/*
+	 * Right now, this is the only place pull_task() is called,
+	 * so we can safely collect pull_task() stats here rather than
+	 * inside pull_task().
+	 */
+	schedstat_add(sd, lb_gained[idle], pulled);
+
+	if (all_pinned)
+		*all_pinned = pinned;
+	return pulled;
+}
+
+/*
+ * find_busiest_group finds and returns the busiest CPU group within the
+ * domain. It calculates and returns the amount of weighted load which
+ * should be moved to restore balance via the imbalance parameter.
+ */
+static struct sched_group *
+find_busiest_group(struct sched_domain *sd, int this_cpu,
+		   unsigned long *imbalance, enum idle_type idle, int *sd_idle)
+{
+	struct sched_group *busiest = NULL, *this = NULL, *group = sd->groups;
+	unsigned long max_load, avg_load, total_load, this_load, total_pwr;
+	unsigned long max_pull;
+	unsigned long busiest_load_per_task, busiest_nr_running;
+	unsigned long this_load_per_task, this_nr_running;
+	int load_idx;
+#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)
+	int power_savings_balance = 1;
+	unsigned long leader_nr_running = 0, min_load_per_task = 0;
+	unsigned long min_nr_running = ULONG_MAX;
+	struct sched_group *group_min = NULL, *group_leader = NULL;
+#endif
+
+	max_load = this_load = total_load = total_pwr = 0;
+	busiest_load_per_task = busiest_nr_running = 0;
+	this_load_per_task = this_nr_running = 0;
+	if (idle == NOT_IDLE)
+		load_idx = sd->busy_idx;
+	else if (idle == NEWLY_IDLE)
+		load_idx = sd->newidle_idx;
+	else
+		load_idx = sd->idle_idx;
+
+	do {
+		unsigned long load, group_capacity;
+		int local_group;
+		int i;
+		unsigned long sum_nr_running, sum_weighted_load;
+
+		local_group = cpu_isset(this_cpu, group->cpumask);
+
+		/* Tally up the load of all CPUs in the group */
+		sum_weighted_load = sum_nr_running = avg_load = 0;
+
+		for_each_cpu_mask(i, group->cpumask) {
+			struct rq *rq = cpu_rq(i);
+
+			if (*sd_idle && !idle_cpu(i))
+				*sd_idle = 0;
+
+			/* Bias balancing toward cpus of our domain */
+			if (local_group)
+				load = target_load(i, load_idx);
+			else
+				load = source_load(i, load_idx);
+
+			avg_load += load;
+			sum_nr_running += rq->nr_running;
+			sum_weighted_load += rq->raw_weighted_load;
+		}
+
+		total_load += avg_load;
+		total_pwr += group->cpu_power;
+
+		/* Adjust by relative CPU power of the group */
+		avg_load = (avg_load * SCHED_LOAD_SCALE) / group->cpu_power;
+
+		group_capacity = group->cpu_power / SCHED_LOAD_SCALE;
+
+		if (local_group) {
+			this_load = avg_load;
+			this = group;
+			this_nr_running = sum_nr_running;
+			this_load_per_task = sum_weighted_load;
+		} else if (avg_load > max_load &&
+			   sum_nr_running > group_capacity) {
+			max_load = avg_load;
+			busiest = group;
+			busiest_nr_running = sum_nr_running;
+			busiest_load_per_task = sum_weighted_load;
+		}
+
+#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)
+		/*
+		 * Busy processors will not participate in power savings
+		 * balance.
+		 */
+ 		if (idle == NOT_IDLE || !(sd->flags & SD_POWERSAVINGS_BALANCE))
+ 			goto group_next;
+
+		/*
+		 * If the local group is idle or completely loaded
+		 * no need to do power savings balance at this domain
+		 */
+		if (local_group && (this_nr_running >= group_capacity ||
+				    !this_nr_running))
+			power_savings_balance = 0;
+
+ 		/*
+		 * If a group is already running at full capacity or idle,
+		 * don't include that group in power savings calculations
+ 		 */
+ 		if (!power_savings_balance || sum_nr_running >= group_capacity
+		    || !sum_nr_running)
+ 			goto group_next;
+
+ 		/*
+		 * Calculate the group which has the least non-idle load.
+ 		 * This is the group from where we need to pick up the load
+ 		 * for saving power
+ 		 */
+ 		if ((sum_nr_running < min_nr_running) ||
+ 		    (sum_nr_running == min_nr_running &&
+		     first_cpu(group->cpumask) <
+		     first_cpu(group_min->cpumask))) {
+ 			group_min = group;
+ 			min_nr_running = sum_nr_running;
+			min_load_per_task = sum_weighted_load /
+						sum_nr_running;
+ 		}
+
+ 		/*
+		 * Calculate the group which is almost near its
+ 		 * capacity but still has some space to pick up some load
+ 		 * from other group and save more power
+ 		 */
+ 		if (sum_nr_running <= group_capacity - 1) {
+ 			if (sum_nr_running > leader_nr_running ||
+ 			    (sum_nr_running == leader_nr_running &&
+ 			     first_cpu(group->cpumask) >
+ 			      first_cpu(group_leader->cpumask))) {
+ 				group_leader = group;
+ 				leader_nr_running = sum_nr_running;
+ 			}
+		}
+group_next:
+#endif
+		group = group->next;
+	} while (group != sd->groups);
+
+	if (!busiest || this_load >= max_load || busiest_nr_running == 0)
+		goto out_balanced;
+
+	avg_load = (SCHED_LOAD_SCALE * total_load) / total_pwr;
+
+	if (this_load >= avg_load ||
+			100*max_load <= sd->imbalance_pct*this_load)
+		goto out_balanced;
+
+	busiest_load_per_task /= busiest_nr_running;
+	/*
+	 * We're trying to get all the cpus to the average_load, so we don't
+	 * want to push ourselves above the average load, nor do we wish to
+	 * reduce the max loaded cpu below the average load, as either of these
+	 * actions would just result in more rebalancing later, and ping-pong
+	 * tasks around. Thus we look for the minimum possible imbalance.
+	 * Negative imbalances (*we* are more loaded than anyone else) will
+	 * be counted as no imbalance for these purposes -- we can't fix that
+	 * by pulling tasks to us.  Be careful of negative numbers as they'll
+	 * appear as very large values with unsigned longs.
+	 */
+	if (max_load <= busiest_load_per_task)
+		goto out_balanced;
+
+	/*
+	 * In the presence of smp nice balancing, certain scenarios can have
+	 * max load less than avg load(as we skip the groups at or below
+	 * its cpu_power, while calculating max_load..)
+	 */
+	if (max_load < avg_load) {
+		*imbalance = 0;
+		goto small_imbalance;
+	}
+
+	/* Don't want to pull so many tasks that a group would go idle */
+	max_pull = min(max_load - avg_load, max_load - busiest_load_per_task);
+
+	/* How much load to actually move to equalise the imbalance */
+	*imbalance = min(max_pull * busiest->cpu_power,
+				(avg_load - this_load) * this->cpu_power)
+			/ SCHED_LOAD_SCALE;
+
+	/*
+	 * if *imbalance is less than the average load per runnable task
+	 * there is no gaurantee that any tasks will be moved so we'll have
+	 * a think about bumping its value to force at least one task to be
+	 * moved
+	 */
+	if (*imbalance < busiest_load_per_task) {
+		unsigned long tmp, pwr_now, pwr_move;
+		unsigned int imbn;
+
+small_imbalance:
+		pwr_move = pwr_now = 0;
+		imbn = 2;
+		if (this_nr_running) {
+			this_load_per_task /= this_nr_running;
+			if (busiest_load_per_task > this_load_per_task)
+				imbn = 1;
+		} else
+			this_load_per_task = SCHED_LOAD_SCALE;
+
+		if (max_load - this_load >= busiest_load_per_task * imbn) {
+			*imbalance = busiest_load_per_task;
+			return busiest;
+		}
+
+		/*
+		 * OK, we don't have enough imbalance to justify moving tasks,
+		 * however we may be able to increase total CPU power used by
+		 * moving them.
+		 */
+
+		pwr_now += busiest->cpu_power *
+			min(busiest_load_per_task, max_load);
+		pwr_now += this->cpu_power *
+			min(this_load_per_task, this_load);
+		pwr_now /= SCHED_LOAD_SCALE;
+
+		/* Amount of load we'd subtract */
+		tmp = busiest_load_per_task*SCHED_LOAD_SCALE/busiest->cpu_power;
+		if (max_load > tmp)
+			pwr_move += busiest->cpu_power *
+				min(busiest_load_per_task, max_load - tmp);
+
+		/* Amount of load we'd add */
+		if (max_load*busiest->cpu_power <
+				busiest_load_per_task*SCHED_LOAD_SCALE)
+			tmp = max_load*busiest->cpu_power/this->cpu_power;
+		else
+			tmp = busiest_load_per_task*SCHED_LOAD_SCALE/this->cpu_power;
+		pwr_move += this->cpu_power*min(this_load_per_task, this_load + tmp);
+		pwr_move /= SCHED_LOAD_SCALE;
+
+		/* Move if we gain throughput */
+		if (pwr_move <= pwr_now)
+			goto out_balanced;
+
+		*imbalance = busiest_load_per_task;
+	}
+
+	return busiest;
+
+out_balanced:
+#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)
+	if (idle == NOT_IDLE || !(sd->flags & SD_POWERSAVINGS_BALANCE))
+		goto ret;
+
+	if (this == group_leader && group_leader != group_min) {
+		*imbalance = min_load_per_task;
+		return group_min;
+	}
+ret:
+#endif
+	*imbalance = 0;
+	return NULL;
+}
+
+/*
+ * find_busiest_queue - find the busiest runqueue among the cpus in group.
+ */
+static struct rq *
+find_busiest_queue(struct sched_group *group, enum idle_type idle,
+		   unsigned long imbalance)
+{
+	struct rq *busiest = NULL, *rq;
+	unsigned long max_load = 0;
+	int i;
+
+	for_each_cpu_mask(i, group->cpumask) {
+		rq = cpu_rq(i);
+
+		if (rq->nr_running == 1 && rq->raw_weighted_load > imbalance)
+			continue;
+
+		if (rq->raw_weighted_load > max_load) {
+			max_load = rq->raw_weighted_load;
+			busiest = rq;
+		}
+	}
+
+	return busiest;
+}
+
+/*
+ * Max backoff if we encounter pinned tasks. Pretty arbitrary value, but
+ * so long as it is large enough.
+ */
+#define MAX_PINNED_INTERVAL	512
+
+static inline unsigned long minus_1_or_zero(unsigned long n)
+{
+	return n > 0 ? n - 1 : 0;
+}
+
+/*
+ * Check this_cpu to ensure it is balanced within domain. Attempt to move
+ * tasks if there is an imbalance.
+ *
+ * Called with this_rq unlocked.
+ */
+static int load_balance(int this_cpu, struct rq *this_rq,
+			struct sched_domain *sd, enum idle_type idle)
+{
+	int nr_moved, all_pinned = 0, active_balance = 0, sd_idle = 0;
+	struct sched_group *group;
+	unsigned long imbalance;
+	struct rq *busiest;
+
+	if (idle != NOT_IDLE && sd->flags & SD_SHARE_CPUPOWER &&
+	    !sched_smt_power_savings)
+		sd_idle = 1;
+
+	schedstat_inc(sd, lb_cnt[idle]);
+
+	group = find_busiest_group(sd, this_cpu, &imbalance, idle, &sd_idle);
+	if (!group) {
+		schedstat_inc(sd, lb_nobusyg[idle]);
+		goto out_balanced;
+	}
+
+	busiest = find_busiest_queue(group, idle, imbalance);
+	if (!busiest) {
+		schedstat_inc(sd, lb_nobusyq[idle]);
+		goto out_balanced;
+	}
+
+	BUG_ON(busiest == this_rq);
+
+	schedstat_add(sd, lb_imbalance[idle], imbalance);
+
+	nr_moved = 0;
+	if (busiest->nr_running > 1) {
+		/*
+		 * Attempt to move tasks. If find_busiest_group has found
+		 * an imbalance but busiest->nr_running <= 1, the group is
+		 * still unbalanced. nr_moved simply stays zero, so it is
+		 * correctly treated as an imbalance.
+		 */
+		double_rq_lock(this_rq, busiest);
+		nr_moved = move_tasks(this_rq, this_cpu, busiest,
+				      minus_1_or_zero(busiest->nr_running),
+				      imbalance, sd, idle, &all_pinned);
+		double_rq_unlock(this_rq, busiest);
+
+		/* All tasks on this runqueue were pinned by CPU affinity */
+		if (unlikely(all_pinned))
+			goto out_balanced;
+	}
+
+	if (!nr_moved) {
+		schedstat_inc(sd, lb_failed[idle]);
+		sd->nr_balance_failed++;
+
+		if (unlikely(sd->nr_balance_failed > sd->cache_nice_tries+2)) {
+
+			spin_lock(&busiest->lock);
+
+			/* don't kick the migration_thread, if the curr
+			 * task on busiest cpu can't be moved to this_cpu
+			 */
+			if (!cpu_isset(this_cpu, busiest->curr->cpus_allowed)) {
+				spin_unlock(&busiest->lock);
+				all_pinned = 1;
+				goto out_one_pinned;
+			}
+
+			if (!busiest->active_balance) {
+				busiest->active_balance = 1;
+				busiest->push_cpu = this_cpu;
+				active_balance = 1;
+			}
+			spin_unlock(&busiest->lock);
+			if (active_balance)
+				wake_up_process(busiest->migration_thread);
+
+			/*
+			 * We've kicked active balancing, reset the failure
+			 * counter.
+			 */
+			sd->nr_balance_failed = sd->cache_nice_tries+1;
+		}
+	} else
+		sd->nr_balance_failed = 0;
+
+	if (likely(!active_balance)) {
+		/* We were unbalanced, so reset the balancing interval */
+		sd->balance_interval = sd->min_interval;
+	} else {
+		/*
+		 * If we've begun active balancing, start to back off. This
+		 * case may not be covered by the all_pinned logic if there
+		 * is only 1 task on the busy runqueue (because we don't call
+		 * move_tasks).
+		 */
+		if (sd->balance_interval < sd->max_interval)
+			sd->balance_interval *= 2;
+	}
+
+	if (!nr_moved && !sd_idle && sd->flags & SD_SHARE_CPUPOWER &&
+	    !sched_smt_power_savings)
+		return -1;
+	return nr_moved;
+
+out_balanced:
+	schedstat_inc(sd, lb_balanced[idle]);
+
+	sd->nr_balance_failed = 0;
+
+out_one_pinned:
+	/* tune up the balancing interval */
+	if ((all_pinned && sd->balance_interval < MAX_PINNED_INTERVAL) ||
+			(sd->balance_interval < sd->max_interval))
+		sd->balance_interval *= 2;
+
+	if (!sd_idle && sd->flags & SD_SHARE_CPUPOWER &&
+			!sched_smt_power_savings)
+		return -1;
+	return 0;
+}
+
+/*
+ * Check this_cpu to ensure it is balanced within domain. Attempt to move
+ * tasks if there is an imbalance.
+ *
+ * Called from schedule when this_rq is about to become idle (NEWLY_IDLE).
+ * this_rq is locked.
+ */
+static int
+load_balance_newidle(int this_cpu, struct rq *this_rq, struct sched_domain *sd)
+{
+	struct sched_group *group;
+	struct rq *busiest = NULL;
+	unsigned long imbalance;
+	int nr_moved = 0;
+	int sd_idle = 0;
+
+	if (sd->flags & SD_SHARE_CPUPOWER && !sched_smt_power_savings)
+		sd_idle = 1;
+
+	schedstat_inc(sd, lb_cnt[NEWLY_IDLE]);
+	group = find_busiest_group(sd, this_cpu, &imbalance, NEWLY_IDLE, &sd_idle);
+	if (!group) {
+		schedstat_inc(sd, lb_nobusyg[NEWLY_IDLE]);
+		goto out_balanced;
+	}
+
+	busiest = find_busiest_queue(group, NEWLY_IDLE, imbalance);
+	if (!busiest) {
+		schedstat_inc(sd, lb_nobusyq[NEWLY_IDLE]);
+		goto out_balanced;
+	}
+
+	BUG_ON(busiest == this_rq);
+
+	schedstat_add(sd, lb_imbalance[NEWLY_IDLE], imbalance);
+
+	nr_moved = 0;
+	if (busiest->nr_running > 1) {
+		/* Attempt to move tasks */
+		double_lock_balance(this_rq, busiest);
+		nr_moved = move_tasks(this_rq, this_cpu, busiest,
+					minus_1_or_zero(busiest->nr_running),
+					imbalance, sd, NEWLY_IDLE, NULL);
+		spin_unlock(&busiest->lock);
+	}
+
+	if (!nr_moved) {
+		schedstat_inc(sd, lb_failed[NEWLY_IDLE]);
+		if (!sd_idle && sd->flags & SD_SHARE_CPUPOWER)
+			return -1;
+	} else
+		sd->nr_balance_failed = 0;
+
+	return nr_moved;
+
+out_balanced:
+	schedstat_inc(sd, lb_balanced[NEWLY_IDLE]);
+	if (!sd_idle && sd->flags & SD_SHARE_CPUPOWER &&
+					!sched_smt_power_savings)
+		return -1;
+	sd->nr_balance_failed = 0;
+
+	return 0;
+}
+
+/*
+ * idle_balance is called by schedule() if this_cpu is about to become
+ * idle. Attempts to pull tasks from other CPUs.
+ */
+static void idle_balance(int this_cpu, struct rq *this_rq)
+{
+	struct sched_domain *sd;
+
+	for_each_domain(this_cpu, sd) {
+		if (sd->flags & SD_BALANCE_NEWIDLE) {
+			/* If we've pulled tasks over stop searching: */
+			if (load_balance_newidle(this_cpu, this_rq, sd))
+				break;
+		}
+	}
+}
+
+/*
+ * active_load_balance is run by migration threads. It pushes running tasks
+ * off the busiest CPU onto idle CPUs. It requires at least 1 task to be
+ * running on each physical CPU where possible, and avoids physical /
+ * logical imbalances.
+ *
+ * Called with busiest_rq locked.
+ */
+static void active_load_balance(struct rq *busiest_rq, int busiest_cpu)
+{
+	int target_cpu = busiest_rq->push_cpu;
+	struct sched_domain *sd;
+	struct rq *target_rq;
+
+	/* Is there any task to move? */
+	if (busiest_rq->nr_running <= 1)
+		return;
+
+	target_rq = cpu_rq(target_cpu);
+
+	/*
+	 * This condition is "impossible", if it occurs
+	 * we need to fix it.  Originally reported by
+	 * Bjorn Helgaas on a 128-cpu setup.
+	 */
+	BUG_ON(busiest_rq == target_rq);
+
+	/* move a task from busiest_rq to target_rq */
+	double_lock_balance(busiest_rq, target_rq);
+
+	/* Search for an sd spanning us and the target CPU. */
+	for_each_domain(target_cpu, sd) {
+		if ((sd->flags & SD_LOAD_BALANCE) &&
+		    cpu_isset(busiest_cpu, sd->span))
+				break;
+	}
+
+	if (likely(sd)) {
+		schedstat_inc(sd, alb_cnt);
+
+		if (move_tasks(target_rq, target_cpu, busiest_rq, 1,
+			       RTPRIO_TO_LOAD_WEIGHT(100), sd, SCHED_IDLE,
+			       NULL))
+			schedstat_inc(sd, alb_pushed);
+		else
+			schedstat_inc(sd, alb_failed);
+	}
+	spin_unlock(&target_rq->lock);
+}
+
+/*
+ * rebalance_tick will get called every timer tick, on every CPU.
+ *
+ * It checks each scheduling domain to see if it is due to be balanced,
+ * and initiates a balancing operation if so.
+ *
+ * Balancing parameters are set up in arch_init_sched_domains.
+ */
+
+/* Don't have all balancing operations going off at once: */
+static inline unsigned long cpu_offset(int cpu)
+{
+	return jiffies + cpu * HZ / NR_CPUS;
+}
+
+static void
+rebalance_tick(int this_cpu, struct rq *this_rq, enum idle_type idle)
+{
+	unsigned long this_load, interval, j = cpu_offset(this_cpu);
+	struct sched_domain *sd;
+	int i, scale;
+
+	this_load = this_rq->raw_weighted_load;
+
+	/* Update our load: */
+	for (i = 0, scale = 1; i < 3; i++, scale <<= 1) {
+		unsigned long old_load, new_load;
+
+		old_load = this_rq->cpu_load[i];
+		new_load = this_load;
+		/*
+		 * Round up the averaging division if load is increasing. This
+		 * prevents us from getting stuck on 9 if the load is 10, for
+		 * example.
+		 */
+		if (new_load > old_load)
+			new_load += scale-1;
+		this_rq->cpu_load[i] = (old_load*(scale-1) + new_load) / scale;
+	}
+
+	for_each_domain(this_cpu, sd) {
+		if (!(sd->flags & SD_LOAD_BALANCE))
+			continue;
+
+		interval = sd->balance_interval;
+		if (idle != SCHED_IDLE)
+			interval *= sd->busy_factor;
+
+		/* scale ms to jiffies */
+		interval = msecs_to_jiffies(interval);
+		if (unlikely(!interval))
+			interval = 1;
+
+		if (j - sd->last_balance >= interval) {
+			if (load_balance(this_cpu, this_rq, sd, idle)) {
+				/*
+				 * We've pulled tasks over so either we're no
+				 * longer idle, or one of our SMT siblings is
+				 * not idle.
+				 */
+				idle = NOT_IDLE;
+			}
+			sd->last_balance += interval;
+		}
+	}
+}
+#else
+/*
+ * on UP we do not need to balance between CPUs:
+ */
+static inline void rebalance_tick(int cpu, struct rq *rq, enum idle_type idle)
+{
+}
+static inline void idle_balance(int cpu, struct rq *rq)
+{
+}
+#endif
+
+static inline int wake_priority_sleeper(struct rq *rq)
+{
+	int ret = 0;
+
+#ifdef CONFIG_SCHED_SMT
+	spin_lock(&rq->lock);
+	/*
+	 * If an SMT sibling task has been put to sleep for priority
+	 * reasons reschedule the idle task to see if it can now run.
+	 */
+	if (rq->nr_running) {
+		resched_task(rq->idle);
+		ret = 1;
+	}
+	spin_unlock(&rq->lock);
+#endif
+	return ret;
+}
+
+DEFINE_PER_CPU(struct kernel_stat, kstat);
+
+EXPORT_PER_CPU_SYMBOL(kstat);
+
+/*
+ * This is called on clock ticks and on context switches.
+ * Bank in p->sched_time the ns elapsed since the last tick or switch.
+ */
+static inline void
+update_cpu_clock(struct task_struct *p, struct rq *rq, unsigned long long now)
+{
+	p->sched_time += now - max(p->timestamp, rq->timestamp_last_tick);
+}
+
+/*
+ * Return current->sched_time plus any more ns on the sched_clock
+ * that have not yet been banked.
+ */
+unsigned long long current_sched_time(const struct task_struct *p)
+{
+	unsigned long long ns;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	ns = max(p->timestamp, task_rq(p)->timestamp_last_tick);
+	ns = p->sched_time + sched_clock() - ns;
+	local_irq_restore(flags);
+
+	return ns;
+}
+
+/*
+ * We place interactive tasks back into the active array, if possible.
+ *
+ * To guarantee that this does not starve expired tasks we ignore the
+ * interactivity of a task if the first expired task had to wait more
+ * than a 'reasonable' amount of time. This deadline timeout is
+ * load-dependent, as the frequency of array switched decreases with
+ * increasing number of running tasks. We also ignore the interactivity
+ * if a better static_prio task has expired:
+ */
+static inline int expired_starving(struct rq *rq)
+{
+	if (rq->curr->static_prio > rq->best_expired_prio)
+		return 1;
+	if (!STARVATION_LIMIT || !rq->expired_timestamp)
+		return 0;
+	if (jiffies - rq->expired_timestamp > STARVATION_LIMIT * rq->nr_running)
+		return 1;
+	return 0;
+}
+
+/*
+ * Account user cpu time to a process.
+ * @p: the process that the cpu time gets accounted to
+ * @hardirq_offset: the offset to subtract from hardirq_count()
+ * @cputime: the cpu time spent in user space since the last update
+ */
+void account_user_time(struct task_struct *p, cputime_t cputime)
+{
+	struct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;
+	cputime64_t tmp;
+
+	p->utime = cputime_add(p->utime, cputime);
+
+	/* Add user time to cpustat. */
+	tmp = cputime_to_cputime64(cputime);
+	if (TASK_NICE(p) > 0)
+		cpustat->nice = cputime64_add(cpustat->nice, tmp);
+	else
+		cpustat->user = cputime64_add(cpustat->user, tmp);
+}
+
+/*
+ * Account system cpu time to a process.
+ * @p: the process that the cpu time gets accounted to
+ * @hardirq_offset: the offset to subtract from hardirq_count()
+ * @cputime: the cpu time spent in kernel space since the last update
+ */
+void account_system_time(struct task_struct *p, int hardirq_offset,
+			 cputime_t cputime)
+{
+	struct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;
+	struct rq *rq = this_rq();
+	cputime64_t tmp;
+
+	p->stime = cputime_add(p->stime, cputime);
+
+	/* Add system time to cpustat. */
+	tmp = cputime_to_cputime64(cputime);
+	if (hardirq_count() - hardirq_offset)
+		cpustat->irq = cputime64_add(cpustat->irq, tmp);
+	else if (softirq_count())
+		cpustat->softirq = cputime64_add(cpustat->softirq, tmp);
+	else if (p != rq->idle)
+		cpustat->system = cputime64_add(cpustat->system, tmp);
+	else if (atomic_read(&rq->nr_iowait) > 0)
+		cpustat->iowait = cputime64_add(cpustat->iowait, tmp);
+	else
+		cpustat->idle = cputime64_add(cpustat->idle, tmp);
+	/* Account for system time used */
+	acct_update_integrals(p);
+}
+
+/*
+ * Account for involuntary wait time.
+ * @p: the process from which the cpu time has been stolen
+ * @steal: the cpu time spent in involuntary wait
+ */
+void account_steal_time(struct task_struct *p, cputime_t steal)
+{
+	struct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;
+	cputime64_t tmp = cputime_to_cputime64(steal);
+	struct rq *rq = this_rq();
+
+	if (p == rq->idle) {
+		p->stime = cputime_add(p->stime, steal);
+		if (atomic_read(&rq->nr_iowait) > 0)
+			cpustat->iowait = cputime64_add(cpustat->iowait, tmp);
+		else
+			cpustat->idle = cputime64_add(cpustat->idle, tmp);
+	} else
+		cpustat->steal = cputime64_add(cpustat->steal, tmp);
+}
+
+/*
+ * This function gets called by the timer code, with HZ frequency.
+ * We call it with interrupts disabled.
+ *
+ * It also gets called by the fork code, when changing the parent's
+ * timeslices.
+ */
+void scheduler_tick(void)
+{
+	unsigned long long now = sched_clock();
+	struct task_struct *p = current;
+	int cpu = smp_processor_id();
+	struct rq *rq = cpu_rq(cpu);
+
+	update_cpu_clock(p, rq, now);
+
+	rq->timestamp_last_tick = now;
+
+	if (p == rq->idle) {
+		if (wake_priority_sleeper(rq))
+			goto out;
+		rebalance_tick(cpu, rq, SCHED_IDLE);
+		return;
+	}
+
+	/* Task might have expired already, but not scheduled off yet */
+	if (p->array != rq->active) {
+		set_tsk_need_resched(p);
+		goto out;
+	}
+	spin_lock(&rq->lock);
+	/*
+	 * The task was running during this tick - update the
+	 * time slice counter. Note: we do not update a thread's
+	 * priority until it either goes to sleep or uses up its
+	 * timeslice. This makes it possible for interactive tasks
+	 * to use up their timeslices at their highest priority levels.
+	 */
+	if (rt_task(p)) {
+		/*
+		 * RR tasks need a special form of timeslice management.
+		 * FIFO tasks have no timeslices.
+		 */
+		if ((p->policy == SCHED_RR) && !--p->time_slice) {
+			p->time_slice = task_timeslice(p);
+			p->first_time_slice = 0;
+			set_tsk_need_resched(p);
+
+			/* put it at the end of the queue: */
+			requeue_task(p, rq->active);
+		}
+		goto out_unlock;
+	}
+	if (!--p->time_slice) {
+		dequeue_task(p, rq->active);
+		set_tsk_need_resched(p);
+		p->prio = effective_prio(p);
+		p->time_slice = task_timeslice(p);
+		p->first_time_slice = 0;
+
+		if (!rq->expired_timestamp)
+			rq->expired_timestamp = jiffies;
+		if (!TASK_INTERACTIVE(p) || expired_starving(rq)) {
+			enqueue_task(p, rq->expired);
+			if (p->static_prio < rq->best_expired_prio)
+				rq->best_expired_prio = p->static_prio;
+		} else
+			enqueue_task(p, rq->active);
+	} else {
+		/*
+		 * Prevent a too long timeslice allowing a task to monopolize
+		 * the CPU. We do this by splitting up the timeslice into
+		 * smaller pieces.
+		 *
+		 * Note: this does not mean the task's timeslices expire or
+		 * get lost in any way, they just might be preempted by
+		 * another task of equal priority. (one with higher
+		 * priority would have preempted this task already.) We
+		 * requeue this task to the end of the list on this priority
+		 * level, which is in essence a round-robin of tasks with
+		 * equal priority.
+		 *
+		 * This only applies to tasks in the interactive
+		 * delta range with at least TIMESLICE_GRANULARITY to requeue.
+		 */
+		if (TASK_INTERACTIVE(p) && !((task_timeslice(p) -
+			p->time_slice) % TIMESLICE_GRANULARITY(p)) &&
+			(p->time_slice >= TIMESLICE_GRANULARITY(p)) &&
+			(p->array == rq->active)) {
+
+			requeue_task(p, rq->active);
+			set_tsk_need_resched(p);
+		}
+	}
+out_unlock:
+	spin_unlock(&rq->lock);
+out:
+	rebalance_tick(cpu, rq, NOT_IDLE);
+}
+
+#ifdef CONFIG_SCHED_SMT
+static inline void wakeup_busy_runqueue(struct rq *rq)
+{
+	/* If an SMT runqueue is sleeping due to priority reasons wake it up */
+	if (rq->curr == rq->idle && rq->nr_running)
+		resched_task(rq->idle);
+}
+
+/*
+ * Called with interrupt disabled and this_rq's runqueue locked.
+ */
+static void wake_sleeping_dependent(int this_cpu)
+{
+	struct sched_domain *tmp, *sd = NULL;
+	int i;
+
+	for_each_domain(this_cpu, tmp) {
+		if (tmp->flags & SD_SHARE_CPUPOWER) {
+			sd = tmp;
+			break;
+		}
+	}
+
+	if (!sd)
+		return;
+
+	for_each_cpu_mask(i, sd->span) {
+		struct rq *smt_rq = cpu_rq(i);
+
+		if (i == this_cpu)
+			continue;
+		if (unlikely(!spin_trylock(&smt_rq->lock)))
+			continue;
+
+		wakeup_busy_runqueue(smt_rq);
+		spin_unlock(&smt_rq->lock);
+	}
+}
+
+/*
+ * number of 'lost' timeslices this task wont be able to fully
+ * utilize, if another task runs on a sibling. This models the
+ * slowdown effect of other tasks running on siblings:
+ */
+static inline unsigned long
+smt_slice(struct task_struct *p, struct sched_domain *sd)
+{
+	return p->time_slice * (100 - sd->per_cpu_gain) / 100;
+}
+
+/*
+ * To minimise lock contention and not have to drop this_rq's runlock we only
+ * trylock the sibling runqueues and bypass those runqueues if we fail to
+ * acquire their lock. As we only trylock the normal locking order does not
+ * need to be obeyed.
+ */
+static int
+dependent_sleeper(int this_cpu, struct rq *this_rq, struct task_struct *p)
+{
+	struct sched_domain *tmp, *sd = NULL;
+	int ret = 0, i;
+
+	/* kernel/rt threads do not participate in dependent sleeping */
+	if (!p->mm || rt_task(p))
+		return 0;
+
+	for_each_domain(this_cpu, tmp) {
+		if (tmp->flags & SD_SHARE_CPUPOWER) {
+			sd = tmp;
+			break;
+		}
+	}
+
+	if (!sd)
+		return 0;
+
+	for_each_cpu_mask(i, sd->span) {
+		struct task_struct *smt_curr;
+		struct rq *smt_rq;
+
+		if (i == this_cpu)
+			continue;
+
+		smt_rq = cpu_rq(i);
+		if (unlikely(!spin_trylock(&smt_rq->lock)))
+			continue;
+
+		smt_curr = smt_rq->curr;
+
+		if (!smt_curr->mm)
+			goto unlock;
+
+		/*
+		 * If a user task with lower static priority than the
+		 * running task on the SMT sibling is trying to schedule,
+		 * delay it till there is proportionately less timeslice
+		 * left of the sibling task to prevent a lower priority
+		 * task from using an unfair proportion of the
+		 * physical cpu's resources. -ck
+		 */
+		if (rt_task(smt_curr)) {
+			/*
+			 * With real time tasks we run non-rt tasks only
+			 * per_cpu_gain% of the time.
+			 */
+			if ((jiffies % DEF_TIMESLICE) >
+				(sd->per_cpu_gain * DEF_TIMESLICE / 100))
+					ret = 1;
+		} else {
+			if (smt_curr->static_prio < p->static_prio &&
+				!TASK_PREEMPTS_CURR(p, smt_rq) &&
+				smt_slice(smt_curr, sd) > task_timeslice(p))
+					ret = 1;
+		}
+unlock:
+		spin_unlock(&smt_rq->lock);
+	}
+	return ret;
+}
+#else
+static inline void wake_sleeping_dependent(int this_cpu)
+{
+}
+static inline int
+dependent_sleeper(int this_cpu, struct rq *this_rq, struct task_struct *p)
+{
+	return 0;
+}
+#endif
+
+#if defined(CONFIG_PREEMPT) && defined(CONFIG_DEBUG_PREEMPT)
+
+void fastcall add_preempt_count(int val)
+{
+	/*
+	 * Underflow?
+	 */
+	if (DEBUG_LOCKS_WARN_ON((preempt_count() < 0)))
+		return;
+	preempt_count() += val;
+	/*
+	 * Spinlock count overflowing soon?
+	 */
+	DEBUG_LOCKS_WARN_ON((preempt_count() & PREEMPT_MASK) >= PREEMPT_MASK-10);
+}
+EXPORT_SYMBOL(add_preempt_count);
+
+void fastcall sub_preempt_count(int val)
+{
+	/*
+	 * Underflow?
+	 */
+	if (DEBUG_LOCKS_WARN_ON(val > preempt_count()))
+		return;
+	/*
+	 * Is the spinlock portion underflowing?
+	 */
+	if (DEBUG_LOCKS_WARN_ON((val < PREEMPT_MASK) &&
+			!(preempt_count() & PREEMPT_MASK)))
+		return;
+
+	preempt_count() -= val;
+}
+EXPORT_SYMBOL(sub_preempt_count);
+
+#endif
+
+static inline int interactive_sleep(enum sleep_type sleep_type)
+{
+	return (sleep_type == SLEEP_INTERACTIVE ||
+		sleep_type == SLEEP_INTERRUPTED);
+}
+
+/*
+ * schedule() is the main scheduler function.
+ */
+asmlinkage void __sched schedule(void)
+{
+	struct task_struct *prev, *next;
+	struct prio_array *array;
+	struct list_head *queue;
+	unsigned long long now;
+	unsigned long run_time;
+	int cpu, idx, new_prio;
+	long *switch_count;
+	struct rq *rq;
+
+	/*
+	 * Test if we are atomic.  Since do_exit() needs to call into
+	 * schedule() atomically, we ignore that path for now.
+	 * Otherwise, whine if we are scheduling when we should not be.
+	 */
+	if (unlikely(in_atomic() && !current->exit_state)) {
+		printk(KERN_ERR "BUG: scheduling while atomic: "
+			"%s/0x%08x/%d\n",
+			current->comm, preempt_count(), current->pid);
+		dump_stack();
+	}
+	profile_hit(SCHED_PROFILING, __builtin_return_address(0));
+
+need_resched:
+	preempt_disable();
+	prev = current;
+	release_kernel_lock(prev);
+need_resched_nonpreemptible:
+	rq = this_rq();
+
+	/*
+	 * The idle thread is not allowed to schedule!
+	 * Remove this check after it has been exercised a bit.
+	 */
+	if (unlikely(prev == rq->idle) && prev->state != TASK_RUNNING) {
+		printk(KERN_ERR "bad: scheduling from the idle thread!\n");
+		dump_stack();
+	}
+
+	schedstat_inc(rq, sched_cnt);
+	now = sched_clock();
+	if (likely((long long)(now - prev->timestamp) < NS_MAX_SLEEP_AVG)) {
+		run_time = now - prev->timestamp;
+		if (unlikely((long long)(now - prev->timestamp) < 0))
+			run_time = 0;
+	} else
+		run_time = NS_MAX_SLEEP_AVG;
+
+	/*
+	 * Tasks charged proportionately less run_time at high sleep_avg to
+	 * delay them losing their interactive status
+	 */
+	run_time /= (CURRENT_BONUS(prev) ? : 1);
+
+	spin_lock_irq(&rq->lock);
+
+	if (unlikely(prev->flags & PF_DEAD))
+		prev->state = EXIT_DEAD;
+
+	switch_count = &prev->nivcsw;
+	if (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {
+		switch_count = &prev->nvcsw;
+		if (unlikely((prev->state & TASK_INTERRUPTIBLE) &&
+				unlikely(signal_pending(prev))))
+			prev->state = TASK_RUNNING;
+		else {
+			if (prev->state == TASK_UNINTERRUPTIBLE)
+				rq->nr_uninterruptible++;
+			deactivate_task(prev, rq);
+		}
+	}
+
+	cpu = smp_processor_id();
+	if (unlikely(!rq->nr_running)) {
+		idle_balance(cpu, rq);
+		if (!rq->nr_running) {
+			next = rq->idle;
+			rq->expired_timestamp = 0;
+			wake_sleeping_dependent(cpu);
+			goto switch_tasks;
+		}
+	}
+
+	array = rq->active;
+	if (unlikely(!array->nr_active)) {
+		/*
+		 * Switch the active and expired arrays.
+		 */
+		schedstat_inc(rq, sched_switch);
+		rq->active = rq->expired;
+		rq->expired = array;
+		array = rq->active;
+		rq->expired_timestamp = 0;
+		rq->best_expired_prio = MAX_PRIO;
+	}
+
+	idx = sched_find_first_bit(array->bitmap);
+	queue = array->queue + idx;
+	next = list_entry(queue->next, struct task_struct, run_list);
+
+	if (!rt_task(next) && interactive_sleep(next->sleep_type)) {
+		unsigned long long delta = now - next->timestamp;
+		if (unlikely((long long)(now - next->timestamp) < 0))
+			delta = 0;
+
+		if (next->sleep_type == SLEEP_INTERACTIVE)
+			delta = delta * (ON_RUNQUEUE_WEIGHT * 128 / 100) / 128;
+
+		array = next->array;
+		new_prio = recalc_task_prio(next, next->timestamp + delta);
+
+		if (unlikely(next->prio != new_prio)) {
+			dequeue_task(next, array);
+			next->prio = new_prio;
+			enqueue_task(next, array);
+		}
+	}
+	next->sleep_type = SLEEP_NORMAL;
+	if (dependent_sleeper(cpu, rq, next))
+		next = rq->idle;
+switch_tasks:
+	if (next == rq->idle)
+		schedstat_inc(rq, sched_goidle);
+	prefetch(next);
+	prefetch_stack(next);
+	clear_tsk_need_resched(prev);
+	rcu_qsctr_inc(task_cpu(prev));
+
+	update_cpu_clock(prev, rq, now);
+
+	prev->sleep_avg -= run_time;
+	if ((long)prev->sleep_avg <= 0)
+		prev->sleep_avg = 0;
+	prev->timestamp = prev->last_ran = now;
+
+	sched_info_switch(prev, next);
+	if (likely(prev != next)) {
+		next->timestamp = now;
+		rq->nr_switches++;
+		rq->curr = next;
+		++*switch_count;
+
+		prepare_task_switch(rq, next);
+		prev = context_switch(rq, prev, next);
+		barrier();
+		/*
+		 * this_rq must be evaluated again because prev may have moved
+		 * CPUs since it called schedule(), thus the 'rq' on its stack
+		 * frame will be invalid.
+		 */
+		finish_task_switch(this_rq(), prev);
+	} else
+		spin_unlock_irq(&rq->lock);
+
+	prev = current;
+	if (unlikely(reacquire_kernel_lock(prev) < 0))
+		goto need_resched_nonpreemptible;
+	preempt_enable_no_resched();
+	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
+		goto need_resched;
+}
+EXPORT_SYMBOL(schedule);
+
+#ifdef CONFIG_PREEMPT
+/*
+ * this is the entry point to schedule() from in-kernel preemption
+ * off of preempt_enable.  Kernel preemptions off return from interrupt
+ * occur there and call schedule directly.
+ */
+asmlinkage void __sched preempt_schedule(void)
+{
+	struct thread_info *ti = current_thread_info();
+#ifdef CONFIG_PREEMPT_BKL
+	struct task_struct *task = current;
+	int saved_lock_depth;
+#endif
+	/*
+	 * If there is a non-zero preempt_count or interrupts are disabled,
+	 * we do not want to preempt the current task.  Just return..
+	 */
+	if (unlikely(ti->preempt_count || irqs_disabled()))
+		return;
+
+need_resched:
+	add_preempt_count(PREEMPT_ACTIVE);
+	/*
+	 * We keep the big kernel semaphore locked, but we
+	 * clear ->lock_depth so that schedule() doesnt
+	 * auto-release the semaphore:
+	 */
+#ifdef CONFIG_PREEMPT_BKL
+	saved_lock_depth = task->lock_depth;
+	task->lock_depth = -1;
+#endif
+	schedule();
+#ifdef CONFIG_PREEMPT_BKL
+	task->lock_depth = saved_lock_depth;
+#endif
+	sub_preempt_count(PREEMPT_ACTIVE);
+
+	/* we could miss a preemption opportunity between schedule and now */
+	barrier();
+	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
+		goto need_resched;
+}
+EXPORT_SYMBOL(preempt_schedule);
+
+/*
+ * this is the entry point to schedule() from kernel preemption
+ * off of irq context.
+ * Note, that this is called and return with irqs disabled. This will
+ * protect us against recursive calling from irq.
+ */
+asmlinkage void __sched preempt_schedule_irq(void)
+{
+	struct thread_info *ti = current_thread_info();
+#ifdef CONFIG_PREEMPT_BKL
+	struct task_struct *task = current;
+	int saved_lock_depth;
+#endif
+	/* Catch callers which need to be fixed */
+	BUG_ON(ti->preempt_count || !irqs_disabled());
+
+need_resched:
+	add_preempt_count(PREEMPT_ACTIVE);
+	/*
+	 * We keep the big kernel semaphore locked, but we
+	 * clear ->lock_depth so that schedule() doesnt
+	 * auto-release the semaphore:
+	 */
+#ifdef CONFIG_PREEMPT_BKL
+	saved_lock_depth = task->lock_depth;
+	task->lock_depth = -1;
+#endif
+	local_irq_enable();
+	schedule();
+	local_irq_disable();
+#ifdef CONFIG_PREEMPT_BKL
+	task->lock_depth = saved_lock_depth;
+#endif
+	sub_preempt_count(PREEMPT_ACTIVE);
+
+	/* we could miss a preemption opportunity between schedule and now */
+	barrier();
+	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
+		goto need_resched;
+}
+
+#endif /* CONFIG_PREEMPT */
+
+int default_wake_function(wait_queue_t *curr, unsigned mode, int sync,
+			  void *key)
+{
+	return try_to_wake_up(curr->private, mode, sync);
+}
+EXPORT_SYMBOL(default_wake_function);
+
+/*
+ * The core wakeup function.  Non-exclusive wakeups (nr_exclusive == 0) just
+ * wake everything up.  If it's an exclusive wakeup (nr_exclusive == small +ve
+ * number) then we wake all the non-exclusive tasks and one exclusive task.
+ *
+ * There are circumstances in which we can try to wake a task which has already
+ * started to run but is not in state TASK_RUNNING.  try_to_wake_up() returns
+ * zero in this (rare) case, and we handle it by continuing to scan the queue.
+ */
+static void __wake_up_common(wait_queue_head_t *q, unsigned int mode,
+			     int nr_exclusive, int sync, void *key)
+{
+	struct list_head *tmp, *next;
+
+	list_for_each_safe(tmp, next, &q->task_list) {
+		wait_queue_t *curr = list_entry(tmp, wait_queue_t, task_list);
+		unsigned flags = curr->flags;
+
+		if (curr->func(curr, mode, sync, key) &&
+				(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
+			break;
+	}
+}
+
+/**
+ * __wake_up - wake up threads blocked on a waitqueue.
+ * @q: the waitqueue
+ * @mode: which threads
+ * @nr_exclusive: how many wake-one or wake-many threads to wake up
+ * @key: is directly passed to the wakeup function
+ */
+void fastcall __wake_up(wait_queue_head_t *q, unsigned int mode,
+			int nr_exclusive, void *key)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&q->lock, flags);
+	__wake_up_common(q, mode, nr_exclusive, 0, key);
+	spin_unlock_irqrestore(&q->lock, flags);
+}
+EXPORT_SYMBOL(__wake_up);
+
+/*
+ * Same as __wake_up but called with the spinlock in wait_queue_head_t held.
+ */
+void fastcall __wake_up_locked(wait_queue_head_t *q, unsigned int mode)
+{
+	__wake_up_common(q, mode, 1, 0, NULL);
+}
+
+/**
+ * __wake_up_sync - wake up threads blocked on a waitqueue.
+ * @q: the waitqueue
+ * @mode: which threads
+ * @nr_exclusive: how many wake-one or wake-many threads to wake up
+ *
+ * The sync wakeup differs that the waker knows that it will schedule
+ * away soon, so while the target thread will be woken up, it will not
+ * be migrated to another CPU - ie. the two threads are 'synchronized'
+ * with each other. This can prevent needless bouncing between CPUs.
+ *
+ * On UP it can prevent extra preemption.
+ */
+void fastcall
+__wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
+{
+	unsigned long flags;
+	int sync = 1;
+
+	if (unlikely(!q))
+		return;
+
+	if (unlikely(!nr_exclusive))
+		sync = 0;
+
+	spin_lock_irqsave(&q->lock, flags);
+	__wake_up_common(q, mode, nr_exclusive, sync, NULL);
+	spin_unlock_irqrestore(&q->lock, flags);
+}
+EXPORT_SYMBOL_GPL(__wake_up_sync);	/* For internal use only */
+
+void fastcall complete(struct completion *x)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&x->wait.lock, flags);
+	x->done++;
+	__wake_up_common(&x->wait, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE,
+			 1, 0, NULL);
+	spin_unlock_irqrestore(&x->wait.lock, flags);
+}
+EXPORT_SYMBOL(complete);
+
+void fastcall complete_all(struct completion *x)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&x->wait.lock, flags);
+	x->done += UINT_MAX/2;
+	__wake_up_common(&x->wait, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE,
+			 0, 0, NULL);
+	spin_unlock_irqrestore(&x->wait.lock, flags);
+}
+EXPORT_SYMBOL(complete_all);
+
+void fastcall __sched wait_for_completion(struct completion *x)
+{
+	might_sleep();
+
+	spin_lock_irq(&x->wait.lock);
+	if (!x->done) {
+		DECLARE_WAITQUEUE(wait, current);
+
+		wait.flags |= WQ_FLAG_EXCLUSIVE;
+		__add_wait_queue_tail(&x->wait, &wait);
+		do {
+			__set_current_state(TASK_UNINTERRUPTIBLE);
+			spin_unlock_irq(&x->wait.lock);
+			schedule();
+			spin_lock_irq(&x->wait.lock);
+		} while (!x->done);
+		__remove_wait_queue(&x->wait, &wait);
+	}
+	x->done--;
+	spin_unlock_irq(&x->wait.lock);
+}
+EXPORT_SYMBOL(wait_for_completion);
+
+unsigned long fastcall __sched
+wait_for_completion_timeout(struct completion *x, unsigned long timeout)
+{
+	might_sleep();
+
+	spin_lock_irq(&x->wait.lock);
+	if (!x->done) {
+		DECLARE_WAITQUEUE(wait, current);
+
+		wait.flags |= WQ_FLAG_EXCLUSIVE;
+		__add_wait_queue_tail(&x->wait, &wait);
+		do {
+			__set_current_state(TASK_UNINTERRUPTIBLE);
+			spin_unlock_irq(&x->wait.lock);
+			timeout = schedule_timeout(timeout);
+			spin_lock_irq(&x->wait.lock);
+			if (!timeout) {
+				__remove_wait_queue(&x->wait, &wait);
+				goto out;
+			}
+		} while (!x->done);
+		__remove_wait_queue(&x->wait, &wait);
+	}
+	x->done--;
+out:
+	spin_unlock_irq(&x->wait.lock);
+	return timeout;
+}
+EXPORT_SYMBOL(wait_for_completion_timeout);
+
+int fastcall __sched wait_for_completion_interruptible(struct completion *x)
+{
+	int ret = 0;
+
+	might_sleep();
+
+	spin_lock_irq(&x->wait.lock);
+	if (!x->done) {
+		DECLARE_WAITQUEUE(wait, current);
+
+		wait.flags |= WQ_FLAG_EXCLUSIVE;
+		__add_wait_queue_tail(&x->wait, &wait);
+		do {
+			if (signal_pending(current)) {
+				ret = -ERESTARTSYS;
+				__remove_wait_queue(&x->wait, &wait);
+				goto out;
+			}
+			__set_current_state(TASK_INTERRUPTIBLE);
+			spin_unlock_irq(&x->wait.lock);
+			schedule();
+			spin_lock_irq(&x->wait.lock);
+		} while (!x->done);
+		__remove_wait_queue(&x->wait, &wait);
+	}
+	x->done--;
+out:
+	spin_unlock_irq(&x->wait.lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(wait_for_completion_interruptible);
+
+unsigned long fastcall __sched
+wait_for_completion_interruptible_timeout(struct completion *x,
+					  unsigned long timeout)
+{
+	might_sleep();
+
+	spin_lock_irq(&x->wait.lock);
+	if (!x->done) {
+		DECLARE_WAITQUEUE(wait, current);
+
+		wait.flags |= WQ_FLAG_EXCLUSIVE;
+		__add_wait_queue_tail(&x->wait, &wait);
+		do {
+			if (signal_pending(current)) {
+				timeout = -ERESTARTSYS;
+				__remove_wait_queue(&x->wait, &wait);
+				goto out;
+			}
+			__set_current_state(TASK_INTERRUPTIBLE);
+			spin_unlock_irq(&x->wait.lock);
+			timeout = schedule_timeout(timeout);
+			spin_lock_irq(&x->wait.lock);
+			if (!timeout) {
+				__remove_wait_queue(&x->wait, &wait);
+				goto out;
+			}
+		} while (!x->done);
+		__remove_wait_queue(&x->wait, &wait);
+	}
+	x->done--;
+out:
+	spin_unlock_irq(&x->wait.lock);
+	return timeout;
+}
+EXPORT_SYMBOL(wait_for_completion_interruptible_timeout);
+
+
+#define	SLEEP_ON_VAR					\
+	unsigned long flags;				\
+	wait_queue_t wait;				\
+	init_waitqueue_entry(&wait, current);
+
+#define SLEEP_ON_HEAD					\
+	spin_lock_irqsave(&q->lock,flags);		\
+	__add_wait_queue(q, &wait);			\
+	spin_unlock(&q->lock);
+
+#define	SLEEP_ON_TAIL					\
+	spin_lock_irq(&q->lock);			\
+	__remove_wait_queue(q, &wait);			\
+	spin_unlock_irqrestore(&q->lock, flags);
+
+void fastcall __sched interruptible_sleep_on(wait_queue_head_t *q)
+{
+	SLEEP_ON_VAR
+
+	current->state = TASK_INTERRUPTIBLE;
+
+	SLEEP_ON_HEAD
+	schedule();
+	SLEEP_ON_TAIL
+}
+EXPORT_SYMBOL(interruptible_sleep_on);
+
+long fastcall __sched
+interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
+{
+	SLEEP_ON_VAR
+
+	current->state = TASK_INTERRUPTIBLE;
+
+	SLEEP_ON_HEAD
+	timeout = schedule_timeout(timeout);
+	SLEEP_ON_TAIL
+
+	return timeout;
+}
+EXPORT_SYMBOL(interruptible_sleep_on_timeout);
+
+void fastcall __sched sleep_on(wait_queue_head_t *q)
+{
+	SLEEP_ON_VAR
+
+	current->state = TASK_UNINTERRUPTIBLE;
+
+	SLEEP_ON_HEAD
+	schedule();
+	SLEEP_ON_TAIL
+}
+EXPORT_SYMBOL(sleep_on);
+
+long fastcall __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)
+{
+	SLEEP_ON_VAR
+
+	current->state = TASK_UNINTERRUPTIBLE;
+
+	SLEEP_ON_HEAD
+	timeout = schedule_timeout(timeout);
+	SLEEP_ON_TAIL
+
+	return timeout;
+}
+
+EXPORT_SYMBOL(sleep_on_timeout);
+
+#ifdef CONFIG_RT_MUTEXES
+
+/*
+ * rt_mutex_setprio - set the current priority of a task
+ * @p: task
+ * @prio: prio value (kernel-internal form)
+ *
+ * This function changes the 'effective' priority of a task. It does
+ * not touch ->normal_prio like __setscheduler().
+ *
+ * Used by the rt_mutex code to implement priority inheritance logic.
+ */
+void rt_mutex_setprio(struct task_struct *p, int prio)
+{
+	struct prio_array *array;
+	unsigned long flags;
+	struct rq *rq;
+	int oldprio;
+
+	BUG_ON(prio < 0 || prio > MAX_PRIO);
+
+	rq = task_rq_lock(p, &flags);
+
+	oldprio = p->prio;
+	array = p->array;
+	if (array)
+		dequeue_task(p, array);
+	p->prio = prio;
+
+	if (array) {
+		/*
+		 * If changing to an RT priority then queue it
+		 * in the active array!
+		 */
+		if (rt_task(p))
+			array = rq->active;
+		enqueue_task(p, array);
+		/*
+		 * Reschedule if we are currently running on this runqueue and
+		 * our priority decreased, or if we are not currently running on
+		 * this runqueue and our priority is higher than the current's
+		 */
+		if (task_running(rq, p)) {
+			if (p->prio > oldprio)
+				resched_task(rq->curr);
+		} else if (TASK_PREEMPTS_CURR(p, rq))
+			resched_task(rq->curr);
+	}
+	task_rq_unlock(rq, &flags);
+}
+
+#endif
+
+void set_user_nice(struct task_struct *p, long nice)
+{
+	struct prio_array *array;
+	int old_prio, delta;
+	unsigned long flags;
+	struct rq *rq;
+
+	if (TASK_NICE(p) == nice || nice < -20 || nice > 19)
+		return;
+	/*
+	 * We have to be careful, if called from sys_setpriority(),
+	 * the task might be in the middle of scheduling on another CPU.
+	 */
+	rq = task_rq_lock(p, &flags);
+	/*
+	 * The RT priorities are set via sched_setscheduler(), but we still
+	 * allow the 'normal' nice value to be set - but as expected
+	 * it wont have any effect on scheduling until the task is
+	 * not SCHED_NORMAL/SCHED_BATCH:
+	 */
+	if (has_rt_policy(p)) {
+		p->static_prio = NICE_TO_PRIO(nice);
+		goto out_unlock;
+	}
+	array = p->array;
+	if (array) {
+		dequeue_task(p, array);
+		dec_raw_weighted_load(rq, p);
+	}
+
+	p->static_prio = NICE_TO_PRIO(nice);
+	set_load_weight(p);
+	old_prio = p->prio;
+	p->prio = effective_prio(p);
+	delta = p->prio - old_prio;
+
+	if (array) {
+		enqueue_task(p, array);
+		inc_raw_weighted_load(rq, p);
+		/*
+		 * If the task increased its priority or is running and
+		 * lowered its priority, then reschedule its CPU:
+		 */
+		if (delta < 0 || (delta > 0 && task_running(rq, p)))
+			resched_task(rq->curr);
+	}
+out_unlock:
+	task_rq_unlock(rq, &flags);
+}
+EXPORT_SYMBOL(set_user_nice);
+
+/*
+ * can_nice - check if a task can reduce its nice value
+ * @p: task
+ * @nice: nice value
+ */
+int can_nice(const struct task_struct *p, const int nice)
+{
+	/* convert nice value [19,-20] to rlimit style value [1,40] */
+	int nice_rlim = 20 - nice;
+
+	return (nice_rlim <= p->signal->rlim[RLIMIT_NICE].rlim_cur ||
+		capable(CAP_SYS_NICE));
+}
+
+#ifdef __ARCH_WANT_SYS_NICE
+
+/*
+ * sys_nice - change the priority of the current process.
+ * @increment: priority increment
+ *
+ * sys_setpriority is a more generic, but much slower function that
+ * does similar things.
+ */
+asmlinkage long sys_nice(int increment)
+{
+	long nice, retval;
+
+	/*
+	 * Setpriority might change our priority at the same moment.
+	 * We don't have to worry. Conceptually one call occurs first
+	 * and we have a single winner.
+	 */
+	if (increment < -40)
+		increment = -40;
+	if (increment > 40)
+		increment = 40;
+
+	nice = PRIO_TO_NICE(current->static_prio) + increment;
+	if (nice < -20)
+		nice = -20;
+	if (nice > 19)
+		nice = 19;
+
+	if (increment < 0 && !can_nice(current, nice))
+		return -EPERM;
+
+	retval = security_task_setnice(current, nice);
+	if (retval)
+		return retval;
+
+	set_user_nice(current, nice);
+	return 0;
+}
+
+#endif
+
+/**
+ * task_prio - return the priority value of a given task.
+ * @p: the task in question.
+ *
+ * This is the priority value as seen by users in /proc.
+ * RT tasks are offset by -200. Normal tasks are centered
+ * around 0, value goes from -16 to +15.
+ */
+int task_prio(const struct task_struct *p)
+{
+	return p->prio - MAX_RT_PRIO;
+}
+
+/**
+ * task_nice - return the nice value of a given task.
+ * @p: the task in question.
+ */
+int task_nice(const struct task_struct *p)
+{
+	return TASK_NICE(p);
+}
+EXPORT_SYMBOL_GPL(task_nice);
+
+/**
+ * idle_cpu - is a given cpu idle currently?
+ * @cpu: the processor in question.
+ */
+int idle_cpu(int cpu)
+{
+	return cpu_curr(cpu) == cpu_rq(cpu)->idle;
+}
+
+/**
+ * idle_task - return the idle task for a given cpu.
+ * @cpu: the processor in question.
+ */
+struct task_struct *idle_task(int cpu)
+{
+	return cpu_rq(cpu)->idle;
+}
+
+/**
+ * find_process_by_pid - find a process with a matching PID value.
+ * @pid: the pid in question.
+ */
+static inline struct task_struct *find_process_by_pid(pid_t pid)
+{
+	return pid ? find_task_by_pid(pid) : current;
+}
+
+/* Actually do priority change: must hold rq lock. */
+static void __setscheduler(struct task_struct *p, int policy, int prio)
+{
+	BUG_ON(p->array);
+
+	p->policy = policy;
+	p->rt_priority = prio;
+	p->normal_prio = normal_prio(p);
+	/* we are holding p->pi_lock already */
+	p->prio = rt_mutex_getprio(p);
+	/*
+	 * SCHED_BATCH tasks are treated as perpetual CPU hogs:
+	 */
+	if (policy == SCHED_BATCH)
+		p->sleep_avg = 0;
+	set_load_weight(p);
+}
+
+/**
+ * sched_setscheduler - change the scheduling policy and/or RT priority of
+ * a thread.
+ * @p: the task in question.
+ * @policy: new policy.
+ * @param: structure containing the new RT priority.
+ */
+int sched_setscheduler(struct task_struct *p, int policy,
+		       struct sched_param *param)
+{
+	int retval, oldprio, oldpolicy = -1;
+	struct prio_array *array;
+	unsigned long flags;
+	struct rq *rq;
+
+	/* may grab non-irq protected spin_locks */
+	BUG_ON(in_interrupt());
+recheck:
+	/* double check policy once rq lock held */
+	if (policy < 0)
+		policy = oldpolicy = p->policy;
+	else if (policy != SCHED_FIFO && policy != SCHED_RR &&
+			policy != SCHED_NORMAL && policy != SCHED_BATCH)
+		return -EINVAL;
+	/*
+	 * Valid priorities for SCHED_FIFO and SCHED_RR are
+	 * 1..MAX_USER_RT_PRIO-1, valid priority for SCHED_NORMAL and
+	 * SCHED_BATCH is 0.
+	 */
+	if (param->sched_priority < 0 ||
+	    (p->mm && param->sched_priority > MAX_USER_RT_PRIO-1) ||
+	    (!p->mm && param->sched_priority > MAX_RT_PRIO-1))
+		return -EINVAL;
+	if ((policy == SCHED_NORMAL || policy == SCHED_BATCH)
+					!= (param->sched_priority == 0))
+		return -EINVAL;
+
+	/*
+	 * Allow unprivileged RT tasks to decrease priority:
+	 */
+	if (!capable(CAP_SYS_NICE)) {
+		/*
+		 * can't change policy, except between SCHED_NORMAL
+		 * and SCHED_BATCH:
+		 */
+		if (((policy != SCHED_NORMAL && p->policy != SCHED_BATCH) &&
+			(policy != SCHED_BATCH && p->policy != SCHED_NORMAL)) &&
+				!p->signal->rlim[RLIMIT_RTPRIO].rlim_cur)
+			return -EPERM;
+		/* can't increase priority */
+		if ((policy != SCHED_NORMAL && policy != SCHED_BATCH) &&
+		    param->sched_priority > p->rt_priority &&
+		    param->sched_priority >
+				p->signal->rlim[RLIMIT_RTPRIO].rlim_cur)
+			return -EPERM;
+		/* can't change other user's priorities */
+		if ((current->euid != p->euid) &&
+		    (current->euid != p->uid))
+			return -EPERM;
+	}
+
+	retval = security_task_setscheduler(p, policy, param);
+	if (retval)
+		return retval;
+	/*
+	 * make sure no PI-waiters arrive (or leave) while we are
+	 * changing the priority of the task:
+	 */
+	spin_lock_irqsave(&p->pi_lock, flags);
+	/*
+	 * To be able to change p->policy safely, the apropriate
+	 * runqueue lock must be held.
+	 */
+	rq = __task_rq_lock(p);
+	/* recheck policy now with rq lock held */
+	if (unlikely(oldpolicy != -1 && oldpolicy != p->policy)) {
+		policy = oldpolicy = -1;
+		__task_rq_unlock(rq);
+		spin_unlock_irqrestore(&p->pi_lock, flags);
+		goto recheck;
+	}
+	array = p->array;
+	if (array)
+		deactivate_task(p, rq);
+	oldprio = p->prio;
+	__setscheduler(p, policy, param->sched_priority);
+	if (array) {
+		__activate_task(p, rq);
+		/*
+		 * Reschedule if we are currently running on this runqueue and
+		 * our priority decreased, or if we are not currently running on
+		 * this runqueue and our priority is higher than the current's
+		 */
+		if (task_running(rq, p)) {
+			if (p->prio > oldprio)
+				resched_task(rq->curr);
+		} else if (TASK_PREEMPTS_CURR(p, rq))
+			resched_task(rq->curr);
+	}
+	__task_rq_unlock(rq);
+	spin_unlock_irqrestore(&p->pi_lock, flags);
+
+	rt_mutex_adjust_pi(p);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(sched_setscheduler);
+
+static int
+do_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)
+{
+	struct sched_param lparam;
+	struct task_struct *p;
+	int retval;
+
+	if (!param || pid < 0)
+		return -EINVAL;
+	if (copy_from_user(&lparam, param, sizeof(struct sched_param)))
+		return -EFAULT;
+	read_lock_irq(&tasklist_lock);
+	p = find_process_by_pid(pid);
+	if (!p) {
+		read_unlock_irq(&tasklist_lock);
+		return -ESRCH;
+	}
+	get_task_struct(p);
+	read_unlock_irq(&tasklist_lock);
+	retval = sched_setscheduler(p, policy, &lparam);
+	put_task_struct(p);
+
+	return retval;
+}
+
+/**
+ * sys_sched_setscheduler - set/change the scheduler policy and RT priority
+ * @pid: the pid in question.
+ * @policy: new policy.
+ * @param: structure containing the new RT priority.
+ */
+asmlinkage long sys_sched_setscheduler(pid_t pid, int policy,
+				       struct sched_param __user *param)
+{
+	/* negative values for policy are not valid */
+	if (policy < 0)
+		return -EINVAL;
+
+	return do_sched_setscheduler(pid, policy, param);
+}
+
+/**
+ * sys_sched_setparam - set/change the RT priority of a thread
+ * @pid: the pid in question.
+ * @param: structure containing the new RT priority.
+ */
+asmlinkage long sys_sched_setparam(pid_t pid, struct sched_param __user *param)
+{
+	return do_sched_setscheduler(pid, -1, param);
+}
+
+/**
+ * sys_sched_getscheduler - get the policy (scheduling class) of a thread
+ * @pid: the pid in question.
+ */
+asmlinkage long sys_sched_getscheduler(pid_t pid)
+{
+	struct task_struct *p;
+	int retval = -EINVAL;
+
+	if (pid < 0)
+		goto out_nounlock;
+
+	retval = -ESRCH;
+	read_lock(&tasklist_lock);
+	p = find_process_by_pid(pid);
+	if (p) {
+		retval = security_task_getscheduler(p);
+		if (!retval)
+			retval = p->policy;
+	}
+	read_unlock(&tasklist_lock);
+
+out_nounlock:
+	return retval;
+}
+
+/**
+ * sys_sched_getscheduler - get the RT priority of a thread
+ * @pid: the pid in question.
+ * @param: structure containing the RT priority.
+ */
+asmlinkage long sys_sched_getparam(pid_t pid, struct sched_param __user *param)
+{
+	struct sched_param lp;
+	struct task_struct *p;
+	int retval = -EINVAL;
+
+	if (!param || pid < 0)
+		goto out_nounlock;
+
+	read_lock(&tasklist_lock);
+	p = find_process_by_pid(pid);
+	retval = -ESRCH;
+	if (!p)
+		goto out_unlock;
+
+	retval = security_task_getscheduler(p);
+	if (retval)
+		goto out_unlock;
+
+	lp.sched_priority = p->rt_priority;
+	read_unlock(&tasklist_lock);
+
+	/*
+	 * This one might sleep, we cannot do it with a spinlock held ...
+	 */
+	retval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;
+
+out_nounlock:
+	return retval;
+
+out_unlock:
+	read_unlock(&tasklist_lock);
+	return retval;
+}
+
+long sched_setaffinity(pid_t pid, cpumask_t new_mask)
+{
+	cpumask_t cpus_allowed;
+	struct task_struct *p;
+	int retval;
+
+	lock_cpu_hotplug();
+	read_lock(&tasklist_lock);
+
+	p = find_process_by_pid(pid);
+	if (!p) {
+		read_unlock(&tasklist_lock);
+		unlock_cpu_hotplug();
+		return -ESRCH;
+	}
+
+	/*
+	 * It is not safe to call set_cpus_allowed with the
+	 * tasklist_lock held.  We will bump the task_struct's
+	 * usage count and then drop tasklist_lock.
+	 */
+	get_task_struct(p);
+	read_unlock(&tasklist_lock);
+
+	retval = -EPERM;
+	if ((current->euid != p->euid) && (current->euid != p->uid) &&
+			!capable(CAP_SYS_NICE))
+		goto out_unlock;
+
+	retval = security_task_setscheduler(p, 0, NULL);
+	if (retval)
+		goto out_unlock;
+
+	cpus_allowed = cpuset_cpus_allowed(p);
+	cpus_and(new_mask, new_mask, cpus_allowed);
+	retval = set_cpus_allowed(p, new_mask);
+
+out_unlock:
+	put_task_struct(p);
+	unlock_cpu_hotplug();
+	return retval;
+}
+
+static int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,
+			     cpumask_t *new_mask)
+{
+	if (len < sizeof(cpumask_t)) {
+		memset(new_mask, 0, sizeof(cpumask_t));
+	} else if (len > sizeof(cpumask_t)) {
+		len = sizeof(cpumask_t);
+	}
+	return copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;
+}
+
+/**
+ * sys_sched_setaffinity - set the cpu affinity of a process
+ * @pid: pid of the process
+ * @len: length in bytes of the bitmask pointed to by user_mask_ptr
+ * @user_mask_ptr: user-space pointer to the new cpu mask
+ */
+asmlinkage long sys_sched_setaffinity(pid_t pid, unsigned int len,
+				      unsigned long __user *user_mask_ptr)
+{
+	cpumask_t new_mask;
+	int retval;
+
+	retval = get_user_cpu_mask(user_mask_ptr, len, &new_mask);
+	if (retval)
+		return retval;
+
+	return sched_setaffinity(pid, new_mask);
+}
+
+/*
+ * Represents all cpu's present in the system
+ * In systems capable of hotplug, this map could dynamically grow
+ * as new cpu's are detected in the system via any platform specific
+ * method, such as ACPI for e.g.
+ */
+
+cpumask_t cpu_present_map __read_mostly;
+EXPORT_SYMBOL(cpu_present_map);
+
+#ifndef CONFIG_SMP
+cpumask_t cpu_online_map __read_mostly = CPU_MASK_ALL;
+cpumask_t cpu_possible_map __read_mostly = CPU_MASK_ALL;
+#endif
+
+long sched_getaffinity(pid_t pid, cpumask_t *mask)
+{
+	struct task_struct *p;
+	int retval;
+
+	lock_cpu_hotplug();
+	read_lock(&tasklist_lock);
+
+	retval = -ESRCH;
+	p = find_process_by_pid(pid);
+	if (!p)
+		goto out_unlock;
+
+	retval = security_task_getscheduler(p);
+	if (retval)
+		goto out_unlock;
+
+	cpus_and(*mask, p->cpus_allowed, cpu_online_map);
+
+out_unlock:
+	read_unlock(&tasklist_lock);
+	unlock_cpu_hotplug();
+	if (retval)
+		return retval;
+
+	return 0;
+}
+
+/**
+ * sys_sched_getaffinity - get the cpu affinity of a process
+ * @pid: pid of the process
+ * @len: length in bytes of the bitmask pointed to by user_mask_ptr
+ * @user_mask_ptr: user-space pointer to hold the current cpu mask
+ */
+asmlinkage long sys_sched_getaffinity(pid_t pid, unsigned int len,
+				      unsigned long __user *user_mask_ptr)
+{
+	int ret;
+	cpumask_t mask;
+
+	if (len < sizeof(cpumask_t))
+		return -EINVAL;
+
+	ret = sched_getaffinity(pid, &mask);
+	if (ret < 0)
+		return ret;
+
+	if (copy_to_user(user_mask_ptr, &mask, sizeof(cpumask_t)))
+		return -EFAULT;
+
+	return sizeof(cpumask_t);
+}
+
+/**
+ * sys_sched_yield - yield the current processor to other threads.
+ *
+ * this function yields the current CPU by moving the calling thread
+ * to the expired array. If there are no other threads running on this
+ * CPU then this function will return.
+ */
+asmlinkage long sys_sched_yield(void)
+{
+	struct rq *rq = this_rq_lock();
+	struct prio_array *array = current->array, *target = rq->expired;
+
+	schedstat_inc(rq, yld_cnt);
+	/*
+	 * We implement yielding by moving the task into the expired
+	 * queue.
+	 *
+	 * (special rule: RT tasks will just roundrobin in the active
+	 *  array.)
+	 */
+	if (rt_task(current))
+		target = rq->active;
+
+	if (array->nr_active == 1) {
+		schedstat_inc(rq, yld_act_empty);
+		if (!rq->expired->nr_active)
+			schedstat_inc(rq, yld_both_empty);
+	} else if (!rq->expired->nr_active)
+		schedstat_inc(rq, yld_exp_empty);
+
+	if (array != target) {
+		dequeue_task(current, array);
+		enqueue_task(current, target);
+	} else
+		/*
+		 * requeue_task is cheaper so perform that if possible.
+		 */
+		requeue_task(current, array);
+
+	/*
+	 * Since we are going to call schedule() anyway, there's
+	 * no need to preempt or enable interrupts:
+	 */
+	__release(rq->lock);
+	spin_release(&rq->lock.dep_map, 1, _THIS_IP_);
+	_raw_spin_unlock(&rq->lock);
+	preempt_enable_no_resched();
+
+	schedule();
+
+	return 0;
+}
+
+static inline int __resched_legal(void)
+{
+	if (unlikely(preempt_count()))
+		return 0;
+	if (unlikely(system_state != SYSTEM_RUNNING))
+		return 0;
+	return 1;
+}
+
+static void __cond_resched(void)
+{
+#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP
+	__might_sleep(__FILE__, __LINE__);
+#endif
+	/*
+	 * The BKS might be reacquired before we have dropped
+	 * PREEMPT_ACTIVE, which could trigger a second
+	 * cond_resched() call.
+	 */
+	do {
+		add_preempt_count(PREEMPT_ACTIVE);
+		schedule();
+		sub_preempt_count(PREEMPT_ACTIVE);
+	} while (need_resched());
+}
+
+int __sched cond_resched(void)
+{
+	if (need_resched() && __resched_legal()) {
+		__cond_resched();
+		return 1;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(cond_resched);
+
+/*
+ * cond_resched_lock() - if a reschedule is pending, drop the given lock,
+ * call schedule, and on return reacquire the lock.
+ *
+ * This works OK both with and without CONFIG_PREEMPT.  We do strange low-level
+ * operations here to prevent schedule() from being called twice (once via
+ * spin_unlock(), once by hand).
+ */
+int cond_resched_lock(spinlock_t *lock)
+{
+	int ret = 0;
+
+	if (need_lockbreak(lock)) {
+		spin_unlock(lock);
+		cpu_relax();
+		ret = 1;
+		spin_lock(lock);
+	}
+	if (need_resched() && __resched_legal()) {
+		spin_release(&lock->dep_map, 1, _THIS_IP_);
+		_raw_spin_unlock(lock);
+		preempt_enable_no_resched();
+		__cond_resched();
+		ret = 1;
+		spin_lock(lock);
+	}
+	return ret;
+}
+EXPORT_SYMBOL(cond_resched_lock);
+
+int __sched cond_resched_softirq(void)
+{
+	BUG_ON(!in_softirq());
+
+	if (need_resched() && __resched_legal()) {
+		raw_local_irq_disable();
+		_local_bh_enable();
+		raw_local_irq_enable();
+		__cond_resched();
+		local_bh_disable();
+		return 1;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(cond_resched_softirq);
+
+/**
+ * yield - yield the current processor to other threads.
+ *
+ * this is a shortcut for kernel-space yielding - it marks the
+ * thread runnable and calls sys_sched_yield().
+ */
+void __sched yield(void)
+{
+	set_current_state(TASK_RUNNING);
+	sys_sched_yield();
+}
+EXPORT_SYMBOL(yield);
+
+/*
+ * This task is about to go to sleep on IO.  Increment rq->nr_iowait so
+ * that process accounting knows that this is a task in IO wait state.
+ *
+ * But don't do that if it is a deliberate, throttling IO wait (this task
+ * has set its backing_dev_info: the queue against which it should throttle)
+ */
+void __sched io_schedule(void)
+{
+	struct rq *rq = &__raw_get_cpu_var(runqueues);
+
+	delayacct_blkio_start();
+	atomic_inc(&rq->nr_iowait);
+	schedule();
+	atomic_dec(&rq->nr_iowait);
+	delayacct_blkio_end();
+}
+EXPORT_SYMBOL(io_schedule);
+
+long __sched io_schedule_timeout(long timeout)
+{
+	struct rq *rq = &__raw_get_cpu_var(runqueues);
+	long ret;
+
+	delayacct_blkio_start();
+	atomic_inc(&rq->nr_iowait);
+	ret = schedule_timeout(timeout);
+	atomic_dec(&rq->nr_iowait);
+	delayacct_blkio_end();
+	return ret;
+}
+
+/**
+ * sys_sched_get_priority_max - return maximum RT priority.
+ * @policy: scheduling class.
+ *
+ * this syscall returns the maximum rt_priority that can be used
+ * by a given scheduling class.
+ */
+asmlinkage long sys_sched_get_priority_max(int policy)
+{
+	int ret = -EINVAL;
+
+	switch (policy) {
+	case SCHED_FIFO:
+	case SCHED_RR:
+		ret = MAX_USER_RT_PRIO-1;
+		break;
+	case SCHED_NORMAL:
+	case SCHED_BATCH:
+		ret = 0;
+		break;
+	}
+	return ret;
+}
+
+/**
+ * sys_sched_get_priority_min - return minimum RT priority.
+ * @policy: scheduling class.
+ *
+ * this syscall returns the minimum rt_priority that can be used
+ * by a given scheduling class.
+ */
+asmlinkage long sys_sched_get_priority_min(int policy)
+{
+	int ret = -EINVAL;
+
+	switch (policy) {
+	case SCHED_FIFO:
+	case SCHED_RR:
+		ret = 1;
+		break;
+	case SCHED_NORMAL:
+	case SCHED_BATCH:
+		ret = 0;
+	}
+	return ret;
+}
+
+/**
+ * sys_sched_rr_get_interval - return the default timeslice of a process.
+ * @pid: pid of the process.
+ * @interval: userspace pointer to the timeslice value.
+ *
+ * this syscall writes the default timeslice value of a given process
+ * into the user-space timespec buffer. A value of '0' means infinity.
+ */
+asmlinkage
+long sys_sched_rr_get_interval(pid_t pid, struct timespec __user *interval)
+{
+	struct task_struct *p;
+	int retval = -EINVAL;
+	struct timespec t;
+
+	if (pid < 0)
+		goto out_nounlock;
+
+	retval = -ESRCH;
+	read_lock(&tasklist_lock);
+	p = find_process_by_pid(pid);
+	if (!p)
+		goto out_unlock;
+
+	retval = security_task_getscheduler(p);
+	if (retval)
+		goto out_unlock;
+
+	jiffies_to_timespec(p->policy == SCHED_FIFO ?
+				0 : task_timeslice(p), &t);
+	read_unlock(&tasklist_lock);
+	retval = copy_to_user(interval, &t, sizeof(t)) ? -EFAULT : 0;
+out_nounlock:
+	return retval;
+out_unlock:
+	read_unlock(&tasklist_lock);
+	return retval;
+}
+
+static inline struct task_struct *eldest_child(struct task_struct *p)
+{
+	if (list_empty(&p->children))
+		return NULL;
+	return list_entry(p->children.next,struct task_struct,sibling);
+}
+
+static inline struct task_struct *older_sibling(struct task_struct *p)
+{
+	if (p->sibling.prev==&p->parent->children)
+		return NULL;
+	return list_entry(p->sibling.prev,struct task_struct,sibling);
+}
+
+static inline struct task_struct *younger_sibling(struct task_struct *p)
+{
+	if (p->sibling.next==&p->parent->children)
+		return NULL;
+	return list_entry(p->sibling.next,struct task_struct,sibling);
+}
+
+static const char stat_nam[] = "RSDTtZX";
+
+static void show_task(struct task_struct *p)
+{
+	struct task_struct *relative;
+	unsigned long free = 0;
+	unsigned state;
+
+	state = p->state ? __ffs(p->state) + 1 : 0;
+	printk("%-13.13s %c", p->comm,
+		state < sizeof(stat_nam) - 1 ? stat_nam[state] : '?');
+#if (BITS_PER_LONG == 32)
+	if (state == TASK_RUNNING)
+		printk(" running ");
+	else
+		printk(" %08lX ", thread_saved_pc(p));
+#else
+	if (state == TASK_RUNNING)
+		printk("  running task   ");
+	else
+		printk(" %016lx ", thread_saved_pc(p));
+#endif
+#ifdef CONFIG_DEBUG_STACK_USAGE
+	{
+		unsigned long *n = end_of_stack(p);
+		while (!*n)
+			n++;
+		free = (unsigned long)n - (unsigned long)end_of_stack(p);
+	}
+#endif
+	printk("%5lu %5d %6d ", free, p->pid, p->parent->pid);
+	if ((relative = eldest_child(p)))
+		printk("%5d ", relative->pid);
+	else
+		printk("      ");
+	if ((relative = younger_sibling(p)))
+		printk("%7d", relative->pid);
+	else
+		printk("       ");
+	if ((relative = older_sibling(p)))
+		printk(" %5d", relative->pid);
+	else
+		printk("      ");
+	if (!p->mm)
+		printk(" (L-TLB)\n");
+	else
+		printk(" (NOTLB)\n");
+
+	if (state != TASK_RUNNING)
+		show_stack(p, NULL);
+}
+
+void show_state(void)
+{
+	struct task_struct *g, *p;
+
+#if (BITS_PER_LONG == 32)
+	printk("\n"
+	       "                                               sibling\n");
+	printk("  task             PC      pid father child younger older\n");
+#else
+	printk("\n"
+	       "                                                       sibling\n");
+	printk("  task                 PC          pid father child younger older\n");
+#endif
+	read_lock(&tasklist_lock);
+	do_each_thread(g, p) {
+		/*
+		 * reset the NMI-timeout, listing all files on a slow
+		 * console might take alot of time:
+		 */
+		touch_nmi_watchdog();
+		show_task(p);
+	} while_each_thread(g, p);
+
+	read_unlock(&tasklist_lock);
+	debug_show_all_locks();
+}
+
+/**
+ * init_idle - set up an idle thread for a given CPU
+ * @idle: task in question
+ * @cpu: cpu the idle task belongs to
+ *
+ * NOTE: this function does not set the idle thread's NEED_RESCHED
+ * flag, to make booting more robust.
+ */
+void __devinit init_idle(struct task_struct *idle, int cpu)
+{
+	struct rq *rq = cpu_rq(cpu);
+	unsigned long flags;
+
+	idle->timestamp = sched_clock();
+	idle->sleep_avg = 0;
+	idle->array = NULL;
+	idle->prio = idle->normal_prio = MAX_PRIO;
+	idle->state = TASK_RUNNING;
+	idle->cpus_allowed = cpumask_of_cpu(cpu);
+	set_task_cpu(idle, cpu);
+
+	spin_lock_irqsave(&rq->lock, flags);
+	rq->curr = rq->idle = idle;
+#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)
+	idle->oncpu = 1;
+#endif
+	spin_unlock_irqrestore(&rq->lock, flags);
+
+	/* Set the preempt count _outside_ the spinlocks! */
+#if defined(CONFIG_PREEMPT) && !defined(CONFIG_PREEMPT_BKL)
+	task_thread_info(idle)->preempt_count = (idle->lock_depth >= 0);
+#else
+	task_thread_info(idle)->preempt_count = 0;
+#endif
+}
+
+/*
+ * In a system that switches off the HZ timer nohz_cpu_mask
+ * indicates which cpus entered this state. This is used
+ * in the rcu update to wait only for active cpus. For system
+ * which do not switch off the HZ timer nohz_cpu_mask should
+ * always be CPU_MASK_NONE.
+ */
+cpumask_t nohz_cpu_mask = CPU_MASK_NONE;
+
+#ifdef CONFIG_SMP
+/*
+ * This is how migration works:
+ *
+ * 1) we queue a struct migration_req structure in the source CPU's
+ *    runqueue and wake up that CPU's migration thread.
+ * 2) we down() the locked semaphore => thread blocks.
+ * 3) migration thread wakes up (implicitly it forces the migrated
+ *    thread off the CPU)
+ * 4) it gets the migration request and checks whether the migrated
+ *    task is still in the wrong runqueue.
+ * 5) if it's in the wrong runqueue then the migration thread removes
+ *    it and puts it into the right queue.
+ * 6) migration thread up()s the semaphore.
+ * 7) we wake up and the migration is done.
+ */
+
+/*
+ * Change a given task's CPU affinity. Migrate the thread to a
+ * proper CPU and schedule it away if the CPU it's executing on
+ * is removed from the allowed bitmask.
+ *
+ * NOTE: the caller must have a valid reference to the task, the
+ * task must not exit() & deallocate itself prematurely.  The
+ * call is not atomic; no spinlocks may be held.
+ */
+int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
+{
+	struct migration_req req;
+	unsigned long flags;
+	struct rq *rq;
+	int ret = 0;
+
+	rq = task_rq_lock(p, &flags);
+	if (!cpus_intersects(new_mask, cpu_online_map)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	p->cpus_allowed = new_mask;
+	/* Can the task run on the task's current CPU? If so, we're done */
+	if (cpu_isset(task_cpu(p), new_mask))
+		goto out;
+
+	if (migrate_task(p, any_online_cpu(new_mask), &req)) {
+		/* Need help from migration thread: drop lock and wait. */
+		task_rq_unlock(rq, &flags);
+		wake_up_process(rq->migration_thread);
+		wait_for_completion(&req.done);
+		tlb_migrate_finish(p->mm);
+		return 0;
+	}
+out:
+	task_rq_unlock(rq, &flags);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(set_cpus_allowed);
+
+/*
+ * Move (not current) task off this cpu, onto dest cpu.  We're doing
+ * this because either it can't run here any more (set_cpus_allowed()
+ * away from this CPU, or CPU going down), or because we're
+ * attempting to rebalance this task on exec (sched_exec).
+ *
+ * So we race with normal scheduler movements, but that's OK, as long
+ * as the task is no longer on this CPU.
+ *
+ * Returns non-zero if task was successfully migrated.
+ */
+static int __migrate_task(struct task_struct *p, int src_cpu, int dest_cpu)
+{
+	struct rq *rq_dest, *rq_src;
+	int ret = 0;
+
+	if (unlikely(cpu_is_offline(dest_cpu)))
+		return ret;
+
+	rq_src = cpu_rq(src_cpu);
+	rq_dest = cpu_rq(dest_cpu);
+
+	double_rq_lock(rq_src, rq_dest);
+	/* Already moved. */
+	if (task_cpu(p) != src_cpu)
+		goto out;
+	/* Affinity changed (again). */
+	if (!cpu_isset(dest_cpu, p->cpus_allowed))
+		goto out;
+
+	set_task_cpu(p, dest_cpu);
+	if (p->array) {
+		/*
+		 * Sync timestamp with rq_dest's before activating.
+		 * The same thing could be achieved by doing this step
+		 * afterwards, and pretending it was a local activate.
+		 * This way is cleaner and logically correct.
+		 */
+		p->timestamp = p->timestamp - rq_src->timestamp_last_tick
+				+ rq_dest->timestamp_last_tick;
+		deactivate_task(p, rq_src);
+		__activate_task(p, rq_dest);
+		if (TASK_PREEMPTS_CURR(p, rq_dest))
+			resched_task(rq_dest->curr);
+	}
+	ret = 1;
+out:
+	double_rq_unlock(rq_src, rq_dest);
+	return ret;
+}
+
+/*
+ * migration_thread - this is a highprio system thread that performs
+ * thread migration by bumping thread off CPU then 'pushing' onto
+ * another runqueue.
+ */
+static int migration_thread(void *data)
+{
+	int cpu = (long)data;
+	struct rq *rq;
+
+	rq = cpu_rq(cpu);
+	BUG_ON(rq->migration_thread != current);
+
+	set_current_state(TASK_INTERRUPTIBLE);
+	while (!kthread_should_stop()) {
+		struct migration_req *req;
+		struct list_head *head;
+
+		try_to_freeze();
+
+		spin_lock_irq(&rq->lock);
+
+		if (cpu_is_offline(cpu)) {
+			spin_unlock_irq(&rq->lock);
+			goto wait_to_die;
+		}
+
+		if (rq->active_balance) {
+			active_load_balance(rq, cpu);
+			rq->active_balance = 0;
+		}
+
+		head = &rq->migration_queue;
+
+		if (list_empty(head)) {
+			spin_unlock_irq(&rq->lock);
+			schedule();
+			set_current_state(TASK_INTERRUPTIBLE);
+			continue;
+		}
+		req = list_entry(head->next, struct migration_req, list);
+		list_del_init(head->next);
+
+		spin_unlock(&rq->lock);
+		__migrate_task(req->task, cpu, req->dest_cpu);
+		local_irq_enable();
+
+		complete(&req->done);
+	}
+	__set_current_state(TASK_RUNNING);
+	return 0;
+
+wait_to_die:
+	/* Wait for kthread_stop */
+	set_current_state(TASK_INTERRUPTIBLE);
+	while (!kthread_should_stop()) {
+		schedule();
+		set_current_state(TASK_INTERRUPTIBLE);
+	}
+	__set_current_state(TASK_RUNNING);
+	return 0;
+}
+
+#ifdef CONFIG_HOTPLUG_CPU
+/* Figure out where task on dead CPU should go, use force if neccessary. */
+static void move_task_off_dead_cpu(int dead_cpu, struct task_struct *p)
+{
+	unsigned long flags;
+	cpumask_t mask;
+	struct rq *rq;
+	int dest_cpu;
+
+restart:
+	/* On same node? */
+	mask = node_to_cpumask(cpu_to_node(dead_cpu));
+	cpus_and(mask, mask, p->cpus_allowed);
+	dest_cpu = any_online_cpu(mask);
+
+	/* On any allowed CPU? */
+	if (dest_cpu == NR_CPUS)
+		dest_cpu = any_online_cpu(p->cpus_allowed);
+
+	/* No more Mr. Nice Guy. */
+	if (dest_cpu == NR_CPUS) {
+		rq = task_rq_lock(p, &flags);
+		cpus_setall(p->cpus_allowed);
+		dest_cpu = any_online_cpu(p->cpus_allowed);
+		task_rq_unlock(rq, &flags);
+
+		/*
+		 * Don't tell them about moving exiting tasks or
+		 * kernel threads (both mm NULL), since they never
+		 * leave kernel.
+		 */
+		if (p->mm && printk_ratelimit())
+			printk(KERN_INFO "process %d (%s) no "
+			       "longer affine to cpu%d\n",
+			       p->pid, p->comm, dead_cpu);
+	}
+	if (!__migrate_task(p, dead_cpu, dest_cpu))
+		goto restart;
+}
+
+/*
+ * While a dead CPU has no uninterruptible tasks queued at this point,
+ * it might still have a nonzero ->nr_uninterruptible counter, because
+ * for performance reasons the counter is not stricly tracking tasks to
+ * their home CPUs. So we just add the counter to another CPU's counter,
+ * to keep the global sum constant after CPU-down:
+ */
+static void migrate_nr_uninterruptible(struct rq *rq_src)
+{
+	struct rq *rq_dest = cpu_rq(any_online_cpu(CPU_MASK_ALL));
+	unsigned long flags;
+
+	local_irq_save(flags);
+	double_rq_lock(rq_src, rq_dest);
+	rq_dest->nr_uninterruptible += rq_src->nr_uninterruptible;
+	rq_src->nr_uninterruptible = 0;
+	double_rq_unlock(rq_src, rq_dest);
+	local_irq_restore(flags);
+}
+
+/* Run through task list and migrate tasks from the dead cpu. */
+static void migrate_live_tasks(int src_cpu)
+{
+	struct task_struct *p, *t;
+
+	write_lock_irq(&tasklist_lock);
+
+	do_each_thread(t, p) {
+		if (p == current)
+			continue;
+
+		if (task_cpu(p) == src_cpu)
+			move_task_off_dead_cpu(src_cpu, p);
+	} while_each_thread(t, p);
+
+	write_unlock_irq(&tasklist_lock);
+}
+
+/* Schedules idle task to be the next runnable task on current CPU.
+ * It does so by boosting its priority to highest possible and adding it to
+ * the _front_ of the runqueue. Used by CPU offline code.
+ */
+void sched_idle_next(void)
+{
+	int this_cpu = smp_processor_id();
+	struct rq *rq = cpu_rq(this_cpu);
+	struct task_struct *p = rq->idle;
+	unsigned long flags;
+
+	/* cpu has to be offline */
+	BUG_ON(cpu_online(this_cpu));
+
+	/*
+	 * Strictly not necessary since rest of the CPUs are stopped by now
+	 * and interrupts disabled on the current cpu.
+	 */
+	spin_lock_irqsave(&rq->lock, flags);
+
+	__setscheduler(p, SCHED_FIFO, MAX_RT_PRIO-1);
+
+	/* Add idle task to the _front_ of its priority queue: */
+	__activate_idle_task(p, rq);
+
+	spin_unlock_irqrestore(&rq->lock, flags);
+}
+
+/*
+ * Ensures that the idle task is using init_mm right before its cpu goes
+ * offline.
+ */
+void idle_task_exit(void)
+{
+	struct mm_struct *mm = current->active_mm;
+
+	BUG_ON(cpu_online(smp_processor_id()));
+
+	if (mm != &init_mm)
+		switch_mm(mm, &init_mm, current);
+	mmdrop(mm);
+}
+
+static void migrate_dead(unsigned int dead_cpu, struct task_struct *p)
+{
+	struct rq *rq = cpu_rq(dead_cpu);
+
+	/* Must be exiting, otherwise would be on tasklist. */
+	BUG_ON(p->exit_state != EXIT_ZOMBIE && p->exit_state != EXIT_DEAD);
+
+	/* Cannot have done final schedule yet: would have vanished. */
+	BUG_ON(p->flags & PF_DEAD);
+
+	get_task_struct(p);
+
+	/*
+	 * Drop lock around migration; if someone else moves it,
+	 * that's OK.  No task can be added to this CPU, so iteration is
+	 * fine.
+	 */
+	spin_unlock_irq(&rq->lock);
+	move_task_off_dead_cpu(dead_cpu, p);
+	spin_lock_irq(&rq->lock);
+
+	put_task_struct(p);
+}
+
+/* release_task() removes task from tasklist, so we won't find dead tasks. */
+static void migrate_dead_tasks(unsigned int dead_cpu)
+{
+	struct rq *rq = cpu_rq(dead_cpu);
+	unsigned int arr, i;
+
+	for (arr = 0; arr < 2; arr++) {
+		for (i = 0; i < MAX_PRIO; i++) {
+			struct list_head *list = &rq->arrays[arr].queue[i];
+
+			while (!list_empty(list))
+				migrate_dead(dead_cpu, list_entry(list->next,
+					     struct task_struct, run_list));
+		}
+	}
+}
+#endif /* CONFIG_HOTPLUG_CPU */
+
+#if defined(CONFIG_DEBUG_KERNEL) && defined(CONFIG_SYSCTL)
+static struct ctl_table sd_ctl_dir[] = {
+	{1, "sched_domain", NULL, 0, 0755, NULL, },
+	{0,},
+};
+
+static struct ctl_table sd_ctl_root[] = {
+	{1, "kernel", NULL, 0, 0755, sd_ctl_dir, },
+	{0,},
+};
+
+static struct ctl_table *sd_alloc_ctl_entry(int n)
+{
+	struct ctl_table *entry =
+		kmalloc(n * sizeof(struct ctl_table), GFP_KERNEL);
+	BUG_ON(!entry);
+	memset(entry, 0, n * sizeof(struct ctl_table));
+	return entry;
+}
+
+static void set_table_entry(struct ctl_table *entry, int ctl_name,
+			const char *procname, void *data, int maxlen,
+			mode_t mode, proc_handler *proc_handler)
+{
+	entry->ctl_name = ctl_name;
+	entry->procname = procname;
+	entry->data = data;
+	entry->maxlen = maxlen;
+	entry->mode = mode;
+	entry->proc_handler = proc_handler;
+}
+
+static struct ctl_table *
+sd_alloc_ctl_domain_table(struct sched_domain *sd)
+{
+	struct ctl_table *table;
+	table = sd_alloc_ctl_entry(14);
+
+	set_table_entry(&table[0], 1, "min_interval", &sd->min_interval,
+		sizeof(long), 0644, proc_doulongvec_minmax);
+	set_table_entry(&table[1], 2, "max_interval", &sd->max_interval,
+		sizeof(long), 0644, proc_doulongvec_minmax);
+	set_table_entry(&table[2], 3, "busy_idx", &sd->busy_idx,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[3], 4, "idle_idx", &sd->idle_idx,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[4], 5, "newidle_idx", &sd->newidle_idx,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[5], 6, "wake_idx", &sd->wake_idx,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[6], 7, "forkexec_idx", &sd->forkexec_idx,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[7], 8, "busy_factor", &sd->busy_factor,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[8], 9, "imbalance_pct", &sd->imbalance_pct,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[9], 10, "cache_hot_time", &sd->cache_hot_time,
+		sizeof(long long), 0644, proc_doulongvec_minmax);
+	set_table_entry(&table[10], 11, "cache_nice_tries", &sd->cache_nice_tries,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[11], 12, "per_cpu_gain", &sd->per_cpu_gain,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[12], 13, "flags", &sd->flags,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	return table;
+}
+
+static ctl_table *sd_alloc_ctl_cpu_table(int cpu)
+{
+	struct sched_domain *sd;
+	int domain_num = 0, i;
+	struct ctl_table *entry, *table;
+	char buf[32];
+	for_each_domain(cpu, sd)
+		domain_num++;
+	entry = table = sd_alloc_ctl_entry(domain_num + 1);
+
+	i = 0;
+	for_each_domain(cpu, sd) {
+		snprintf(buf, 32, "domain%d", i);
+		entry->ctl_name = i + 1;
+		entry->procname = kstrdup(buf, GFP_KERNEL);
+		entry->mode = 0755;
+		entry->child = sd_alloc_ctl_domain_table(sd);
+		entry++;
+		i++;
+	}
+	return table;
+}
+
+static struct ctl_table_header *sd_sysctl_header;
+static void init_sched_domain_sysctl(void)
+{
+	int i, cpu_num = num_online_cpus();
+	char buf[32];
+	struct ctl_table *entry = sd_alloc_ctl_entry(cpu_num + 1);
+
+	sd_ctl_dir[0].child = entry;
+
+	for (i = 0; i < cpu_num; i++, entry++) {
+		snprintf(buf, 32, "cpu%d", i);
+		entry->ctl_name = i + 1;
+		entry->procname = kstrdup(buf, GFP_KERNEL);
+		entry->mode = 0755;
+		entry->child = sd_alloc_ctl_cpu_table(i);
+	}
+	sd_sysctl_header = register_sysctl_table(sd_ctl_root, 0);
+}
+#else
+static void init_sched_domain_sysctl(void)
+{
+}
+#endif
+
+/*
+ * migration_call - callback that gets triggered when a CPU is added.
+ * Here we can start up the necessary migration thread for the new CPU.
+ */
+static int __cpuinit
+migration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)
+{
+	struct task_struct *p;
+	int cpu = (long)hcpu;
+	unsigned long flags;
+	struct rq *rq;
+
+	switch (action) {
+	case CPU_UP_PREPARE:
+		p = kthread_create(migration_thread, hcpu, "migration/%d",cpu);
+		if (IS_ERR(p))
+			return NOTIFY_BAD;
+		p->flags |= PF_NOFREEZE;
+		kthread_bind(p, cpu);
+		/* Must be high prio: stop_machine expects to yield to it. */
+		rq = task_rq_lock(p, &flags);
+		__setscheduler(p, SCHED_FIFO, MAX_RT_PRIO-1);
+		task_rq_unlock(rq, &flags);
+		cpu_rq(cpu)->migration_thread = p;
+		break;
+
+	case CPU_ONLINE:
+		/* Strictly unneccessary, as first user will wake it. */
+		wake_up_process(cpu_rq(cpu)->migration_thread);
+		break;
+
+#ifdef CONFIG_HOTPLUG_CPU
+	case CPU_UP_CANCELED:
+		if (!cpu_rq(cpu)->migration_thread)
+			break;
+		/* Unbind it from offline cpu so it can run.  Fall thru. */
+		kthread_bind(cpu_rq(cpu)->migration_thread,
+			     any_online_cpu(cpu_online_map));
+		kthread_stop(cpu_rq(cpu)->migration_thread);
+		cpu_rq(cpu)->migration_thread = NULL;
+		break;
+
+	case CPU_DEAD:
+		migrate_live_tasks(cpu);
+		rq = cpu_rq(cpu);
+		kthread_stop(rq->migration_thread);
+		rq->migration_thread = NULL;
+		/* Idle task back to normal (off runqueue, low prio) */
+		rq = task_rq_lock(rq->idle, &flags);
+		deactivate_task(rq->idle, rq);
+		rq->idle->static_prio = MAX_PRIO;
+		__setscheduler(rq->idle, SCHED_NORMAL, 0);
+		migrate_dead_tasks(cpu);
+		task_rq_unlock(rq, &flags);
+		migrate_nr_uninterruptible(rq);
+		BUG_ON(rq->nr_running != 0);
+
+		/* No need to migrate the tasks: it was best-effort if
+		 * they didn't do lock_cpu_hotplug().  Just wake up
+		 * the requestors. */
+		spin_lock_irq(&rq->lock);
+		while (!list_empty(&rq->migration_queue)) {
+			struct migration_req *req;
+
+			req = list_entry(rq->migration_queue.next,
+					 struct migration_req, list);
+			list_del_init(&req->list);
+			complete(&req->done);
+		}
+		spin_unlock_irq(&rq->lock);
+		break;
+#endif
+	}
+	return NOTIFY_OK;
+}
+
+/* Register at highest priority so that task migration (migrate_all_tasks)
+ * happens before everything else.
+ */
+static struct notifier_block __cpuinitdata migration_notifier = {
+	.notifier_call = migration_call,
+	.priority = 10
+};
+
+int __init migration_init(void)
+{
+	void *cpu = (void *)(long)smp_processor_id();
+
+	/* Start one for the boot CPU: */
+	migration_call(&migration_notifier, CPU_UP_PREPARE, cpu);
+	migration_call(&migration_notifier, CPU_ONLINE, cpu);
+	register_cpu_notifier(&migration_notifier);
+
+	return 0;
+}
+#endif
+
+#ifdef CONFIG_SMP
+#undef SCHED_DOMAIN_DEBUG
+#ifdef SCHED_DOMAIN_DEBUG
+static void sched_domain_debug(struct sched_domain *sd, int cpu)
+{
+	int level = 0;
+
+	if (!sd) {
+		printk(KERN_DEBUG "CPU%d attaching NULL sched-domain.\n", cpu);
+		return;
+	}
+
+	printk(KERN_DEBUG "CPU%d attaching sched-domain:\n", cpu);
+
+	do {
+		int i;
+		char str[NR_CPUS];
+		struct sched_group *group = sd->groups;
+		cpumask_t groupmask;
+
+		cpumask_scnprintf(str, NR_CPUS, sd->span);
+		cpus_clear(groupmask);
+
+		printk(KERN_DEBUG);
+		for (i = 0; i < level + 1; i++)
+			printk(" ");
+		printk("domain %d: ", level);
+
+		if (!(sd->flags & SD_LOAD_BALANCE)) {
+			printk("does not load-balance\n");
+			if (sd->parent)
+				printk(KERN_ERR "ERROR: !SD_LOAD_BALANCE domain has parent");
+			break;
+		}
+
+		printk("span %s\n", str);
+
+		if (!cpu_isset(cpu, sd->span))
+			printk(KERN_ERR "ERROR: domain->span does not contain CPU%d\n", cpu);
+		if (!cpu_isset(cpu, group->cpumask))
+			printk(KERN_ERR "ERROR: domain->groups does not contain CPU%d\n", cpu);
+
+		printk(KERN_DEBUG);
+		for (i = 0; i < level + 2; i++)
+			printk(" ");
+		printk("groups:");
+		do {
+			if (!group) {
+				printk("\n");
+				printk(KERN_ERR "ERROR: group is NULL\n");
+				break;
+			}
+
+			if (!group->cpu_power) {
+				printk("\n");
+				printk(KERN_ERR "ERROR: domain->cpu_power not set\n");
+			}
+
+			if (!cpus_weight(group->cpumask)) {
+				printk("\n");
+				printk(KERN_ERR "ERROR: empty group\n");
+			}
+
+			if (cpus_intersects(groupmask, group->cpumask)) {
+				printk("\n");
+				printk(KERN_ERR "ERROR: repeated CPUs\n");
+			}
+
+			cpus_or(groupmask, groupmask, group->cpumask);
+
+			cpumask_scnprintf(str, NR_CPUS, group->cpumask);
+			printk(" %s", str);
+
+			group = group->next;
+		} while (group != sd->groups);
+		printk("\n");
+
+		if (!cpus_equal(sd->span, groupmask))
+			printk(KERN_ERR "ERROR: groups don't span domain->span\n");
+
+		level++;
+		sd = sd->parent;
+
+		if (sd) {
+			if (!cpus_subset(groupmask, sd->span))
+				printk(KERN_ERR "ERROR: parent span is not a superset of domain->span\n");
+		}
+
+	} while (sd);
+}
+#else
+# define sched_domain_debug(sd, cpu) do { } while (0)
+#endif
+
+static int sd_degenerate(struct sched_domain *sd)
+{
+	if (cpus_weight(sd->span) == 1)
+		return 1;
+
+	/* Following flags need at least 2 groups */
+	if (sd->flags & (SD_LOAD_BALANCE |
+			 SD_BALANCE_NEWIDLE |
+			 SD_BALANCE_FORK |
+			 SD_BALANCE_EXEC)) {
+		if (sd->groups != sd->groups->next)
+			return 0;
+	}
+
+	/* Following flags don't use groups */
+	if (sd->flags & (SD_WAKE_IDLE |
+			 SD_WAKE_AFFINE |
+			 SD_WAKE_BALANCE))
+		return 0;
+
+	return 1;
+}
+
+static int
+sd_parent_degenerate(struct sched_domain *sd, struct sched_domain *parent)
+{
+	unsigned long cflags = sd->flags, pflags = parent->flags;
+
+	if (sd_degenerate(parent))
+		return 1;
+
+	if (!cpus_equal(sd->span, parent->span))
+		return 0;
+
+	/* Does parent contain flags not in child? */
+	/* WAKE_BALANCE is a subset of WAKE_AFFINE */
+	if (cflags & SD_WAKE_AFFINE)
+		pflags &= ~SD_WAKE_BALANCE;
+	/* Flags needing groups don't count if only 1 group in parent */
+	if (parent->groups == parent->groups->next) {
+		pflags &= ~(SD_LOAD_BALANCE |
+				SD_BALANCE_NEWIDLE |
+				SD_BALANCE_FORK |
+				SD_BALANCE_EXEC);
+	}
+	if (~cflags & pflags)
+		return 0;
+
+	return 1;
+}
+
+/*
+ * Attach the domain 'sd' to 'cpu' as its base domain.  Callers must
+ * hold the hotplug lock.
+ */
+static void cpu_attach_domain(struct sched_domain *sd, int cpu)
+{
+	struct rq *rq = cpu_rq(cpu);
+	struct sched_domain *tmp;
+
+	/* Remove the sched domains which do not contribute to scheduling. */
+	for (tmp = sd; tmp; tmp = tmp->parent) {
+		struct sched_domain *parent = tmp->parent;
+		if (!parent)
+			break;
+		if (sd_parent_degenerate(tmp, parent))
+			tmp->parent = parent->parent;
+	}
+
+	if (sd && sd_degenerate(sd))
+		sd = sd->parent;
+
+	sched_domain_debug(sd, cpu);
+
+	rcu_assign_pointer(rq->sd, sd);
+}
+
+/* cpus with isolated domains */
+static cpumask_t __devinitdata cpu_isolated_map = CPU_MASK_NONE;
+
+/* Setup the mask of cpus configured for isolated domains */
+static int __init isolated_cpu_setup(char *str)
+{
+	int ints[NR_CPUS], i;
+
+	str = get_options(str, ARRAY_SIZE(ints), ints);
+	cpus_clear(cpu_isolated_map);
+	for (i = 1; i <= ints[0]; i++)
+		if (ints[i] < NR_CPUS)
+			cpu_set(ints[i], cpu_isolated_map);
+	return 1;
+}
+
+__setup ("isolcpus=", isolated_cpu_setup);
+
+/*
+ * init_sched_build_groups takes an array of groups, the cpumask we wish
+ * to span, and a pointer to a function which identifies what group a CPU
+ * belongs to. The return value of group_fn must be a valid index into the
+ * groups[] array, and must be >= 0 and < NR_CPUS (due to the fact that we
+ * keep track of groups covered with a cpumask_t).
+ *
+ * init_sched_build_groups will build a circular linked list of the groups
+ * covered by the given span, and will set each group's ->cpumask correctly,
+ * and ->cpu_power to 0.
+ */
+static void init_sched_build_groups(struct sched_group groups[], cpumask_t span,
+				    int (*group_fn)(int cpu))
+{
+	struct sched_group *first = NULL, *last = NULL;
+	cpumask_t covered = CPU_MASK_NONE;
+	int i;
+
+	for_each_cpu_mask(i, span) {
+		int group = group_fn(i);
+		struct sched_group *sg = &groups[group];
+		int j;
+
+		if (cpu_isset(i, covered))
+			continue;
+
+		sg->cpumask = CPU_MASK_NONE;
+		sg->cpu_power = 0;
+
+		for_each_cpu_mask(j, span) {
+			if (group_fn(j) != group)
+				continue;
+
+			cpu_set(j, covered);
+			cpu_set(j, sg->cpumask);
+		}
+		if (!first)
+			first = sg;
+		if (last)
+			last->next = sg;
+		last = sg;
+	}
+	last->next = first;
+}
+
+#define SD_NODES_PER_DOMAIN 16
+
+/*
+ * Self-tuning task migration cost measurement between source and target CPUs.
+ *
+ * This is done by measuring the cost of manipulating buffers of varying
+ * sizes. For a given buffer-size here are the steps that are taken:
+ *
+ * 1) the source CPU reads+dirties a shared buffer
+ * 2) the target CPU reads+dirties the same shared buffer
+ *
+ * We measure how long they take, in the following 4 scenarios:
+ *
+ *  - source: CPU1, target: CPU2 | cost1
+ *  - source: CPU2, target: CPU1 | cost2
+ *  - source: CPU1, target: CPU1 | cost3
+ *  - source: CPU2, target: CPU2 | cost4
+ *
+ * We then calculate the cost3+cost4-cost1-cost2 difference - this is
+ * the cost of migration.
+ *
+ * We then start off from a small buffer-size and iterate up to larger
+ * buffer sizes, in 5% steps - measuring each buffer-size separately, and
+ * doing a maximum search for the cost. (The maximum cost for a migration
+ * normally occurs when the working set size is around the effective cache
+ * size.)
+ */
+#define SEARCH_SCOPE		2
+#define MIN_CACHE_SIZE		(64*1024U)
+#define DEFAULT_CACHE_SIZE	(5*1024*1024U)
+#define ITERATIONS		1
+#define SIZE_THRESH		130
+#define COST_THRESH		130
+
+/*
+ * The migration cost is a function of 'domain distance'. Domain
+ * distance is the number of steps a CPU has to iterate down its
+ * domain tree to share a domain with the other CPU. The farther
+ * two CPUs are from each other, the larger the distance gets.
+ *
+ * Note that we use the distance only to cache measurement results,
+ * the distance value is not used numerically otherwise. When two
+ * CPUs have the same distance it is assumed that the migration
+ * cost is the same. (this is a simplification but quite practical)
+ */
+#define MAX_DOMAIN_DISTANCE 32
+
+static unsigned long long migration_cost[MAX_DOMAIN_DISTANCE] =
+		{ [ 0 ... MAX_DOMAIN_DISTANCE-1 ] =
+/*
+ * Architectures may override the migration cost and thus avoid
+ * boot-time calibration. Unit is nanoseconds. Mostly useful for
+ * virtualized hardware:
+ */
+#ifdef CONFIG_DEFAULT_MIGRATION_COST
+			CONFIG_DEFAULT_MIGRATION_COST
+#else
+			-1LL
+#endif
+};
+
+/*
+ * Allow override of migration cost - in units of microseconds.
+ * E.g. migration_cost=1000,2000,3000 will set up a level-1 cost
+ * of 1 msec, level-2 cost of 2 msecs and level3 cost of 3 msecs:
+ */
+static int __init migration_cost_setup(char *str)
+{
+	int ints[MAX_DOMAIN_DISTANCE+1], i;
+
+	str = get_options(str, ARRAY_SIZE(ints), ints);
+
+	printk("#ints: %d\n", ints[0]);
+	for (i = 1; i <= ints[0]; i++) {
+		migration_cost[i-1] = (unsigned long long)ints[i]*1000;
+		printk("migration_cost[%d]: %Ld\n", i-1, migration_cost[i-1]);
+	}
+	return 1;
+}
+
+__setup ("migration_cost=", migration_cost_setup);
+
+/*
+ * Global multiplier (divisor) for migration-cutoff values,
+ * in percentiles. E.g. use a value of 150 to get 1.5 times
+ * longer cache-hot cutoff times.
+ *
+ * (We scale it from 100 to 128 to long long handling easier.)
+ */
+
+#define MIGRATION_FACTOR_SCALE 128
+
+static unsigned int migration_factor = MIGRATION_FACTOR_SCALE;
+
+static int __init setup_migration_factor(char *str)
+{
+	get_option(&str, &migration_factor);
+	migration_factor = migration_factor * MIGRATION_FACTOR_SCALE / 100;
+	return 1;
+}
+
+__setup("migration_factor=", setup_migration_factor);
+
+/*
+ * Estimated distance of two CPUs, measured via the number of domains
+ * we have to pass for the two CPUs to be in the same span:
+ */
+static unsigned long domain_distance(int cpu1, int cpu2)
+{
+	unsigned long distance = 0;
+	struct sched_domain *sd;
+
+	for_each_domain(cpu1, sd) {
+		WARN_ON(!cpu_isset(cpu1, sd->span));
+		if (cpu_isset(cpu2, sd->span))
+			return distance;
+		distance++;
+	}
+	if (distance >= MAX_DOMAIN_DISTANCE) {
+		WARN_ON(1);
+		distance = MAX_DOMAIN_DISTANCE-1;
+	}
+
+	return distance;
+}
+
+static unsigned int migration_debug;
+
+static int __init setup_migration_debug(char *str)
+{
+	get_option(&str, &migration_debug);
+	return 1;
+}
+
+__setup("migration_debug=", setup_migration_debug);
+
+/*
+ * Maximum cache-size that the scheduler should try to measure.
+ * Architectures with larger caches should tune this up during
+ * bootup. Gets used in the domain-setup code (i.e. during SMP
+ * bootup).
+ */
+unsigned int max_cache_size;
+
+static int __init setup_max_cache_size(char *str)
+{
+	get_option(&str, &max_cache_size);
+	return 1;
+}
+
+__setup("max_cache_size=", setup_max_cache_size);
+
+/*
+ * Dirty a big buffer in a hard-to-predict (for the L2 cache) way. This
+ * is the operation that is timed, so we try to generate unpredictable
+ * cachemisses that still end up filling the L2 cache:
+ */
+static void touch_cache(void *__cache, unsigned long __size)
+{
+	unsigned long size = __size/sizeof(long), chunk1 = size/3,
+			chunk2 = 2*size/3;
+	unsigned long *cache = __cache;
+	int i;
+
+	for (i = 0; i < size/6; i += 8) {
+		switch (i % 6) {
+			case 0: cache[i]++;
+			case 1: cache[size-1-i]++;
+			case 2: cache[chunk1-i]++;
+			case 3: cache[chunk1+i]++;
+			case 4: cache[chunk2-i]++;
+			case 5: cache[chunk2+i]++;
+		}
+	}
+}
+
+/*
+ * Measure the cache-cost of one task migration. Returns in units of nsec.
+ */
+static unsigned long long
+measure_one(void *cache, unsigned long size, int source, int target)
+{
+	cpumask_t mask, saved_mask;
+	unsigned long long t0, t1, t2, t3, cost;
+
+	saved_mask = current->cpus_allowed;
+
+	/*
+	 * Flush source caches to RAM and invalidate them:
+	 */
+	sched_cacheflush();
+
+	/*
+	 * Migrate to the source CPU:
+	 */
+	mask = cpumask_of_cpu(source);
+	set_cpus_allowed(current, mask);
+	WARN_ON(smp_processor_id() != source);
+
+	/*
+	 * Dirty the working set:
+	 */
+	t0 = sched_clock();
+	touch_cache(cache, size);
+	t1 = sched_clock();
+
+	/*
+	 * Migrate to the target CPU, dirty the L2 cache and access
+	 * the shared buffer. (which represents the working set
+	 * of a migrated task.)
+	 */
+	mask = cpumask_of_cpu(target);
+	set_cpus_allowed(current, mask);
+	WARN_ON(smp_processor_id() != target);
+
+	t2 = sched_clock();
+	touch_cache(cache, size);
+	t3 = sched_clock();
+
+	cost = t1-t0 + t3-t2;
+
+	if (migration_debug >= 2)
+		printk("[%d->%d]: %8Ld %8Ld %8Ld => %10Ld.\n",
+			source, target, t1-t0, t1-t0, t3-t2, cost);
+	/*
+	 * Flush target caches to RAM and invalidate them:
+	 */
+	sched_cacheflush();
+
+	set_cpus_allowed(current, saved_mask);
+
+	return cost;
+}
+
+/*
+ * Measure a series of task migrations and return the average
+ * result. Since this code runs early during bootup the system
+ * is 'undisturbed' and the average latency makes sense.
+ *
+ * The algorithm in essence auto-detects the relevant cache-size,
+ * so it will properly detect different cachesizes for different
+ * cache-hierarchies, depending on how the CPUs are connected.
+ *
+ * Architectures can prime the upper limit of the search range via
+ * max_cache_size, otherwise the search range defaults to 20MB...64K.
+ */
+static unsigned long long
+measure_cost(int cpu1, int cpu2, void *cache, unsigned int size)
+{
+	unsigned long long cost1, cost2;
+	int i;
+
+	/*
+	 * Measure the migration cost of 'size' bytes, over an
+	 * average of 10 runs:
+	 *
+	 * (We perturb the cache size by a small (0..4k)
+	 *  value to compensate size/alignment related artifacts.
+	 *  We also subtract the cost of the operation done on
+	 *  the same CPU.)
+	 */
+	cost1 = 0;
+
+	/*
+	 * dry run, to make sure we start off cache-cold on cpu1,
+	 * and to get any vmalloc pagefaults in advance:
+	 */
+	measure_one(cache, size, cpu1, cpu2);
+	for (i = 0; i < ITERATIONS; i++)
+		cost1 += measure_one(cache, size - i*1024, cpu1, cpu2);
+
+	measure_one(cache, size, cpu2, cpu1);
+	for (i = 0; i < ITERATIONS; i++)
+		cost1 += measure_one(cache, size - i*1024, cpu2, cpu1);
+
+	/*
+	 * (We measure the non-migrating [cached] cost on both
+	 *  cpu1 and cpu2, to handle CPUs with different speeds)
+	 */
+	cost2 = 0;
+
+	measure_one(cache, size, cpu1, cpu1);
+	for (i = 0; i < ITERATIONS; i++)
+		cost2 += measure_one(cache, size - i*1024, cpu1, cpu1);
+
+	measure_one(cache, size, cpu2, cpu2);
+	for (i = 0; i < ITERATIONS; i++)
+		cost2 += measure_one(cache, size - i*1024, cpu2, cpu2);
+
+	/*
+	 * Get the per-iteration migration cost:
+	 */
+	do_div(cost1, 2*ITERATIONS);
+	do_div(cost2, 2*ITERATIONS);
+
+	return cost1 - cost2;
+}
+
+static unsigned long long measure_migration_cost(int cpu1, int cpu2)
+{
+	unsigned long long max_cost = 0, fluct = 0, avg_fluct = 0;
+	unsigned int max_size, size, size_found = 0;
+	long long cost = 0, prev_cost;
+	void *cache;
+
+	/*
+	 * Search from max_cache_size*5 down to 64K - the real relevant
+	 * cachesize has to lie somewhere inbetween.
+	 */
+	if (max_cache_size) {
+		max_size = max(max_cache_size * SEARCH_SCOPE, MIN_CACHE_SIZE);
+		size = max(max_cache_size / SEARCH_SCOPE, MIN_CACHE_SIZE);
+	} else {
+		/*
+		 * Since we have no estimation about the relevant
+		 * search range
+		 */
+		max_size = DEFAULT_CACHE_SIZE * SEARCH_SCOPE;
+		size = MIN_CACHE_SIZE;
+	}
+
+	if (!cpu_online(cpu1) || !cpu_online(cpu2)) {
+		printk("cpu %d and %d not both online!\n", cpu1, cpu2);
+		return 0;
+	}
+
+	/*
+	 * Allocate the working set:
+	 */
+	cache = vmalloc(max_size);
+	if (!cache) {
+		printk("could not vmalloc %d bytes for cache!\n", 2*max_size);
+		return 1000000; /* return 1 msec on very small boxen */
+	}
+
+	while (size <= max_size) {
+		prev_cost = cost;
+		cost = measure_cost(cpu1, cpu2, cache, size);
+
+		/*
+		 * Update the max:
+		 */
+		if (cost > 0) {
+			if (max_cost < cost) {
+				max_cost = cost;
+				size_found = size;
+			}
+		}
+		/*
+		 * Calculate average fluctuation, we use this to prevent
+		 * noise from triggering an early break out of the loop:
+		 */
+		fluct = abs(cost - prev_cost);
+		avg_fluct = (avg_fluct + fluct)/2;
+
+		if (migration_debug)
+			printk("-> [%d][%d][%7d] %3ld.%ld [%3ld.%ld] (%ld): (%8Ld %8Ld)\n",
+				cpu1, cpu2, size,
+				(long)cost / 1000000,
+				((long)cost / 100000) % 10,
+				(long)max_cost / 1000000,
+				((long)max_cost / 100000) % 10,
+				domain_distance(cpu1, cpu2),
+				cost, avg_fluct);
+
+		/*
+		 * If we iterated at least 20% past the previous maximum,
+		 * and the cost has dropped by more than 20% already,
+		 * (taking fluctuations into account) then we assume to
+		 * have found the maximum and break out of the loop early:
+		 */
+		if (size_found && (size*100 > size_found*SIZE_THRESH))
+			if (cost+avg_fluct <= 0 ||
+				max_cost*100 > (cost+avg_fluct)*COST_THRESH) {
+
+				if (migration_debug)
+					printk("-> found max.\n");
+				break;
+			}
+		/*
+		 * Increase the cachesize in 10% steps:
+		 */
+		size = size * 10 / 9;
+	}
+
+	if (migration_debug)
+		printk("[%d][%d] working set size found: %d, cost: %Ld\n",
+			cpu1, cpu2, size_found, max_cost);
+
+	vfree(cache);
+
+	/*
+	 * A task is considered 'cache cold' if at least 2 times
+	 * the worst-case cost of migration has passed.
+	 *
+	 * (this limit is only listened to if the load-balancing
+	 * situation is 'nice' - if there is a large imbalance we
+	 * ignore it for the sake of CPU utilization and
+	 * processing fairness.)
+	 */
+	return 2 * max_cost * migration_factor / MIGRATION_FACTOR_SCALE;
+}
+
+static void calibrate_migration_costs(const cpumask_t *cpu_map)
+{
+	int cpu1 = -1, cpu2 = -1, cpu, orig_cpu = raw_smp_processor_id();
+	unsigned long j0, j1, distance, max_distance = 0;
+	struct sched_domain *sd;
+
+	j0 = jiffies;
+
+	/*
+	 * First pass - calculate the cacheflush times:
+	 */
+	for_each_cpu_mask(cpu1, *cpu_map) {
+		for_each_cpu_mask(cpu2, *cpu_map) {
+			if (cpu1 == cpu2)
+				continue;
+			distance = domain_distance(cpu1, cpu2);
+			max_distance = max(max_distance, distance);
+			/*
+			 * No result cached yet?
+			 */
+			if (migration_cost[distance] == -1LL)
+				migration_cost[distance] =
+					measure_migration_cost(cpu1, cpu2);
+		}
+	}
+	/*
+	 * Second pass - update the sched domain hierarchy with
+	 * the new cache-hot-time estimations:
+	 */
+	for_each_cpu_mask(cpu, *cpu_map) {
+		distance = 0;
+		for_each_domain(cpu, sd) {
+			sd->cache_hot_time = migration_cost[distance];
+			distance++;
+		}
+	}
+	/*
+	 * Print the matrix:
+	 */
+	if (migration_debug)
+		printk("migration: max_cache_size: %d, cpu: %d MHz:\n",
+			max_cache_size,
+#ifdef CONFIG_X86
+			cpu_khz/1000
+#else
+			-1
+#endif
+		);
+	if (system_state == SYSTEM_BOOTING) {
+		printk("migration_cost=");
+		for (distance = 0; distance <= max_distance; distance++) {
+			if (distance)
+				printk(",");
+			printk("%ld", (long)migration_cost[distance] / 1000);
+		}
+		printk("\n");
+	}
+	j1 = jiffies;
+	if (migration_debug)
+		printk("migration: %ld seconds\n", (j1-j0)/HZ);
+
+	/*
+	 * Move back to the original CPU. NUMA-Q gets confused
+	 * if we migrate to another quad during bootup.
+	 */
+	if (raw_smp_processor_id() != orig_cpu) {
+		cpumask_t mask = cpumask_of_cpu(orig_cpu),
+			saved_mask = current->cpus_allowed;
+
+		set_cpus_allowed(current, mask);
+		set_cpus_allowed(current, saved_mask);
+	}
+}
+
+#ifdef CONFIG_NUMA
+
+/**
+ * find_next_best_node - find the next node to include in a sched_domain
+ * @node: node whose sched_domain we're building
+ * @used_nodes: nodes already in the sched_domain
+ *
+ * Find the next node to include in a given scheduling domain.  Simply
+ * finds the closest node not already in the @used_nodes map.
+ *
+ * Should use nodemask_t.
+ */
+static int find_next_best_node(int node, unsigned long *used_nodes)
+{
+	int i, n, val, min_val, best_node = 0;
+
+	min_val = INT_MAX;
+
+	for (i = 0; i < MAX_NUMNODES; i++) {
+		/* Start at @node */
+		n = (node + i) % MAX_NUMNODES;
+
+		if (!nr_cpus_node(n))
+			continue;
+
+		/* Skip already used nodes */
+		if (test_bit(n, used_nodes))
+			continue;
+
+		/* Simple min distance search */
+		val = node_distance(node, n);
+
+		if (val < min_val) {
+			min_val = val;
+			best_node = n;
+		}
+	}
+
+	set_bit(best_node, used_nodes);
+	return best_node;
+}
+
+/**
+ * sched_domain_node_span - get a cpumask for a node's sched_domain
+ * @node: node whose cpumask we're constructing
+ * @size: number of nodes to include in this span
+ *
+ * Given a node, construct a good cpumask for its sched_domain to span.  It
+ * should be one that prevents unnecessary balancing, but also spreads tasks
+ * out optimally.
+ */
+static cpumask_t sched_domain_node_span(int node)
+{
+	DECLARE_BITMAP(used_nodes, MAX_NUMNODES);
+	cpumask_t span, nodemask;
+	int i;
+
+	cpus_clear(span);
+	bitmap_zero(used_nodes, MAX_NUMNODES);
+
+	nodemask = node_to_cpumask(node);
+	cpus_or(span, span, nodemask);
+	set_bit(node, used_nodes);
+
+	for (i = 1; i < SD_NODES_PER_DOMAIN; i++) {
+		int next_node = find_next_best_node(node, used_nodes);
+
+		nodemask = node_to_cpumask(next_node);
+		cpus_or(span, span, nodemask);
+	}
+
+	return span;
+}
+#endif
+
+int sched_smt_power_savings = 0, sched_mc_power_savings = 0;
+
+/*
+ * SMT sched-domains:
+ */
+#ifdef CONFIG_SCHED_SMT
+static DEFINE_PER_CPU(struct sched_domain, cpu_domains);
+static struct sched_group sched_group_cpus[NR_CPUS];
+
+static int cpu_to_cpu_group(int cpu)
+{
+	return cpu;
+}
+#endif
+
+/*
+ * multi-core sched-domains:
+ */
+#ifdef CONFIG_SCHED_MC
+static DEFINE_PER_CPU(struct sched_domain, core_domains);
+static struct sched_group *sched_group_core_bycpu[NR_CPUS];
+#endif
+
+#if defined(CONFIG_SCHED_MC) && defined(CONFIG_SCHED_SMT)
+static int cpu_to_core_group(int cpu)
+{
+	return first_cpu(cpu_sibling_map[cpu]);
+}
+#elif defined(CONFIG_SCHED_MC)
+static int cpu_to_core_group(int cpu)
+{
+	return cpu;
+}
+#endif
+
+static DEFINE_PER_CPU(struct sched_domain, phys_domains);
+static struct sched_group *sched_group_phys_bycpu[NR_CPUS];
+
+static int cpu_to_phys_group(int cpu)
+{
+#ifdef CONFIG_SCHED_MC
+	cpumask_t mask = cpu_coregroup_map(cpu);
+	return first_cpu(mask);
+#elif defined(CONFIG_SCHED_SMT)
+	return first_cpu(cpu_sibling_map[cpu]);
+#else
+	return cpu;
+#endif
+}
+
+#ifdef CONFIG_NUMA
+/*
+ * The init_sched_build_groups can't handle what we want to do with node
+ * groups, so roll our own. Now each node has its own list of groups which
+ * gets dynamically allocated.
+ */
+static DEFINE_PER_CPU(struct sched_domain, node_domains);
+static struct sched_group **sched_group_nodes_bycpu[NR_CPUS];
+
+static DEFINE_PER_CPU(struct sched_domain, allnodes_domains);
+static struct sched_group *sched_group_allnodes_bycpu[NR_CPUS];
+
+static int cpu_to_allnodes_group(int cpu)
+{
+	return cpu_to_node(cpu);
+}
+static void init_numa_sched_groups_power(struct sched_group *group_head)
+{
+	struct sched_group *sg = group_head;
+	int j;
+
+	if (!sg)
+		return;
+next_sg:
+	for_each_cpu_mask(j, sg->cpumask) {
+		struct sched_domain *sd;
+
+		sd = &per_cpu(phys_domains, j);
+		if (j != first_cpu(sd->groups->cpumask)) {
+			/*
+			 * Only add "power" once for each
+			 * physical package.
+			 */
+			continue;
+		}
+
+		sg->cpu_power += sd->groups->cpu_power;
+	}
+	sg = sg->next;
+	if (sg != group_head)
+		goto next_sg;
+}
+#endif
+
+/* Free memory allocated for various sched_group structures */
+static void free_sched_groups(const cpumask_t *cpu_map)
+{
+	int cpu;
+#ifdef CONFIG_NUMA
+	int i;
+
+	for_each_cpu_mask(cpu, *cpu_map) {
+		struct sched_group *sched_group_allnodes
+			= sched_group_allnodes_bycpu[cpu];
+		struct sched_group **sched_group_nodes
+			= sched_group_nodes_bycpu[cpu];
+
+		if (sched_group_allnodes) {
+			kfree(sched_group_allnodes);
+			sched_group_allnodes_bycpu[cpu] = NULL;
+		}
+
+		if (!sched_group_nodes)
+			continue;
+
+		for (i = 0; i < MAX_NUMNODES; i++) {
+			cpumask_t nodemask = node_to_cpumask(i);
+			struct sched_group *oldsg, *sg = sched_group_nodes[i];
+
+			cpus_and(nodemask, nodemask, *cpu_map);
+			if (cpus_empty(nodemask))
+				continue;
+
+			if (sg == NULL)
+				continue;
+			sg = sg->next;
+next_sg:
+			oldsg = sg;
+			sg = sg->next;
+			kfree(oldsg);
+			if (oldsg != sched_group_nodes[i])
+				goto next_sg;
+		}
+		kfree(sched_group_nodes);
+		sched_group_nodes_bycpu[cpu] = NULL;
+	}
+#endif
+	for_each_cpu_mask(cpu, *cpu_map) {
+		if (sched_group_phys_bycpu[cpu]) {
+			kfree(sched_group_phys_bycpu[cpu]);
+			sched_group_phys_bycpu[cpu] = NULL;
+		}
+#ifdef CONFIG_SCHED_MC
+		if (sched_group_core_bycpu[cpu]) {
+			kfree(sched_group_core_bycpu[cpu]);
+			sched_group_core_bycpu[cpu] = NULL;
+		}
+#endif
+	}
+}
+
+/*
+ * Build sched domains for a given set of cpus and attach the sched domains
+ * to the individual cpus
+ */
+static int build_sched_domains(const cpumask_t *cpu_map)
+{
+	int i;
+	struct sched_group *sched_group_phys = NULL;
+#ifdef CONFIG_SCHED_MC
+	struct sched_group *sched_group_core = NULL;
+#endif
+#ifdef CONFIG_NUMA
+	struct sched_group **sched_group_nodes = NULL;
+	struct sched_group *sched_group_allnodes = NULL;
+
+	/*
+	 * Allocate the per-node list of sched groups
+	 */
+	sched_group_nodes = kzalloc(sizeof(struct sched_group*)*MAX_NUMNODES,
+					   GFP_KERNEL);
+	if (!sched_group_nodes) {
+		printk(KERN_WARNING "Can not alloc sched group node list\n");
+		return -ENOMEM;
+	}
+	sched_group_nodes_bycpu[first_cpu(*cpu_map)] = sched_group_nodes;
+#endif
+
+	/*
+	 * Set up domains for cpus specified by the cpu_map.
+	 */
+	for_each_cpu_mask(i, *cpu_map) {
+		int group;
+		struct sched_domain *sd = NULL, *p;
+		cpumask_t nodemask = node_to_cpumask(cpu_to_node(i));
+
+		cpus_and(nodemask, nodemask, *cpu_map);
+
+#ifdef CONFIG_NUMA
+		if (cpus_weight(*cpu_map)
+				> SD_NODES_PER_DOMAIN*cpus_weight(nodemask)) {
+			if (!sched_group_allnodes) {
+				sched_group_allnodes
+					= kmalloc(sizeof(struct sched_group)
+							* MAX_NUMNODES,
+						  GFP_KERNEL);
+				if (!sched_group_allnodes) {
+					printk(KERN_WARNING
+					"Can not alloc allnodes sched group\n");
+					goto error;
+				}
+				sched_group_allnodes_bycpu[i]
+						= sched_group_allnodes;
+			}
+			sd = &per_cpu(allnodes_domains, i);
+			*sd = SD_ALLNODES_INIT;
+			sd->span = *cpu_map;
+			group = cpu_to_allnodes_group(i);
+			sd->groups = &sched_group_allnodes[group];
+			p = sd;
+		} else
+			p = NULL;
+
+		sd = &per_cpu(node_domains, i);
+		*sd = SD_NODE_INIT;
+		sd->span = sched_domain_node_span(cpu_to_node(i));
+		sd->parent = p;
+		cpus_and(sd->span, sd->span, *cpu_map);
+#endif
+
+		if (!sched_group_phys) {
+			sched_group_phys
+				= kmalloc(sizeof(struct sched_group) * NR_CPUS,
+					  GFP_KERNEL);
+			if (!sched_group_phys) {
+				printk (KERN_WARNING "Can not alloc phys sched"
+						     "group\n");
+				goto error;
+			}
+			sched_group_phys_bycpu[i] = sched_group_phys;
+		}
+
+		p = sd;
+		sd = &per_cpu(phys_domains, i);
+		group = cpu_to_phys_group(i);
+		*sd = SD_CPU_INIT;
+		sd->span = nodemask;
+		sd->parent = p;
+		sd->groups = &sched_group_phys[group];
+
+#ifdef CONFIG_SCHED_MC
+		if (!sched_group_core) {
+			sched_group_core
+				= kmalloc(sizeof(struct sched_group) * NR_CPUS,
+					  GFP_KERNEL);
+			if (!sched_group_core) {
+				printk (KERN_WARNING "Can not alloc core sched"
+						     "group\n");
+				goto error;
+			}
+			sched_group_core_bycpu[i] = sched_group_core;
+		}
+
+		p = sd;
+		sd = &per_cpu(core_domains, i);
+		group = cpu_to_core_group(i);
+		*sd = SD_MC_INIT;
+		sd->span = cpu_coregroup_map(i);
+		cpus_and(sd->span, sd->span, *cpu_map);
+		sd->parent = p;
+		sd->groups = &sched_group_core[group];
+#endif
+
+#ifdef CONFIG_SCHED_SMT
+		p = sd;
+		sd = &per_cpu(cpu_domains, i);
+		group = cpu_to_cpu_group(i);
+		*sd = SD_SIBLING_INIT;
+		sd->span = cpu_sibling_map[i];
+		cpus_and(sd->span, sd->span, *cpu_map);
+		sd->parent = p;
+		sd->groups = &sched_group_cpus[group];
+#endif
+	}
+
+#ifdef CONFIG_SCHED_SMT
+	/* Set up CPU (sibling) groups */
+	for_each_cpu_mask(i, *cpu_map) {
+		cpumask_t this_sibling_map = cpu_sibling_map[i];
+		cpus_and(this_sibling_map, this_sibling_map, *cpu_map);
+		if (i != first_cpu(this_sibling_map))
+			continue;
+
+		init_sched_build_groups(sched_group_cpus, this_sibling_map,
+						&cpu_to_cpu_group);
+	}
+#endif
+
+#ifdef CONFIG_SCHED_MC
+	/* Set up multi-core groups */
+	for_each_cpu_mask(i, *cpu_map) {
+		cpumask_t this_core_map = cpu_coregroup_map(i);
+		cpus_and(this_core_map, this_core_map, *cpu_map);
+		if (i != first_cpu(this_core_map))
+			continue;
+		init_sched_build_groups(sched_group_core, this_core_map,
+					&cpu_to_core_group);
+	}
+#endif
+
+
+	/* Set up physical groups */
+	for (i = 0; i < MAX_NUMNODES; i++) {
+		cpumask_t nodemask = node_to_cpumask(i);
+
+		cpus_and(nodemask, nodemask, *cpu_map);
+		if (cpus_empty(nodemask))
+			continue;
+
+		init_sched_build_groups(sched_group_phys, nodemask,
+						&cpu_to_phys_group);
+	}
+
+#ifdef CONFIG_NUMA
+	/* Set up node groups */
+	if (sched_group_allnodes)
+		init_sched_build_groups(sched_group_allnodes, *cpu_map,
+					&cpu_to_allnodes_group);
+
+	for (i = 0; i < MAX_NUMNODES; i++) {
+		/* Set up node groups */
+		struct sched_group *sg, *prev;
+		cpumask_t nodemask = node_to_cpumask(i);
+		cpumask_t domainspan;
+		cpumask_t covered = CPU_MASK_NONE;
+		int j;
+
+		cpus_and(nodemask, nodemask, *cpu_map);
+		if (cpus_empty(nodemask)) {
+			sched_group_nodes[i] = NULL;
+			continue;
+		}
+
+		domainspan = sched_domain_node_span(i);
+		cpus_and(domainspan, domainspan, *cpu_map);
+
+		sg = kmalloc_node(sizeof(struct sched_group), GFP_KERNEL, i);
+		if (!sg) {
+			printk(KERN_WARNING "Can not alloc domain group for "
+				"node %d\n", i);
+			goto error;
+		}
+		sched_group_nodes[i] = sg;
+		for_each_cpu_mask(j, nodemask) {
+			struct sched_domain *sd;
+			sd = &per_cpu(node_domains, j);
+			sd->groups = sg;
+		}
+		sg->cpu_power = 0;
+		sg->cpumask = nodemask;
+		sg->next = sg;
+		cpus_or(covered, covered, nodemask);
+		prev = sg;
+
+		for (j = 0; j < MAX_NUMNODES; j++) {
+			cpumask_t tmp, notcovered;
+			int n = (i + j) % MAX_NUMNODES;
+
+			cpus_complement(notcovered, covered);
+			cpus_and(tmp, notcovered, *cpu_map);
+			cpus_and(tmp, tmp, domainspan);
+			if (cpus_empty(tmp))
+				break;
+
+			nodemask = node_to_cpumask(n);
+			cpus_and(tmp, tmp, nodemask);
+			if (cpus_empty(tmp))
+				continue;
+
+			sg = kmalloc_node(sizeof(struct sched_group),
+					  GFP_KERNEL, i);
+			if (!sg) {
+				printk(KERN_WARNING
+				"Can not alloc domain group for node %d\n", j);
+				goto error;
+			}
+			sg->cpu_power = 0;
+			sg->cpumask = tmp;
+			sg->next = prev->next;
+			cpus_or(covered, covered, tmp);
+			prev->next = sg;
+			prev = sg;
+		}
+	}
+#endif
+
+	/* Calculate CPU power for physical packages and nodes */
+#ifdef CONFIG_SCHED_SMT
+	for_each_cpu_mask(i, *cpu_map) {
+		struct sched_domain *sd;
+		sd = &per_cpu(cpu_domains, i);
+		sd->groups->cpu_power = SCHED_LOAD_SCALE;
+	}
+#endif
+#ifdef CONFIG_SCHED_MC
+	for_each_cpu_mask(i, *cpu_map) {
+		int power;
+		struct sched_domain *sd;
+		sd = &per_cpu(core_domains, i);
+		if (sched_smt_power_savings)
+			power = SCHED_LOAD_SCALE * cpus_weight(sd->groups->cpumask);
+		else
+			power = SCHED_LOAD_SCALE + (cpus_weight(sd->groups->cpumask)-1)
+					    * SCHED_LOAD_SCALE / 10;
+		sd->groups->cpu_power = power;
+	}
+#endif
+
+	for_each_cpu_mask(i, *cpu_map) {
+		struct sched_domain *sd;
+#ifdef CONFIG_SCHED_MC
+		sd = &per_cpu(phys_domains, i);
+		if (i != first_cpu(sd->groups->cpumask))
+			continue;
+
+		sd->groups->cpu_power = 0;
+		if (sched_mc_power_savings || sched_smt_power_savings) {
+			int j;
+
+ 			for_each_cpu_mask(j, sd->groups->cpumask) {
+				struct sched_domain *sd1;
+ 				sd1 = &per_cpu(core_domains, j);
+ 				/*
+ 			 	 * for each core we will add once
+ 				 * to the group in physical domain
+ 			 	 */
+  	 			if (j != first_cpu(sd1->groups->cpumask))
+ 					continue;
+
+ 				if (sched_smt_power_savings)
+   					sd->groups->cpu_power += sd1->groups->cpu_power;
+ 				else
+   					sd->groups->cpu_power += SCHED_LOAD_SCALE;
+   			}
+ 		} else
+ 			/*
+ 			 * This has to be < 2 * SCHED_LOAD_SCALE
+ 			 * Lets keep it SCHED_LOAD_SCALE, so that
+ 			 * while calculating NUMA group's cpu_power
+ 			 * we can simply do
+ 			 *  numa_group->cpu_power += phys_group->cpu_power;
+ 			 *
+ 			 * See "only add power once for each physical pkg"
+ 			 * comment below
+ 			 */
+ 			sd->groups->cpu_power = SCHED_LOAD_SCALE;
+#else
+		int power;
+		sd = &per_cpu(phys_domains, i);
+		if (sched_smt_power_savings)
+			power = SCHED_LOAD_SCALE * cpus_weight(sd->groups->cpumask);
+		else
+			power = SCHED_LOAD_SCALE;
+		sd->groups->cpu_power = power;
+#endif
+	}
+
+#ifdef CONFIG_NUMA
+	for (i = 0; i < MAX_NUMNODES; i++)
+		init_numa_sched_groups_power(sched_group_nodes[i]);
+
+	init_numa_sched_groups_power(sched_group_allnodes);
+#endif
+
+	/* Attach the domains */
+	for_each_cpu_mask(i, *cpu_map) {
+		struct sched_domain *sd;
+#ifdef CONFIG_SCHED_SMT
+		sd = &per_cpu(cpu_domains, i);
+#elif defined(CONFIG_SCHED_MC)
+		sd = &per_cpu(core_domains, i);
+#else
+		sd = &per_cpu(phys_domains, i);
+#endif
+		cpu_attach_domain(sd, i);
+	}
+	/*
+	 * Tune cache-hot values:
+	 */
+	calibrate_migration_costs(cpu_map);
+
+	return 0;
+
+error:
+	free_sched_groups(cpu_map);
+	return -ENOMEM;
+}
+/*
+ * Set up scheduler domains and groups.  Callers must hold the hotplug lock.
+ */
+static int arch_init_sched_domains(const cpumask_t *cpu_map)
+{
+	cpumask_t cpu_default_map;
+	int err;
+
+	/*
+	 * Setup mask for cpus without special case scheduling requirements.
+	 * For now this just excludes isolated cpus, but could be used to
+	 * exclude other special cases in the future.
+	 */
+	cpus_andnot(cpu_default_map, *cpu_map, cpu_isolated_map);
+
+	err = build_sched_domains(&cpu_default_map);
+
+	return err;
+}
+
+static void arch_destroy_sched_domains(const cpumask_t *cpu_map)
+{
+	free_sched_groups(cpu_map);
+}
+
+/*
+ * Detach sched domains from a group of cpus specified in cpu_map
+ * These cpus will now be attached to the NULL domain
+ */
+static void detach_destroy_domains(const cpumask_t *cpu_map)
+{
+	int i;
+
+	for_each_cpu_mask(i, *cpu_map)
+		cpu_attach_domain(NULL, i);
+	synchronize_sched();
+	arch_destroy_sched_domains(cpu_map);
+}
+
+/*
+ * Partition sched domains as specified by the cpumasks below.
+ * This attaches all cpus from the cpumasks to the NULL domain,
+ * waits for a RCU quiescent period, recalculates sched
+ * domain information and then attaches them back to the
+ * correct sched domains
+ * Call with hotplug lock held
+ */
+int partition_sched_domains(cpumask_t *partition1, cpumask_t *partition2)
+{
+	cpumask_t change_map;
+	int err = 0;
+
+	cpus_and(*partition1, *partition1, cpu_online_map);
+	cpus_and(*partition2, *partition2, cpu_online_map);
+	cpus_or(change_map, *partition1, *partition2);
+
+	/* Detach sched domains from all of the affected cpus */
+	detach_destroy_domains(&change_map);
+	if (!cpus_empty(*partition1))
+		err = build_sched_domains(partition1);
+	if (!err && !cpus_empty(*partition2))
+		err = build_sched_domains(partition2);
+
+	return err;
+}
+
+#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)
+int arch_reinit_sched_domains(void)
+{
+	int err;
+
+	lock_cpu_hotplug();
+	detach_destroy_domains(&cpu_online_map);
+	err = arch_init_sched_domains(&cpu_online_map);
+	unlock_cpu_hotplug();
+
+	return err;
+}
+
+static ssize_t sched_power_savings_store(const char *buf, size_t count, int smt)
+{
+	int ret;
+
+	if (buf[0] != '0' && buf[0] != '1')
+		return -EINVAL;
+
+	if (smt)
+		sched_smt_power_savings = (buf[0] == '1');
+	else
+		sched_mc_power_savings = (buf[0] == '1');
+
+	ret = arch_reinit_sched_domains();
+
+	return ret ? ret : count;
+}
+
+int sched_create_sysfs_power_savings_entries(struct sysdev_class *cls)
+{
+	int err = 0;
+
+#ifdef CONFIG_SCHED_SMT
+	if (smt_capable())
+		err = sysfs_create_file(&cls->kset.kobj,
+					&attr_sched_smt_power_savings.attr);
+#endif
+#ifdef CONFIG_SCHED_MC
+	if (!err && mc_capable())
+		err = sysfs_create_file(&cls->kset.kobj,
+					&attr_sched_mc_power_savings.attr);
+#endif
+	return err;
+}
+#endif
+
+#ifdef CONFIG_SCHED_MC
+static ssize_t sched_mc_power_savings_show(struct sys_device *dev, char *page)
+{
+	return sprintf(page, "%u\n", sched_mc_power_savings);
+}
+static ssize_t sched_mc_power_savings_store(struct sys_device *dev,
+					    const char *buf, size_t count)
+{
+	return sched_power_savings_store(buf, count, 0);
+}
+SYSDEV_ATTR(sched_mc_power_savings, 0644, sched_mc_power_savings_show,
+	    sched_mc_power_savings_store);
+#endif
+
+#ifdef CONFIG_SCHED_SMT
+static ssize_t sched_smt_power_savings_show(struct sys_device *dev, char *page)
+{
+	return sprintf(page, "%u\n", sched_smt_power_savings);
+}
+static ssize_t sched_smt_power_savings_store(struct sys_device *dev,
+					     const char *buf, size_t count)
+{
+	return sched_power_savings_store(buf, count, 1);
+}
+SYSDEV_ATTR(sched_smt_power_savings, 0644, sched_smt_power_savings_show,
+	    sched_smt_power_savings_store);
+#endif
+
+
+#ifdef CONFIG_HOTPLUG_CPU
+/*
+ * Force a reinitialization of the sched domains hierarchy.  The domains
+ * and groups cannot be updated in place without racing with the balancing
+ * code, so we temporarily attach all running cpus to the NULL domain
+ * which will prevent rebalancing while the sched domains are recalculated.
+ */
+static int update_sched_domains(struct notifier_block *nfb,
+				unsigned long action, void *hcpu)
+{
+	switch (action) {
+	case CPU_UP_PREPARE:
+	case CPU_DOWN_PREPARE:
+		detach_destroy_domains(&cpu_online_map);
+		return NOTIFY_OK;
+
+	case CPU_UP_CANCELED:
+	case CPU_DOWN_FAILED:
+	case CPU_ONLINE:
+	case CPU_DEAD:
+		/*
+		 * Fall through and re-initialise the domains.
+		 */
+		break;
+	default:
+		return NOTIFY_DONE;
+	}
+
+	/* The hotplug lock is already held by cpu_up/cpu_down */
+	arch_init_sched_domains(&cpu_online_map);
+
+	return NOTIFY_OK;
+}
+#endif
+
+void __init sched_init_smp(void)
+{
+	lock_cpu_hotplug();
+	arch_init_sched_domains(&cpu_online_map);
+	unlock_cpu_hotplug();
+	/* XXX: Theoretical race here - CPU may be hotplugged now */
+	hotcpu_notifier(update_sched_domains, 0);
+	init_sched_domain_sysctl();
+}
+#else
+void __init sched_init_smp(void)
+{
+}
+#endif /* CONFIG_SMP */
+
+int in_sched_functions(unsigned long addr)
+{
+	/* Linker adds these: start and end of __sched functions */
+	extern char __sched_text_start[], __sched_text_end[];
+
+	return in_lock_functions(addr) ||
+		(addr >= (unsigned long)__sched_text_start
+		&& addr < (unsigned long)__sched_text_end);
+}
+
+void __init sched_init(void)
+{
+	int i, j, k;
+
+	for_each_possible_cpu(i) {
+		struct prio_array *array;
+		struct rq *rq;
+
+		rq = cpu_rq(i);
+		spin_lock_init(&rq->lock);
+		lockdep_set_class(&rq->lock, &rq->rq_lock_key);
+		rq->nr_running = 0;
+		rq->active = rq->arrays;
+		rq->expired = rq->arrays + 1;
+		rq->best_expired_prio = MAX_PRIO;
+
+#ifdef CONFIG_SMP
+		rq->sd = NULL;
+		for (j = 1; j < 3; j++)
+			rq->cpu_load[j] = 0;
+		rq->active_balance = 0;
+		rq->push_cpu = 0;
+		rq->migration_thread = NULL;
+		INIT_LIST_HEAD(&rq->migration_queue);
+#endif
+		atomic_set(&rq->nr_iowait, 0);
+
+		for (j = 0; j < 2; j++) {
+			array = rq->arrays + j;
+			for (k = 0; k < MAX_PRIO; k++) {
+				INIT_LIST_HEAD(array->queue + k);
+				__clear_bit(k, array->bitmap);
+			}
+			// delimiter for bitsearch
+			__set_bit(MAX_PRIO, array->bitmap);
+		}
+	}
+
+	set_load_weight(&init_task);
+	/*
+	 * The boot idle thread does lazy MMU switching as well:
+	 */
+	atomic_inc(&init_mm.mm_count);
+	enter_lazy_tlb(&init_mm, current);
+
+	/*
+	 * Make us the idle thread. Technically, schedule() should not be
+	 * called from this thread, however somewhere below it might be,
+	 * but because we are the idle thread, we just pick up running again
+	 * when this runqueue becomes "idle".
+	 */
+	init_idle(current, smp_processor_id());
+}
+
+#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP
+void __might_sleep(char *file, int line)
+{
+#ifdef in_atomic
+	static unsigned long prev_jiffy;	/* ratelimiting */
+
+	if ((in_atomic() || irqs_disabled()) &&
+	    system_state == SYSTEM_RUNNING && !oops_in_progress) {
+		if (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)
+			return;
+		prev_jiffy = jiffies;
+		printk(KERN_ERR "BUG: sleeping function called from invalid"
+				" context at %s:%d\n", file, line);
+		printk("in_atomic():%d, irqs_disabled():%d\n",
+			in_atomic(), irqs_disabled());
+		dump_stack();
+	}
+#endif
+}
+EXPORT_SYMBOL(__might_sleep);
+#endif
+
+#ifdef CONFIG_MAGIC_SYSRQ
+void normalize_rt_tasks(void)
+{
+	struct prio_array *array;
+	struct task_struct *p;
+	unsigned long flags;
+	struct rq *rq;
+
+	read_lock_irq(&tasklist_lock);
+	for_each_process(p) {
+		if (!rt_task(p))
+			continue;
+
+		spin_lock_irqsave(&p->pi_lock, flags);
+		rq = __task_rq_lock(p);
+
+		array = p->array;
+		if (array)
+			deactivate_task(p, task_rq(p));
+		__setscheduler(p, SCHED_NORMAL, 0);
+		if (array) {
+			__activate_task(p, task_rq(p));
+			resched_task(rq->curr);
+		}
+
+		__task_rq_unlock(rq);
+		spin_unlock_irqrestore(&p->pi_lock, flags);
+	}
+	read_unlock_irq(&tasklist_lock);
+}
+
+#endif /* CONFIG_MAGIC_SYSRQ */
+
+#ifdef CONFIG_IA64
+/*
+ * These functions are only useful for the IA64 MCA handling.
+ *
+ * They can only be called when the whole system has been
+ * stopped - every CPU needs to be quiescent, and no scheduling
+ * activity can take place. Using them for anything else would
+ * be a serious bug, and as a result, they aren't even visible
+ * under any other configuration.
+ */
+
+/**
+ * curr_task - return the current task for a given cpu.
+ * @cpu: the processor in question.
+ *
+ * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!
+ */
+struct task_struct *curr_task(int cpu)
+{
+	return cpu_curr(cpu);
+}
+
+/**
+ * set_curr_task - set the current task for a given cpu.
+ * @cpu: the processor in question.
+ * @p: the task pointer to set.
+ *
+ * Description: This function must only be used when non-maskable interrupts
+ * are serviced on a separate stack.  It allows the architecture to switch the
+ * notion of the current task on a cpu in a non-blocking manner.  This function
+ * must be called with all CPU's synchronized, and interrupts disabled, the
+ * and caller must save the original value of the current task (see
+ * curr_task() above) and restore that value before reenabling interrupts and
+ * re-starting the system.
+ *
+ * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!
+ */
+void set_curr_task(int cpu, struct task_struct *p)
+{
+	cpu_curr(cpu) = p;
+}
+
+#endif
+
+/* Ingosched Scheduler Code End. */
diff -urN oldtree/kernel/sched_staircase.c newtree/kernel/sched_staircase.c
--- oldtree/kernel/sched_staircase.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/kernel/sched_staircase.c	2006-07-12 19:00:11.000000000 -0400
@@ -0,0 +1,6622 @@
+/*
+ * kernel/sched_staircase.c
+ * ( Staircase v16 + 2.6.17-mm6 )
+ *
+ *  Port History:
+ *  2006-05-29: Staircase ported to the -mm kernel by cheater-conrad.
+ *
+ *  Staircase History:
+ *  2006-06-18  Staircase scheduling policy by Con Kolivas with help
+ *              from William Lee Irwin III, Zwane Mwaikambo & Peter Williams.
+ *              Staircase v16
+ *
+ *  No-Sources Update History:
+ *  2006-06-10  Updated features from 2.6.17-rc6-mm2
+ *  2006-06-21  Updated to staircase v16, and updated features from 2.6.17-mm1
+ *  2006-06-27  Updated features from 2.6.17-mm2
+ *  2006-07-04  Updated features from 2.6.17-mm6
+ *  2006-07-07  Added staircase SCHED_INTERACTIVE and SCHED_COMPUTE tunable.
+ */
+
+/* Staircase Scheduler Code Begin: */
+
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/nmi.h>
+#include <linux/init.h>
+#include <asm/uaccess.h>
+#include <linux/highmem.h>
+#include <linux/smp_lock.h>
+#include <asm/mmu_context.h>
+#include <linux/interrupt.h>
+#include <linux/capability.h>
+#include <linux/completion.h>
+#include <linux/kernel_stat.h>
+#include <linux/debug_locks.h>
+#include <linux/security.h>
+#include <linux/notifier.h>
+#include <linux/profile.h>
+#include <linux/suspend.h>
+#include <linux/vmalloc.h>
+#include <linux/blkdev.h>
+#include <linux/delay.h>
+#include <linux/smp.h>
+#include <linux/threads.h>
+#include <linux/timer.h>
+#include <linux/rcupdate.h>
+#include <linux/cpu.h>
+#include <linux/cpuset.h>
+#include <linux/percpu.h>
+#include <linux/kthread.h>
+#include <linux/seq_file.h>
+#include <linux/sysctl.h>
+#include <linux/syscalls.h>
+#include <linux/times.h>
+#include <linux/acct.h>
+#include <linux/kprobes.h>
+#include <linux/delayacct.h>
+#include <asm/tlb.h>
+
+#include <asm/unistd.h>
+
+/*
+ * sched_interactive - sysctl which allows interactive tasks to have bonus
+ * raise its priority.
+ * sched_compute - sysctl which enables long timeslices and delayed preemption
+ * for compute server usage.
+ */
+int sched_interactive __read_mostly = 1;
+int sched_compute __read_mostly;
+
+/*
+ * CACHE_DELAY is the time preemption is delayed in sched_compute mode
+ * and is set to a nominal 10ms.
+ */
+#define CACHE_DELAY    (10 * (HZ) / 1001 + 1)
+
+/*
+ * Convert user-nice values [ -20 ... 0 ... 19 ]
+ * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],
+ * and back.
+ */
+#define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 20)
+#define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 20)
+#define TASK_NICE(p)		PRIO_TO_NICE((p)->static_prio)
+
+/*
+ * 'User priority' is the nice value converted to something we
+ * can work with better when scaling various scheduler parameters,
+ * it's a [ 0 ... 39 ] range.
+ */
+#define USER_PRIO(p)		((p)-MAX_RT_PRIO)
+#define TASK_USER_PRIO(p)	USER_PRIO((p)->static_prio)
+#define MAX_USER_PRIO		(USER_PRIO(MAX_PRIO))
+
+/*
+ * Some helpers for converting nanosecond timing to jiffy resolution
+ */
+#define NSJIFFY                       (1000000000 / HZ)       /* One jiffy in ns */
+#define NS_TO_JIFFIES(TIME)   ((TIME) / NSJIFFY)
+#define JIFFIES_TO_NS(TIME)   ((TIME) * NSJIFFY)
+#define TASK_PREEMPTS_CURR(p, rq) \
+	((p)->prio < (rq)->curr->prio)
+
+/*
+ * This is the time all tasks within the same priority round robin.
+ * Set to a minimum of 6ms. It is 10 times longer in compute mode.
+ */
+#define _RR_INTERVAL           ((6 * HZ / 1001) + 1)
+#define RR_INTERVAL            (_RR_INTERVAL * (1 + 9 * sched_compute))
+#define DEF_TIMESLICE         (RR_INTERVAL * 19)
+
+
+/* These are the runqueue data structures: */
+
+/*
+ * This is the main, per-CPU runqueue data structure.
+ *
+ * Locking rule: those places that want to lock multiple runqueues
+ * (such as the load balancing or the thread migration code), lock
+ * acquire operations must be ordered by ascending &runqueue.
+ */
+struct rq {
+	spinlock_t lock;
+
+	/*
+	 * nr_running and cpu_load should be in the same cacheline because
+	 * remote CPUs use both these fields when doing load calculation.
+	 */
+	unsigned long nr_running;
+	unsigned long raw_weighted_load;
+#ifdef CONFIG_SMP
+	unsigned long cpu_load[3];
+#endif
+	unsigned long long nr_switches;
+
+	/*
+	 * This is part of a global counter where only the total sum
+	 * over all CPUs matters. A task can increase this counter on
+	 * one CPU and if it got migrated afterwards it may decrease
+	 * it on another CPU. Always updated under the runqueue lock:
+	 */
+	unsigned long nr_uninterruptible;
+
+	unsigned long long timestamp_last_tick;
+        unsigned short cache_ticks, preempted;
+	struct task_struct *curr, *idle;
+	struct mm_struct *prev_mm;
+	unsigned long bitmap[BITS_TO_LONGS(MAX_PRIO + 1)];
+	struct list_head queue[MAX_PRIO];
+	atomic_t nr_iowait;
+
+#ifdef CONFIG_SMP
+	struct sched_domain *sd;
+
+	/* For active balancing */
+	int active_balance;
+	int push_cpu;
+
+	struct task_struct *migration_thread;
+	struct list_head migration_queue;
+	int cpu;
+#endif
+
+#ifdef CONFIG_SCHEDSTATS
+	/* latency stats */
+	struct sched_info rq_sched_info;
+
+	/* sys_sched_yield() stats */
+	unsigned long yld_exp_empty;
+	unsigned long yld_act_empty;
+	unsigned long yld_both_empty;
+	unsigned long yld_cnt;
+
+	/* schedule() stats */
+	unsigned long sched_switch;
+	unsigned long sched_cnt;
+	unsigned long sched_goidle;
+
+	/* try_to_wake_up() stats */
+	unsigned long ttwu_cnt;
+	unsigned long ttwu_local;
+#endif
+        struct lock_class_key rq_lock_key;
+};
+
+static DEFINE_PER_CPU(struct rq, runqueues);
+
+/*
+ * The domain tree (rq->sd) is protected by RCU's quiescent state transition.
+ * See detach_destroy_domains: synchronize_sched for details.
+ *
+ * The domain tree of any CPU may only be accessed from within
+ * preempt-disabled sections.
+ */
+#define for_each_domain(cpu, __sd) \
+        for (__sd = rcu_dereference(cpu_rq(cpu)->sd); __sd; __sd = __sd->parent)
+
+#define cpu_rq(cpu)		(&per_cpu(runqueues, (cpu)))
+#define this_rq()		(&__get_cpu_var(runqueues))
+#define task_rq(p)		cpu_rq(task_cpu(p))
+#define cpu_curr(cpu)		(cpu_rq(cpu)->curr)
+
+#ifndef prepare_arch_switch
+# define prepare_arch_switch(next)	do { } while (0)
+#endif
+#ifndef finish_arch_switch
+# define finish_arch_switch(prev)	do { } while (0)
+#endif
+
+#ifndef __ARCH_WANT_UNLOCKED_CTXSW
+static inline int task_running(struct rq *rq, struct task_struct *p)
+{
+	return rq->curr == p;
+}
+
+static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
+{
+}
+
+static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
+{
+#ifdef CONFIG_DEBUG_SPINLOCK
+	/* this is a valid case when another task releases the spinlock */
+	rq->lock.owner = current;
+#endif
+        /*
+         * If we are tracking spinlock dependencies then we have to
+         * fix up the runqueue lock - which gets 'carried over' from
+         * prev into current:
+         */
+        spin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);
+
+	spin_unlock_irq(&rq->lock);
+}
+
+#else /* __ARCH_WANT_UNLOCKED_CTXSW */
+static inline int task_running(struct *rq, struct task_struct *p)
+{
+#ifdef CONFIG_SMP
+	return p->oncpu;
+#else
+	return rq->curr == p;
+#endif
+}
+
+static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
+{
+#ifdef CONFIG_SMP
+	/*
+	 * We can optimise this out completely for !SMP, because the
+	 * SMP rebalancing from interrupt is the only thing that cares
+	 * here.
+	 */
+	next->oncpu = 1;
+#endif
+#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW
+	spin_unlock_irq(&rq->lock);
+#else
+	spin_unlock(&rq->lock);
+#endif
+}
+
+static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
+{
+#ifdef CONFIG_SMP
+	/*
+	 * After ->oncpu is cleared, the task can be moved to a different CPU.
+	 * We must ensure this doesn't happen until the switch is completely
+	 * finished.
+	 */
+	smp_wmb();
+	prev->oncpu = 0;
+#endif
+#ifndef __ARCH_WANT_INTERRUPTS_ON_CTXSW
+	local_irq_enable();
+#endif
+}
+#endif /* __ARCH_WANT_UNLOCKED_CTXSW */
+
+/*
+ * __task_rq_lock - lock the runqueue a given task resides on.
+ * Must be called interrupts disabled.
+ */
+static struct rq *__task_rq_lock(struct task_struct *p)
+	__acquires(rq->lock)
+{
+	struct rq *rq;
+
+repeat_lock_task:
+	rq = task_rq(p);
+	spin_lock(&rq->lock);
+	if (unlikely(rq != task_rq(p))) {
+		spin_unlock(&rq->lock);
+		goto repeat_lock_task;
+	}
+	return rq;
+}
+
+/*
+ * task_rq_lock - lock the runqueue a given task resides on and disable
+ * interrupts.  Note the ordering: we can safely lookup the task_rq without
+ * explicitly disabling preemption.
+ */
+static inline struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)
+	__acquires(rq->lock)
+{
+	struct rq *rq;
+
+repeat_lock_task:
+	local_irq_save(*flags);
+	rq = task_rq(p);
+	spin_lock(&rq->lock);
+	if (unlikely(rq != task_rq(p))) {
+		spin_unlock_irqrestore(&rq->lock, *flags);
+		goto repeat_lock_task;
+	}
+	return rq;
+}
+
+static inline void __task_rq_unlock(struct rq *rq)
+	__releases(rq->lock)
+{
+	spin_unlock(&rq->lock);
+}
+
+static inline void task_rq_unlock(struct rq *rq, unsigned long *flags)
+	__releases(rq->lock)
+{
+	spin_unlock_irqrestore(&rq->lock, *flags);
+}
+
+#ifdef CONFIG_SCHEDSTATS
+/*
+ * bump this up when changing the output format or the meaning of an existing
+ * format, so that tools can adapt (or abort)
+ */
+#define SCHEDSTAT_VERSION 12
+
+static int show_schedstat(struct seq_file *seq, void *v)
+{
+	int cpu;
+
+	seq_printf(seq, "version %d\n", SCHEDSTAT_VERSION);
+	seq_printf(seq, "timestamp %lu\n", jiffies);
+	for_each_online_cpu(cpu) {
+		struct rq *rq = cpu_rq(cpu);
+#ifdef CONFIG_SMP
+		struct sched_domain *sd;
+		int dcnt = 0;
+#endif
+
+		/* runqueue-specific stats */
+		seq_printf(seq,
+		    "cpu%d %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu",
+		    cpu, rq->yld_both_empty,
+		    rq->yld_act_empty, rq->yld_exp_empty, rq->yld_cnt,
+		    rq->sched_switch, rq->sched_cnt, rq->sched_goidle,
+		    rq->ttwu_cnt, rq->ttwu_local,
+		    rq->rq_sched_info.cpu_time,
+		    rq->rq_sched_info.run_delay, rq->rq_sched_info.pcnt);
+
+		seq_printf(seq, "\n");
+
+#ifdef CONFIG_SMP
+		/* domain-specific stats */
+		preempt_disable();
+		for_each_domain(cpu, sd) {
+			enum idle_type itype;
+			char mask_str[NR_CPUS];
+
+			cpumask_scnprintf(mask_str, NR_CPUS, sd->span);
+			seq_printf(seq, "domain%d %s", dcnt++, mask_str);
+			for (itype = SCHED_IDLE; itype < MAX_IDLE_TYPES;
+					itype++) {
+				seq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu",
+				    sd->lb_cnt[itype],
+				    sd->lb_balanced[itype],
+				    sd->lb_failed[itype],
+				    sd->lb_imbalance[itype],
+				    sd->lb_gained[itype],
+				    sd->lb_hot_gained[itype],
+				    sd->lb_nobusyq[itype],
+				    sd->lb_nobusyg[itype]);
+			}
+			seq_printf(seq, " %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu %lu\n",
+			    sd->alb_cnt, sd->alb_failed, sd->alb_pushed,
+			    sd->sbe_cnt, sd->sbe_balanced, sd->sbe_pushed,
+			    sd->sbf_cnt, sd->sbf_balanced, sd->sbf_pushed,
+			    sd->ttwu_wake_remote, sd->ttwu_move_affine, sd->ttwu_move_balance);
+		}
+		preempt_enable();
+#endif
+	}
+	return 0;
+}
+
+static int schedstat_open(struct inode *inode, struct file *file)
+{
+	unsigned int size = PAGE_SIZE * (1 + num_online_cpus() / 32);
+	char *buf = kmalloc(size, GFP_KERNEL);
+	struct seq_file *m;
+	int res;
+
+	if (!buf)
+		return -ENOMEM;
+	res = single_open(file, show_schedstat, NULL);
+	if (!res) {
+		m = file->private_data;
+		m->buf = buf;
+		m->size = size;
+	} else
+		kfree(buf);
+	return res;
+}
+
+struct file_operations proc_schedstat_operations = {
+	.open    = schedstat_open,
+	.read    = seq_read,
+	.llseek  = seq_lseek,
+	.release = single_release,
+};
+
+/*
+ * Expects runqueue lock to be held for atomicity of update
+ */
+static inline void
+rq_sched_info_arrive(struct rq *rq, unsigned long delta_jiffies)
+{
+	if (rq) {
+		rq->rq_sched_info.run_delay += delta_jiffies;
+		rq->rq_sched_info.pcnt++;
+	}
+}
+
+/*
+ * Expects runqueue lock to be held for atomicity of update
+ */
+static inline void
+rq_sched_info_depart(struct rq *rq, unsigned long delta_jiffies)
+{
+	if (rq)
+		rq->rq_sched_info.cpu_time += delta_jiffies;
+}
+# define schedstat_inc(rq, field)	do { (rq)->field++; } while (0)
+# define schedstat_add(rq, field, amt)	do { (rq)->field += (amt); } while (0)
+#else /* !CONFIG_SCHEDSTATS */
+static inline void
+rq_sched_info_arrive(struct rq *rq, unsigned long delta_jiffies)
+{}
+static inline void
+rq_sched_info_depart(struct rq *rq, unsigned long delta_jiffies)
+{}
+# define schedstat_inc(rq, field)	do { } while (0)
+# define schedstat_add(rq, field, amt)	do { } while (0)
+#endif
+
+/*
+ * rq_lock - lock a given runqueue and disable interrupts.
+ */
+static inline struct rq *this_rq_lock(void)
+	__acquires(rq->lock)
+{
+	struct rq *rq;
+
+	local_irq_disable();
+	rq = this_rq();
+	spin_lock(&rq->lock);
+
+	return rq;
+}
+
+#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
+/*
+ * Called when a process is dequeued and given the cpu.
+ *
+ * This function is only called from sched_info_arrive(), rather than
+ * dequeue_task(). Even though a task may be queued and dequeued multiple
+ * times as it is shuffled about, we're really interested in knowing how
+ * long it was from the *first* time it was queued to the time that it
+ * finally hit a cpu.
+ */
+static inline void sched_info_dequeued(struct task_struct *t)
+{
+	t->sched_info.last_queued = 0;
+}
+
+/*
+ * Called when a task finally hits the cpu.  We can now calculate how
+ * long it was waiting to run.  We also note when it began so that we
+ * can keep stats on how long its timeslice is.
+ */
+static void sched_info_arrive(struct task_struct *t)
+{
+	unsigned long now = jiffies, delta_jiffies = 0;
+
+	if (t->sched_info.last_queued)
+		delta_jiffies = now - t->sched_info.last_queued;
+	sched_info_dequeued(t);
+	t->sched_info.run_delay += delta_jiffies;
+	t->sched_info.last_arrival = now;
+	t->sched_info.pcnt++;
+
+	rq_sched_info_arrive(task_rq(t), delta_jiffies);
+}
+
+/*
+ * Called when a process is queued
+ * The time is noted and later used to determine how long we had to wait for
+ * us to reach the cpu.
+ * It is unusual but not impossible for tasks to be dequeued and immediately
+ * requeued: this can happen in sched_yield(),
+ * set_user_nice(), and even load_balance() as it moves tasks from runqueue
+ * to runqueue.
+ *
+ * This function is only called from enqueue_task(), but also only updates
+ * the timestamp if it is already not set.  It's assumed that
+ * sched_info_dequeued() will clear that stamp when appropriate.
+ */
+static inline void sched_info_queued(struct task_struct *t)
+{
+	if (unlikely(sched_info_on()))
+		if (!t->sched_info.last_queued)
+			t->sched_info.last_queued = jiffies;
+}
+
+/*
+ * Called when a process ceases being the active-running process, either
+ * voluntarily or involuntarily.  Now we can calculate how long we ran.
+ */
+static inline void sched_info_depart(struct task_struct *t)
+{
+	unsigned long delta_jiffies = jiffies - t->sched_info.last_arrival;
+
+	t->sched_info.cpu_time += delta_jiffies;
+	rq_sched_info_depart(task_rq(t), delta_jiffies);
+}
+
+/*
+ * Called when tasks are switched involuntarily due, typically, to expiring
+ * their time slice.  (This may also be called when switching to or from
+ * the idle task.)  We are only called when prev != next.
+ */
+static inline void __sched_info_switch(struct task_struct *prev, struct task_struct *next)
+{
+	struct rq *rq = task_rq(prev);
+
+	/*
+	 * prev now departs the cpu.  It's not interesting to record
+	 * stats about how efficient we were at scheduling the idle
+	 * process, however.
+	 */
+	if (prev != rq->idle)
+		sched_info_depart(prev);
+
+	if (next != rq->idle)
+		sched_info_arrive(next);
+}
+static inline void 
+sched_info_switch(struct task_struct *prev, struct task_struct *next)
+{
+	if (unlikely(sched_info_on()))
+		__sched_info_switch(prev, next);
+}
+#else
+#define sched_info_queued(t)		do { } while (0)
+#define sched_info_switch(t, next)	do { } while (0)
+#endif /* CONFIG_SCHEDSTATS || CONFIG_TASK_DELAY_ACCT */
+
+/*
+ * Get nanosecond clock difference without overflowing unsigned long.
+ */
+static unsigned long ns_diff(unsigned long long v1, unsigned long long v2)
+{
+	unsigned long long vdiff;
+	if (likely(v1 >= v2)) {
+		vdiff = v1 - v2;
+#if BITS_PER_LONG < 64
+		if (vdiff > (1 << 31))
+			vdiff = 1 << 31;
+#endif
+	} else {
+		/*
+		 * Rarely the clock appears to go backwards. There should
+		 * always be a positive difference so return 1.
+		 */
+		vdiff = 1;
+	}
+	return (unsigned long)vdiff;
+}
+
+static inline int task_queued(const struct task_struct *task)
+{
+	return !list_empty(&task->run_list);
+}
+
+/*
+ * Adding/removing a task to/from a runqueue:
+ */
+static void dequeue_task(struct task_struct *p, struct rq *rq)
+{
+	list_del_init(&p->run_list);
+	if (list_empty(rq->queue + p->prio))
+		__clear_bit(p->prio, rq->bitmap);
+	p->ns_debit = 0;
+}
+
+static void enqueue_task(struct task_struct *p, struct rq *rq)
+{
+	sched_info_queued(p);
+	list_add_tail(&p->run_list, rq->queue + p->prio);
+	__set_bit(p->prio, rq->bitmap);
+}
+
+/*
+ * Put task to the end of the run list without the overhead of dequeue
+ * followed by enqueue.
+ */
+static void requeue_task(struct task_struct *p, struct rq *rq, int prio)
+{
+	list_move_tail(&p->run_list, rq->queue + prio);
+	if (p->prio != prio) {
+		if (list_empty(rq->queue + p->prio))
+			__clear_bit(p->prio, rq->bitmap);
+		p->prio = prio;
+		__set_bit(prio, rq->bitmap);
+	}
+	p->ns_debit = 0;
+}
+
+static inline void enqueue_task_head(struct task_struct *p, struct rq *rq)
+{
+	list_add(&p->run_list, rq->queue + p->prio);
+	__set_bit(p->prio, rq->bitmap);
+}
+
+static unsigned int rr_interval(const struct task_struct *p)
+{
+	int nice = TASK_NICE(p);
+
+	if (nice < 0 && !rt_task(p))
+		return RR_INTERVAL * (20 - nice) / 20;
+	return RR_INTERVAL;
+}
+
+/*
+ * slice - the duration a task runs before getting requeued at its best
+ * priority and has its bonus decremented.
+ */
+static unsigned int slice(const struct task_struct *p)
+{
+	unsigned int slice, rr;
+
+	slice = rr = rr_interval(p);
+	if (likely(!rt_task(p)))
+		slice += (39 - TASK_USER_PRIO(p)) * rr;
+	return slice;
+}
+
+/*
+ * Bonus - How much higher than its base priority an interactive task can run.
+ */
+static inline unsigned int bonus(const struct task_struct *p)
+{
+	return TASK_USER_PRIO(p);
+}
+
+/*
+ * __normal_prio - return the priority that is based on the static
+ * priority but is modified by bonuses/penalties.
+ * The priority normally decreases by one each rr_interval().
+ * As the bonus increases the initial priority starts at a higher "stair" or
+ * priority.
+ */
+static inline int __normal_prio(struct task_struct *p)
+{
+	int prio;
+	unsigned int full_slice, used_slice = 0;
+	unsigned int best_bonus, rr;
+
+	full_slice = slice(p);
+	if (full_slice > p->slice)
+		used_slice = full_slice - p->slice;
+	best_bonus = bonus(p);
+	prio = MAX_RT_PRIO + best_bonus;
+	/* SCHED_BATCH tasks have their bonus ignored */
+        if (sched_interactive && !sched_compute && !batch_task(p))
+		prio -= p->bonus;
+	rr = rr_interval(p);
+	prio += used_slice / rr;
+	if (prio > MIN_USER_PRIO)
+		prio = MIN_USER_PRIO;
+	return prio;
+}
+
+/*
+ * To aid in avoiding the subversion of "niceness" due to uneven distribution
+ * of tasks with abnormal "nice" values across CPUs the contribution that
+ * each task makes to its run queue's load is weighted according to its
+ * scheduling class and "nice" value.  For SCHED_NORMAL tasks this is just a
+ * scaled version of the new time slice allocation that they receive on time
+ * slice expiry etc.
+ */
+
+/*
+ * Assume: static_prio_timeslice(NICE_TO_PRIO(0)) == DEF_TIMESLICE
+ * If static_prio_timeslice() is ever changed to break this assumption then
+ * this code will need modification
+ */
+#define TIME_SLICE_NICE_ZERO DEF_TIMESLICE
+#define LOAD_WEIGHT(lp) \
+	(((lp) * SCHED_LOAD_SCALE) / TIME_SLICE_NICE_ZERO)
+#define TASK_LOAD_WEIGHT(p)	LOAD_WEIGHT(slice(p))
+#define RTPRIO_TO_LOAD_WEIGHT(rp)	\
+	(LOAD_WEIGHT((RR_INTERVAL + 20 + (rp))))
+
+static void set_load_weight(struct task_struct *p)
+{
+	if (rt_task(p)) {
+#ifdef CONFIG_SMP
+		if (p == task_rq(p)->migration_thread)
+			/*
+			 * The migration thread does the actual balancing.
+			 * Giving its load any weight will skew balancing
+			 * adversely.
+			 */
+			p->load_weight = 0;
+		else
+#endif
+			p->load_weight = RTPRIO_TO_LOAD_WEIGHT(p->rt_priority);
+	} else
+		p->load_weight = TASK_LOAD_WEIGHT(p);
+}
+
+static inline void inc_raw_weighted_load(struct rq *rq, const struct task_struct *p)
+{
+	rq->raw_weighted_load += p->load_weight;
+}
+
+static inline void dec_raw_weighted_load(struct rq *rq, const struct task_struct *p)
+{
+	rq->raw_weighted_load -= p->load_weight;
+}
+
+static inline void inc_nr_running(struct task_struct *p, struct rq *rq)
+{
+	rq->nr_running++;
+	inc_raw_weighted_load(rq, p);
+}
+
+static inline void dec_nr_running(struct task_struct *p, struct rq *rq)
+{
+	rq->nr_running--;
+	dec_raw_weighted_load(rq, p);
+}
+
+/*
+ * Calculate the expected normal priority: i.e. priority
+ * without taking RT-inheritance into account. Might be
+ * boosted by interactivity modifiers. Changes upon fork,
+ * setprio syscalls, and whenever the interactivity
+ * estimator recalculates.
+ */
+static inline int normal_prio(struct task_struct *p)
+{
+	int prio;
+
+	if (p->policy != SCHED_NORMAL && p->policy != SCHED_BATCH)
+		prio = MAX_RT_PRIO-1 - p->rt_priority;
+	else
+		prio = __normal_prio(p);
+	return prio;
+}
+
+/*
+ * Calculate the current priority, i.e. the priority
+ * taken into account by the scheduler. This value might
+ * be boosted by RT tasks, or might be boosted by
+ * bonus modifiers. Will be RT if the task got
+ * RT-boosted. If not then it returns p->normal_prio.
+ */
+static int effective_prio(struct task_struct *p)
+{
+	p->normal_prio = normal_prio(p);
+	/*
+	 * If we are RT tasks or we were boosted to RT priority,
+	 * keep the priority unchanged. Otherwise, update priority
+	 * to the normal priority:
+	 */
+	if (!rt_prio(p->prio))
+		return p->normal_prio;
+	return p->prio;
+}
+
+/*
+ * __activate_task - move a task to the runqueue.
+ */
+static void __activate_task(struct task_struct *p, struct rq *rq)
+{
+	enqueue_task(p, rq);
+	inc_nr_running(p, rq);
+}
+
+/*
+ * __activate_idle_task - move idle task to the _front_ of runqueue.
+ */
+static inline void __activate_idle_task(struct task_struct *p, struct rq *rq)
+{
+	enqueue_task_head(p, rq);
+	inc_nr_running(p, rq);
+}
+
+/*
+ * We increase our bonus by sleeping more than the time we ran.
+ * The ratio of sleep to run gives us the cpu% that we last ran and determines
+ * the maximum bonus we can acquire.
+ */
+static void inc_bonus(struct task_struct *p, unsigned long totalrun, unsigned long sleep)
+{
+	unsigned int best_bonus = sleep / (totalrun + 1);
+
+	if (p->bonus >= best_bonus)
+		return;
+	best_bonus = bonus(p);
+	if (p->bonus < best_bonus)
+		p->bonus++;
+}
+
+static inline void dec_bonus(struct task_struct *p)
+{
+	p->totalrun = 0;
+	if (p->bonus)
+		p->bonus--;
+}
+
+static inline void continue_slice(struct task_struct *p)
+{
+	unsigned long total_run = NS_TO_JIFFIES(p->totalrun);
+
+	if (total_run >= p->slice || p->prio == MIN_USER_PRIO)
+		dec_bonus(p);
+	else {
+		unsigned long remainder;
+
+		p->slice -= total_run;
+		if (p->slice <= p->time_slice)
+			dec_bonus(p);
+		remainder = p->slice % rr_interval(p);
+		if (remainder)
+			p->time_slice = remainder;
+	}
+}
+
+/*
+ * recalc_task_prio - this checks for tasks that run ultra short timeslices
+ * or have just forked a thread/process and make them continue their old
+ * slice instead of starting a new one at high priority.
+ */
+static inline void recalc_task_prio(struct task_struct *p, const unsigned long long now)
+{
+	/* Double the systime to account for missed sub-jiffy time */
+	unsigned long ns_systime = JIFFIES_TO_NS(p->systime) * 2;
+	unsigned long sleep_time = ns_diff(now, p->timestamp);
+
+	/*
+	 * Add the total for this last scheduled run (p->runtime) and system
+	 * time (p->systime) done on behalf of p to the running total so far
+	 * used (p->totalrun).
+	 */
+	p->totalrun += p->runtime + ns_systime;
+
+	/* systime is unintentionally seen as sleep, subtract it */
+	if (likely(ns_systime < sleep_time))
+		sleep_time -= ns_systime;
+	else
+		sleep_time = 0;
+
+	if (unlikely(p->flags & PF_FORKED))
+		sleep_time = 0;
+
+	/*
+	 * If we sleep longer than our running total and have not set the
+	 * PF_NONSLEEP flag we gain a bonus.
+	 */
+	if (sleep_time >= p->totalrun && !(p->flags & PF_NONSLEEP)) {
+		inc_bonus(p, p->totalrun, sleep_time);
+		p->totalrun = 0;
+		return;
+	}
+
+	/* We elevate priority by the amount of time we slept. */
+	p->totalrun -= sleep_time;
+	continue_slice(p);
+}
+
+/*
+ * activate_task - move a task to the runqueue and do priority recalculation
+ *
+ * Update all the scheduling statistics stuff. (priority modifiers, etc.)
+ */
+static void activate_task(struct task_struct *p, struct rq *rq, int local)
+{
+	unsigned long long now = sched_clock();
+	unsigned long rr = rr_interval(p);
+
+#ifdef CONFIG_SMP
+	if (!local) {
+		/* Compensate for drifting sched_clock */
+		struct rq *this_rq = this_rq();
+		now = (now - this_rq->timestamp_last_tick)
+			+ rq->timestamp_last_tick;
+	}
+#endif
+	p->slice = slice(p);
+	p->time_slice = p->slice % rr ? : rr;
+	if (!rt_task(p)) {
+		recalc_task_prio(p, now);
+		p->flags &= ~(PF_NONSLEEP | PF_FORKED);
+		p->systime = 0;
+		p->prio = effective_prio(p);
+	}
+	p->timestamp = now;
+
+	__activate_task(p, rq);
+}
+
+/*
+ * deactivate_task - remove a task from the runqueue.
+ */
+static void deactivate_task(struct task_struct *p, struct rq *rq)
+{
+	dec_nr_running(p, rq);
+	dequeue_task(p, rq);
+}
+
+/*
+ * resched_task - mark a task 'to be rescheduled now'.
+ *
+ * On UP this means the setting of the need_resched flag, on SMP it
+ * might also involve a cross-CPU call to trigger the scheduler on
+ * the target CPU.
+ */
+
+#ifndef tsk_is_polling
+#define tsk_is_polling(t) test_tsk_thread_flag(t, TIF_POLLING_NRFLAG)
+#endif
+
+#ifdef CONFIG_SMP
+static void resched_task(struct task_struct *p)
+{
+	int cpu;
+
+	assert_spin_locked(&task_rq(p)->lock);
+
+	if (unlikely(test_tsk_thread_flag(p, TIF_NEED_RESCHED)))
+		return;
+
+	set_tsk_thread_flag(p, TIF_NEED_RESCHED);
+
+	cpu = task_cpu(p);
+	if (cpu == smp_processor_id())
+		return;
+
+	/* NEED_RESCHED must be visible before we test polling */
+	smp_mb();
+        if (!tsk_is_polling(p))
+		smp_send_reschedule(cpu);
+}
+#else
+static inline void resched_task(struct task_struct *p)
+{
+	assert_spin_locked(&task_rq(p)->lock);
+	set_tsk_need_resched(p);
+}
+#endif
+
+/**
+ * task_curr - is this task currently executing on a CPU?
+ * @p: the task in question.
+ */
+inline int task_curr(const struct task_struct *p)
+{
+	return cpu_curr(task_cpu(p)) == p;
+}
+
+/* Used instead of source_load when we know the type == 0 */
+unsigned long weighted_cpuload(const int cpu)
+{
+	return cpu_rq(cpu)->raw_weighted_load;
+}
+
+#ifdef CONFIG_SMP
+struct migration_req {
+	struct list_head list;
+
+	struct task_struct *task;
+	int dest_cpu;
+
+	struct completion done;
+};
+
+/*
+ * The task's runqueue lock must be held.
+ * Returns true if you have to wait for migration thread.
+ */
+static int migrate_task(struct task_struct *p, int dest_cpu, struct migration_req *req)
+{
+	struct rq *rq = task_rq(p);
+
+	/*
+	 * If the task is not on a runqueue (and not running), then
+	 * it is sufficient to simply update the task's cpu field.
+	 */
+	if (!task_queued(p) && !task_running(rq, p)) {
+		set_task_cpu(p, dest_cpu);
+		return 0;
+	}
+
+	init_completion(&req->done);
+	req->task = p;
+	req->dest_cpu = dest_cpu;
+	list_add(&req->list, &rq->migration_queue);
+
+	return 1;
+}
+
+/*
+ * wait_task_inactive - wait for a thread to unschedule.
+ *
+ * The caller must ensure that the task *will* unschedule sometime soon,
+ * else this function might spin for a *long* time. This function can't
+ * be called with interrupts off, or it may introduce deadlock with
+ * smp_call_function() if an IPI is sent by the same process we are
+ * waiting to become inactive.
+ */
+void wait_task_inactive(struct task_struct *p)
+{
+	unsigned long flags;
+	struct rq *rq;
+	int preempted;
+
+repeat:
+	rq = task_rq_lock(p, &flags);
+	/* Must be off runqueue entirely, not preempted. */
+	if (unlikely(task_queued(p) || task_running(rq, p))) {
+		/* If it's preempted, we yield.  It could be a while. */
+		preempted = !task_running(rq, p);
+		task_rq_unlock(rq, &flags);
+		cpu_relax();
+		if (preempted)
+			yield();
+		goto repeat;
+	}
+	task_rq_unlock(rq, &flags);
+}
+
+/***
+ * kick_process - kick a running thread to enter/exit the kernel
+ * @p: the to-be-kicked thread
+ *
+ * Cause a process which is running on another CPU to enter
+ * kernel-mode, without any delay. (to get signals handled.)
+ *
+ * NOTE: this function doesnt have to take the runqueue lock,
+ * because all it wants to ensure is that the remote task enters
+ * the kernel. If the IPI races and the task has been migrated
+ * to another CPU then no harm is done and the purpose has been
+ * achieved as well.
+ */
+void kick_process(struct task_struct *p)
+{
+	int cpu;
+
+	preempt_disable();
+	cpu = task_cpu(p);
+	if ((cpu != smp_processor_id()) && task_curr(p))
+		smp_send_reschedule(cpu);
+	preempt_enable();
+}
+
+/*
+ * Return a low guess at the load of a migration-source cpu weighted
+ * according to the scheduling class and "nice" value.
+ *
+ * We want to under-estimate the load of migration sources, to
+ * balance conservatively.
+ */
+static inline unsigned long source_load(int cpu, int type)
+{
+	struct rq *rq = cpu_rq(cpu);
+
+	if (type == 0)
+		return rq->raw_weighted_load;
+
+	return min(rq->cpu_load[type-1], rq->raw_weighted_load);
+}
+
+/*
+ * Return a high guess at the load of a migration-target cpu weighted
+ * according to the scheduling class and "nice" value.
+ */
+static inline unsigned long target_load(int cpu, int type)
+{
+	struct rq *rq = cpu_rq(cpu);
+
+	if (type == 0)
+		return rq->raw_weighted_load;
+
+	return max(rq->cpu_load[type-1], rq->raw_weighted_load);
+}
+
+/*
+ * Return the average load per task on the cpu's run queue
+ */
+static inline unsigned long cpu_avg_load_per_task(int cpu)
+{
+	struct rq *rq = cpu_rq(cpu);
+	unsigned long n = rq->nr_running;
+
+	return n ? rq->raw_weighted_load / n : SCHED_LOAD_SCALE;
+}
+
+/*
+ * find_idlest_group finds and returns the least busy CPU group within the
+ * domain.
+ */
+static struct sched_group *
+find_idlest_group(struct sched_domain *sd, struct task_struct *p, int this_cpu)
+{
+	struct sched_group *idlest = NULL, *this = NULL, *group = sd->groups;
+	unsigned long min_load = ULONG_MAX, this_load = 0;
+	int load_idx = sd->forkexec_idx;
+	int imbalance = 100 + (sd->imbalance_pct-100)/2;
+
+	do {
+		unsigned long load, avg_load;
+		int local_group;
+		int i;
+
+		/* Skip over this group if it has no CPUs allowed */
+		if (!cpus_intersects(group->cpumask, p->cpus_allowed))
+			goto nextgroup;
+
+		local_group = cpu_isset(this_cpu, group->cpumask);
+
+		/* Tally up the load of all CPUs in the group */
+		avg_load = 0;
+
+		for_each_cpu_mask(i, group->cpumask) {
+			/* Bias balancing toward cpus of our domain */
+			if (local_group)
+				load = source_load(i, load_idx);
+			else
+				load = target_load(i, load_idx);
+
+			avg_load += load;
+		}
+
+		/* Adjust by relative CPU power of the group */
+		avg_load = (avg_load * SCHED_LOAD_SCALE) / group->cpu_power;
+
+		if (local_group) {
+			this_load = avg_load;
+			this = group;
+		} else if (avg_load < min_load) {
+			min_load = avg_load;
+			idlest = group;
+		}
+nextgroup:
+		group = group->next;
+	} while (group != sd->groups);
+
+	if (!idlest || 100*this_load < imbalance*min_load)
+		return NULL;
+	return idlest;
+}
+
+/*
+ * find_idlest_queue - find the idlest runqueue among the cpus in group.
+ */
+static int
+find_idlest_cpu(struct sched_group *group, struct task_struct *p, int this_cpu)
+{
+	cpumask_t tmp;
+	unsigned long load, min_load = ULONG_MAX;
+	int idlest = -1;
+	int i;
+
+	/* Traverse only the allowed CPUs */
+	cpus_and(tmp, group->cpumask, p->cpus_allowed);
+
+	for_each_cpu_mask(i, tmp) {
+		load = weighted_cpuload(i);
+
+		if (load < min_load || (load == min_load && i == this_cpu)) {
+			min_load = load;
+			idlest = i;
+		}
+	}
+
+	return idlest;
+}
+
+/*
+ * sched_balance_self: balance the current task (running on cpu) in domains
+ * that have the 'flag' flag set. In practice, this is SD_BALANCE_FORK and
+ * SD_BALANCE_EXEC.
+ *
+ * Balance, ie. select the least loaded group.
+ *
+ * Returns the target CPU number, or the same CPU if no balancing is needed.
+ *
+ * preempt must be disabled.
+ */
+static int sched_balance_self(int cpu, int flag)
+{
+	struct task_struct *t = current;
+	struct sched_domain *tmp, *sd = NULL;
+
+	for_each_domain(cpu, tmp) {
+		if (tmp->flags & flag)
+			sd = tmp;
+        }
+
+	while (sd) {
+		cpumask_t span;
+		struct sched_group *group;
+		int new_cpu;
+		int weight;
+
+		span = sd->span;
+		group = find_idlest_group(sd, t, cpu);
+		if (!group)
+			goto nextlevel;
+
+		new_cpu = find_idlest_cpu(group, t, cpu);
+		if (new_cpu == -1 || new_cpu == cpu)
+			goto nextlevel;
+
+		/* Now try balancing at a lower domain level */
+		cpu = new_cpu;
+nextlevel:
+		sd = NULL;
+		weight = cpus_weight(span);
+		for_each_domain(cpu, tmp) {
+			if (weight <= cpus_weight(tmp->span))
+				break;
+			if (tmp->flags & flag)
+				sd = tmp;
+		}
+		/* while loop will break here if sd == NULL */
+	}
+
+	return cpu;
+}
+
+#endif /* CONFIG_SMP */
+
+/*
+ * wake_idle() will wake a task on an idle cpu if task->cpu is
+ * not idle and an idle cpu is available.  The span of cpus to
+ * search starts with cpus closest then further out as needed,
+ * so we always favor a closer, idle cpu.
+ *
+ * Returns the CPU we should wake onto.
+ */
+#if defined(ARCH_HAS_SCHED_WAKE_IDLE)
+static int wake_idle(int cpu, struct task_struct *p)
+{
+	cpumask_t tmp;
+	struct sched_domain *sd;
+	int i;
+
+	if (idle_cpu(cpu))
+		return cpu;
+
+	for_each_domain(cpu, sd) {
+		if (sd->flags & SD_WAKE_IDLE) {
+			cpus_and(tmp, sd->span, p->cpus_allowed);
+			for_each_cpu_mask(i, tmp) {
+				if (idle_cpu(i))
+					return i;
+			}
+		}
+		else
+			break;
+	}
+	return cpu;
+}
+#else
+static inline int wake_idle(int cpu, struct task_struct *p)
+{
+	return cpu;
+}
+#endif
+
+/*
+ * Check to see if p preempts rq->curr and resched if it does. In compute
+ * mode we do not preempt for at least CACHE_DELAY and set rq->preempted.
+ */
+static void fastcall preempt(const struct task_struct *p, struct rq *rq)
+{
+        struct task_struct *curr = rq->curr;
+ 
+        if (p->prio >= curr->prio)
+                return;
+        if (!sched_compute || rq->cache_ticks >= CACHE_DELAY || !p->mm ||
+            rt_task(p) || curr == rq->idle) {
+                resched_task(curr);
+                return;
+        }
+        rq->preempted = 1;
+}
+
+/***
+ * try_to_wake_up - wake up a thread
+ * @p: the to-be-woken-up thread
+ * @state: the mask of task states that can be woken
+ * @sync: do a synchronous wakeup?
+ *
+ * Put it on the run-queue if it's not already there. The "current"
+ * thread is always on the run-queue (except when the actual
+ * re-schedule is in progress), and as such you're allowed to do
+ * the simpler "current->state = TASK_RUNNING" to mark yourself
+ * runnable without the overhead of this.
+ *
+ * returns failure only if the task is already active.
+ */
+static int try_to_wake_up(struct task_struct *p, unsigned int state, int sync)
+{
+	int cpu, this_cpu, success = 0;
+	unsigned long flags;
+	long old_state;
+	struct rq *rq;
+#ifdef CONFIG_SMP
+	struct sched_domain *sd, *this_sd = NULL;
+        unsigned long load, this_load;
+	int new_cpu;
+#endif
+
+	rq = task_rq_lock(p, &flags);
+	old_state = p->state;
+	if (!(old_state & state))
+		goto out;
+
+	if (task_queued(p))
+		goto out_running;
+
+	cpu = task_cpu(p);
+	this_cpu = smp_processor_id();
+
+#ifdef CONFIG_SMP
+	if (unlikely(task_running(rq, p)))
+		goto out_activate;
+
+	new_cpu = cpu;
+
+	schedstat_inc(rq, ttwu_cnt);
+	if (cpu == this_cpu) {
+		schedstat_inc(rq, ttwu_local);
+		goto out_set_cpu;
+	}
+
+	for_each_domain(this_cpu, sd) {
+		if (cpu_isset(cpu, sd->span)) {
+			schedstat_inc(sd, ttwu_wake_remote);
+			this_sd = sd;
+			break;
+		}
+	}
+
+	if (unlikely(!cpu_isset(this_cpu, p->cpus_allowed)))
+		goto out_set_cpu;
+
+	/*
+	 * Check for affine wakeup and passive balancing possibilities.
+	 */
+	if (this_sd) {
+		int idx = this_sd->wake_idx;
+		unsigned int imbalance;
+
+		imbalance = 100 + (this_sd->imbalance_pct - 100) / 2;
+
+		load = source_load(cpu, idx);
+		this_load = target_load(this_cpu, idx);
+
+		new_cpu = this_cpu; /* Wake to this CPU if we can */
+
+		if (this_sd->flags & SD_WAKE_AFFINE) {
+			unsigned long tl = this_load;
+			unsigned long tl_per_task = cpu_avg_load_per_task(this_cpu);
+
+			/*
+			 * If sync wakeup then subtract the (maximum possible)
+			 * effect of the currently running task from the load
+			 * of the current CPU:
+			 */
+			if (sync)
+				tl -= current->load_weight;
+
+			if ((tl <= load &&
+				tl + target_load(cpu, idx) <= tl_per_task) ||
+				100*(tl + p->load_weight) <= imbalance*load) {
+				/*
+				 * This domain has SD_WAKE_AFFINE and
+				 * p is cache cold in this domain, and
+				 * there is no bad imbalance.
+				 */
+				schedstat_inc(this_sd, ttwu_move_affine);
+				goto out_set_cpu;
+			}
+		}
+
+		/*
+		 * Start passive balancing when half the imbalance_pct
+		 * limit is reached.
+		 */
+		if (this_sd->flags & SD_WAKE_BALANCE) {
+			if (imbalance*this_load <= 100*load) {
+				schedstat_inc(this_sd, ttwu_move_balance);
+				goto out_set_cpu;
+			}
+		}
+	}
+
+	new_cpu = cpu; /* Could not wake to this_cpu. Wake to cpu instead */
+out_set_cpu:
+	new_cpu = wake_idle(new_cpu, p);
+	if (new_cpu != cpu) {
+		set_task_cpu(p, new_cpu);
+		task_rq_unlock(rq, &flags);
+		/* might preempt at this point */
+		rq = task_rq_lock(p, &flags);
+		old_state = p->state;
+		if (!(old_state & state))
+			goto out;
+		if (task_queued(p))
+			goto out_running;
+
+		this_cpu = smp_processor_id();
+		cpu = task_cpu(p);
+	}
+
+out_activate:
+#endif /* CONFIG_SMP */
+	if (old_state == TASK_UNINTERRUPTIBLE)
+		rq->nr_uninterruptible--;
+
+	activate_task(p, rq, cpu == this_cpu);
+	/*
+	 * Sync wakeups (i.e. those types of wakeups where the waker
+	 * has indicated that it will leave the CPU in short order)
+	 * don't trigger a preemption, if the woken up task will run on
+	 * this cpu. (in this case the 'I will reschedule' promise of
+	 * the waker guarantees that the freshly woken up task is going
+	 * to be considered on this CPU.)
+	 */
+	if (!sync || cpu != this_cpu)
+		preempt(p, rq);
+	success = 1;
+
+out_running:
+	p->state = TASK_RUNNING;
+out:
+	task_rq_unlock(rq, &flags);
+
+	return success;
+}
+
+int fastcall wake_up_process(struct task_struct *p)
+{
+	return try_to_wake_up(p, TASK_STOPPED | TASK_TRACED |
+				 TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE, 0);
+}
+EXPORT_SYMBOL(wake_up_process);
+
+int fastcall wake_up_state(struct task_struct *p, unsigned int state)
+{
+	return try_to_wake_up(p, state, 0);
+}
+
+/*
+ * Perform scheduler related setup for a newly forked process p.
+ * p is forked by current.
+ */
+void fastcall sched_fork(struct task_struct *p, int clone_flags)
+{
+	int cpu = get_cpu();
+
+#ifdef CONFIG_SMP
+	cpu = sched_balance_self(cpu, SD_BALANCE_FORK);
+#endif
+	set_task_cpu(p, cpu);
+
+	/*
+	 * We mark the process as running here, but have not actually
+	 * inserted it onto the runqueue yet. This guarantees that
+	 * nobody will actually run it, and a signal or other external
+	 * event cannot wake it up and insert it on the runqueue either.
+	 */
+	p->state = TASK_RUNNING;
+
+	/*
+	 * Make sure we do not leak PI boosting priority to the child:
+	 */
+	p->prio = current->normal_prio;
+
+	INIT_LIST_HEAD(&p->run_list);
+#if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
+	if (unlikely(sched_info_on()))
+		memset(&p->sched_info, 0, sizeof(p->sched_info));
+#endif
+#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)
+	p->oncpu = 0;
+#endif
+#ifdef CONFIG_PREEMPT
+	/* Want to start with kernel preemption disabled. */
+	task_thread_info(p)->preempt_count = 1;
+#endif
+	put_cpu();
+}
+
+/*
+ * wake_up_new_task - wake up a newly created task for the first time.
+ *
+ * This function will do some initial scheduler statistics housekeeping
+ * that must be done for every newly created context, then puts the task
+ * on the runqueue and wakes it.
+ */
+void fastcall wake_up_new_task(struct task_struct *p, unsigned long clone_flags)
+{
+	unsigned long flags;
+	int this_cpu, cpu;
+	struct rq *rq, *this_rq;
+
+	rq = task_rq_lock(p, &flags);
+	BUG_ON(p->state != TASK_RUNNING);
+	this_cpu = smp_processor_id();
+	cpu = task_cpu(p);
+
+	/* Forked process gets no bonus to prevent fork bombs. */
+	p->bonus = 0;
+	current->flags |= PF_FORKED;
+
+	if (likely(cpu == this_cpu)) {
+		activate_task(p, rq, 1);
+		if (!(clone_flags & CLONE_VM)) {
+			/*
+			 * The VM isn't cloned, so we're in a good position to
+			 * do child-runs-first in anticipation of an exec. This
+			 * usually avoids a lot of COW overhead.
+			 */
+			set_need_resched();
+		}
+		/*
+		 * We skip the following code due to cpu == this_cpu
+	 	 *
+		 *   task_rq_unlock(rq, &flags);
+		 *   this_rq = task_rq_lock(current, &flags);
+		 */
+		this_rq = rq;
+	} else {
+		this_rq = cpu_rq(this_cpu);
+
+		/*
+		 * Not the local CPU - must adjust timestamp. This should
+		 * get optimised away in the !CONFIG_SMP case.
+		 */
+		p->timestamp = (p->timestamp - this_rq->timestamp_last_tick)
+					+ rq->timestamp_last_tick;
+		activate_task(p, rq, 0);
+		preempt(p, rq);
+
+		/*
+		 * Parent and child are on different CPUs, now get the
+		 * parent runqueue to update the parent's ->flags:
+		 */
+		task_rq_unlock(rq, &flags);
+		this_rq = task_rq_lock(current, &flags);
+	}
+	task_rq_unlock(this_rq, &flags);
+}
+
+/**
+ * prepare_task_switch - prepare to switch tasks
+ * @rq: the runqueue preparing to switch
+ * @next: the task we are going to switch to.
+ *
+ * This is called with the rq lock held and interrupts off. It must
+ * be paired with a subsequent finish_task_switch after the context
+ * switch.
+ *
+ * prepare_task_switch sets up locking and calls architecture specific
+ * hooks.
+ */
+static inline void prepare_task_switch(struct rq *rq, struct task_struct *next)
+{
+	prepare_lock_switch(rq, next);
+	prepare_arch_switch(next);
+}
+
+/**
+ * finish_task_switch - clean up after a task-switch
+ * @rq: runqueue associated with task-switch
+ * @prev: the thread we just switched away from.
+ *
+ * finish_task_switch must be called after the context switch, paired
+ * with a prepare_task_switch call before the context switch.
+ * finish_task_switch will reconcile locking set up by prepare_task_switch,
+ * and do any other architecture-specific cleanup actions.
+ *
+ * Note that we may have delayed dropping an mm in context_switch(). If
+ * so, we finish that here outside of the runqueue lock.  (Doing it
+ * with the lock held can cause deadlocks; see schedule() for
+ * details.)
+ */
+static inline void finish_task_switch(struct rq *rq, struct task_struct *prev)
+	__releases(rq->lock)
+{
+	struct mm_struct *mm = rq->prev_mm;
+	unsigned long prev_task_flags;
+
+	rq->prev_mm = NULL;
+
+	/*
+	 * A task struct has one reference for the use as "current".
+	 * If a task dies, then it sets EXIT_ZOMBIE in tsk->exit_state and
+	 * calls schedule one last time. The schedule call will never return,
+	 * and the scheduled task must drop that reference.
+	 * The test for EXIT_ZOMBIE must occur while the runqueue locks are
+	 * still held, otherwise prev could be scheduled on another cpu, die
+	 * there before we look at prev->state, and then the reference would
+	 * be dropped twice.
+	 *		Manfred Spraul <manfred@colorfullife.com>
+	 */
+	prev_task_flags = prev->flags;
+	finish_arch_switch(prev);
+	finish_lock_switch(rq, prev);
+	if (mm)
+		mmdrop(mm);
+	if (unlikely(prev_task_flags & PF_DEAD)) {
+		/*
+		 * Remove function-return probe instances associated with this
+		 * task and put them back on the free list.
+	 	 */
+		kprobe_flush_task(prev);
+		put_task_struct(prev);
+	}
+}
+
+/**
+ * schedule_tail - first thing a freshly forked thread must call.
+ * @prev: the thread we just switched away from.
+ */
+asmlinkage void schedule_tail(struct task_struct *prev)
+	__releases(rq->lock)
+{
+	struct rq *rq = this_rq();
+
+	finish_task_switch(rq, prev);
+#ifdef __ARCH_WANT_UNLOCKED_CTXSW
+	/* In this case, finish_task_switch does not reenable preemption */
+	preempt_enable();
+#endif
+	if (current->set_child_tid)
+		put_user(current->pid, current->set_child_tid);
+}
+
+/*
+ * context_switch - switch to the new MM and the new
+ * thread's register state.
+ */
+static inline struct task_struct * 
+context_switch(struct rq *rq, struct task_struct *prev,
+              struct task_struct *next)
+{
+	struct mm_struct *mm = next->mm;
+	struct mm_struct *oldmm = prev->active_mm;
+
+	if (unlikely(!mm)) {
+		next->active_mm = oldmm;
+		atomic_inc(&oldmm->mm_count);
+		enter_lazy_tlb(oldmm, next);
+	} else
+		switch_mm(oldmm, mm, next);
+
+	if (unlikely(!prev->mm)) {
+		prev->active_mm = NULL;
+		WARN_ON(rq->prev_mm);
+		rq->prev_mm = oldmm;
+	}
+        spin_release(&rq->lock.dep_map, 1, _THIS_IP_);
+
+	/* Here we just switch the register state and the stack. */
+	switch_to(prev, next, prev);
+
+	return prev;
+}
+
+/*
+ * nr_running, nr_uninterruptible and nr_context_switches:
+ *
+ * externally visible scheduler statistics: current number of runnable
+ * threads, current number of uninterruptible-sleeping threads, total
+ * number of context switches performed since bootup.
+ */
+unsigned long nr_running(void)
+{
+	unsigned long i, sum = 0;
+
+	for_each_online_cpu(i)
+		sum += cpu_rq(i)->nr_running;
+
+	return sum;
+}
+
+unsigned long nr_uninterruptible(void)
+{
+	unsigned long i, sum = 0;
+
+	for_each_possible_cpu(i)
+		sum += cpu_rq(i)->nr_uninterruptible;
+
+	/*
+	 * Since we read the counters lockless, it might be slightly
+	 * inaccurate. Do not allow it to go below zero though:
+	 */
+	if (unlikely((long)sum < 0))
+		sum = 0;
+
+	return sum;
+}
+
+unsigned long long nr_context_switches(void)
+{
+	int i;
+	unsigned long long sum = 0;
+
+	for_each_possible_cpu(i)
+		sum += cpu_rq(i)->nr_switches;
+
+	return sum;
+}
+
+unsigned long nr_iowait(void)
+{
+	unsigned long i, sum = 0;
+
+	for_each_possible_cpu(i)
+		sum += atomic_read(&cpu_rq(i)->nr_iowait);
+
+	return sum;
+}
+
+unsigned long nr_active(void)
+{
+	unsigned long i, running = 0, uninterruptible = 0;
+
+	for_each_online_cpu(i) {
+		running += cpu_rq(i)->nr_running;
+		uninterruptible += cpu_rq(i)->nr_uninterruptible;
+	}
+
+	if (unlikely((long)uninterruptible < 0))
+		uninterruptible = 0;
+
+	return running + uninterruptible;
+}
+
+#ifdef CONFIG_SMP
+
+
+/*
+ * Is this task likely cache-hot:
+ */
+static inline int
+task_hot(struct task_struct *p, unsigned long long now, struct sched_domain *sd)
+{
+        return (long long)(now - p->timestamp) < (long long)sd->cache_hot_time;
+}
+
+/*
+ * double_rq_lock - safely lock two runqueues
+ *
+ * We must take them in cpu order to match code in
+ * dependent_sleeper and wake_dependent_sleeper.
+ *
+ * Note this does not disable interrupts like task_rq_lock,
+ * you need to do so manually before calling.
+ */
+static void double_rq_lock(struct rq *rq1, struct rq *rq2)
+	__acquires(rq1->lock)
+	__acquires(rq2->lock)
+{
+	if (rq1 == rq2) {
+		spin_lock(&rq1->lock);
+		__acquire(rq2->lock);	/* Fake it out ;) */
+	} else {
+                if (rq1 < rq2) {
+			spin_lock(&rq1->lock);
+			spin_lock(&rq2->lock);
+		} else {
+			spin_lock(&rq2->lock);
+			spin_lock(&rq1->lock);
+		}
+	}
+}
+
+/*
+ * double_rq_unlock - safely unlock two runqueues
+ *
+ * Note this does not restore interrupts like task_rq_unlock,
+ * you need to do so manually after calling.
+ */
+static void double_rq_unlock(struct rq *rq1, struct rq *rq2)
+	__releases(rq1->lock)
+	__releases(rq2->lock)
+{
+	spin_unlock(&rq1->lock);
+	if (rq1 != rq2)
+		spin_unlock(&rq2->lock);
+	else
+		__release(rq2->lock);
+}
+
+/*
+ * double_lock_balance - lock the busiest runqueue, this_rq is locked already.
+ */
+static void double_lock_balance(struct rq *this_rq, struct rq *busiest)
+	__releases(this_rq->lock)
+	__acquires(busiest->lock)
+	__acquires(this_rq->lock)
+{
+	if (unlikely(!spin_trylock(&busiest->lock))) {
+                if (busiest < this_rq) {
+			spin_unlock(&this_rq->lock);
+			spin_lock(&busiest->lock);
+			spin_lock(&this_rq->lock);
+		} else
+			spin_lock(&busiest->lock);
+	}
+}
+
+/*
+ * If dest_cpu is allowed for this process, migrate the task to it.
+ * This is accomplished by forcing the cpu_allowed mask to only
+ * allow dest_cpu, which will force the cpu onto dest_cpu.  Then
+ * the cpu_allowed mask is restored.
+ */
+static void sched_migrate_task(struct task_struct *p, int dest_cpu)
+{
+	struct migration_req req;
+	unsigned long flags;
+        struct rq *rq;
+
+	rq = task_rq_lock(p, &flags);
+	if (!cpu_isset(dest_cpu, p->cpus_allowed)
+	    || unlikely(cpu_is_offline(dest_cpu)))
+		goto out;
+
+	/* force the process onto the specified CPU */
+	if (migrate_task(p, dest_cpu, &req)) {
+		/* Need to wait for migration thread (might exit: take ref). */
+		struct task_struct *mt = rq->migration_thread;
+
+		get_task_struct(mt);
+		task_rq_unlock(rq, &flags);
+		wake_up_process(mt);
+		put_task_struct(mt);
+		wait_for_completion(&req.done);
+
+		return;
+	}
+out:
+	task_rq_unlock(rq, &flags);
+}
+
+/*
+ * sched_exec - execve() is a valuable balancing opportunity, because at
+ * this point the task has the smallest effective memory and cache footprint.
+ */
+void sched_exec(void)
+{
+	int new_cpu, this_cpu = get_cpu();
+	new_cpu = sched_balance_self(this_cpu, SD_BALANCE_EXEC);
+	put_cpu();
+	if (new_cpu != this_cpu)
+		sched_migrate_task(current, new_cpu);
+}
+
+/*
+ * pull_task - move a task from a remote runqueue to the local runqueue.
+ * Both runqueues must be locked.
+ */
+static void 
+pull_task(struct rq *src_rq, struct task_struct *p, struct rq *this_rq,int this_cpu)
+{
+	dequeue_task(p, src_rq);
+	dec_nr_running(p, src_rq);
+	set_task_cpu(p, this_cpu);
+	inc_nr_running(p, this_rq);
+	enqueue_task(p, this_rq);
+	p->timestamp = (p->timestamp - src_rq->timestamp_last_tick)
+				+ this_rq->timestamp_last_tick;
+	/*
+	 * Note that idle threads have a prio of MAX_PRIO, for this test
+	 * to be always true for them.
+	 */
+	preempt(p, this_rq);
+}
+
+/*
+ * can_migrate_task - may task p from runqueue rq be migrated to this_cpu?
+ */
+static
+int can_migrate_task(struct task_struct *p, struct rq *rq, int this_cpu,
+		     struct sched_domain *sd, enum idle_type idle,
+		     int *all_pinned)
+{
+	/*
+	 * We do not migrate tasks that are:
+	 * 1) running (obviously), or
+	 * 2) cannot be migrated to this CPU due to cpus_allowed, or
+	 * 3) are cache-hot on their current CPU.
+	 */
+	if (!cpu_isset(this_cpu, p->cpus_allowed))
+		return 0;
+	*all_pinned = 0;
+
+	if (task_running(rq, p))
+		return 0;
+
+	/*
+	 * Aggressive migration if:
+	 * 1) task is cache cold, or
+	 * 2) too many balance attempts have failed.
+	 */
+
+	if (sd->nr_balance_failed > sd->cache_nice_tries)
+		return 1;
+
+	if (task_hot(p, rq->timestamp_last_tick, sd))
+		return 0;
+	return 1;
+}
+
+#define rq_best_prio(rq) min((rq)->curr->prio, (rq)->best_expired_prio)
+
+/*
+ * move_tasks tries to move up to max_nr_move tasks and max_load_move weighted
+ * load from busiest to this_rq, as part of a balancing operation within
+ * "domain". Returns the number of tasks moved.
+ *
+ * Called with both runqueues locked.
+ */
+static int move_tasks(struct rq *this_rq, int this_cpu, struct rq *busiest,
+                      unsigned long max_nr_move, unsigned long max_load_move,
+                      struct sched_domain *sd, enum idle_type idle,
+                      int *all_pinned)
+{
+        struct list_head *head, *curr;
+        int idx, pulled = 0, pinned = 0, this_min_prio;
+        struct task_struct *tmp;
+        long rem_load_move;
+
+        if (max_nr_move == 0 || max_load_move == 0)
+                goto out;
+
+        rem_load_move = max_load_move;
+        pinned = 1;
+        this_min_prio = this_rq->curr->prio;
+
+        /* Start searching at priority 0: */
+        idx = 0;
+skip_bitmap:
+        if (!idx)
+                idx = sched_find_first_bit(busiest->bitmap);
+        else
+                idx = find_next_bit(busiest->bitmap, MAX_PRIO, idx);
+        if (idx >= MAX_PRIO)
+                goto out;
+
+        head = busiest->queue + idx;
+        curr = head->prev;
+skip_queue:
+        tmp = list_entry(curr, struct task_struct, run_list);
+
+        curr = curr->prev;
+
+        /*
+         * To help distribute high priority tasks accross CPUs we don't
+         * skip a task if it will be the highest priority task (i.e. smallest
+         * prio value) on its new queue regardless of its load weight
+         */
+        if ((idx >= this_min_prio && tmp->load_weight > rem_load_move) ||
+            !can_migrate_task(tmp, busiest, this_cpu, sd, idle, &pinned)) {
+                if (curr != head)
+                        goto skip_queue;
+                idx++;
+                goto skip_bitmap;
+        }
+
+#ifdef CONFIG_SCHEDSTATS
+        if (task_hot(tmp, busiest->timestamp_last_tick, sd))
+                schedstat_inc(sd, lb_hot_gained[idle]);
+#endif
+
+        pull_task(busiest, tmp, this_rq, this_cpu);
+        pulled++;
+        rem_load_move -= tmp->load_weight;
+
+        /*
+         * We only want to steal up to the prescribed number of tasks
+         * and the prescribed amount of weighted load.
+         */
+        if (pulled < max_nr_move && rem_load_move > 0) {
+                if (idx < this_min_prio)
+                        this_min_prio = idx;
+                if (curr != head)
+                        goto skip_queue;
+                idx++;
+                goto skip_bitmap;
+        }
+
+#ifdef CONFIG_SCHEDSTATS
+        if (task_hot(tmp, busiest->timestamp_last_tick, sd))
+                schedstat_inc(sd, lb_hot_gained[idle]);
+#endif
+
+        pull_task(busiest, tmp, this_rq, this_cpu);
+        pulled++;
+        rem_load_move -= tmp->load_weight;
+
+        /*
+         * We only want to steal up to the prescribed number of tasks
+         * and the prescribed amount of weighted load.
+         */
+        if (pulled < max_nr_move && rem_load_move > 0) {
+                if (idx < this_min_prio)
+                        this_min_prio = idx;
+                if (curr != head)
+                        goto skip_queue;
+                idx++;
+                goto skip_bitmap;
+        }
+out:
+        /*
+         * Right now, this is the only place pull_task() is called,
+         * so we can safely collect pull_task() stats here rather than
+         * inside pull_task().
+         */
+        schedstat_add(sd, lb_gained[idle], pulled);
+
+        if (all_pinned)
+                *all_pinned = pinned;
+        return pulled;
+}
+
+/*
+ * find_busiest_group finds and returns the busiest CPU group within the
+ * domain. It calculates and returns the amount of weighted load which 
+ * should be moved to restore balance via the imbalance parameter.
+ */
+static struct sched_group *
+find_busiest_group(struct sched_domain *sd, int this_cpu,
+		   unsigned long *imbalance, enum idle_type idle, int *sd_idle)
+{
+	struct sched_group *busiest = NULL, *this = NULL, *group = sd->groups;
+	unsigned long max_load, avg_load, total_load, this_load, total_pwr;
+	unsigned long max_pull;
+	unsigned long busiest_load_per_task, busiest_nr_running;
+	unsigned long this_load_per_task, this_nr_running;
+	int load_idx;
+
+	max_load = this_load = total_load = total_pwr = 0;
+	busiest_load_per_task = busiest_nr_running = 0;
+	this_load_per_task = this_nr_running = 0;
+	if (idle == NOT_IDLE)
+		load_idx = sd->busy_idx;
+	else if (idle == NEWLY_IDLE)
+		load_idx = sd->newidle_idx;
+	else
+		load_idx = sd->idle_idx;
+
+	do {
+		unsigned long load;
+		int local_group;
+		int i;
+		unsigned long sum_nr_running, sum_weighted_load;
+
+		local_group = cpu_isset(this_cpu, group->cpumask);
+
+		/* Tally up the load of all CPUs in the group */
+		sum_weighted_load = sum_nr_running = avg_load = 0;
+
+		for_each_cpu_mask(i, group->cpumask) {
+			struct rq *rq = cpu_rq(i);
+
+			if (*sd_idle && !idle_cpu(i))
+				*sd_idle = 0;
+
+			/* Bias balancing toward cpus of our domain */
+			if (local_group)
+				load = target_load(i, load_idx);
+			else
+				load = source_load(i, load_idx);
+
+			avg_load += load;
+			sum_nr_running += rq->nr_running;
+			sum_weighted_load += rq->raw_weighted_load;
+		}
+
+		total_load += avg_load;
+		total_pwr += group->cpu_power;
+
+		/* Adjust by relative CPU power of the group */
+		avg_load = (avg_load * SCHED_LOAD_SCALE) / group->cpu_power;
+
+		if (local_group) {
+			this_load = avg_load;
+			this = group;
+			this_nr_running = sum_nr_running;
+			this_load_per_task = sum_weighted_load;
+		} else if (avg_load > max_load &&
+			   sum_nr_running > group->cpu_power / SCHED_LOAD_SCALE) {
+			max_load = avg_load;
+			busiest = group;
+			busiest_nr_running = sum_nr_running;
+			busiest_load_per_task = sum_weighted_load;
+		}
+		group = group->next;
+	} while (group != sd->groups);
+
+	if (!busiest || this_load >= max_load || busiest_nr_running == 0)
+		goto out_balanced;
+
+	avg_load = (SCHED_LOAD_SCALE * total_load) / total_pwr;
+
+	if (this_load >= avg_load ||
+			100*max_load <= sd->imbalance_pct*this_load)
+		goto out_balanced;
+
+	busiest_load_per_task /= busiest_nr_running;
+	/*
+	 * We're trying to get all the cpus to the average_load, so we don't
+	 * want to push ourselves above the average load, nor do we wish to
+	 * reduce the max loaded cpu below the average load, as either of these
+	 * actions would just result in more rebalancing later, and ping-pong
+	 * tasks around. Thus we look for the minimum possible imbalance.
+	 * Negative imbalances (*we* are more loaded than anyone else) will
+	 * be counted as no imbalance for these purposes -- we can't fix that
+	 * by pulling tasks to us.  Be careful of negative numbers as they'll
+	 * appear as very large values with unsigned longs.
+	 */
+	if (max_load <= busiest_load_per_task)
+		goto out_balanced;
+
+	/*
+	 * In the presence of smp nice balancing, certain scenarios can have
+	 * max load less than avg load(as we skip the groups at or below
+	 * its cpu_power, while calculating max_load..)
+	 */
+	if (max_load < avg_load) {
+		*imbalance = 0;
+		goto small_imbalance;
+	}
+
+	/* Don't want to pull so many tasks that a group would go idle */
+	max_pull = min(max_load - avg_load, max_load - busiest_load_per_task);
+
+	/* How much load to actually move to equalise the imbalance */
+	*imbalance = min(max_pull * busiest->cpu_power,
+				(avg_load - this_load) * this->cpu_power)
+			/ SCHED_LOAD_SCALE;
+
+	/*
+	 * if *imbalance is less than the average load per runnable task
+	 * there is no gaurantee that any tasks will be moved so we'll have
+	 * a think about bumping its value to force at least one task to be
+	 * moved
+	 */
+	if (*imbalance < busiest_load_per_task) {
+		unsigned long tmp, pwr_now, pwr_move;
+		unsigned int imbn;
+
+small_imbalance:
+		pwr_move = pwr_now = 0;
+		imbn = 2;
+		if (this_nr_running) {
+			this_load_per_task /= this_nr_running;
+			if (busiest_load_per_task > this_load_per_task)
+				imbn = 1;
+		} else
+			this_load_per_task = SCHED_LOAD_SCALE;
+
+		if (max_load - this_load >= busiest_load_per_task * imbn) {
+			*imbalance = busiest_load_per_task;
+			return busiest;
+		}
+
+		/*
+		 * OK, we don't have enough imbalance to justify moving tasks,
+		 * however we may be able to increase total CPU power used by
+		 * moving them.
+		 */
+
+		pwr_now += busiest->cpu_power *
+			min(busiest_load_per_task, max_load);
+		pwr_now += this->cpu_power *
+			min(this_load_per_task, this_load);
+		pwr_now /= SCHED_LOAD_SCALE;
+
+		/* Amount of load we'd subtract */
+		tmp = busiest_load_per_task*SCHED_LOAD_SCALE/busiest->cpu_power;
+		if (max_load > tmp)
+			pwr_move += busiest->cpu_power *
+				min(busiest_load_per_task, max_load - tmp);
+
+		/* Amount of load we'd add */
+		if (max_load*busiest->cpu_power <
+				busiest_load_per_task*SCHED_LOAD_SCALE)
+			tmp = max_load*busiest->cpu_power/this->cpu_power;
+		else
+			tmp = busiest_load_per_task*SCHED_LOAD_SCALE/this->cpu_power;
+		pwr_move += this->cpu_power*min(this_load_per_task, this_load + tmp);
+		pwr_move /= SCHED_LOAD_SCALE;
+
+		/* Move if we gain throughput */
+		if (pwr_move <= pwr_now)
+			goto out_balanced;
+
+		*imbalance = busiest_load_per_task;
+	}
+
+	return busiest;
+
+out_balanced:
+
+	*imbalance = 0;
+	return NULL;
+}
+
+/*
+ * find_busiest_queue - find the busiest runqueue among the cpus in group.
+ */
+static struct rq *
+find_busiest_queue(struct sched_group *group, enum idle_type idle, 
+                  unsigned long imbalance)
+{
+        struct rq *busiest = NULL, *rq;
+	unsigned long max_load = 0;
+	int i;
+
+	for_each_cpu_mask(i, group->cpumask) {
+		rq = cpu_rq(i);
+
+		if (rq->nr_running == 1 && rq->raw_weighted_load > imbalance)
+			continue;
+
+		if (rq->raw_weighted_load > max_load) {
+			max_load = rq->raw_weighted_load;
+			busiest = rq;
+		}
+	}
+
+	return busiest;
+}
+
+/*
+ * Max backoff if we encounter pinned tasks. Pretty arbitrary value, but
+ * so long as it is large enough.
+ */
+#define MAX_PINNED_INTERVAL	512
+
+static inline unsigned long minus_1_or_zero(unsigned long n)
+{
+        return n > 0 ? n - 1 : 0;
+}
+
+/*
+ * Check this_cpu to ensure it is balanced within domain. Attempt to move
+ * tasks if there is an imbalance.
+ *
+ * Called with this_rq unlocked.
+ */
+static int load_balance(int this_cpu, struct rq *this_rq,
+			struct sched_domain *sd, enum idle_type idle)
+{
+        int nr_moved, all_pinned = 0, active_balance = 0, sd_idle = 0;
+	struct sched_group *group;
+	unsigned long imbalance;
+        struct rq *busiest;
+
+	if (idle != NOT_IDLE && sd->flags & SD_SHARE_CPUPOWER)
+		sd_idle = 1;
+
+	schedstat_inc(sd, lb_cnt[idle]);
+
+	group = find_busiest_group(sd, this_cpu, &imbalance, idle, &sd_idle);
+	if (!group) {
+		schedstat_inc(sd, lb_nobusyg[idle]);
+		goto out_balanced;
+	}
+
+	busiest = find_busiest_queue(group, idle, imbalance);
+	if (!busiest) {
+		schedstat_inc(sd, lb_nobusyq[idle]);
+		goto out_balanced;
+	}
+
+	BUG_ON(busiest == this_rq);
+
+	schedstat_add(sd, lb_imbalance[idle], imbalance);
+
+	nr_moved = 0;
+	if (busiest->nr_running > 1) {
+		/*
+		 * Attempt to move tasks. If find_busiest_group has found
+		 * an imbalance but busiest->nr_running <= 1, the group is
+		 * still unbalanced. nr_moved simply stays zero, so it is
+		 * correctly treated as an imbalance.
+		 */
+		double_rq_lock(this_rq, busiest);
+		nr_moved = move_tasks(this_rq, this_cpu, busiest,
+				      minus_1_or_zero(busiest->nr_running),
+				      imbalance, sd, idle, &all_pinned);
+		double_rq_unlock(this_rq, busiest);
+
+		/* All tasks on this runqueue were pinned by CPU affinity */
+		if (unlikely(all_pinned))
+			goto out_balanced;
+	}
+
+	if (!nr_moved) {
+		schedstat_inc(sd, lb_failed[idle]);
+		sd->nr_balance_failed++;
+
+		if (unlikely(sd->nr_balance_failed > sd->cache_nice_tries+2)) {
+
+			spin_lock(&busiest->lock);
+
+			/* don't kick the migration_thread, if the curr
+			 * task on busiest cpu can't be moved to this_cpu
+			 */
+			if (!cpu_isset(this_cpu, busiest->curr->cpus_allowed)) {
+				spin_unlock(&busiest->lock);
+				all_pinned = 1;
+				goto out_one_pinned;
+			}
+
+			if (!busiest->active_balance) {
+				busiest->active_balance = 1;
+				busiest->push_cpu = this_cpu;
+				active_balance = 1;
+			}
+			spin_unlock(&busiest->lock);
+			if (active_balance)
+				wake_up_process(busiest->migration_thread);
+
+			/*
+			 * We've kicked active balancing, reset the failure
+			 * counter.
+			 */
+			sd->nr_balance_failed = sd->cache_nice_tries+1;
+		}
+	} else
+		sd->nr_balance_failed = 0;
+
+	if (likely(!active_balance)) {
+		/* We were unbalanced, so reset the balancing interval */
+		sd->balance_interval = sd->min_interval;
+	} else {
+		/*
+		 * If we've begun active balancing, start to back off. This
+		 * case may not be covered by the all_pinned logic if there
+		 * is only 1 task on the busy runqueue (because we don't call
+		 * move_tasks).
+		 */
+		if (sd->balance_interval < sd->max_interval)
+			sd->balance_interval *= 2;
+	}
+
+	if (!nr_moved && !sd_idle && sd->flags & SD_SHARE_CPUPOWER)
+		return -1;
+	return nr_moved;
+
+out_balanced:
+	schedstat_inc(sd, lb_balanced[idle]);
+
+	sd->nr_balance_failed = 0;
+
+out_one_pinned:
+	/* tune up the balancing interval */
+	if ((all_pinned && sd->balance_interval < MAX_PINNED_INTERVAL) ||
+			(sd->balance_interval < sd->max_interval))
+		sd->balance_interval *= 2;
+
+	if (!sd_idle && sd->flags & SD_SHARE_CPUPOWER)
+		return -1;
+	return 0;
+}
+
+/*
+ * Check this_cpu to ensure it is balanced within domain. Attempt to move
+ * tasks if there is an imbalance.
+ *
+ * Called from schedule when this_rq is about to become idle (NEWLY_IDLE).
+ * this_rq is locked.
+ */
+static int 
+load_balance_newidle(int this_cpu, struct rq *this_rq, struct sched_domain *sd)
+{
+	struct sched_group *group;
+	struct rq *busiest = NULL;
+	unsigned long imbalance;
+	int nr_moved = 0;
+	int sd_idle = 0;
+
+	if (sd->flags & SD_SHARE_CPUPOWER)
+		sd_idle = 1;
+
+	schedstat_inc(sd, lb_cnt[NEWLY_IDLE]);
+	group = find_busiest_group(sd, this_cpu, &imbalance, NEWLY_IDLE, &sd_idle);
+	if (!group) {
+		schedstat_inc(sd, lb_nobusyg[NEWLY_IDLE]);
+		goto out_balanced;
+	}
+
+	busiest = find_busiest_queue(group, NEWLY_IDLE, imbalance);
+	if (!busiest) {
+		schedstat_inc(sd, lb_nobusyq[NEWLY_IDLE]);
+		goto out_balanced;
+	}
+
+	BUG_ON(busiest == this_rq);
+
+	schedstat_add(sd, lb_imbalance[NEWLY_IDLE], imbalance);
+
+	nr_moved = 0;
+	if (busiest->nr_running > 1) {
+		/* Attempt to move tasks */
+		double_lock_balance(this_rq, busiest);
+		nr_moved = move_tasks(this_rq, this_cpu, busiest,
+					minus_1_or_zero(busiest->nr_running),
+					imbalance, sd, NEWLY_IDLE, NULL);
+		spin_unlock(&busiest->lock);
+	}
+
+	if (!nr_moved) {
+		schedstat_inc(sd, lb_failed[NEWLY_IDLE]);
+		if (!sd_idle && sd->flags & SD_SHARE_CPUPOWER)
+			return -1;
+	} else
+		sd->nr_balance_failed = 0;
+
+	return nr_moved;
+
+out_balanced:
+	schedstat_inc(sd, lb_balanced[NEWLY_IDLE]);
+	if (!sd_idle && sd->flags & SD_SHARE_CPUPOWER)
+		return -1;
+	sd->nr_balance_failed = 0;
+
+	return 0;
+}
+
+/*
+ * idle_balance is called by schedule() if this_cpu is about to become
+ * idle. Attempts to pull tasks from other CPUs.
+ */
+static void idle_balance(int this_cpu, struct rq *this_rq)
+{
+	struct sched_domain *sd;
+
+	for_each_domain(this_cpu, sd) {
+		if (sd->flags & SD_BALANCE_NEWIDLE) {
+                        /* If we've pulled tasks over stop searching: */
+                        if (load_balance_newidle(this_cpu, this_rq, sd))
+				break;
+		}
+	}
+}
+
+/*
+ * active_load_balance is run by migration threads. It pushes running tasks
+ * off the busiest CPU onto idle CPUs. It requires at least 1 task to be
+ * running on each physical CPU where possible, and avoids physical /
+ * logical imbalances.
+ *
+ * Called with busiest_rq locked.
+ */
+static void active_load_balance(struct rq *busiest_rq, int busiest_cpu)
+{
+	int target_cpu = busiest_rq->push_cpu;
+        struct sched_domain *sd;
+        struct rq *target_rq;
+
+        /* Is there any task to move? */
+	if (busiest_rq->nr_running <= 1)
+		return;
+
+	target_rq = cpu_rq(target_cpu);
+
+	/*
+	 * This condition is "impossible", if it occurs
+	 * we need to fix it.  Originally reported by
+	 * Bjorn Helgaas on a 128-cpu setup.
+	 */
+	BUG_ON(busiest_rq == target_rq);
+
+	/* move a task from busiest_rq to target_rq */
+	double_lock_balance(busiest_rq, target_rq);
+
+	/* Search for an sd spanning us and the target CPU. */
+	for_each_domain(target_cpu, sd) {
+		if ((sd->flags & SD_LOAD_BALANCE) &&
+		    cpu_isset(busiest_cpu, sd->span))
+				break;
+        }
+
+        if (likely(sd)) {
+                schedstat_inc(sd, alb_cnt);
+
+                if (move_tasks(target_rq, target_cpu, busiest_rq, 1,
+                               RTPRIO_TO_LOAD_WEIGHT(100), sd, SCHED_IDLE,
+                               NULL))
+                        schedstat_inc(sd, alb_pushed);
+                else
+                        schedstat_inc(sd, alb_failed);
+        }
+	spin_unlock(&target_rq->lock);
+}
+
+/*
+ * rebalance_tick will get called every timer tick, on every CPU.
+ *
+ * It checks each scheduling domain to see if it is due to be balanced,
+ * and initiates a balancing operation if so.
+ *
+ * Balancing parameters are set up in arch_init_sched_domains.
+ */
+
+/* Don't have all balancing operations going off at once: */
+static inline unsigned long cpu_offset(int cpu)
+{
+        return jiffies + cpu * HZ / NR_CPUS;
+}
+
+static void 
+rebalance_tick(int this_cpu, struct rq *this_rq, enum idle_type idle)
+{
+        unsigned long this_load, interval, j = cpu_offset(this_cpu);
+	struct sched_domain *sd;
+	int i, scale;
+
+	this_load = this_rq->raw_weighted_load;
+
+        /* Update our load: */
+        for (i = 0, scale = 1; i < 3; i++, scale <<= 1) {
+                unsigned long old_load, new_load;
+
+		old_load = this_rq->cpu_load[i];
+                new_load = this_load;
+		/*
+		 * Round up the averaging division if load is increasing. This
+		 * prevents us from getting stuck on 9 if the load is 10, for
+		 * example.
+		 */
+		if (new_load > old_load)
+			new_load += scale-1;
+		this_rq->cpu_load[i] = (old_load*(scale-1) + new_load) / scale;
+	}
+
+	for_each_domain(this_cpu, sd) {
+		if (!(sd->flags & SD_LOAD_BALANCE))
+			continue;
+
+		interval = sd->balance_interval;
+		if (idle != SCHED_IDLE)
+			interval *= sd->busy_factor;
+
+		/* scale ms to jiffies */
+		interval = msecs_to_jiffies(interval);
+		if (unlikely(!interval))
+			interval = 1;
+
+		if (j - sd->last_balance >= interval) {
+			if (load_balance(this_cpu, this_rq, sd, idle)) {
+				/*
+				 * We've pulled tasks over so either we're no
+				 * longer idle, or one of our SMT siblings is
+				 * not idle.
+				 */
+				idle = NOT_IDLE;
+			}
+			sd->last_balance += interval;
+		}
+	}
+}
+#else
+/*
+ * on UP we do not need to balance between CPUs:
+ */
+static inline void rebalance_tick(int cpu, struct rq *rq, enum idle_type idle)
+{
+}
+static inline void idle_balance(int cpu, struct rq *rq)
+{
+}
+#endif
+
+static inline int wake_priority_sleeper(struct rq *rq)
+{
+	int ret = 0;
+
+#ifdef CONFIG_SCHED_SMT
+	spin_lock(&rq->lock);
+	/*
+	 * If an SMT sibling task has been put to sleep for priority
+	 * reasons reschedule the idle task to see if it can now run.
+	 */
+	if (rq->nr_running) {
+		resched_task(rq->idle);
+		ret = 1;
+	}
+	spin_unlock(&rq->lock);
+#endif
+	return ret;
+}
+
+DEFINE_PER_CPU(struct kernel_stat, kstat);
+
+EXPORT_PER_CPU_SYMBOL(kstat);
+
+/*
+ * This is called on clock ticks and on context switches.
+ * Bank in p->sched_time the ns elapsed since the last tick or switch.
+ */
+static inline void 
+update_cpu_clock(struct task_struct *p, struct rq *rq, unsigned long long now)
+{
+        p->sched_time += now - max(p->timestamp, rq->timestamp_last_tick);
+}
+
+/*
+ * Return current->sched_time plus any more ns on the sched_clock
+ * that have not yet been banked.
+ */
+unsigned long long current_sched_time(const struct task_struct *p)
+{
+	unsigned long long ns;
+	unsigned long flags;
+
+	local_irq_save(flags);
+	ns = max(p->timestamp, task_rq(p)->timestamp_last_tick);
+	ns = p->sched_time + (sched_clock() - ns);
+	local_irq_restore(flags);
+
+	return ns;
+}
+
+/*
+ * Account user cpu time to a process.
+ * @p: the process that the cpu time gets accounted to
+ * @hardirq_offset: the offset to subtract from hardirq_count()
+ * @cputime: the cpu time spent in user space since the last update
+ */
+void account_user_time(struct task_struct *p, cputime_t cputime)
+{
+	struct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;
+	cputime64_t tmp;
+
+	p->utime = cputime_add(p->utime, cputime);
+
+	/* Add user time to cpustat. */
+	tmp = cputime_to_cputime64(cputime);
+	if (TASK_NICE(p) > 0)
+		cpustat->nice = cputime64_add(cpustat->nice, tmp);
+	else
+		cpustat->user = cputime64_add(cpustat->user, tmp);
+}
+
+/*
+ * Account system cpu time to a process.
+ * @p: the process that the cpu time gets accounted to
+ * @hardirq_offset: the offset to subtract from hardirq_count()
+ * @cputime: the cpu time spent in kernel space since the last update
+ */
+void account_system_time(struct task_struct *p, int hardirq_offset,
+			 cputime_t cputime)
+{
+	struct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;
+	struct rq *rq = this_rq();
+	cputime64_t tmp;
+
+	p->stime = cputime_add(p->stime, cputime);
+
+	/* Add system time to cpustat. */
+	tmp = cputime_to_cputime64(cputime);
+	if (hardirq_count() - hardirq_offset)
+		cpustat->irq = cputime64_add(cpustat->irq, tmp);
+	else if (softirq_count())
+		cpustat->softirq = cputime64_add(cpustat->softirq, tmp);
+	else if (p != rq->idle)
+		cpustat->system = cputime64_add(cpustat->system, tmp);
+	else if (atomic_read(&rq->nr_iowait) > 0)
+		cpustat->iowait = cputime64_add(cpustat->iowait, tmp);
+	else
+		cpustat->idle = cputime64_add(cpustat->idle, tmp);
+	/* Account for system time used */
+	p->systime++;
+	acct_update_integrals(p);
+}
+
+/*
+ * Account for involuntary wait time.
+ * @p: the process from which the cpu time has been stolen
+ * @steal: the cpu time spent in involuntary wait
+ */
+void account_steal_time(struct task_struct *p, cputime_t steal)
+{
+	struct cpu_usage_stat *cpustat = &kstat_this_cpu.cpustat;
+	cputime64_t tmp = cputime_to_cputime64(steal);
+	struct rq *rq = this_rq();
+
+	if (p == rq->idle) {
+		p->stime = cputime_add(p->stime, steal);
+		if (atomic_read(&rq->nr_iowait) > 0)
+			cpustat->iowait = cputime64_add(cpustat->iowait, tmp);
+		else
+			cpustat->idle = cputime64_add(cpustat->idle, tmp);
+	} else
+		cpustat->steal = cputime64_add(cpustat->steal, tmp);
+}
+
+static void time_slice_expired(struct task_struct *p, struct rq *rq)
+{
+	set_tsk_need_resched(p);
+	p->time_slice = rr_interval(p);
+	requeue_task(p, rq, effective_prio(p));
+}
+
+/*
+ * This function gets called by the timer code, with HZ frequency.
+ * We call it with interrupts disabled.
+ */
+void scheduler_tick(void)
+{
+	unsigned long debit;
+	unsigned long long now = sched_clock();
+        struct task_struct *p = current;
+        int cpu = smp_processor_id();
+        struct rq *rq = cpu_rq(cpu);
+
+	update_cpu_clock(p, rq, now);
+
+	rq->timestamp_last_tick = now;
+
+	if (p == rq->idle) {
+		if (wake_priority_sleeper(rq))
+			goto out;
+		rebalance_tick(cpu, rq, SCHED_IDLE);
+		return;
+	}
+
+	/* Task might have expired already, but not scheduled off yet */
+	if (unlikely(!task_queued(p))) {
+		set_tsk_need_resched(p);
+		goto out;
+	}
+        /* SCHED_FIFO tasks never run out of timeslice. */
+	if (unlikely(p->policy == SCHED_FIFO))
+		goto out;
+
+	spin_lock(&rq->lock);
+	debit = ns_diff(rq->timestamp_last_tick, p->timestamp);
+	p->ns_debit += debit;
+	if (p->ns_debit < NSJIFFY)
+		goto out_unlock;
+	p->ns_debit %= NSJIFFY;
+
+	/* Tasks lose bonus each time they use up a full slice(). */
+	if (!--p->slice) {
+		dec_bonus(p);
+		p->slice = slice(p);
+		time_slice_expired(p, rq);
+		goto out_unlock;
+	}
+	/*
+	 * Tasks that run out of time_slice but still have slice left get
+	 * requeued with a lower priority && RR_INTERVAL time_slice.
+	 */
+	if (!--p->time_slice) {
+		time_slice_expired(p, rq);
+		goto out_unlock;
+	}
+        rq->cache_ticks++;
+        if (rq->preempted && rq->cache_ticks >= CACHE_DELAY)
+                set_tsk_need_resched(p);
+
+out_unlock:
+	spin_unlock(&rq->lock);
+out:
+	rebalance_tick(cpu, rq, NOT_IDLE);
+}
+
+#ifdef CONFIG_SCHED_SMT
+static inline void wakeup_busy_runqueue(struct rq *rq)
+{
+	/* If an SMT runqueue is sleeping due to priority reasons wake it up */
+	if (rq->curr == rq->idle && rq->nr_running)
+		resched_task(rq->idle);
+}
+
+/*
+ * Called with interrupt disabled and this_rq's runqueue locked.
+ */
+static void wake_sleeping_dependent(int this_cpu)
+{
+	struct sched_domain *tmp, *sd = NULL;
+	int i;
+
+        for_each_domain(this_cpu, tmp) {
+                if (tmp->flags & SD_SHARE_CPUPOWER) {
+			sd = tmp;
+                        break;
+                }
+        }
+
+	if (!sd)
+		return;
+
+        for_each_cpu_mask(i, sd->span) {
+		struct rq *smt_rq = cpu_rq(i);
+
+                if (i == this_cpu)
+                        continue;
+                if (unlikely(!spin_trylock(&smt_rq->lock)))
+                        continue;
+
+		wakeup_busy_runqueue(smt_rq);
+                spin_unlock(&smt_rq->lock);
+	}
+}
+
+/*
+ * number of 'lost' timeslices this task wont be able to fully
+ * utilise, if another task runs on a sibling. This models the
+ * slowdown effect of other tasks running on siblings:
+ */
+static inline unsigned long 
+smt_slice(struct task_struct *p, struct sched_domain *sd)
+{
+	return p->slice * (100 - sd->per_cpu_gain) / 100;
+}
+
+/*
+ * To minimise lock contention and not have to drop this_rq's runlock we only
+ * trylock the sibling runqueues and bypass those runqueues if we fail to
+ * acquire their lock. As we only trylock the normal locking order does not
+ * need to be obeyed.
+ */
+static int 
+dependent_sleeper(int this_cpu, struct rq *this_rq, struct task_struct *p)
+{
+	struct sched_domain *tmp, *sd = NULL;
+	int ret = 0, i;
+
+        /* kernel/rt threads do not participate in dependent sleeping */
+        if (!p->mm || rt_task(p))
+                return 0;
+ 
+        for_each_domain(this_cpu, tmp) {
+                if (tmp->flags & SD_SHARE_CPUPOWER) {
+			sd = tmp;
+                        break;
+                }
+        }
+
+	if (!sd)
+		return 0;
+
+        for_each_cpu_mask(i, sd->span) {
+                struct task_struct *smt_curr;
+                struct rq *smt_rq;
+
+                if (i == this_cpu)
+                        continue;
+
+                smt_rq = cpu_rq(i);
+                if (unlikely(!spin_trylock(&smt_rq->lock)))
+                        continue;
+
+                smt_curr = smt_rq->curr;
+
+                if (!smt_curr->mm)
+                        goto unlock;
+
+		/*
+		 * If a user task with lower static priority than the
+		 * running task on the SMT sibling is trying to schedule,
+		 * delay it till there is proportionately less timeslice
+		 * left of the sibling task to prevent a lower priority
+		 * task from using an unfair proportion of the
+		 * physical cpu's resources. -ck
+		 */
+		if (rt_task(smt_curr)) {
+			/*
+			 * With real time tasks we run non-rt tasks only
+			 * per_cpu_gain% of the time.
+			 */
+			if ((jiffies % DEF_TIMESLICE) >
+				(sd->per_cpu_gain * DEF_TIMESLICE / 100))
+					ret = 1;
+		} else
+			if (smt_curr->static_prio < p->static_prio &&
+				!TASK_PREEMPTS_CURR(p, smt_rq) &&
+				smt_slice(smt_curr, sd) > slice(p))
+					ret = 1;
+
+unlock:
+                spin_unlock(&smt_rq->lock);
+	}
+	return ret;
+}
+#else
+static inline void wake_sleeping_dependent(int this_cpu)
+{
+}
+
+static inline int 
+dependent_sleeper(int this_cpu, struct rq *this_rq, struct task_struct *p)
+{
+	return 0;
+}
+#endif
+
+#if defined(CONFIG_PREEMPT) && defined(CONFIG_DEBUG_PREEMPT)
+
+void fastcall add_preempt_count(int val)
+{
+	/*
+	 * Underflow?
+	 */
+        if (DEBUG_LOCKS_WARN_ON((preempt_count() < 0)))
+                return;
+	preempt_count() += val;
+	/*
+	 * Spinlock count overflowing soon?
+	 */
+        DEBUG_LOCKS_WARN_ON((preempt_count() & PREEMPT_MASK) >= PREEMPT_MASK-10);
+}
+EXPORT_SYMBOL(add_preempt_count);
+
+void fastcall sub_preempt_count(int val)
+{
+	/*
+	 * Underflow?
+	 */
+        if (DEBUG_LOCKS_WARN_ON(val > preempt_count()))
+                return;
+	/*
+	 * Is the spinlock portion underflowing?
+	 */
+        if (DEBUG_LOCKS_WARN_ON((val < PREEMPT_MASK) &&
+                        !(preempt_count() & PREEMPT_MASK)))
+                return;
+ 
+	preempt_count() -= val;
+}
+EXPORT_SYMBOL(sub_preempt_count);
+
+#endif
+
+/*
+ * schedule() is the main scheduler function.
+ */
+asmlinkage void __sched schedule(void)
+{
+        struct task_struct *prev, *next;
+	struct list_head *queue;
+	unsigned long long now;
+	unsigned long debit;
+	int cpu, idx;
+        long *switch_count;
+        struct rq *rq;
+
+	/*
+	 * Test if we are atomic.  Since do_exit() needs to call into
+	 * schedule() atomically, we ignore that path for now.
+	 * Otherwise, whine if we are scheduling when we should not be.
+	 */
+	if (unlikely(in_atomic() && !current->exit_state)) {
+		printk(KERN_ERR "BUG: scheduling while atomic: "
+			"%s/0x%08x/%d\n",
+			current->comm, preempt_count(), current->pid);
+		dump_stack();
+	}
+	profile_hit(SCHED_PROFILING, __builtin_return_address(0));
+
+need_resched:
+	preempt_disable();
+	prev = current;
+	release_kernel_lock(prev);
+need_resched_nonpreemptible:
+	rq = this_rq();
+
+	/*
+	 * The idle thread is not allowed to schedule!
+	 * Remove this check after it has been exercised a bit.
+	 */
+	if (unlikely(prev == rq->idle) && prev->state != TASK_RUNNING) {
+		printk(KERN_ERR "bad: scheduling from the idle thread!\n");
+		dump_stack();
+	}
+
+	schedstat_inc(rq, sched_cnt);
+	now = sched_clock();
+
+	spin_lock_irq(&rq->lock);
+	prev->runtime = ns_diff(now, prev->timestamp);
+	debit = ns_diff(now, rq->timestamp_last_tick) % NSJIFFY;
+	prev->ns_debit += debit;
+
+	if (unlikely(prev->flags & PF_DEAD))
+		prev->state = EXIT_DEAD;
+
+	switch_count = &prev->nivcsw;
+	if (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {
+		switch_count = &prev->nvcsw;
+		if (unlikely((prev->state & TASK_INTERRUPTIBLE) &&
+				unlikely(signal_pending(prev))))
+			prev->state = TASK_RUNNING;
+		else {
+			if (prev->state == TASK_UNINTERRUPTIBLE) {
+				prev->flags |= PF_NONSLEEP;
+				rq->nr_uninterruptible++;
+			}
+			deactivate_task(prev, rq);
+		}
+	}
+
+	cpu = smp_processor_id();
+	if (unlikely(!rq->nr_running)) {
+		idle_balance(cpu, rq);
+		if (!rq->nr_running) {
+			next = rq->idle;
+                        wake_sleeping_dependent(cpu);
+			goto switch_tasks;
+		}
+	}
+
+	idx = sched_find_first_bit(rq->bitmap);
+	queue = rq->queue + idx;
+	next = list_entry(queue->next, struct task_struct, run_list);
+
+        if (dependent_sleeper(cpu, rq, next))
+                next = rq->idle;
+switch_tasks:
+	if (next == rq->idle)
+		schedstat_inc(rq, sched_goidle);
+        else {
+                prefetch(next);
+                prefetch_stack(next);
+        }
+	prev->timestamp = now;
+	clear_tsk_need_resched(prev);
+	rcu_qsctr_inc(task_cpu(prev));
+
+	update_cpu_clock(prev, rq, now);
+
+	sched_info_switch(prev, next);
+	if (likely(prev != next)) {
+                rq->preempted = rq->cache_ticks = 0;
+		next->timestamp = now;
+		rq->nr_switches++;
+		rq->curr = next;
+		++*switch_count;
+
+		prepare_task_switch(rq, next);
+		prev = context_switch(rq, prev, next);
+		barrier();
+		/*
+		 * this_rq must be evaluated again because prev may have moved
+		 * CPUs since it called schedule(), thus the 'rq' on its stack
+		 * frame will be invalid.
+		 */
+		finish_task_switch(this_rq(), prev);
+	} else
+		spin_unlock_irq(&rq->lock);
+
+	prev = current;
+	if (unlikely(reacquire_kernel_lock(prev) < 0))
+		goto need_resched_nonpreemptible;
+	preempt_enable_no_resched();
+	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
+		goto need_resched;
+}
+EXPORT_SYMBOL(schedule);
+
+#ifdef CONFIG_PREEMPT
+/*
+ * this is the entry point to schedule() from in-kernel preemption
+ * off of preempt_enable.  Kernel preemptions off return from interrupt
+ * occur there and call schedule directly.
+ */
+asmlinkage void __sched preempt_schedule(void)
+{
+	struct thread_info *ti = current_thread_info();
+#ifdef CONFIG_PREEMPT_BKL
+	struct task_struct *task = current;
+	int saved_lock_depth;
+#endif
+	/*
+	 * If there is a non-zero preempt_count or interrupts are disabled,
+	 * we do not want to preempt the current task.  Just return..
+	 */
+	if (unlikely(ti->preempt_count || irqs_disabled()))
+		return;
+
+need_resched:
+	add_preempt_count(PREEMPT_ACTIVE);
+	/*
+	 * We keep the big kernel semaphore locked, but we
+	 * clear ->lock_depth so that schedule() doesnt
+	 * auto-release the semaphore:
+	 */
+#ifdef CONFIG_PREEMPT_BKL
+	saved_lock_depth = task->lock_depth;
+	task->lock_depth = -1;
+#endif
+	schedule();
+#ifdef CONFIG_PREEMPT_BKL
+	task->lock_depth = saved_lock_depth;
+#endif
+	sub_preempt_count(PREEMPT_ACTIVE);
+
+	/* we could miss a preemption opportunity between schedule and now */
+	barrier();
+	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
+		goto need_resched;
+}
+
+EXPORT_SYMBOL(preempt_schedule);
+
+/*
+ * this is is the entry point to schedule() from kernel preemption
+ * off of irq context.
+ * Note, that this is called and return with irqs disabled. This will
+ * protect us against recursive calling from irq.
+ */
+asmlinkage void __sched preempt_schedule_irq(void)
+{
+	struct thread_info *ti = current_thread_info();
+#ifdef CONFIG_PREEMPT_BKL
+	struct task_struct *task = current;
+	int saved_lock_depth;
+#endif
+	/* Catch callers which need to be fixed */
+	BUG_ON(ti->preempt_count || !irqs_disabled());
+
+need_resched:
+	add_preempt_count(PREEMPT_ACTIVE);
+	/*
+	 * We keep the big kernel semaphore locked, but we
+	 * clear ->lock_depth so that schedule() doesnt
+	 * auto-release the semaphore:
+	 */
+#ifdef CONFIG_PREEMPT_BKL
+	saved_lock_depth = task->lock_depth;
+	task->lock_depth = -1;
+#endif
+	local_irq_enable();
+	schedule();
+	local_irq_disable();
+#ifdef CONFIG_PREEMPT_BKL
+	task->lock_depth = saved_lock_depth;
+#endif
+	sub_preempt_count(PREEMPT_ACTIVE);
+
+	/* we could miss a preemption opportunity between schedule and now */
+	barrier();
+	if (unlikely(test_thread_flag(TIF_NEED_RESCHED)))
+		goto need_resched;
+}
+
+#endif /* CONFIG_PREEMPT */
+
+int default_wake_function(wait_queue_t *curr, unsigned mode, int sync,
+			  void *key)
+{
+	struct task_struct *p = curr->private;
+	return try_to_wake_up(p, mode, sync);
+}
+EXPORT_SYMBOL(default_wake_function);
+
+/*
+ * The core wakeup function.  Non-exclusive wakeups (nr_exclusive == 0) just
+ * wake everything up.  If it's an exclusive wakeup (nr_exclusive == small +ve
+ * number) then we wake all the non-exclusive tasks and one exclusive task.
+ *
+ * There are circumstances in which we can try to wake a task which has already
+ * started to run but is not in state TASK_RUNNING.  try_to_wake_up() returns
+ * zero in this (rare) case, and we handle it by continuing to scan the queue.
+ */
+static void __wake_up_common(wait_queue_head_t *q, unsigned int mode,
+			     int nr_exclusive, int sync, void *key)
+{
+	struct list_head *tmp, *next;
+
+	list_for_each_safe(tmp, next, &q->task_list) {
+                wait_queue_t *curr = list_entry(tmp, wait_queue_t, task_list);
+                unsigned flags = curr->flags;
+
+		if (curr->func(curr, mode, sync, key) &&
+                                (flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
+			break;
+	}
+}
+
+/**
+ * __wake_up - wake up threads blocked on a waitqueue.
+ * @q: the waitqueue
+ * @mode: which threads
+ * @nr_exclusive: how many wake-one or wake-many threads to wake up
+ * @key: is directly passed to the wakeup function
+ */
+void fastcall __wake_up(wait_queue_head_t *q, unsigned int mode,
+			int nr_exclusive, void *key)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&q->lock, flags);
+	__wake_up_common(q, mode, nr_exclusive, 0, key);
+	spin_unlock_irqrestore(&q->lock, flags);
+}
+EXPORT_SYMBOL(__wake_up);
+
+/*
+ * Same as __wake_up but called with the spinlock in wait_queue_head_t held.
+ */
+void fastcall __wake_up_locked(wait_queue_head_t *q, unsigned int mode)
+{
+	__wake_up_common(q, mode, 1, 0, NULL);
+}
+
+/**
+ * __wake_up_sync - wake up threads blocked on a waitqueue.
+ * @q: the waitqueue
+ * @mode: which threads
+ * @nr_exclusive: how many wake-one or wake-many threads to wake up
+ *
+ * The sync wakeup differs that the waker knows that it will schedule
+ * away soon, so while the target thread will be woken up, it will not
+ * be migrated to another CPU - ie. the two threads are 'synchronized'
+ * with each other. This can prevent needless bouncing between CPUs.
+ *
+ * On UP it can prevent extra preemption.
+ */
+void fastcall
+__wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
+{
+	unsigned long flags;
+	int sync = 1;
+
+	if (unlikely(!q))
+		return;
+
+	if (unlikely(!nr_exclusive))
+		sync = 0;
+
+	spin_lock_irqsave(&q->lock, flags);
+	__wake_up_common(q, mode, nr_exclusive, sync, NULL);
+	spin_unlock_irqrestore(&q->lock, flags);
+}
+EXPORT_SYMBOL_GPL(__wake_up_sync);	/* For internal use only */
+
+void fastcall complete(struct completion *x)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&x->wait.lock, flags);
+	x->done++;
+	__wake_up_common(&x->wait, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE,
+			 1, 0, NULL);
+	spin_unlock_irqrestore(&x->wait.lock, flags);
+}
+EXPORT_SYMBOL(complete);
+
+void fastcall complete_all(struct completion *x)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&x->wait.lock, flags);
+	x->done += UINT_MAX/2;
+	__wake_up_common(&x->wait, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE,
+			 0, 0, NULL);
+	spin_unlock_irqrestore(&x->wait.lock, flags);
+}
+EXPORT_SYMBOL(complete_all);
+
+void fastcall __sched wait_for_completion(struct completion *x)
+{
+	might_sleep();
+
+	spin_lock_irq(&x->wait.lock);
+	if (!x->done) {
+		DECLARE_WAITQUEUE(wait, current);
+
+		wait.flags |= WQ_FLAG_EXCLUSIVE;
+		__add_wait_queue_tail(&x->wait, &wait);
+		do {
+			__set_current_state(TASK_UNINTERRUPTIBLE);
+			spin_unlock_irq(&x->wait.lock);
+			schedule();
+			spin_lock_irq(&x->wait.lock);
+		} while (!x->done);
+		__remove_wait_queue(&x->wait, &wait);
+	}
+	x->done--;
+	spin_unlock_irq(&x->wait.lock);
+}
+EXPORT_SYMBOL(wait_for_completion);
+
+unsigned long fastcall __sched
+wait_for_completion_timeout(struct completion *x, unsigned long timeout)
+{
+	might_sleep();
+
+	spin_lock_irq(&x->wait.lock);
+	if (!x->done) {
+		DECLARE_WAITQUEUE(wait, current);
+
+		wait.flags |= WQ_FLAG_EXCLUSIVE;
+		__add_wait_queue_tail(&x->wait, &wait);
+		do {
+			__set_current_state(TASK_UNINTERRUPTIBLE);
+			spin_unlock_irq(&x->wait.lock);
+			timeout = schedule_timeout(timeout);
+			spin_lock_irq(&x->wait.lock);
+			if (!timeout) {
+				__remove_wait_queue(&x->wait, &wait);
+				goto out;
+			}
+		} while (!x->done);
+		__remove_wait_queue(&x->wait, &wait);
+	}
+	x->done--;
+out:
+	spin_unlock_irq(&x->wait.lock);
+	return timeout;
+}
+EXPORT_SYMBOL(wait_for_completion_timeout);
+
+int fastcall __sched wait_for_completion_interruptible(struct completion *x)
+{
+	int ret = 0;
+
+	might_sleep();
+
+	spin_lock_irq(&x->wait.lock);
+	if (!x->done) {
+		DECLARE_WAITQUEUE(wait, current);
+
+		wait.flags |= WQ_FLAG_EXCLUSIVE;
+		__add_wait_queue_tail(&x->wait, &wait);
+		do {
+			if (signal_pending(current)) {
+				ret = -ERESTARTSYS;
+				__remove_wait_queue(&x->wait, &wait);
+				goto out;
+			}
+			__set_current_state(TASK_INTERRUPTIBLE);
+			spin_unlock_irq(&x->wait.lock);
+			schedule();
+			spin_lock_irq(&x->wait.lock);
+		} while (!x->done);
+		__remove_wait_queue(&x->wait, &wait);
+	}
+	x->done--;
+out:
+	spin_unlock_irq(&x->wait.lock);
+
+	return ret;
+}
+EXPORT_SYMBOL(wait_for_completion_interruptible);
+
+unsigned long fastcall __sched
+wait_for_completion_interruptible_timeout(struct completion *x,
+					  unsigned long timeout)
+{
+	might_sleep();
+
+	spin_lock_irq(&x->wait.lock);
+	if (!x->done) {
+		DECLARE_WAITQUEUE(wait, current);
+
+		wait.flags |= WQ_FLAG_EXCLUSIVE;
+		__add_wait_queue_tail(&x->wait, &wait);
+		do {
+			if (signal_pending(current)) {
+				timeout = -ERESTARTSYS;
+				__remove_wait_queue(&x->wait, &wait);
+				goto out;
+			}
+			__set_current_state(TASK_INTERRUPTIBLE);
+			spin_unlock_irq(&x->wait.lock);
+			timeout = schedule_timeout(timeout);
+			spin_lock_irq(&x->wait.lock);
+			if (!timeout) {
+				__remove_wait_queue(&x->wait, &wait);
+				goto out;
+			}
+		} while (!x->done);
+		__remove_wait_queue(&x->wait, &wait);
+	}
+	x->done--;
+out:
+	spin_unlock_irq(&x->wait.lock);
+	return timeout;
+}
+EXPORT_SYMBOL(wait_for_completion_interruptible_timeout);
+
+
+#define	SLEEP_ON_VAR					\
+	unsigned long flags;				\
+	wait_queue_t wait;				\
+	init_waitqueue_entry(&wait, current);
+
+#define SLEEP_ON_HEAD					\
+	spin_lock_irqsave(&q->lock,flags);		\
+	__add_wait_queue(q, &wait);			\
+	spin_unlock(&q->lock);
+
+#define	SLEEP_ON_TAIL					\
+	spin_lock_irq(&q->lock);			\
+	__remove_wait_queue(q, &wait);			\
+	spin_unlock_irqrestore(&q->lock, flags);
+
+void fastcall __sched interruptible_sleep_on(wait_queue_head_t *q)
+{
+	SLEEP_ON_VAR
+
+	current->state = TASK_INTERRUPTIBLE;
+
+	SLEEP_ON_HEAD
+	schedule();
+	SLEEP_ON_TAIL
+}
+EXPORT_SYMBOL(interruptible_sleep_on);
+
+long fastcall __sched
+interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
+{
+	SLEEP_ON_VAR
+
+	current->state = TASK_INTERRUPTIBLE;
+
+	SLEEP_ON_HEAD
+	timeout = schedule_timeout(timeout);
+	SLEEP_ON_TAIL
+
+	return timeout;
+}
+EXPORT_SYMBOL(interruptible_sleep_on_timeout);
+
+void fastcall __sched sleep_on(wait_queue_head_t *q)
+{
+	SLEEP_ON_VAR
+
+	current->state = TASK_UNINTERRUPTIBLE;
+
+	SLEEP_ON_HEAD
+	schedule();
+	SLEEP_ON_TAIL
+}
+EXPORT_SYMBOL(sleep_on);
+
+long fastcall __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)
+{
+	SLEEP_ON_VAR
+
+	current->state = TASK_UNINTERRUPTIBLE;
+
+	SLEEP_ON_HEAD
+	timeout = schedule_timeout(timeout);
+	SLEEP_ON_TAIL
+
+	return timeout;
+}
+
+EXPORT_SYMBOL(sleep_on_timeout);
+
+#ifdef CONFIG_RT_MUTEXES
+
+/*
+ * rt_mutex_setprio - set the current priority of a task
+ * @p: task
+ * @prio: prio value (kernel-internal form)
+ *
+ * This function changes the 'effective' priority of a task. It does
+ * not touch ->normal_prio like __setscheduler().
+ *
+ * Used by the rt_mutex code to implement priority inheritance logic.
+ */
+void rt_mutex_setprio(struct task_struct *p, int prio)
+{
+	unsigned long flags;
+	struct rq *rq;
+	int oldprio, queued;
+
+	BUG_ON(prio < 0 || prio > MAX_PRIO);
+
+	rq = task_rq_lock(p, &flags);
+
+	oldprio = p->prio;
+	if ((queued = task_queued(p)))
+		dequeue_task(p, rq);
+	p->prio = prio;
+
+	if (queued) {
+		enqueue_task(p, rq);
+		/*
+		 * Reschedule if we are currently running on this runqueue and
+		 * our priority decreased, or if we are not currently running on
+		 * this runqueue and our priority is higher than the current's
+		 */
+		if (task_running(rq, p)) {
+			if (p->prio > oldprio)
+				resched_task(rq->curr);
+		} else
+			preempt(p, rq);
+	}
+	task_rq_unlock(rq, &flags);
+}
+
+#endif
+
+void set_user_nice(struct task_struct *p, long nice)
+{
+	int queued, old_prio, delta;
+        unsigned long flags;
+        struct rq *rq;
+
+	if (TASK_NICE(p) == nice || nice < -20 || nice > 19)
+		return;
+	/*
+	 * We have to be careful, if called from sys_setpriority(),
+	 * the task might be in the middle of scheduling on another CPU.
+	 */
+	rq = task_rq_lock(p, &flags);
+	/*
+	 * The RT priorities are set via sched_setscheduler(), but we still
+	 * allow the 'normal' nice value to be set - but as expected
+	 * it wont have any effect on scheduling until the task is
+	 * not SCHED_NORMAL/SCHED_BATCH:
+	 */
+	if (has_rt_policy(p)) {
+		p->static_prio = NICE_TO_PRIO(nice);
+		goto out_unlock;
+	}
+        if ((queued = task_queued(p))) {
+                dequeue_task(p, rq);
+                dec_raw_weighted_load(rq, p);
+	}
+
+	p->static_prio = NICE_TO_PRIO(nice);
+	set_load_weight(p);
+        old_prio = p->prio;
+        p->prio = effective_prio(p);
+        delta = p->prio - old_prio;
+        if (p->bonus > bonus(p))
+                p->bonus= bonus(p);
+
+	if (queued) {
+		enqueue_task(p, rq);
+		inc_raw_weighted_load(rq, p);
+		/*
+		 * If the task increased its priority or is running and
+		 * lowered its priority, then reschedule its CPU:
+		 */
+		if (delta < 0 || (delta > 0 && task_running(rq, p)))
+			resched_task(rq->curr);
+	}
+out_unlock:
+	task_rq_unlock(rq, &flags);
+}
+
+EXPORT_SYMBOL(set_user_nice);
+
+/*
+ * can_nice - check if a task can reduce its nice value
+ * @p: task
+ * @nice: nice value
+ */
+int can_nice(const struct task_struct *p, const int nice)
+{
+	/* convert nice value [19,-20] to rlimit style value [1,40] */
+	int nice_rlim = 20 - nice;
+
+	return (nice_rlim <= p->signal->rlim[RLIMIT_NICE].rlim_cur ||
+		capable(CAP_SYS_NICE));
+}
+
+#ifdef __ARCH_WANT_SYS_NICE
+
+/*
+ * sys_nice - change the priority of the current process.
+ * @increment: priority increment
+ *
+ * sys_setpriority is a more generic, but much slower function that
+ * does similar things.
+ */
+asmlinkage long sys_nice(int increment)
+{
+	long nice, retval;
+
+	/*
+	 * Setpriority might change our priority at the same moment.
+	 * We don't have to worry. Conceptually one call occurs first
+	 * and we have a single winner.
+	 */
+	if (increment < -40)
+		increment = -40;
+	if (increment > 40)
+		increment = 40;
+
+	nice = PRIO_TO_NICE(current->static_prio) + increment;
+	if (nice < -20)
+		nice = -20;
+	if (nice > 19)
+		nice = 19;
+
+	if (increment < 0 && !can_nice(current, nice))
+		return -EPERM;
+
+	retval = security_task_setnice(current, nice);
+	if (retval)
+		return retval;
+
+	set_user_nice(current, nice);
+	return 0;
+}
+
+#endif
+
+/**
+ * task_prio - return the priority value of a given task.
+ * @p: the task in question.
+ *
+ * This is the priority value as seen by users in /proc.
+ * RT tasks are offset by -200. Normal tasks are centered
+ * around 0, value goes from -16 to +15.
+ */
+int task_prio(const struct task_struct *p)
+{
+	return p->prio - MAX_RT_PRIO;
+}
+
+/**
+ * task_nice - return the nice value of a given task.
+ * @p: the task in question.
+ */
+int task_nice(const struct task_struct *p)
+{
+	return TASK_NICE(p);
+}
+EXPORT_SYMBOL_GPL(task_nice);
+
+/**
+ * idle_cpu - is a given cpu idle currently?
+ * @cpu: the processor in question.
+ */
+int idle_cpu(int cpu)
+{
+	return cpu_curr(cpu) == cpu_rq(cpu)->idle;
+}
+
+/**
+ * idle_task - return the idle task for a given cpu.
+ * @cpu: the processor in question.
+ */
+struct task_struct *idle_task(int cpu)
+{
+	return cpu_rq(cpu)->idle;
+}
+
+/**
+ * find_process_by_pid - find a process with a matching PID value.
+ * @pid: the pid in question.
+ */
+static inline struct task_struct *find_process_by_pid(pid_t pid)
+{
+	return pid ? find_task_by_pid(pid) : current;
+}
+
+/* Actually do priority change: must hold rq lock. */
+static void __setscheduler(struct task_struct *p, int policy, int prio)
+{
+	BUG_ON(task_queued(p));
+
+	p->policy = policy;
+	p->rt_priority = prio;
+	p->normal_prio = normal_prio(p);
+	/* we are holding p->pi_lock already */
+	p->prio = rt_mutex_getprio(p);
+	set_load_weight(p);
+}
+
+/**
+ * sched_setscheduler - change the scheduling policy and/or RT priority of
+ * a thread.
+ * @p: the task in question.
+ * @policy: new policy.
+ * @param: structure containing the new RT priority.
+ */
+int sched_setscheduler(struct task_struct *p, int policy,
+		       struct sched_param *param)
+{
+	int retval, queued, oldprio, oldpolicy = -1;
+	unsigned long flags;
+	struct rq *rq;
+
+        /* may grab non-irq protected spin_locks */
+        BUG_ON(in_interrupt());
+recheck:
+	/* double check policy once rq lock held */
+	if (policy < 0)
+		policy = oldpolicy = p->policy;
+	else if (policy != SCHED_FIFO && policy != SCHED_RR &&
+			policy != SCHED_NORMAL && policy != SCHED_BATCH)
+		return -EINVAL;
+	/*
+	 * Valid priorities for SCHED_FIFO and SCHED_RR are
+	 * 1..MAX_USER_RT_PRIO-1, valid priority for SCHED_NORMAL and
+	 * SCHED_BATCH is 0.
+	 */
+	if (param->sched_priority < 0 ||
+	    (p->mm && param->sched_priority > MAX_USER_RT_PRIO-1) ||
+	    (!p->mm && param->sched_priority > MAX_RT_PRIO-1))
+		return -EINVAL;
+	if ((policy == SCHED_NORMAL || policy == SCHED_BATCH)
+					!= (param->sched_priority == 0))
+		return -EINVAL;
+
+	/*
+	 * Allow unprivileged RT tasks to decrease priority:
+	 */
+	if (!capable(CAP_SYS_NICE)) {
+		/*
+		 * can't change policy, except between SCHED_NORMAL
+		 * and SCHED_BATCH:
+		 */
+		if (((policy != SCHED_NORMAL && p->policy != SCHED_BATCH) &&
+			(policy != SCHED_BATCH && p->policy != SCHED_NORMAL)) &&
+				!p->signal->rlim[RLIMIT_RTPRIO].rlim_cur)
+			return -EPERM;
+		/* can't increase priority */
+		if ((policy != SCHED_NORMAL && policy != SCHED_BATCH) &&
+		    param->sched_priority > p->rt_priority &&
+		    param->sched_priority >
+				p->signal->rlim[RLIMIT_RTPRIO].rlim_cur)
+			return -EPERM;
+		/* can't change other user's priorities */
+		if ((current->euid != p->euid) &&
+		    (current->euid != p->uid))
+			return -EPERM;
+	}
+
+	retval = security_task_setscheduler(p, policy, param);
+	if (retval)
+		return retval;
+	/*
+	 * make sure no PI-waiters arrive (or leave) while we are
+	 * changing the priority of the task:
+	 */
+	spin_lock_irqsave(&p->pi_lock, flags);
+	/*
+	 * To be able to change p->policy safely, the apropriate
+	 * runqueue lock must be held.
+	 */
+	rq = __task_rq_lock(p);
+	/* recheck policy now with rq lock held */
+	if (unlikely(oldpolicy != -1 && oldpolicy != p->policy)) {
+		policy = oldpolicy = -1;
+		__task_rq_unlock(rq);
+		spin_unlock_irqrestore(&p->pi_lock, flags);
+		goto recheck;
+	}
+	if ((queued = task_queued(p)))
+		deactivate_task(p, rq);
+	oldprio = p->prio;
+	__setscheduler(p, policy, param->sched_priority);
+	if (queued) {
+		__activate_task(p, rq);
+		/*
+		 * Reschedule if we are currently running on this runqueue and
+		 * our priority decreased, or if we are not currently running on
+		 * this runqueue and our priority is higher than the current's
+		 */
+		if (task_running(rq, p)) {
+			if (p->prio > oldprio)
+				resched_task(rq->curr);
+		} else
+			preempt(p, rq);
+	}
+	__task_rq_unlock(rq);
+	spin_unlock_irqrestore(&p->pi_lock, flags);
+
+        rt_mutex_adjust_pi(p);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(sched_setscheduler);
+
+static int
+do_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param)
+{
+	struct sched_param lparam;
+	struct task_struct *p;
+        int retval;
+
+	if (!param || pid < 0)
+		return -EINVAL;
+	if (copy_from_user(&lparam, param, sizeof(struct sched_param)))
+		return -EFAULT;
+	read_lock_irq(&tasklist_lock);
+	p = find_process_by_pid(pid);
+	if (!p) {
+		read_unlock_irq(&tasklist_lock);
+		return -ESRCH;
+	}
+        get_task_struct(p);
+	read_unlock_irq(&tasklist_lock);
+        retval = sched_setscheduler(p, policy, &lparam);
+        put_task_struct(p);
+
+	return retval;
+}
+
+/**
+ * sys_sched_setscheduler - set/change the scheduler policy and RT priority
+ * @pid: the pid in question.
+ * @policy: new policy.
+ * @param: structure containing the new RT priority.
+ */
+asmlinkage long sys_sched_setscheduler(pid_t pid, int policy,
+				       struct sched_param __user *param)
+{
+	/* negative values for policy are not valid */
+	if (policy < 0)
+		return -EINVAL;
+
+	return do_sched_setscheduler(pid, policy, param);
+}
+
+/**
+ * sys_sched_setparam - set/change the RT priority of a thread
+ * @pid: the pid in question.
+ * @param: structure containing the new RT priority.
+ */
+asmlinkage long sys_sched_setparam(pid_t pid, struct sched_param __user *param)
+{
+	return do_sched_setscheduler(pid, -1, param);
+}
+
+/**
+ * sys_sched_getscheduler - get the policy (scheduling class) of a thread
+ * @pid: the pid in question.
+ */
+asmlinkage long sys_sched_getscheduler(pid_t pid)
+{
+        struct task_struct *p;
+	int retval = -EINVAL;
+
+	if (pid < 0)
+		goto out_nounlock;
+
+	retval = -ESRCH;
+	read_lock(&tasklist_lock);
+	p = find_process_by_pid(pid);
+	if (p) {
+		retval = security_task_getscheduler(p);
+		if (!retval)
+			retval = p->policy;
+	}
+	read_unlock(&tasklist_lock);
+
+out_nounlock:
+	return retval;
+}
+
+/**
+ * sys_sched_getscheduler - get the RT priority of a thread
+ * @pid: the pid in question.
+ * @param: structure containing the RT priority.
+ */
+asmlinkage long sys_sched_getparam(pid_t pid, struct sched_param __user *param)
+{
+	struct sched_param lp;
+        struct task_struct *p;
+	int retval = -EINVAL;
+
+	if (!param || pid < 0)
+		goto out_nounlock;
+
+	read_lock(&tasklist_lock);
+	p = find_process_by_pid(pid);
+	retval = -ESRCH;
+	if (!p)
+		goto out_unlock;
+
+	retval = security_task_getscheduler(p);
+	if (retval)
+		goto out_unlock;
+
+	lp.sched_priority = p->rt_priority;
+	read_unlock(&tasklist_lock);
+
+	/*
+	 * This one might sleep, we cannot do it with a spinlock held ...
+	 */
+	retval = copy_to_user(param, &lp, sizeof(*param)) ? -EFAULT : 0;
+
+out_nounlock:
+	return retval;
+
+out_unlock:
+	read_unlock(&tasklist_lock);
+	return retval;
+}
+
+long sched_setaffinity(pid_t pid, cpumask_t new_mask)
+{
+	cpumask_t cpus_allowed;
+        struct task_struct *p;
+        int retval;
+
+	lock_cpu_hotplug();
+	read_lock(&tasklist_lock);
+
+	p = find_process_by_pid(pid);
+	if (!p) {
+		read_unlock(&tasklist_lock);
+		unlock_cpu_hotplug();
+		return -ESRCH;
+	}
+
+	/*
+	 * It is not safe to call set_cpus_allowed with the
+	 * tasklist_lock held.  We will bump the task_struct's
+	 * usage count and then drop tasklist_lock.
+	 */
+	get_task_struct(p);
+	read_unlock(&tasklist_lock);
+
+	retval = -EPERM;
+	if ((current->euid != p->euid) && (current->euid != p->uid) &&
+			!capable(CAP_SYS_NICE))
+		goto out_unlock;
+
+        retval = security_task_setscheduler(p, 0, NULL);
+        if (retval)
+                goto out_unlock;
+
+	cpus_allowed = cpuset_cpus_allowed(p);
+	cpus_and(new_mask, new_mask, cpus_allowed);
+	retval = set_cpus_allowed(p, new_mask);
+
+out_unlock:
+	put_task_struct(p);
+	unlock_cpu_hotplug();
+	return retval;
+}
+
+static int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len,
+			     cpumask_t *new_mask)
+{
+	if (len < sizeof(cpumask_t)) {
+		memset(new_mask, 0, sizeof(cpumask_t));
+	} else if (len > sizeof(cpumask_t)) {
+		len = sizeof(cpumask_t);
+	}
+	return copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0;
+}
+
+/**
+ * sys_sched_setaffinity - set the cpu affinity of a process
+ * @pid: pid of the process
+ * @len: length in bytes of the bitmask pointed to by user_mask_ptr
+ * @user_mask_ptr: user-space pointer to the new cpu mask
+ */
+asmlinkage long sys_sched_setaffinity(pid_t pid, unsigned int len,
+				      unsigned long __user *user_mask_ptr)
+{
+	cpumask_t new_mask;
+	int retval;
+
+	retval = get_user_cpu_mask(user_mask_ptr, len, &new_mask);
+	if (retval)
+		return retval;
+
+	return sched_setaffinity(pid, new_mask);
+}
+
+/*
+ * Represents all cpu's present in the system
+ * In systems capable of hotplug, this map could dynamically grow
+ * as new cpu's are detected in the system via any platform specific
+ * method, such as ACPI for e.g.
+ */
+
+cpumask_t cpu_present_map __read_mostly;
+EXPORT_SYMBOL(cpu_present_map);
+
+#ifndef CONFIG_SMP
+cpumask_t cpu_online_map __read_mostly = CPU_MASK_ALL;
+cpumask_t cpu_possible_map __read_mostly = CPU_MASK_ALL;
+#endif
+
+long sched_getaffinity(pid_t pid, cpumask_t *mask)
+{
+        struct task_struct *p;
+	int retval;
+
+	lock_cpu_hotplug();
+	read_lock(&tasklist_lock);
+
+	retval = -ESRCH;
+	p = find_process_by_pid(pid);
+	if (!p)
+		goto out_unlock;
+
+        retval = security_task_getscheduler(p);
+        if (retval)
+                goto out_unlock;
+ 
+	cpus_and(*mask, p->cpus_allowed, cpu_online_map);
+
+out_unlock:
+	read_unlock(&tasklist_lock);
+	unlock_cpu_hotplug();
+	if (retval)
+		return retval;
+
+	return 0;
+}
+
+/**
+ * sys_sched_getaffinity - get the cpu affinity of a process
+ * @pid: pid of the process
+ * @len: length in bytes of the bitmask pointed to by user_mask_ptr
+ * @user_mask_ptr: user-space pointer to hold the current cpu mask
+ */
+asmlinkage long sys_sched_getaffinity(pid_t pid, unsigned int len,
+				      unsigned long __user *user_mask_ptr)
+{
+	int ret;
+	cpumask_t mask;
+
+	if (len < sizeof(cpumask_t))
+		return -EINVAL;
+
+	ret = sched_getaffinity(pid, &mask);
+	if (ret < 0)
+		return ret;
+
+	if (copy_to_user(user_mask_ptr, &mask, sizeof(cpumask_t)))
+		return -EFAULT;
+
+	return sizeof(cpumask_t);
+}
+
+/**
+ * sys_sched_yield - yield the current processor to other threads.
+ * This function yields the current CPU by dropping the priority of current
+ * to the lowest priority.
+ */
+asmlinkage long sys_sched_yield(void)
+{
+	int newprio;
+	struct rq *rq = this_rq_lock();
+
+	newprio = current->prio;
+	schedstat_inc(rq, yld_cnt);
+	current->slice = slice(current);
+	current->time_slice = rr_interval(current);
+	if (likely(!rt_task(current)))
+		newprio = MIN_USER_PRIO;
+
+	requeue_task(current, rq, newprio);
+
+	/*
+	 * Since we are going to call schedule() anyway, there's
+	 * no need to preempt or enable interrupts:
+	 */
+	__release(rq->lock);
+        spin_release(&rq->lock.dep_map, 1, _THIS_IP_);
+	_raw_spin_unlock(&rq->lock);
+	preempt_enable_no_resched();
+
+	schedule();
+
+	return 0;
+}
+
+static inline int __resched_legal(void)
+{
+        if (unlikely(preempt_count()))
+                return 0;
+        if (unlikely(system_state != SYSTEM_RUNNING))
+                return 0;
+        return 1;
+}
+
+static void __cond_resched(void)
+{
+#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP
+	__might_sleep(__FILE__, __LINE__);
+#endif
+	/*
+	 * The BKS might be reacquired before we have dropped
+	 * PREEMPT_ACTIVE, which could trigger a second
+	 * cond_resched() call.
+	 */
+	do {
+		add_preempt_count(PREEMPT_ACTIVE);
+		schedule();
+		sub_preempt_count(PREEMPT_ACTIVE);
+	} while (need_resched());
+}
+
+int __sched cond_resched(void)
+{
+        if (need_resched() && __resched_legal()) {
+		__cond_resched();
+		return 1;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(cond_resched);
+
+/*
+ * cond_resched_lock() - if a reschedule is pending, drop the given lock,
+ * call schedule, and on return reacquire the lock.
+ *
+ * This works OK both with and without CONFIG_PREEMPT.  We do strange low-level
+ * operations here to prevent schedule() from being called twice (once via
+ * spin_unlock(), once by hand).
+ */
+int cond_resched_lock(spinlock_t *lock)
+{
+	int ret = 0;
+
+	if (need_lockbreak(lock)) {
+		spin_unlock(lock);
+		cpu_relax();
+		ret = 1;
+		spin_lock(lock);
+	}
+        if (need_resched() && __resched_legal()) {
+                spin_release(&lock->dep_map, 1, _THIS_IP_);
+		_raw_spin_unlock(lock);
+		preempt_enable_no_resched();
+		__cond_resched();
+		ret = 1;
+		spin_lock(lock);
+	}
+	return ret;
+}
+EXPORT_SYMBOL(cond_resched_lock);
+
+int __sched cond_resched_softirq(void)
+{
+	BUG_ON(!in_softirq());
+
+        if (need_resched() && __resched_legal()) {
+                raw_local_irq_disable();
+                _local_bh_enable();
+                raw_local_irq_enable();
+		__cond_resched();
+		local_bh_disable();
+		return 1;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(cond_resched_softirq);
+
+/**
+ * yield - yield the current processor to other threads.
+ *
+ * this is a shortcut for kernel-space yielding - it marks the
+ * thread runnable and calls sys_sched_yield().
+ */
+void __sched yield(void)
+{
+	set_current_state(TASK_RUNNING);
+	sys_sched_yield();
+}
+EXPORT_SYMBOL(yield);
+
+/*
+ * This task is about to go to sleep on IO.  Increment rq->nr_iowait so
+ * that process accounting knows that this is a task in IO wait state.
+ *
+ * But don't do that if it is a deliberate, throttling IO wait (this task
+ * has set its backing_dev_info: the queue against which it should throttle)
+ */
+void __sched io_schedule(void)
+{
+	struct rq *rq = &__raw_get_cpu_var(runqueues);
+
+	delayacct_blkio_start();
+	atomic_inc(&rq->nr_iowait);
+	schedule();
+	atomic_dec(&rq->nr_iowait);
+	delayacct_blkio_end();
+}
+EXPORT_SYMBOL(io_schedule);
+
+long __sched io_schedule_timeout(long timeout)
+{
+	struct rq *rq = &__raw_get_cpu_var(runqueues);
+	long ret;
+
+	delayacct_blkio_start();
+	atomic_inc(&rq->nr_iowait);
+	ret = schedule_timeout(timeout);
+	atomic_dec(&rq->nr_iowait);
+	delayacct_blkio_end();
+	return ret;
+}
+
+/**
+ * sys_sched_get_priority_max - return maximum RT priority.
+ * @policy: scheduling class.
+ *
+ * this syscall returns the maximum rt_priority that can be used
+ * by a given scheduling class.
+ */
+asmlinkage long sys_sched_get_priority_max(int policy)
+{
+	int ret = -EINVAL;
+
+	switch (policy) {
+	case SCHED_FIFO:
+	case SCHED_RR:
+		ret = MAX_USER_RT_PRIO-1;
+		break;
+	case SCHED_NORMAL:
+	case SCHED_BATCH:
+		ret = 0;
+		break;
+	}
+	return ret;
+}
+
+/**
+ * sys_sched_get_priority_min - return minimum RT priority.
+ * @policy: scheduling class.
+ *
+ * this syscall returns the minimum rt_priority that can be used
+ * by a given scheduling class.
+ */
+asmlinkage long sys_sched_get_priority_min(int policy)
+{
+	int ret = -EINVAL;
+
+	switch (policy) {
+	case SCHED_FIFO:
+	case SCHED_RR:
+		ret = 1;
+		break;
+	case SCHED_NORMAL:
+	case SCHED_BATCH:
+		ret = 0;
+	}
+	return ret;
+}
+
+/**
+ * sys_sched_rr_get_interval - return the default timeslice of a process.
+ * @pid: pid of the process.
+ * @interval: userspace pointer to the timeslice value.
+ *
+ * this syscall writes the default timeslice value of a given process
+ * into the user-space timespec buffer. A value of '0' means infinity.
+ */
+asmlinkage
+long sys_sched_rr_get_interval(pid_t pid, struct timespec __user *interval)
+{
+        struct task_struct *p;
+	int retval = -EINVAL;
+	struct timespec t;
+
+	if (pid < 0)
+		goto out_nounlock;
+
+	retval = -ESRCH;
+	read_lock(&tasklist_lock);
+	p = find_process_by_pid(pid);
+	if (!p)
+		goto out_unlock;
+
+	retval = security_task_getscheduler(p);
+	if (retval)
+		goto out_unlock;
+
+	jiffies_to_timespec(p->policy == SCHED_FIFO ?
+				0 : slice(p), &t);
+	read_unlock(&tasklist_lock);
+	retval = copy_to_user(interval, &t, sizeof(t)) ? -EFAULT : 0;
+out_nounlock:
+	return retval;
+out_unlock:
+	read_unlock(&tasklist_lock);
+	return retval;
+}
+
+static inline struct task_struct *eldest_child(struct task_struct *p)
+{
+	if (list_empty(&p->children)) 
+                return NULL;
+	return list_entry(p->children.next,struct task_struct,sibling);
+}
+
+static inline struct task_struct *older_sibling(struct task_struct *p)
+{
+	if (p->sibling.prev==&p->parent->children) 
+                return NULL;
+	return list_entry(p->sibling.prev,struct task_struct,sibling);
+}
+
+static inline struct task_struct *younger_sibling(struct task_struct *p)
+{
+	if (p->sibling.next==&p->parent->children) 
+                return NULL;
+	return list_entry(p->sibling.next,struct task_struct,sibling);
+}
+
+static const char stat_nam[] = "RSDTtZX";
+
+static void show_task(struct task_struct *p)
+{
+        struct task_struct *relative;
+	unsigned long free = 0;
+        unsigned state;
+
+	state = p->state ? __ffs(p->state) + 1 : 0;
+        printk("%-13.13s %c", p->comm,
+                state < sizeof(stat_nam) - 1 ? stat_nam[state] : '?');
+#if (BITS_PER_LONG == 32)
+	if (state == TASK_RUNNING)
+		printk(" running ");
+	else
+		printk(" %08lX ", thread_saved_pc(p));
+#else
+	if (state == TASK_RUNNING)
+		printk("  running task   ");
+	else
+		printk(" %016lx ", thread_saved_pc(p));
+#endif
+#ifdef CONFIG_DEBUG_STACK_USAGE
+	{
+		unsigned long *n = end_of_stack(p);
+		while (!*n)
+			n++;
+		free = (unsigned long)n - (unsigned long)end_of_stack(p);
+	}
+#endif
+	printk("%5lu %5d %6d ", free, p->pid, p->parent->pid);
+	if ((relative = eldest_child(p)))
+		printk("%5d ", relative->pid);
+	else
+		printk("      ");
+	if ((relative = younger_sibling(p)))
+		printk("%7d", relative->pid);
+	else
+		printk("       ");
+	if ((relative = older_sibling(p)))
+		printk(" %5d", relative->pid);
+	else
+		printk("      ");
+	if (!p->mm)
+		printk(" (L-TLB)\n");
+	else
+		printk(" (NOTLB)\n");
+
+	if (state != TASK_RUNNING)
+		show_stack(p, NULL);
+}
+
+void show_state(void)
+{
+	struct task_struct *g, *p;
+
+#if (BITS_PER_LONG == 32)
+	printk("\n"
+	       "                                               sibling\n");
+	printk("  task             PC      pid father child younger older\n");
+#else
+	printk("\n"
+	       "                                                       sibling\n");
+	printk("  task                 PC          pid father child younger older\n");
+#endif
+	read_lock(&tasklist_lock);
+	do_each_thread(g, p) {
+		/*
+		 * reset the NMI-timeout, listing all files on a slow
+		 * console might take alot of time:
+		 */
+		touch_nmi_watchdog();
+		show_task(p);
+	} while_each_thread(g, p);
+
+	read_unlock(&tasklist_lock);
+	debug_show_all_locks();
+}
+
+/**
+ * init_idle - set up an idle thread for a given CPU
+ * @idle: task in question
+ * @cpu: cpu the idle task belongs to
+ *
+ * NOTE: this function does not set the idle thread's NEED_RESCHED
+ * flag, to make booting more robust.
+ */
+void __devinit init_idle(struct task_struct *idle, int cpu)
+{
+	struct rq *rq = cpu_rq(cpu);
+	unsigned long flags;
+
+	idle->timestamp = sched_clock();
+	idle->prio = idle->normal_prio = MAX_PRIO;
+	idle->state = TASK_RUNNING;
+	idle->cpus_allowed = cpumask_of_cpu(cpu);
+	set_task_cpu(idle, cpu);
+
+	spin_lock_irqsave(&rq->lock, flags);
+	rq->curr = rq->idle = idle;
+#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)
+	idle->oncpu = 1;
+#endif
+	spin_unlock_irqrestore(&rq->lock, flags);
+
+	/* Set the preempt count _outside_ the spinlocks! */
+#if defined(CONFIG_PREEMPT) && !defined(CONFIG_PREEMPT_BKL)
+	task_thread_info(idle)->preempt_count = (idle->lock_depth >= 0);
+#else
+	task_thread_info(idle)->preempt_count = 0;
+#endif
+}
+
+/*
+ * In a system that switches off the HZ timer nohz_cpu_mask
+ * indicates which cpus entered this state. This is used
+ * in the rcu update to wait only for active cpus. For system
+ * which do not switch off the HZ timer nohz_cpu_mask should
+ * always be CPU_MASK_NONE.
+ */
+cpumask_t nohz_cpu_mask = CPU_MASK_NONE;
+
+#ifdef CONFIG_SMP
+/*
+ * This is how migration works:
+ *
+ * 1) we queue a struct migration_req structure in the source CPU's
+ *    runqueue and wake up that CPU's migration thread.
+ * 2) we down() the locked semaphore => thread blocks.
+ * 3) migration thread wakes up (implicitly it forces the migrated
+ *    thread off the CPU)
+ * 4) it gets the migration request and checks whether the migrated
+ *    task is still in the wrong runqueue.
+ * 5) if it's in the wrong runqueue then the migration thread removes
+ *    it and puts it into the right queue.
+ * 6) migration thread up()s the semaphore.
+ * 7) we wake up and the migration is done.
+ */
+
+/*
+ * Change a given task's CPU affinity. Migrate the thread to a
+ * proper CPU and schedule it away if the CPU it's executing on
+ * is removed from the allowed bitmask.
+ *
+ * NOTE: the caller must have a valid reference to the task, the
+ * task must not exit() & deallocate itself prematurely.  The
+ * call is not atomic; no spinlocks may be held.
+ */
+int set_cpus_allowed(struct task_struct *p, cpumask_t new_mask)
+{
+        struct migration_req req;
+	unsigned long flags;
+        struct rq *rq;
+	int ret = 0;
+
+	rq = task_rq_lock(p, &flags);
+	if (!cpus_intersects(new_mask, cpu_online_map)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	p->cpus_allowed = new_mask;
+	/* Can the task run on the task's current CPU? If so, we're done */
+	if (cpu_isset(task_cpu(p), new_mask))
+		goto out;
+
+	if (migrate_task(p, any_online_cpu(new_mask), &req)) {
+		/* Need help from migration thread: drop lock and wait. */
+		task_rq_unlock(rq, &flags);
+		wake_up_process(rq->migration_thread);
+		wait_for_completion(&req.done);
+		tlb_migrate_finish(p->mm);
+		return 0;
+	}
+out:
+	task_rq_unlock(rq, &flags);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(set_cpus_allowed);
+
+/*
+ * Move (not current) task off this cpu, onto dest cpu.  We're doing
+ * this because either it can't run here any more (set_cpus_allowed()
+ * away from this CPU, or CPU going down), or because we're
+ * attempting to rebalance this task on exec (sched_exec).
+ *
+ * So we race with normal scheduler movements, but that's OK, as long
+ * as the task is no longer on this CPU.
+ *
+ * Returns non-zero if task was successfully migrated.
+ */
+static int __migrate_task(struct task_struct *p, int src_cpu, int dest_cpu)
+{
+        struct rq *rq_dest, *rq_src;
+        int ret = 0;
+
+	if (unlikely(cpu_is_offline(dest_cpu)))
+		return ret;
+
+	rq_src = cpu_rq(src_cpu);
+	rq_dest = cpu_rq(dest_cpu);
+
+	double_rq_lock(rq_src, rq_dest);
+	/* Already moved. */
+	if (task_cpu(p) != src_cpu)
+		goto out;
+	/* Affinity changed (again). */
+	if (!cpu_isset(dest_cpu, p->cpus_allowed))
+		goto out;
+
+	set_task_cpu(p, dest_cpu);
+	if (task_queued(p)) {
+		/*
+		 * Sync timestamp with rq_dest's before activating.
+		 * The same thing could be achieved by doing this step
+		 * afterwards, and pretending it was a local activate.
+		 * This way is cleaner and logically correct.
+		 */
+		p->timestamp = p->timestamp - rq_src->timestamp_last_tick
+				+ rq_dest->timestamp_last_tick;
+		deactivate_task(p, rq_src);
+		__activate_task(p, rq_dest);
+		preempt(p, rq_dest);
+	}
+
+        ret = 1;
+out:
+	double_rq_unlock(rq_src, rq_dest);
+        return ret;
+}
+
+/*
+ * migration_thread - this is a highprio system thread that performs
+ * thread migration by bumping thread off CPU then 'pushing' onto
+ * another runqueue.
+ */
+static int migration_thread(void *data)
+{
+	int cpu = (long)data;
+        struct rq *rq;
+
+	rq = cpu_rq(cpu);
+	BUG_ON(rq->migration_thread != current);
+
+	set_current_state(TASK_INTERRUPTIBLE);
+	while (!kthread_should_stop()) {
+                struct migration_req *req;
+		struct list_head *head;
+
+		try_to_freeze();
+
+		spin_lock_irq(&rq->lock);
+
+		if (cpu_is_offline(cpu)) {
+			spin_unlock_irq(&rq->lock);
+			goto wait_to_die;
+		}
+
+		if (rq->active_balance) {
+			active_load_balance(rq, cpu);
+			rq->active_balance = 0;
+		}
+
+		head = &rq->migration_queue;
+
+		if (list_empty(head)) {
+			spin_unlock_irq(&rq->lock);
+			schedule();
+			set_current_state(TASK_INTERRUPTIBLE);
+			continue;
+		}
+		req = list_entry(head->next, struct migration_req, list);
+		list_del_init(head->next);
+
+		spin_unlock(&rq->lock);
+		__migrate_task(req->task, cpu, req->dest_cpu);
+		local_irq_enable();
+
+		complete(&req->done);
+	}
+	__set_current_state(TASK_RUNNING);
+	return 0;
+
+wait_to_die:
+	/* Wait for kthread_stop */
+	set_current_state(TASK_INTERRUPTIBLE);
+	while (!kthread_should_stop()) {
+		schedule();
+		set_current_state(TASK_INTERRUPTIBLE);
+	}
+	__set_current_state(TASK_RUNNING);
+	return 0;
+}
+
+#ifdef CONFIG_HOTPLUG_CPU
+/* Figure out where task on dead CPU should go, use force if neccessary. */
+static void move_task_off_dead_cpu(int dead_cpu, struct task_struct *p)
+{
+        unsigned long flags;
+	cpumask_t mask;
+        struct rq *rq;
+        int dest_cpu;
+
+restart:
+	/* On same node? */
+	mask = node_to_cpumask(cpu_to_node(dead_cpu));
+	cpus_and(mask, mask, p->cpus_allowed);
+	dest_cpu = any_online_cpu(mask);
+
+	/* On any allowed CPU? */
+	if (dest_cpu == NR_CPUS)
+		dest_cpu = any_online_cpu(p->cpus_allowed);
+
+	/* No more Mr. Nice Guy. */
+	if (dest_cpu == NR_CPUS) {
+                rq = task_rq_lock(p, &flags);
+		cpus_setall(p->cpus_allowed);
+		dest_cpu = any_online_cpu(p->cpus_allowed);
+                task_rq_unlock(rq, &flags);
+
+		/*
+		 * Don't tell them about moving exiting tasks or
+		 * kernel threads (both mm NULL), since they never
+		 * leave kernel.
+		 */
+		if (p->mm && printk_ratelimit())
+			printk(KERN_INFO "process %d (%s) no "
+			       "longer affine to cpu%d\n",
+			       p->pid, p->comm, dead_cpu);
+	}
+        if (!__migrate_task(p, dead_cpu, dest_cpu))
+                goto restart;
+}
+
+/*
+ * While a dead CPU has no uninterruptible tasks queued at this point,
+ * it might still have a nonzero ->nr_uninterruptible counter, because
+ * for performance reasons the counter is not stricly tracking tasks to
+ * their home CPUs. So we just add the counter to another CPU's counter,
+ * to keep the global sum constant after CPU-down:
+ */
+static void migrate_nr_uninterruptible(struct rq *rq_src)
+{
+	struct rq *rq_dest = cpu_rq(any_online_cpu(CPU_MASK_ALL));
+	unsigned long flags;
+
+	local_irq_save(flags);
+	double_rq_lock(rq_src, rq_dest);
+	rq_dest->nr_uninterruptible += rq_src->nr_uninterruptible;
+	rq_src->nr_uninterruptible = 0;
+	double_rq_unlock(rq_src, rq_dest);
+	local_irq_restore(flags);
+}
+
+/* Run through task list and migrate tasks from the dead cpu. */
+static void migrate_live_tasks(int src_cpu)
+{
+	struct task_struct *p, *t;
+
+	write_lock_irq(&tasklist_lock);
+
+	do_each_thread(t, p) {
+		if (p == current)
+			continue;
+
+		if (task_cpu(p) == src_cpu)
+			move_task_off_dead_cpu(src_cpu, p);
+	} while_each_thread(t, p);
+
+	write_unlock_irq(&tasklist_lock);
+}
+
+/* Schedules idle task to be the next runnable task on current CPU.
+ * It does so by boosting its priority to highest possible and adding it to
+ * the _front_ of the runqueue. Used by CPU offline code.
+ */
+void sched_idle_next(void)
+{
+        int this_cpu = smp_processor_id();
+        struct rq *rq = cpu_rq(this_cpu);
+	struct task_struct *p = rq->idle;
+	unsigned long flags;
+
+	/* cpu has to be offline */
+	BUG_ON(cpu_online(this_cpu));
+
+        /*
+         * Strictly not necessary since rest of the CPUs are stopped by now
+         * and interrupts disabled on the current cpu.
+	 */
+	spin_lock_irqsave(&rq->lock, flags);
+
+	__setscheduler(p, SCHED_FIFO, MAX_RT_PRIO-1);
+
+        /* Add idle task to the _front_ of its priority queue: */
+	__activate_idle_task(p, rq);
+
+	spin_unlock_irqrestore(&rq->lock, flags);
+}
+
+/*
+ * Ensures that the idle task is using init_mm right before its cpu goes
+ * offline.
+ */
+void idle_task_exit(void)
+{
+	struct mm_struct *mm = current->active_mm;
+
+	BUG_ON(cpu_online(smp_processor_id()));
+
+	if (mm != &init_mm)
+		switch_mm(mm, &init_mm, current);
+	mmdrop(mm);
+}
+
+static void migrate_dead(unsigned int dead_cpu, struct task_struct *p)
+{
+	struct rq *rq = cpu_rq(dead_cpu);
+
+	/* Must be exiting, otherwise would be on tasklist. */
+	BUG_ON(p->exit_state != EXIT_ZOMBIE && p->exit_state != EXIT_DEAD);
+
+	/* Cannot have done final schedule yet: would have vanished. */
+	BUG_ON(p->flags & PF_DEAD);
+
+	get_task_struct(p);
+
+	/*
+	 * Drop lock around migration; if someone else moves it,
+	 * that's OK.  No task can be added to this CPU, so iteration is
+	 * fine.
+	 */
+	spin_unlock_irq(&rq->lock);
+	move_task_off_dead_cpu(dead_cpu, p);
+	spin_lock_irq(&rq->lock);
+
+	put_task_struct(p);
+}
+
+/* release_task() removes task from tasklist, so we won't find dead tasks. */
+static void migrate_dead_tasks(unsigned int dead_cpu)
+{
+        struct rq *rq = cpu_rq(dead_cpu);
+        unsigned int arr, i;
+
+	for (arr = 0; arr < 2; arr++) {
+		for (i = 0; i < MAX_PRIO; i++) {
+			struct list_head *list = &rq->queue[i];
+
+			while (!list_empty(list))
+                                migrate_dead(dead_cpu, list_entry(list->next,
+                                             struct task_struct, run_list));
+		}
+	}
+}
+#endif /* CONFIG_HOTPLUG_CPU */
+
+#if defined(CONFIG_DEBUG_KERNEL) && defined(CONFIG_SYSCTL)
+static struct ctl_table sd_ctl_dir[] = {
+	{1, "sched_domain", NULL, 0, 0755, NULL, },
+	{0,},
+};
+
+static struct ctl_table sd_ctl_root[] = {
+	{1, "kernel", NULL, 0, 0755, sd_ctl_dir, },
+	{0,},
+};
+
+static struct ctl_table *sd_alloc_ctl_entry(int n)
+{
+	struct ctl_table *entry =
+		kmalloc(n * sizeof(struct ctl_table), GFP_KERNEL);
+	BUG_ON(!entry);
+	memset(entry, 0, n * sizeof(struct ctl_table));
+	return entry;
+}
+
+static void set_table_entry(struct ctl_table *entry, int ctl_name,
+			const char *procname, void *data, int maxlen,
+			mode_t mode, proc_handler *proc_handler)
+{
+	entry->ctl_name = ctl_name;
+	entry->procname = procname;
+	entry->data = data;
+	entry->maxlen = maxlen;
+	entry->mode = mode;
+	entry->proc_handler = proc_handler;
+}
+
+static struct ctl_table *
+sd_alloc_ctl_domain_table(struct sched_domain *sd)
+{
+	struct ctl_table *table;
+	table = sd_alloc_ctl_entry(14);
+
+	set_table_entry(&table[0], 1, "min_interval", &sd->min_interval,
+		sizeof(long), 0644, proc_doulongvec_minmax);
+	set_table_entry(&table[1], 2, "max_interval", &sd->max_interval,
+		sizeof(long), 0644, proc_doulongvec_minmax);
+	set_table_entry(&table[2], 3, "busy_idx", &sd->busy_idx,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[3], 4, "idle_idx", &sd->idle_idx,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[4], 5, "newidle_idx", &sd->newidle_idx,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[5], 6, "wake_idx", &sd->wake_idx,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[6], 7, "forkexec_idx", &sd->forkexec_idx,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[7], 8, "busy_factor", &sd->busy_factor,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[8], 9, "imbalance_pct", &sd->imbalance_pct,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[9], 10, "cache_hot_time", &sd->cache_hot_time,
+		sizeof(long long), 0644, proc_doulongvec_minmax);
+	set_table_entry(&table[10], 11, "cache_nice_tries", &sd->cache_nice_tries,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[11], 12, "per_cpu_gain", &sd->per_cpu_gain,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	set_table_entry(&table[12], 13, "flags", &sd->flags,
+		sizeof(int), 0644, proc_dointvec_minmax);
+	return table;
+}
+
+static ctl_table *sd_alloc_ctl_cpu_table(int cpu)
+{
+	struct sched_domain *sd;
+	int domain_num = 0, i;
+	struct ctl_table *entry, *table;
+	char buf[32];
+	for_each_domain(cpu, sd)
+		domain_num++;
+	entry = table = sd_alloc_ctl_entry(domain_num + 1);
+
+	i = 0;
+	for_each_domain(cpu, sd) {
+		snprintf(buf, 32, "domain%d", i);
+		entry->ctl_name = i + 1;
+		entry->procname = kstrdup(buf, GFP_KERNEL);
+		entry->mode = 0755;
+		entry->child = sd_alloc_ctl_domain_table(sd);
+		entry++;
+		i++;
+	}
+	return table;
+}
+
+static struct ctl_table_header *sd_sysctl_header;
+static void init_sched_domain_sysctl(void)
+{
+	int i, cpu_num = num_online_cpus();
+	char buf[32];
+	struct ctl_table *entry = sd_alloc_ctl_entry(cpu_num + 1);
+
+	sd_ctl_dir[0].child = entry;
+
+	for (i = 0; i < cpu_num; i++, entry++) {
+		snprintf(buf, 32, "cpu%d", i);
+		entry->ctl_name = i + 1;
+		entry->procname = kstrdup(buf, GFP_KERNEL);
+		entry->mode = 0755;
+		entry->child = sd_alloc_ctl_cpu_table(i);
+	}
+	sd_sysctl_header = register_sysctl_table(sd_ctl_root, 0);
+}
+#else
+static void init_sched_domain_sysctl(void)
+{
+}
+#endif
+
+/*
+ * migration_call - callback that gets triggered when a CPU is added.
+ * Here we can start up the necessary migration thread for the new CPU.
+ */
+static int __cpuinit
+migration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)
+{
+	struct task_struct *p;
+        int cpu = (long)hcpu;
+	unsigned long flags;
+        struct rq *rq;
+
+	switch (action) {
+	case CPU_UP_PREPARE:
+		p = kthread_create(migration_thread, hcpu, "migration/%d",cpu);
+		if (IS_ERR(p))
+			return NOTIFY_BAD;
+		p->flags |= PF_NOFREEZE;
+		kthread_bind(p, cpu);
+		/* Must be high prio: stop_machine expects to yield to it. */
+		rq = task_rq_lock(p, &flags);
+		__setscheduler(p, SCHED_FIFO, MAX_RT_PRIO-1);
+		task_rq_unlock(rq, &flags);
+		cpu_rq(cpu)->migration_thread = p;
+		break;
+
+	case CPU_ONLINE:
+		/* Strictly unneccessary, as first user will wake it. */
+		wake_up_process(cpu_rq(cpu)->migration_thread);
+		break;
+
+#ifdef CONFIG_HOTPLUG_CPU
+	case CPU_UP_CANCELED:
+                if (!cpu_rq(cpu)->migration_thread)
+                        break;
+		/* Unbind it from offline cpu so it can run.  Fall thru. */
+		kthread_bind(cpu_rq(cpu)->migration_thread,
+			     any_online_cpu(cpu_online_map));
+		kthread_stop(cpu_rq(cpu)->migration_thread);
+		cpu_rq(cpu)->migration_thread = NULL;
+		break;
+	case CPU_DEAD:
+		migrate_live_tasks(cpu);
+		rq = cpu_rq(cpu);
+		kthread_stop(rq->migration_thread);
+		rq->migration_thread = NULL;
+		/* Idle task back to normal (off runqueue, low prio) */
+		rq = task_rq_lock(rq->idle, &flags);
+		deactivate_task(rq->idle, rq);
+		rq->idle->static_prio = MAX_PRIO;
+		__setscheduler(rq->idle, SCHED_NORMAL, 0);
+		migrate_dead_tasks(cpu);
+		task_rq_unlock(rq, &flags);
+		migrate_nr_uninterruptible(rq);
+		BUG_ON(rq->nr_running != 0);
+
+		/* No need to migrate the tasks: it was best-effort if
+		 * they didn't do lock_cpu_hotplug().  Just wake up
+		 * the requestors. */
+		spin_lock_irq(&rq->lock);
+		while (!list_empty(&rq->migration_queue)) {
+			struct migration_req *req;
+
+			req = list_entry(rq->migration_queue.next,
+					 struct migration_req, list);
+			list_del_init(&req->list);
+			complete(&req->done);
+		}
+		spin_unlock_irq(&rq->lock);
+		break;
+#endif
+	}
+	return NOTIFY_OK;
+}
+
+/* Register at highest priority so that task migration (migrate_all_tasks)
+ * happens before everything else.
+ */
+static struct notifier_block __cpuinitdata migration_notifier = {
+	.notifier_call = migration_call,
+	.priority = 10
+};
+
+int __init migration_init(void)
+{
+	void *cpu = (void *)(long)smp_processor_id();
+
+	/* Start one for the boot CPU: */
+	migration_call(&migration_notifier, CPU_UP_PREPARE, cpu);
+	migration_call(&migration_notifier, CPU_ONLINE, cpu);
+	register_cpu_notifier(&migration_notifier);
+
+	return 0;
+}
+#endif
+
+#ifdef CONFIG_SMP
+#undef SCHED_DOMAIN_DEBUG
+#ifdef SCHED_DOMAIN_DEBUG
+static void sched_domain_debug(struct sched_domain *sd, int cpu)
+{
+	int level = 0;
+
+	if (!sd) {
+		printk(KERN_DEBUG "CPU%d attaching NULL sched-domain.\n", cpu);
+		return;
+	}
+
+	printk(KERN_DEBUG "CPU%d attaching sched-domain:\n", cpu);
+
+	do {
+		int i;
+		char str[NR_CPUS];
+		struct sched_group *group = sd->groups;
+		cpumask_t groupmask;
+
+		cpumask_scnprintf(str, NR_CPUS, sd->span);
+		cpus_clear(groupmask);
+
+		printk(KERN_DEBUG);
+		for (i = 0; i < level + 1; i++)
+			printk(" ");
+		printk("domain %d: ", level);
+
+		if (!(sd->flags & SD_LOAD_BALANCE)) {
+			printk("does not load-balance\n");
+			if (sd->parent)
+				printk(KERN_ERR "ERROR: !SD_LOAD_BALANCE domain has parent");
+			break;
+		}
+
+		printk("span %s\n", str);
+
+		if (!cpu_isset(cpu, sd->span))
+			printk(KERN_ERR "ERROR: domain->span does not contain CPU%d\n", cpu);
+		if (!cpu_isset(cpu, group->cpumask))
+			printk(KERN_ERR "ERROR: domain->groups does not contain CPU%d\n", cpu);
+
+		printk(KERN_DEBUG);
+		for (i = 0; i < level + 2; i++)
+			printk(" ");
+		printk("groups:");
+		do {
+			if (!group) {
+				printk("\n");
+				printk(KERN_ERR "ERROR: group is NULL\n");
+				break;
+			}
+
+			if (!group->cpu_power) {
+				printk("\n");
+				printk(KERN_ERR "ERROR: domain->cpu_power not set\n");
+			}
+
+			if (!cpus_weight(group->cpumask)) {
+				printk("\n");
+				printk(KERN_ERR "ERROR: empty group\n");
+			}
+
+			if (cpus_intersects(groupmask, group->cpumask)) {
+				printk("\n");
+				printk(KERN_ERR "ERROR: repeated CPUs\n");
+			}
+
+			cpus_or(groupmask, groupmask, group->cpumask);
+
+			cpumask_scnprintf(str, NR_CPUS, group->cpumask);
+			printk(" %s", str);
+
+			group = group->next;
+		} while (group != sd->groups);
+		printk("\n");
+
+		if (!cpus_equal(sd->span, groupmask))
+			printk(KERN_ERR "ERROR: groups don't span domain->span\n");
+
+		level++;
+		sd = sd->parent;
+
+		if (sd) {
+			if (!cpus_subset(groupmask, sd->span))
+				printk(KERN_ERR "ERROR: parent span is not a superset of domain->span\n");
+		}
+
+	} while (sd);
+}
+#else
+# define sched_domain_debug(sd, cpu) do { } while (0)
+#endif
+
+static int sd_degenerate(struct sched_domain *sd)
+{
+	if (cpus_weight(sd->span) == 1)
+		return 1;
+
+	/* Following flags need at least 2 groups */
+	if (sd->flags & (SD_LOAD_BALANCE |
+			 SD_BALANCE_NEWIDLE |
+			 SD_BALANCE_FORK |
+			 SD_BALANCE_EXEC)) {
+		if (sd->groups != sd->groups->next)
+			return 0;
+	}
+
+	/* Following flags don't use groups */
+	if (sd->flags & (SD_WAKE_IDLE |
+			 SD_WAKE_AFFINE |
+			 SD_WAKE_BALANCE))
+		return 0;
+
+	return 1;
+}
+
+static int
+sd_parent_degenerate(struct sched_domain *sd, struct sched_domain *parent)
+{
+	unsigned long cflags = sd->flags, pflags = parent->flags;
+
+	if (sd_degenerate(parent))
+		return 1;
+
+	if (!cpus_equal(sd->span, parent->span))
+		return 0;
+
+	/* Does parent contain flags not in child? */
+	/* WAKE_BALANCE is a subset of WAKE_AFFINE */
+	if (cflags & SD_WAKE_AFFINE)
+		pflags &= ~SD_WAKE_BALANCE;
+	/* Flags needing groups don't count if only 1 group in parent */
+	if (parent->groups == parent->groups->next) {
+		pflags &= ~(SD_LOAD_BALANCE |
+				SD_BALANCE_NEWIDLE |
+				SD_BALANCE_FORK |
+				SD_BALANCE_EXEC);
+	}
+	if (~cflags & pflags)
+		return 0;
+
+	return 1;
+}
+
+/*
+ * Attach the domain 'sd' to 'cpu' as its base domain.  Callers must
+ * hold the hotplug lock.
+ */
+static void cpu_attach_domain(struct sched_domain *sd, int cpu)
+{
+	struct rq *rq = cpu_rq(cpu);
+	struct sched_domain *tmp;
+
+	/* Remove the sched domains which do not contribute to scheduling. */
+	for (tmp = sd; tmp; tmp = tmp->parent) {
+		struct sched_domain *parent = tmp->parent;
+		if (!parent)
+			break;
+		if (sd_parent_degenerate(tmp, parent))
+			tmp->parent = parent->parent;
+	}
+
+	if (sd && sd_degenerate(sd))
+		sd = sd->parent;
+
+	sched_domain_debug(sd, cpu);
+
+	rcu_assign_pointer(rq->sd, sd);
+}
+
+/* cpus with isolated domains */
+static cpumask_t __devinitdata cpu_isolated_map = CPU_MASK_NONE;
+
+/* Setup the mask of cpus configured for isolated domains */
+static int __init isolated_cpu_setup(char *str)
+{
+	int ints[NR_CPUS], i;
+
+	str = get_options(str, ARRAY_SIZE(ints), ints);
+	cpus_clear(cpu_isolated_map);
+	for (i = 1; i <= ints[0]; i++)
+		if (ints[i] < NR_CPUS)
+			cpu_set(ints[i], cpu_isolated_map);
+	return 1;
+}
+
+__setup ("isolcpus=", isolated_cpu_setup);
+
+/*
+ * init_sched_build_groups takes an array of groups, the cpumask we wish
+ * to span, and a pointer to a function which identifies what group a CPU
+ * belongs to. The return value of group_fn must be a valid index into the
+ * groups[] array, and must be >= 0 and < NR_CPUS (due to the fact that we
+ * keep track of groups covered with a cpumask_t).
+ *
+ * init_sched_build_groups will build a circular linked list of the groups
+ * covered by the given span, and will set each group's ->cpumask correctly,
+ * and ->cpu_power to 0.
+ */
+static void init_sched_build_groups(struct sched_group groups[], cpumask_t span,
+				    int (*group_fn)(int cpu))
+{
+	struct sched_group *first = NULL, *last = NULL;
+	cpumask_t covered = CPU_MASK_NONE;
+	int i;
+
+	for_each_cpu_mask(i, span) {
+		int group = group_fn(i);
+		struct sched_group *sg = &groups[group];
+		int j;
+
+		if (cpu_isset(i, covered))
+			continue;
+
+		sg->cpumask = CPU_MASK_NONE;
+		sg->cpu_power = 0;
+
+		for_each_cpu_mask(j, span) {
+			if (group_fn(j) != group)
+				continue;
+
+			cpu_set(j, covered);
+			cpu_set(j, sg->cpumask);
+		}
+		if (!first)
+			first = sg;
+		if (last)
+			last->next = sg;
+		last = sg;
+	}
+	last->next = first;
+}
+
+#define SD_NODES_PER_DOMAIN 16
+
+/*
+ * Self-tuning task migration cost measurement between source and target CPUs.
+ *
+ * This is done by measuring the cost of manipulating buffers of varying
+ * sizes. For a given buffer-size here are the steps that are taken:
+ *
+ * 1) the source CPU reads+dirties a shared buffer
+ * 2) the target CPU reads+dirties the same shared buffer
+ *
+ * We measure how long they take, in the following 4 scenarios:
+ *
+ *  - source: CPU1, target: CPU2 | cost1
+ *  - source: CPU2, target: CPU1 | cost2
+ *  - source: CPU1, target: CPU1 | cost3
+ *  - source: CPU2, target: CPU2 | cost4
+ *
+ * We then calculate the cost3+cost4-cost1-cost2 difference - this is
+ * the cost of migration.
+ *
+ * We then start off from a small buffer-size and iterate up to larger
+ * buffer sizes, in 5% steps - measuring each buffer-size separately, and
+ * doing a maximum search for the cost. (The maximum cost for a migration
+ * normally occurs when the working set size is around the effective cache
+ * size.)
+ */
+#define SEARCH_SCOPE		2
+#define MIN_CACHE_SIZE		(64*1024U)
+#define DEFAULT_CACHE_SIZE	(5*1024*1024U)
+#define ITERATIONS		1
+#define SIZE_THRESH		130
+#define COST_THRESH		130
+
+/*
+ * The migration cost is a function of 'domain distance'. Domain
+ * distance is the number of steps a CPU has to iterate down its
+ * domain tree to share a domain with the other CPU. The farther
+ * two CPUs are from each other, the larger the distance gets.
+ *
+ * Note that we use the distance only to cache measurement results,
+ * the distance value is not used numerically otherwise. When two
+ * CPUs have the same distance it is assumed that the migration
+ * cost is the same. (this is a simplification but quite practical)
+ */
+#define MAX_DOMAIN_DISTANCE 32
+
+static unsigned long long migration_cost[MAX_DOMAIN_DISTANCE] =
+		{ [ 0 ... MAX_DOMAIN_DISTANCE-1 ] =
+/*
+ * Architectures may override the migration cost and thus avoid
+ * boot-time calibration. Unit is nanoseconds. Mostly useful for
+ * virtualized hardware:
+ */
+#ifdef CONFIG_DEFAULT_MIGRATION_COST
+			CONFIG_DEFAULT_MIGRATION_COST
+#else
+			-1LL
+#endif
+};
+
+/*
+ * Allow override of migration cost - in units of microseconds.
+ * E.g. migration_cost=1000,2000,3000 will set up a level-1 cost
+ * of 1 msec, level-2 cost of 2 msecs and level3 cost of 3 msecs:
+ */
+static int __init migration_cost_setup(char *str)
+{
+	int ints[MAX_DOMAIN_DISTANCE+1], i;
+
+	str = get_options(str, ARRAY_SIZE(ints), ints);
+
+	printk("#ints: %d\n", ints[0]);
+	for (i = 1; i <= ints[0]; i++) {
+		migration_cost[i-1] = (unsigned long long)ints[i]*1000;
+		printk("migration_cost[%d]: %Ld\n", i-1, migration_cost[i-1]);
+	}
+	return 1;
+}
+
+__setup ("migration_cost=", migration_cost_setup);
+
+/*
+ * Global multiplier (divisor) for migration-cutoff values,
+ * in percentiles. E.g. use a value of 150 to get 1.5 times
+ * longer cache-hot cutoff times.
+ *
+ * (We scale it from 100 to 128 to long long handling easier.)
+ */
+
+#define MIGRATION_FACTOR_SCALE 128
+
+static unsigned int migration_factor = MIGRATION_FACTOR_SCALE;
+
+static int __init setup_migration_factor(char *str)
+{
+	get_option(&str, &migration_factor);
+	migration_factor = migration_factor * MIGRATION_FACTOR_SCALE / 100;
+	return 1;
+}
+
+__setup("migration_factor=", setup_migration_factor);
+
+/*
+ * Estimated distance of two CPUs, measured via the number of domains
+ * we have to pass for the two CPUs to be in the same span:
+ */
+static unsigned long domain_distance(int cpu1, int cpu2)
+{
+	unsigned long distance = 0;
+	struct sched_domain *sd;
+
+	for_each_domain(cpu1, sd) {
+		WARN_ON(!cpu_isset(cpu1, sd->span));
+		if (cpu_isset(cpu2, sd->span))
+			return distance;
+		distance++;
+	}
+	if (distance >= MAX_DOMAIN_DISTANCE) {
+		WARN_ON(1);
+		distance = MAX_DOMAIN_DISTANCE-1;
+	}
+
+	return distance;
+}
+
+static unsigned int migration_debug;
+
+static int __init setup_migration_debug(char *str)
+{
+	get_option(&str, &migration_debug);
+	return 1;
+}
+
+__setup("migration_debug=", setup_migration_debug);
+
+/*
+ * Maximum cache-size that the scheduler should try to measure.
+ * Architectures with larger caches should tune this up during
+ * bootup. Gets used in the domain-setup code (i.e. during SMP
+ * bootup).
+ */
+unsigned int max_cache_size;
+
+static int __init setup_max_cache_size(char *str)
+{
+	get_option(&str, &max_cache_size);
+	return 1;
+}
+
+__setup("max_cache_size=", setup_max_cache_size);
+
+/*
+ * Dirty a big buffer in a hard-to-predict (for the L2 cache) way. This
+ * is the operation that is timed, so we try to generate unpredictable
+ * cachemisses that still end up filling the L2 cache:
+ */
+static void touch_cache(void *__cache, unsigned long __size)
+{
+	unsigned long size = __size/sizeof(long), chunk1 = size/3,
+			chunk2 = 2*size/3;
+	unsigned long *cache = __cache;
+	int i;
+
+	for (i = 0; i < size/6; i += 8) {
+		switch (i % 6) {
+			case 0: cache[i]++;
+			case 1: cache[size-1-i]++;
+			case 2: cache[chunk1-i]++;
+			case 3: cache[chunk1+i]++;
+			case 4: cache[chunk2-i]++;
+			case 5: cache[chunk2+i]++;
+		}
+	}
+}
+
+/*
+ * Measure the cache-cost of one task migration. Returns in units of nsec.
+ */
+static unsigned long long
+measure_one(void *cache, unsigned long size, int source, int target)
+{
+	cpumask_t mask, saved_mask;
+	unsigned long long t0, t1, t2, t3, cost;
+
+	saved_mask = current->cpus_allowed;
+
+	/*
+	 * Flush source caches to RAM and invalidate them:
+	 */
+	sched_cacheflush();
+
+	/*
+	 * Migrate to the source CPU:
+	 */
+	mask = cpumask_of_cpu(source);
+	set_cpus_allowed(current, mask);
+	WARN_ON(smp_processor_id() != source);
+
+	/*
+	 * Dirty the working set:
+	 */
+	t0 = sched_clock();
+	touch_cache(cache, size);
+	t1 = sched_clock();
+
+	/*
+	 * Migrate to the target CPU, dirty the L2 cache and access
+	 * the shared buffer. (which represents the working set
+	 * of a migrated task.)
+	 */
+	mask = cpumask_of_cpu(target);
+	set_cpus_allowed(current, mask);
+	WARN_ON(smp_processor_id() != target);
+
+	t2 = sched_clock();
+	touch_cache(cache, size);
+	t3 = sched_clock();
+
+	cost = t1-t0 + t3-t2;
+
+	if (migration_debug >= 2)
+		printk("[%d->%d]: %8Ld %8Ld %8Ld => %10Ld.\n",
+			source, target, t1-t0, t1-t0, t3-t2, cost);
+	/*
+	 * Flush target caches to RAM and invalidate them:
+	 */
+	sched_cacheflush();
+
+	set_cpus_allowed(current, saved_mask);
+
+	return cost;
+}
+
+/*
+ * Measure a series of task migrations and return the average
+ * result. Since this code runs early during bootup the system
+ * is 'undisturbed' and the average latency makes sense.
+ *
+ * The algorithm in essence auto-detects the relevant cache-size,
+ * so it will properly detect different cachesizes for different
+ * cache-hierarchies, depending on how the CPUs are connected.
+ *
+ * Architectures can prime the upper limit of the search range via
+ * max_cache_size, otherwise the search range defaults to 20MB...64K.
+ */
+static unsigned long long
+measure_cost(int cpu1, int cpu2, void *cache, unsigned int size)
+{
+	unsigned long long cost1, cost2;
+	int i;
+
+	/*
+	 * Measure the migration cost of 'size' bytes, over an
+	 * average of 10 runs:
+	 *
+	 * (We perturb the cache size by a small (0..4k)
+	 *  value to compensate size/alignment related artifacts.
+	 *  We also subtract the cost of the operation done on
+	 *  the same CPU.)
+	 */
+	cost1 = 0;
+
+	/*
+	 * dry run, to make sure we start off cache-cold on cpu1,
+	 * and to get any vmalloc pagefaults in advance:
+	 */
+	measure_one(cache, size, cpu1, cpu2);
+	for (i = 0; i < ITERATIONS; i++)
+		cost1 += measure_one(cache, size - i*1024, cpu1, cpu2);
+
+	measure_one(cache, size, cpu2, cpu1);
+	for (i = 0; i < ITERATIONS; i++)
+		cost1 += measure_one(cache, size - i*1024, cpu2, cpu1);
+
+	/*
+	 * (We measure the non-migrating [cached] cost on both
+	 *  cpu1 and cpu2, to handle CPUs with different speeds)
+	 */
+	cost2 = 0;
+
+	measure_one(cache, size, cpu1, cpu1);
+	for (i = 0; i < ITERATIONS; i++)
+		cost2 += measure_one(cache, size - i*1024, cpu1, cpu1);
+
+	measure_one(cache, size, cpu2, cpu2);
+	for (i = 0; i < ITERATIONS; i++)
+		cost2 += measure_one(cache, size - i*1024, cpu2, cpu2);
+
+	/*
+	 * Get the per-iteration migration cost:
+	 */
+	do_div(cost1, 2*ITERATIONS);
+	do_div(cost2, 2*ITERATIONS);
+
+	return cost1 - cost2;
+}
+
+static unsigned long long measure_migration_cost(int cpu1, int cpu2)
+{
+	unsigned long long max_cost = 0, fluct = 0, avg_fluct = 0;
+	unsigned int max_size, size, size_found = 0;
+	long long cost = 0, prev_cost;
+	void *cache;
+
+	/*
+	 * Search from max_cache_size*5 down to 64K - the real relevant
+	 * cachesize has to lie somewhere inbetween.
+	 */
+	if (max_cache_size) {
+		max_size = max(max_cache_size * SEARCH_SCOPE, MIN_CACHE_SIZE);
+		size = max(max_cache_size / SEARCH_SCOPE, MIN_CACHE_SIZE);
+	} else {
+		/*
+		 * Since we have no estimation about the relevant
+		 * search range
+		 */
+		max_size = DEFAULT_CACHE_SIZE * SEARCH_SCOPE;
+		size = MIN_CACHE_SIZE;
+	}
+
+	if (!cpu_online(cpu1) || !cpu_online(cpu2)) {
+		printk("cpu %d and %d not both online!\n", cpu1, cpu2);
+		return 0;
+	}
+
+	/*
+	 * Allocate the working set:
+	 */
+	cache = vmalloc(max_size);
+	if (!cache) {
+		printk("could not vmalloc %d bytes for cache!\n", 2*max_size);
+		return 1000000; /* return 1 msec on very small boxen */
+	}
+
+	while (size <= max_size) {
+		prev_cost = cost;
+		cost = measure_cost(cpu1, cpu2, cache, size);
+
+		/*
+		 * Update the max:
+		 */
+		if (cost > 0) {
+			if (max_cost < cost) {
+				max_cost = cost;
+				size_found = size;
+			}
+		}
+		/*
+		 * Calculate average fluctuation, we use this to prevent
+		 * noise from triggering an early break out of the loop:
+		 */
+		fluct = abs(cost - prev_cost);
+		avg_fluct = (avg_fluct + fluct)/2;
+
+		if (migration_debug)
+			printk("-> [%d][%d][%7d] %3ld.%ld [%3ld.%ld] (%ld): (%8Ld %8Ld)\n",
+				cpu1, cpu2, size,
+				(long)cost / 1000000,
+				((long)cost / 100000) % 10,
+				(long)max_cost / 1000000,
+				((long)max_cost / 100000) % 10,
+				domain_distance(cpu1, cpu2),
+				cost, avg_fluct);
+
+		/*
+		 * If we iterated at least 20% past the previous maximum,
+		 * and the cost has dropped by more than 20% already,
+		 * (taking fluctuations into account) then we assume to
+		 * have found the maximum and break out of the loop early:
+		 */
+		if (size_found && (size*100 > size_found*SIZE_THRESH))
+			if (cost+avg_fluct <= 0 ||
+				max_cost*100 > (cost+avg_fluct)*COST_THRESH) {
+
+				if (migration_debug)
+					printk("-> found max.\n");
+				break;
+			}
+		/*
+		 * Increase the cachesize in 10% steps:
+		 */
+		size = size * 10 / 9;
+	}
+
+	if (migration_debug)
+		printk("[%d][%d] working set size found: %d, cost: %Ld\n",
+			cpu1, cpu2, size_found, max_cost);
+
+	vfree(cache);
+
+	/*
+	 * A task is considered 'cache cold' if at least 2 times
+	 * the worst-case cost of migration has passed.
+	 *
+	 * (this limit is only listened to if the load-balancing
+	 * situation is 'nice' - if there is a large imbalance we
+	 * ignore it for the sake of CPU utilization and
+	 * processing fairness.)
+	 */
+	return 2 * max_cost * migration_factor / MIGRATION_FACTOR_SCALE;
+}
+
+static void calibrate_migration_costs(const cpumask_t *cpu_map)
+{
+	int cpu1 = -1, cpu2 = -1, cpu, orig_cpu = raw_smp_processor_id();
+	unsigned long j0, j1, distance, max_distance = 0;
+	struct sched_domain *sd;
+
+	j0 = jiffies;
+
+	/*
+	 * First pass - calculate the cacheflush times:
+	 */
+	for_each_cpu_mask(cpu1, *cpu_map) {
+		for_each_cpu_mask(cpu2, *cpu_map) {
+			if (cpu1 == cpu2)
+				continue;
+			distance = domain_distance(cpu1, cpu2);
+			max_distance = max(max_distance, distance);
+			/*
+			 * No result cached yet?
+			 */
+			if (migration_cost[distance] == -1LL)
+				migration_cost[distance] =
+					measure_migration_cost(cpu1, cpu2);
+		}
+	}
+	/*
+	 * Second pass - update the sched domain hierarchy with
+	 * the new cache-hot-time estimations:
+	 */
+	for_each_cpu_mask(cpu, *cpu_map) {
+		distance = 0;
+		for_each_domain(cpu, sd) {
+			sd->cache_hot_time = migration_cost[distance];
+			distance++;
+		}
+	}
+	/*
+	 * Print the matrix:
+	 */
+	if (migration_debug)
+		printk("migration: max_cache_size: %d, cpu: %d MHz:\n",
+			max_cache_size,
+#ifdef CONFIG_X86
+			cpu_khz/1000
+#else
+			-1
+#endif
+		);
+	if (system_state == SYSTEM_BOOTING) {
+		printk("migration_cost=");
+		for (distance = 0; distance <= max_distance; distance++) {
+			if (distance)
+				printk(",");
+			printk("%ld", (long)migration_cost[distance] / 1000);
+		}
+		printk("\n");
+	}
+	j1 = jiffies;
+	if (migration_debug)
+		printk("migration: %ld seconds\n", (j1-j0)/HZ);
+
+	/*
+	 * Move back to the original CPU. NUMA-Q gets confused
+	 * if we migrate to another quad during bootup.
+	 */
+	if (raw_smp_processor_id() != orig_cpu) {
+		cpumask_t mask = cpumask_of_cpu(orig_cpu),
+			saved_mask = current->cpus_allowed;
+
+		set_cpus_allowed(current, mask);
+		set_cpus_allowed(current, saved_mask);
+	}
+}
+
+#ifdef CONFIG_NUMA
+
+/**
+ * find_next_best_node - find the next node to include in a sched_domain
+ * @node: node whose sched_domain we're building
+ * @used_nodes: nodes already in the sched_domain
+ *
+ * Find the next node to include in a given scheduling domain.  Simply
+ * finds the closest node not already in the @used_nodes map.
+ *
+ * Should use nodemask_t.
+ */
+static int find_next_best_node(int node, unsigned long *used_nodes)
+{
+	int i, n, val, min_val, best_node = 0;
+
+	min_val = INT_MAX;
+
+	for (i = 0; i < MAX_NUMNODES; i++) {
+		/* Start at @node */
+		n = (node + i) % MAX_NUMNODES;
+
+		if (!nr_cpus_node(n))
+			continue;
+
+		/* Skip already used nodes */
+		if (test_bit(n, used_nodes))
+			continue;
+
+		/* Simple min distance search */
+		val = node_distance(node, n);
+
+		if (val < min_val) {
+			min_val = val;
+			best_node = n;
+		}
+	}
+
+	set_bit(best_node, used_nodes);
+	return best_node;
+}
+
+/**
+ * sched_domain_node_span - get a cpumask for a node's sched_domain
+ * @node: node whose cpumask we're constructing
+ * @size: number of nodes to include in this span
+ *
+ * Given a node, construct a good cpumask for its sched_domain to span.  It
+ * should be one that prevents unnecessary balancing, but also spreads tasks
+ * out optimally.
+ */
+static cpumask_t sched_domain_node_span(int node)
+{
+	DECLARE_BITMAP(used_nodes, MAX_NUMNODES);
+        cpumask_t span, nodemask;
+        int i;
+
+	cpus_clear(span);
+	bitmap_zero(used_nodes, MAX_NUMNODES);
+
+	nodemask = node_to_cpumask(node);
+	cpus_or(span, span, nodemask);
+	set_bit(node, used_nodes);
+
+	for (i = 1; i < SD_NODES_PER_DOMAIN; i++) {
+        	int next_node = find_next_best_node(node, used_nodes);
+	
+        	nodemask = node_to_cpumask(next_node);
+		cpus_or(span, span, nodemask);
+	}
+
+	return span;
+}
+#endif
+
+int sched_smt_power_savings = 0, sched_mc_power_savings = 0;
+
+/*
+ * SMT sched-domains:
+ */
+#ifdef CONFIG_SCHED_SMT
+static DEFINE_PER_CPU(struct sched_domain, cpu_domains);
+static struct sched_group sched_group_cpus[NR_CPUS];
+
+static int cpu_to_cpu_group(int cpu)
+{
+	return cpu;
+}
+#endif
+
+/*
+ * multi-core sched-domains:
+ */
+#ifdef CONFIG_SCHED_MC
+static DEFINE_PER_CPU(struct sched_domain, core_domains);
+static struct sched_group *sched_group_core_bycpu[NR_CPUS];
+#endif
+
+#if defined(CONFIG_SCHED_MC) && defined(CONFIG_SCHED_SMT)
+static int cpu_to_core_group(int cpu)
+{
+	return first_cpu(cpu_sibling_map[cpu]);
+}
+#elif defined(CONFIG_SCHED_MC)
+static int cpu_to_core_group(int cpu)
+{
+	return cpu;
+}
+#endif
+
+static DEFINE_PER_CPU(struct sched_domain, phys_domains);
+static struct sched_group *sched_group_phys_bycpu[NR_CPUS];
+
+static int cpu_to_phys_group(int cpu)
+{
+#ifdef CONFIG_SCHED_MC
+	cpumask_t mask = cpu_coregroup_map(cpu);
+	return first_cpu(mask);
+#elif defined(CONFIG_SCHED_SMT)
+	return first_cpu(cpu_sibling_map[cpu]);
+#else
+	return cpu;
+#endif
+}
+
+#ifdef CONFIG_NUMA
+/*
+ * The init_sched_build_groups can't handle what we want to do with node
+ * groups, so roll our own. Now each node has its own list of groups which
+ * gets dynamically allocated.
+ */
+static DEFINE_PER_CPU(struct sched_domain, node_domains);
+static struct sched_group **sched_group_nodes_bycpu[NR_CPUS];
+
+static DEFINE_PER_CPU(struct sched_domain, allnodes_domains);
+static struct sched_group *sched_group_allnodes_bycpu[NR_CPUS];
+
+static int cpu_to_allnodes_group(int cpu)
+{
+	return cpu_to_node(cpu);
+}
+static void init_numa_sched_groups_power(struct sched_group *group_head)
+{
+	struct sched_group *sg = group_head;
+	int j;
+
+	if (!sg)
+		return;
+next_sg:
+	for_each_cpu_mask(j, sg->cpumask) {
+		struct sched_domain *sd;
+
+		sd = &per_cpu(phys_domains, j);
+		if (j != first_cpu(sd->groups->cpumask)) {
+			/*
+			 * Only add "power" once for each
+			 * physical package.
+			 */
+			continue;
+		}
+
+		sg->cpu_power += sd->groups->cpu_power;
+	}
+	sg = sg->next;
+	if (sg != group_head)
+		goto next_sg;
+}
+#endif
+
+/* Free memory allocated for various sched_group structures */
+static void free_sched_groups(const cpumask_t *cpu_map)
+{
+	int cpu;
+#ifdef CONFIG_NUMA
+	int i;
+
+	for_each_cpu_mask(cpu, *cpu_map) {
+		struct sched_group *sched_group_allnodes
+			= sched_group_allnodes_bycpu[cpu];
+		struct sched_group **sched_group_nodes
+			= sched_group_nodes_bycpu[cpu];
+
+		if (sched_group_allnodes) {
+			kfree(sched_group_allnodes);
+			sched_group_allnodes_bycpu[cpu] = NULL;
+		}
+
+		if (!sched_group_nodes)
+			continue;
+
+		for (i = 0; i < MAX_NUMNODES; i++) {
+			cpumask_t nodemask = node_to_cpumask(i);
+			struct sched_group *oldsg, *sg = sched_group_nodes[i];
+
+			cpus_and(nodemask, nodemask, *cpu_map);
+			if (cpus_empty(nodemask))
+				continue;
+
+			if (sg == NULL)
+				continue;
+			sg = sg->next;
+next_sg:
+			oldsg = sg;
+			sg = sg->next;
+			kfree(oldsg);
+			if (oldsg != sched_group_nodes[i])
+				goto next_sg;
+		}
+		kfree(sched_group_nodes);
+		sched_group_nodes_bycpu[cpu] = NULL;
+	}
+#endif
+	for_each_cpu_mask(cpu, *cpu_map) {
+		if (sched_group_phys_bycpu[cpu]) {
+			kfree(sched_group_phys_bycpu[cpu]);
+			sched_group_phys_bycpu[cpu] = NULL;
+		}
+#ifdef CONFIG_SCHED_MC
+		if (sched_group_core_bycpu[cpu]) {
+			kfree(sched_group_core_bycpu[cpu]);
+			sched_group_core_bycpu[cpu] = NULL;
+		}
+#endif
+	}
+}
+
+/*
+ * Build sched domains for a given set of cpus and attach the sched domains
+ * to the individual cpus
+ */
+static int build_sched_domains(const cpumask_t *cpu_map)
+{
+	int i;
+	struct sched_group *sched_group_phys = NULL;
+#ifdef CONFIG_SCHED_MC
+	struct sched_group *sched_group_core = NULL;
+#endif
+#ifdef CONFIG_NUMA
+	struct sched_group **sched_group_nodes = NULL;
+	struct sched_group *sched_group_allnodes = NULL;
+
+	/*
+	 * Allocate the per-node list of sched groups
+	 */
+	sched_group_nodes = kzalloc(sizeof(struct sched_group*)*MAX_NUMNODES,
+					   GFP_KERNEL);
+	if (!sched_group_nodes) {
+		printk(KERN_WARNING "Can not alloc sched group node list\n");
+		return -ENOMEM;
+	}
+	sched_group_nodes_bycpu[first_cpu(*cpu_map)] = sched_group_nodes;
+#endif
+
+	/*
+	 * Set up domains for cpus specified by the cpu_map.
+	 */
+	for_each_cpu_mask(i, *cpu_map) {
+		int group;
+		struct sched_domain *sd = NULL, *p;
+		cpumask_t nodemask = node_to_cpumask(cpu_to_node(i));
+
+		cpus_and(nodemask, nodemask, *cpu_map);
+
+#ifdef CONFIG_NUMA
+		if (cpus_weight(*cpu_map)
+				> SD_NODES_PER_DOMAIN*cpus_weight(nodemask)) {
+			if (!sched_group_allnodes) {
+				sched_group_allnodes
+					= kmalloc(sizeof(struct sched_group)
+							* MAX_NUMNODES,
+						  GFP_KERNEL);
+				if (!sched_group_allnodes) {
+					printk(KERN_WARNING
+					"Can not alloc allnodes sched group\n");
+					goto error;
+				}
+				sched_group_allnodes_bycpu[i]
+						= sched_group_allnodes;
+			}
+			sd = &per_cpu(allnodes_domains, i);
+			*sd = SD_ALLNODES_INIT;
+			sd->span = *cpu_map;
+			group = cpu_to_allnodes_group(i);
+			sd->groups = &sched_group_allnodes[group];
+			p = sd;
+		} else
+			p = NULL;
+
+		sd = &per_cpu(node_domains, i);
+		*sd = SD_NODE_INIT;
+		sd->span = sched_domain_node_span(cpu_to_node(i));
+		sd->parent = p;
+		cpus_and(sd->span, sd->span, *cpu_map);
+#endif
+
+		if (!sched_group_phys) {
+			sched_group_phys
+				= kmalloc(sizeof(struct sched_group) * NR_CPUS,
+					  GFP_KERNEL);
+			if (!sched_group_phys) {
+				printk (KERN_WARNING "Can not alloc phys sched"
+						     "group\n");
+				goto error;
+			}
+			sched_group_phys_bycpu[i] = sched_group_phys;
+		}
+
+		p = sd;
+		sd = &per_cpu(phys_domains, i);
+		group = cpu_to_phys_group(i);
+		*sd = SD_CPU_INIT;
+		sd->span = nodemask;
+		sd->parent = p;
+		sd->groups = &sched_group_phys[group];
+
+#ifdef CONFIG_SCHED_MC
+		if (!sched_group_core) {
+			sched_group_core
+				= kmalloc(sizeof(struct sched_group) * NR_CPUS,
+					  GFP_KERNEL);
+			if (!sched_group_core) {
+				printk (KERN_WARNING "Can not alloc core sched"
+						     "group\n");
+				goto error;
+			}
+			sched_group_core_bycpu[i] = sched_group_core;
+		}
+
+		p = sd;
+		sd = &per_cpu(core_domains, i);
+		group = cpu_to_core_group(i);
+		*sd = SD_MC_INIT;
+		sd->span = cpu_coregroup_map(i);
+		cpus_and(sd->span, sd->span, *cpu_map);
+		sd->parent = p;
+		sd->groups = &sched_group_core[group];
+#endif
+
+#ifdef CONFIG_SCHED_SMT
+		p = sd;
+		sd = &per_cpu(cpu_domains, i);
+		group = cpu_to_cpu_group(i);
+		*sd = SD_SIBLING_INIT;
+		sd->span = cpu_sibling_map[i];
+		cpus_and(sd->span, sd->span, *cpu_map);
+		sd->parent = p;
+		sd->groups = &sched_group_cpus[group];
+#endif
+	}
+
+#ifdef CONFIG_SCHED_SMT
+	/* Set up CPU (sibling) groups */
+	for_each_cpu_mask(i, *cpu_map) {
+		cpumask_t this_sibling_map = cpu_sibling_map[i];
+		cpus_and(this_sibling_map, this_sibling_map, *cpu_map);
+		if (i != first_cpu(this_sibling_map))
+			continue;
+
+		init_sched_build_groups(sched_group_cpus, this_sibling_map,
+						&cpu_to_cpu_group);
+	}
+#endif
+
+#ifdef CONFIG_SCHED_MC
+	/* Set up multi-core groups */
+	for_each_cpu_mask(i, *cpu_map) {
+		cpumask_t this_core_map = cpu_coregroup_map(i);
+		cpus_and(this_core_map, this_core_map, *cpu_map);
+		if (i != first_cpu(this_core_map))
+			continue;
+		init_sched_build_groups(sched_group_core, this_core_map,
+					&cpu_to_core_group);
+	}
+#endif
+
+
+	/* Set up physical groups */
+	for (i = 0; i < MAX_NUMNODES; i++) {
+		cpumask_t nodemask = node_to_cpumask(i);
+
+		cpus_and(nodemask, nodemask, *cpu_map);
+		if (cpus_empty(nodemask))
+			continue;
+
+		init_sched_build_groups(sched_group_phys, nodemask,
+						&cpu_to_phys_group);
+	}
+
+#ifdef CONFIG_NUMA
+	/* Set up node groups */
+	if (sched_group_allnodes)
+		init_sched_build_groups(sched_group_allnodes, *cpu_map,
+					&cpu_to_allnodes_group);
+
+	for (i = 0; i < MAX_NUMNODES; i++) {
+		/* Set up node groups */
+		struct sched_group *sg, *prev;
+		cpumask_t nodemask = node_to_cpumask(i);
+		cpumask_t domainspan;
+		cpumask_t covered = CPU_MASK_NONE;
+		int j;
+
+		cpus_and(nodemask, nodemask, *cpu_map);
+		if (cpus_empty(nodemask)) {
+			sched_group_nodes[i] = NULL;
+			continue;
+		}
+
+		domainspan = sched_domain_node_span(i);
+		cpus_and(domainspan, domainspan, *cpu_map);
+
+		sg = kmalloc_node(sizeof(struct sched_group), GFP_KERNEL, i);
+		if (!sg) {
+			printk(KERN_WARNING "Can not alloc domain group for "
+				"node %d\n", i);
+			goto error;
+		}
+		sched_group_nodes[i] = sg;
+		for_each_cpu_mask(j, nodemask) {
+			struct sched_domain *sd;
+			sd = &per_cpu(node_domains, j);
+			sd->groups = sg;
+		}
+		sg->cpu_power = 0;
+		sg->cpumask = nodemask;
+		sg->next = sg;
+		cpus_or(covered, covered, nodemask);
+		prev = sg;
+
+		for (j = 0; j < MAX_NUMNODES; j++) {
+			cpumask_t tmp, notcovered;
+			int n = (i + j) % MAX_NUMNODES;
+
+			cpus_complement(notcovered, covered);
+			cpus_and(tmp, notcovered, *cpu_map);
+			cpus_and(tmp, tmp, domainspan);
+			if (cpus_empty(tmp))
+				break;
+
+			nodemask = node_to_cpumask(n);
+			cpus_and(tmp, tmp, nodemask);
+			if (cpus_empty(tmp))
+				continue;
+
+			sg = kmalloc_node(sizeof(struct sched_group),
+					  GFP_KERNEL, i);
+			if (!sg) {
+				printk(KERN_WARNING
+				"Can not alloc domain group for node %d\n", j);
+				goto error;
+			}
+			sg->cpu_power = 0;
+			sg->cpumask = tmp;
+			sg->next = prev->next;
+			cpus_or(covered, covered, tmp);
+			prev->next = sg;
+			prev = sg;
+		}
+	}
+#endif
+
+	/* Calculate CPU power for physical packages and nodes */
+#ifdef CONFIG_SCHED_SMT
+	for_each_cpu_mask(i, *cpu_map) {
+		struct sched_domain *sd;
+		sd = &per_cpu(cpu_domains, i);
+		sd->groups->cpu_power = SCHED_LOAD_SCALE;
+        }
+#endif
+#ifdef CONFIG_SCHED_MC
+        for_each_cpu_mask(i, *cpu_map) {
+                int power;
+                struct sched_domain *sd;
+                sd = &per_cpu(core_domains, i);
+                if (sched_smt_power_savings)
+                        power = SCHED_LOAD_SCALE * cpus_weight(sd->groups->cpumask);
+                else
+                        power = SCHED_LOAD_SCALE + (cpus_weight(sd->groups->cpumask)-1)
+                                            * SCHED_LOAD_SCALE / 10;
+                sd->groups->cpu_power = power;
+        }
+#endif
+        for_each_cpu_mask(i, *cpu_map) {
+                struct sched_domain *sd;
+#ifdef CONFIG_SCHED_MC
+                sd = &per_cpu(phys_domains, i);
+                if (i != first_cpu(sd->groups->cpumask))
+                        continue;
+
+                sd->groups->cpu_power = 0;
+                if (sched_mc_power_savings || sched_smt_power_savings) {
+                        int j;
+
+                        for_each_cpu_mask(j, sd->groups->cpumask) {
+                                struct sched_domain *sd1;
+                                sd1 = &per_cpu(core_domains, j);
+                                /*
+                                 * for each core we will add once
+                                 * to the group in physical domain
+                                 */
+                                if (j != first_cpu(sd1->groups->cpumask))
+                                        continue;
+
+                                if (sched_smt_power_savings)
+                                        sd->groups->cpu_power += sd1->groups->cpu_power;
+                                else
+                                        sd->groups->cpu_power += SCHED_LOAD_SCALE;
+                        }
+                } else
+                        /*
+                         * This has to be < 2 * SCHED_LOAD_SCALE
+                         * Lets keep it SCHED_LOAD_SCALE, so that
+                         * while calculating NUMA group's cpu_power
+                         * we can simply do
+                         *  numa_group->cpu_power += phys_group->cpu_power;
+                         *
+                         * See "only add power once for each physical pkg"
+                         * comment below
+                         */
+                        sd->groups->cpu_power = SCHED_LOAD_SCALE;
+#else
+                int power;
+                sd = &per_cpu(phys_domains, i);
+                if (sched_smt_power_savings)
+                        power = SCHED_LOAD_SCALE * cpus_weight(sd->groups->cpumask);
+                else
+                        power = SCHED_LOAD_SCALE;
+                sd->groups->cpu_power = power;
+#endif
+	}
+
+#ifdef CONFIG_NUMA
+	for (i = 0; i < MAX_NUMNODES; i++)
+		init_numa_sched_groups_power(sched_group_nodes[i]);
+
+	init_numa_sched_groups_power(sched_group_allnodes);
+#endif
+
+	/* Attach the domains */
+	for_each_cpu_mask(i, *cpu_map) {
+		struct sched_domain *sd;
+#ifdef CONFIG_SCHED_SMT
+		sd = &per_cpu(cpu_domains, i);
+#elif defined(CONFIG_SCHED_MC)
+		sd = &per_cpu(core_domains, i);
+#else
+		sd = &per_cpu(phys_domains, i);
+#endif
+		cpu_attach_domain(sd, i);
+	}
+	/*
+	 * Tune cache-hot values:
+	 */
+	calibrate_migration_costs(cpu_map);
+
+	return 0;
+
+error:
+	free_sched_groups(cpu_map);
+	return -ENOMEM;
+}
+/*
+ * Set up scheduler domains and groups.  Callers must hold the hotplug lock.
+ */
+static int arch_init_sched_domains(const cpumask_t *cpu_map)
+{
+	cpumask_t cpu_default_map;
+	int err;
+
+	/*
+	 * Setup mask for cpus without special case scheduling requirements.
+	 * For now this just excludes isolated cpus, but could be used to
+	 * exclude other special cases in the future.
+	 */
+	cpus_andnot(cpu_default_map, *cpu_map, cpu_isolated_map);
+
+	err = build_sched_domains(&cpu_default_map);
+
+	return err;
+}
+
+static void arch_destroy_sched_domains(const cpumask_t *cpu_map)
+{
+	free_sched_groups(cpu_map);
+}
+
+/*
+ * Detach sched domains from a group of cpus specified in cpu_map
+ * These cpus will now be attached to the NULL domain
+ */
+static void detach_destroy_domains(const cpumask_t *cpu_map)
+{
+	int i;
+
+	for_each_cpu_mask(i, *cpu_map)
+		cpu_attach_domain(NULL, i);
+	synchronize_sched();
+	arch_destroy_sched_domains(cpu_map);
+}
+
+/*
+ * Partition sched domains as specified by the cpumasks below.
+ * This attaches all cpus from the cpumasks to the NULL domain,
+ * waits for a RCU quiescent period, recalculates sched
+ * domain information and then attaches them back to the
+ * correct sched domains
+ * Call with hotplug lock held
+ */
+int partition_sched_domains(cpumask_t *partition1, cpumask_t *partition2)
+{
+	cpumask_t change_map;
+	int err = 0;
+
+	cpus_and(*partition1, *partition1, cpu_online_map);
+	cpus_and(*partition2, *partition2, cpu_online_map);
+	cpus_or(change_map, *partition1, *partition2);
+
+	/* Detach sched domains from all of the affected cpus */
+	detach_destroy_domains(&change_map);
+	if (!cpus_empty(*partition1))
+		err = build_sched_domains(partition1);
+	if (!err && !cpus_empty(*partition2))
+		err = build_sched_domains(partition2);
+
+	return err;
+}
+
+#if defined(CONFIG_SCHED_MC) || defined(CONFIG_SCHED_SMT)
+int arch_reinit_sched_domains(void)
+{
+        int err;
+
+        lock_cpu_hotplug();
+        detach_destroy_domains(&cpu_online_map);
+        err = arch_init_sched_domains(&cpu_online_map);
+        unlock_cpu_hotplug();
+
+        return err;
+}
+
+static ssize_t sched_power_savings_store(const char *buf, size_t count, int smt)
+{
+        int ret;
+
+        if (buf[0] != '0' && buf[0] != '1')
+                return -EINVAL;
+
+        if (smt)
+                sched_smt_power_savings = (buf[0] == '1');
+        else
+                sched_mc_power_savings = (buf[0] == '1');
+
+        ret = arch_reinit_sched_domains();
+
+        return ret ? ret : count;
+}
+int sched_create_sysfs_power_savings_entries(struct sysdev_class *cls)
+{
+        int err = 0;
+#ifdef CONFIG_SCHED_SMT
+        if (smt_capable())
+                err = sysfs_create_file(&cls->kset.kobj,
+                                        &attr_sched_smt_power_savings.attr);
+#endif
+#ifdef CONFIG_SCHED_MC
+        if (!err && mc_capable())
+                err = sysfs_create_file(&cls->kset.kobj,
+                                        &attr_sched_mc_power_savings.attr);
+#endif
+        return err;
+}
+#endif
+
+#ifdef CONFIG_SCHED_MC
+static ssize_t sched_mc_power_savings_show(struct sys_device *dev, char *page)
+{
+        return sprintf(page, "%u\n", sched_mc_power_savings);
+}
+static ssize_t sched_mc_power_savings_store(struct sys_device *dev,
+                                            const char *buf, size_t count)
+{
+        return sched_power_savings_store(buf, count, 0);
+}
+SYSDEV_ATTR(sched_mc_power_savings, 0644, sched_mc_power_savings_show,
+            sched_mc_power_savings_store);
+#endif
+
+#ifdef CONFIG_SCHED_SMT
+static ssize_t sched_smt_power_savings_show(struct sys_device *dev, char *page)
+{
+        return sprintf(page, "%u\n", sched_smt_power_savings);
+}
+static ssize_t sched_smt_power_savings_store(struct sys_device *dev,
+                                             const char *buf, size_t count)
+{
+        return sched_power_savings_store(buf, count, 1);
+}
+SYSDEV_ATTR(sched_smt_power_savings, 0644, sched_smt_power_savings_show,
+            sched_smt_power_savings_store);
+#endif
+
+#ifdef CONFIG_HOTPLUG_CPU
+/*
+ * Force a reinitialization of the sched domains hierarchy.  The domains
+ * and groups cannot be updated in place without racing with the balancing
+ * code, so we temporarily attach all running cpus to the NULL domain
+ * which will prevent rebalancing while the sched domains are recalculated.
+ */
+static int update_sched_domains(struct notifier_block *nfb,
+				unsigned long action, void *hcpu)
+{
+	switch (action) {
+	case CPU_UP_PREPARE:
+	case CPU_DOWN_PREPARE:
+		detach_destroy_domains(&cpu_online_map);
+		return NOTIFY_OK;
+
+	case CPU_UP_CANCELED:
+	case CPU_DOWN_FAILED:
+	case CPU_ONLINE:
+	case CPU_DEAD:
+		/*
+		 * Fall through and re-initialise the domains.
+		 */
+		break;
+	default:
+		return NOTIFY_DONE;
+	}
+
+	/* The hotplug lock is already held by cpu_up/cpu_down */
+	arch_init_sched_domains(&cpu_online_map);
+
+	return NOTIFY_OK;
+}
+#endif
+
+void __init sched_init_smp(void)
+{
+	lock_cpu_hotplug();
+	arch_init_sched_domains(&cpu_online_map);
+	unlock_cpu_hotplug();
+	/* XXX: Theoretical race here - CPU may be hotplugged now */
+	hotcpu_notifier(update_sched_domains, 0);
+	init_sched_domain_sysctl();
+}
+#else
+void __init sched_init_smp(void)
+{
+}
+#endif /* CONFIG_SMP */
+
+int in_sched_functions(unsigned long addr)
+{
+	/* Linker adds these: start and end of __sched functions */
+	extern char __sched_text_start[], __sched_text_end[];
+
+	return in_lock_functions(addr) ||
+		(addr >= (unsigned long)__sched_text_start
+		&& addr < (unsigned long)__sched_text_end);
+}
+
+void __init sched_init(void)
+{
+	int i, j;
+
+	for_each_possible_cpu(i) {
+                struct rq *rq;
+		rq = cpu_rq(i);
+                spin_lock_init(&rq->lock);
+                lockdep_set_class(&rq->lock, &rq->rq_lock_key);
+                rq->nr_running = rq->cache_ticks = rq->preempted = 0;
+
+#ifdef CONFIG_SMP
+		rq->sd = NULL;
+		for (j = 1; j < 3; j++)
+			rq->cpu_load[j] = 0;
+		rq->active_balance = 0;
+		rq->push_cpu = 0;
+		rq->migration_thread = NULL;
+		INIT_LIST_HEAD(&rq->migration_queue);
+#endif
+		atomic_set(&rq->nr_iowait, 0);
+		for (j = 0; j < MAX_PRIO; j++)
+			INIT_LIST_HEAD(&rq->queue[j]);
+		memset(rq->bitmap, 0, BITS_TO_LONGS(MAX_PRIO)*sizeof(long));
+		/* delimiter for bitsearch */
+		__set_bit(MAX_PRIO, rq->bitmap);
+	}
+
+	set_load_weight(&init_task);
+	/*
+	 * The boot idle thread does lazy MMU switching as well:
+	 */
+	atomic_inc(&init_mm.mm_count);
+	enter_lazy_tlb(&init_mm, current);
+
+	/*
+	 * Make us the idle thread. Technically, schedule() should not be
+	 * called from this thread, however somewhere below it might be,
+	 * but because we are the idle thread, we just pick up running again
+	 * when this runqueue becomes "idle".
+	 */
+	init_idle(current, smp_processor_id());
+}
+
+#ifdef CONFIG_DEBUG_SPINLOCK_SLEEP
+void __might_sleep(char *file, int line)
+{
+#ifdef in_atomic
+	static unsigned long prev_jiffy;	/* ratelimiting */
+
+	if ((in_atomic() || irqs_disabled()) &&
+	    system_state == SYSTEM_RUNNING && !oops_in_progress) {
+		if (time_before(jiffies, prev_jiffy + HZ) && prev_jiffy)
+			return;
+		prev_jiffy = jiffies;
+		printk(KERN_ERR "BUG: sleeping function called from invalid"
+				" context at %s:%d\n", file, line);
+		printk("in_atomic():%d, irqs_disabled():%d\n",
+			in_atomic(), irqs_disabled());
+		dump_stack();
+	}
+#endif
+}
+EXPORT_SYMBOL(__might_sleep);
+#endif
+
+#ifdef CONFIG_MAGIC_SYSRQ
+void normalize_rt_tasks(void)
+{
+	struct task_struct *p;
+	unsigned long flags;
+	struct rq *rq;
+	int queued;
+
+	read_lock_irq(&tasklist_lock);
+	for_each_process(p) {
+		if (!rt_task(p))
+			continue;
+
+		spin_lock_irqsave(&p->pi_lock, flags);
+		rq = __task_rq_lock(p);
+
+		if ((queued = task_queued(p)))
+			deactivate_task(p, task_rq(p));
+		__setscheduler(p, SCHED_NORMAL, 0);
+		if (queued) {
+			__activate_task(p, task_rq(p));
+			resched_task(rq->curr);
+		}
+
+		__task_rq_unlock(rq);
+		spin_unlock_irqrestore(&p->pi_lock, flags);
+	}
+	read_unlock_irq(&tasklist_lock);
+}
+
+#endif /* CONFIG_MAGIC_SYSRQ */
+
+#ifdef CONFIG_IA64
+/*
+ * These functions are only useful for the IA64 MCA handling.
+ *
+ * They can only be called when the whole system has been
+ * stopped - every CPU needs to be quiescent, and no scheduling
+ * activity can take place. Using them for anything else would
+ * be a serious bug, and as a result, they aren't even visible
+ * under any other configuration.
+ */
+
+/**
+ * curr_task - return the current task for a given cpu.
+ * @cpu: the processor in question.
+ *
+ * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!
+ */
+struct task_struct *curr_task(int cpu)
+{
+	return cpu_curr(cpu);
+}
+
+/**
+ * set_curr_task - set the current task for a given cpu.
+ * @cpu: the processor in question.
+ * @p: the task pointer to set.
+ *
+ * Description: This function must only be used when non-maskable interrupts
+ * are serviced on a separate stack.  It allows the architecture to switch the
+ * notion of the current task on a cpu in a non-blocking manner.  This function
+ * must be called with all CPU's synchronized, and interrupts disabled, the
+ * and caller must save the original value of the current task (see
+ * curr_task() above) and restore that value before reenabling interrupts and
+ * re-starting the system.
+ *
+ * ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED!
+ */
+void set_curr_task(int cpu, struct task_struct *p)
+{
+	cpu_curr(cpu) = p;
+}
+
+#endif
+
+/* Staircase Scheduler Code End. */
diff -urN oldtree/kernel/sysctl.c newtree/kernel/sysctl.c
--- oldtree/kernel/sysctl.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/kernel/sysctl.c	2006-07-12 19:02:29.000000000 -0400
@@ -99,6 +99,9 @@
 #ifdef CONFIG_KMOD
 extern char modprobe_path[];
 #endif
+#ifdef CONFIG_FB_SPLASH
+extern char fbsplash_path[];
+#endif
 #ifdef CONFIG_CHR_DEV_SG
 extern int sg_big_buff;
 #endif
@@ -473,6 +476,17 @@
 		.strategy	= &sysctl_string,
 	},
 #endif
+#ifdef CONFIG_FB_SPLASH
+	{
+		.ctl_name	= KERN_FBSPLASH,
+		.procname	= "fbsplash",
+		.data		= &fbsplash_path,
+		.maxlen		= KMOD_PATH_LEN,
+		.mode		= 0644,
+		.proc_handler	= &proc_dostring,
+		.strategy	= &sysctl_string,
+	},
+#endif
 #ifdef CONFIG_CHR_DEV_SG
 	{
 		.ctl_name	= KERN_SG_BIG_BUFF,
@@ -687,6 +701,24 @@
 		.mode		= 0444,
 		.proc_handler	= &proc_dointvec,
 	},
+#ifdef CONFIG_STAIRCASE
+        {
+                .ctl_name       = KERN_INTERACTIVE,
+                .procname       = "interactive",
+                .data           = &sched_interactive,
+                .maxlen         = sizeof (int),
+                .mode           = 0644,
+                .proc_handler   = &proc_dointvec,
+        },
+        {
+                .ctl_name       = KERN_COMPUTE,
+                .procname       = "compute",
+                .data           = &sched_compute,
+                .maxlen         = sizeof (int),
+                .mode           = 0644,
+                .proc_handler   = &proc_dointvec,
+        },
+#endif
 #if defined(CONFIG_X86_LOCAL_APIC) && defined(CONFIG_X86)
 	{
 		.ctl_name       = KERN_UNKNOWN_NMI_PANIC,
@@ -873,16 +905,24 @@
 		.proc_handler	= &proc_dointvec,
 	},
 	{
-		.ctl_name	= VM_SWAPPINESS,
-		.procname	= "swappiness",
-		.data		= &vm_swappiness,
-		.maxlen		= sizeof(vm_swappiness),
+		.ctl_name	= VM_MAPPED,
+		.procname	= "mapped",
+		.data		= &vm_mapped,
+		.maxlen		= sizeof(vm_mapped),
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec_minmax,
 		.strategy	= &sysctl_intvec,
 		.extra1		= &zero,
 		.extra2		= &one_hundred,
 	},
+	{
+		.ctl_name	= VM_HARDMAPLIMIT,
+		.procname	= "hardmaplimit",
+		.data		= &vm_hardmaplimit,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
 #ifdef CONFIG_HUGETLB_PAGE
 	 {
 		.ctl_name	= VM_HUGETLB_PAGES,
diff -urN oldtree/lib/Kconfig.debug newtree/lib/Kconfig.debug
--- oldtree/lib/Kconfig.debug	2006-07-05 10:06:57.000000000 -0400
+++ newtree/lib/Kconfig.debug	2006-07-12 19:01:43.000000000 -0400
@@ -399,6 +399,22 @@
 
 	  See Documentation/synchro-test.txt.
 
+config HIDE_FALSE_POSITIVES
+       bool "Hide gcc false positives of unititialized variables"
+       depends on DEBUG_KERNEL
+       help
+         gcc sometimes shows that a variable is uninitialized when the logic
+         actually does initialize it before use.  The kernel has lots of these
+         warnings.  This option hides those warnings that were actually looked
+         at by a human, and decided (right or wrong) that this variable is indeed
+         properly initialized.
+
+         If you are a developer that doesn't care about these warnings, and trust
+         that the one that marked these variables, did so correctly.  Then you
+         may turn on this option, to look for your own mistakes.
+
+         Otherwise, say N
+
 config RCU_TORTURE_TEST
 	tristate "torture tests for RCU"
 	depends on DEBUG_KERNEL
diff -urN oldtree/lib/statistic.c newtree/lib/statistic.c
--- oldtree/lib/statistic.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/lib/statistic.c	2006-07-12 19:03:38.000000000 -0400
@@ -666,7 +666,7 @@
 		struct file *file, struct statistic_interface **interface,
 		struct statistic_file_private **private)
 {
-	*interface = inode->u.generic_ip;
+	*interface = inode->i_private;
 	BUG_ON(!interface);
 	*private = kzalloc(sizeof(struct statistic_file_private), GFP_KERNEL);
 	if (unlikely(!*private))
@@ -749,7 +749,7 @@
 
 static int statistic_def_close(struct inode *inode, struct file *file)
 {
-	struct statistic_interface *interface = inode->u.generic_ip;
+	struct statistic_interface *interface = inode->i_private;
 	struct statistic_file_private *private = file->private_data;
 	struct sgrb_seg *seg, *seg_nl;
 	int offset;
diff -urN oldtree/lib/vsprintf.c newtree/lib/vsprintf.c
--- oldtree/lib/vsprintf.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/lib/vsprintf.c	2006-07-12 19:03:53.000000000 -0400
@@ -283,12 +283,12 @@
 	}
 
 	str = buf;
-	end = buf + size - 1;
+	end = buf + size;
 
 	/* Make sure end is always >= buf */
-	if (end < buf - 1) {
+	if (end < buf) {
 		end = ((void *)-1);
-		size = end - buf + 1;
+		size = end - buf;
 	}
 
 	for (; *fmt ; ++fmt) {
diff -urN oldtree/mm/memory.c newtree/mm/memory.c
--- oldtree/mm/memory.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/mm/memory.c	2006-07-12 19:01:08.000000000 -0400
@@ -1176,6 +1176,7 @@
 	} while (pgd++, addr = next, addr != end);
 	return err;
 }
+EXPORT_SYMBOL_GPL(zeromap_page_range);
 
 pte_t * fastcall get_locked_pte(struct mm_struct *mm, unsigned long addr, spinlock_t **ptl)
 {
diff -urN oldtree/mm/mmap.c newtree/mm/mmap.c
--- oldtree/mm/mmap.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/mm/mmap.c	2006-07-12 19:01:08.000000000 -0400
@@ -2002,6 +2002,7 @@
 	vma_link(mm, vma, prev, rb_link, rb_parent);
 	return 0;
 }
+EXPORT_SYMBOL_GPL(insert_vm_struct);
 
 /*
  * Copy the vma structure to a new location in the same mm,
diff -urN oldtree/mm/page_alloc.c newtree/mm/page_alloc.c
--- oldtree/mm/page_alloc.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/mm/page_alloc.c	2006-07-12 19:02:48.000000000 -0400
@@ -1396,6 +1396,7 @@
 			" min:%lukB"
 			" low:%lukB"
 			" high:%lukB"
+			" lots:%lukB"
 			" active:%lukB"
 			" inactive:%lukB"
 			" present:%lukB"
@@ -1407,6 +1408,7 @@
 			K(zone->pages_min),
 			K(zone->pages_low),
 			K(zone->pages_high),
+			K(zone->pages_lots),
 			K(zone->nr_active),
 			K(zone->nr_inactive),
 			K(zone->present_pages),
@@ -2333,6 +2335,7 @@
 
 		zone->pages_low   = zone->pages_min + (tmp >> 2);
 		zone->pages_high  = zone->pages_min + (tmp >> 1);
+		zone->pages_lots  = zone->pages_min + tmp;
 		spin_unlock_irqrestore(&zone->lru_lock, flags);
 	}
 
diff -urN oldtree/mm/vmscan.c newtree/mm/vmscan.c
--- oldtree/mm/vmscan.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/mm/vmscan.c	2006-07-12 19:02:48.000000000 -0400
@@ -62,7 +62,7 @@
 	 * whole list at once. */
 	int swap_cluster_max;
 
-	int swappiness;
+	int mapped;
 };
 
 #define lru_to_page(_head) (list_entry((_head)->prev, struct page, lru))
@@ -96,10 +96,12 @@
 #endif
 
 /*
- * From 0 .. 100.  Higher means more swappy.
+ * From 0 .. 100.  Lower means more swappy.
  */
-int vm_swappiness = 60;
-long vm_total_pages;	/* The total number of pages which the VM controls */
+int vm_mapped __read_mostly = 66;
+int vm_hardmaplimit __read_mostly = 1;
+long vm_total_pages;
+/* static long total_memory __read_mostly; */
 
 static LIST_HEAD(shrinker_list);
 static DECLARE_RWSEM(shrinker_rwsem);
@@ -760,10 +762,14 @@
 		 * The distress ratio is important - we don't want to start
 		 * going oom.
 		 *
-		 * A 100% value of vm_swappiness overrides this algorithm
-		 * altogether.
+		 * This distress value is ignored if we apply a hardmaplimit except
+		 * in extreme distress.
+		 *
+		 * A 0% value of vm_mapped overrides this algorithm altogether.
 		 */
-		swap_tendency = mapped_ratio / 2 + distress + sc->swappiness;
+		swap_tendency = mapped_ratio * 100 / (sc->mapped + 1);
+		if (!vm_hardmaplimit || distress == 100)
+			swap_tendency += distress;
 
 		/*
 		 * Now use this metric to decide whether to start moving mapped
@@ -978,7 +984,7 @@
 		.may_writepage = !laptop_mode,
 		.swap_cluster_max = SWAP_CLUSTER_MAX,
 		.may_swap = 1,
-		.swappiness = vm_swappiness,
+		.mapped = vm_mapped,
 	};
 
 	delay_swap_prefetch();
@@ -1072,7 +1078,7 @@
 		.gfp_mask = GFP_KERNEL,
 		.may_swap = 1,
 		.swap_cluster_max = SWAP_CLUSTER_MAX,
-		.swappiness = vm_swappiness,
+		.mapped = vm_mapped,
 	};
 
 loop_again:
@@ -1103,6 +1109,7 @@
 		 */
 		for (i = pgdat->nr_zones - 1; i >= 0; i--) {
 			struct zone *zone = pgdat->node_zones + i;
+			unsigned long watermark;
 
 			if (!populated_zone(zone))
 				continue;
@@ -1110,11 +1117,18 @@
 			if (zone->all_unreclaimable && priority != DEF_PRIORITY)
 				continue;
 
-			if (!zone_watermark_ok(zone, order, zone->pages_high,
-					       0, 0)) {
+			/*
+			 * The watermark is relaxed depending on the
+			 * level of "priority" till it drops to
+			 * pages_high.
+			 */
+			watermark = zone->pages_high + (zone->pages_high *
+				    priority / DEF_PRIORITY);
+			if (!zone_watermark_ok(zone, order, watermark, 0, 0)) {
 				end_zone = i;
 				goto scan;
 			}
+
 		}
 		goto out;
 scan:
@@ -1136,6 +1150,7 @@
 		for (i = 0; i <= end_zone; i++) {
 			struct zone *zone = pgdat->node_zones + i;
 			int nr_slab;
+			unsigned long watermark;
 
 			if (!populated_zone(zone))
 				continue;
@@ -1143,7 +1158,10 @@
 			if (zone->all_unreclaimable && priority != DEF_PRIORITY)
 				continue;
 
-			if (!zone_watermark_ok(zone, order, zone->pages_high,
+			watermark = zone->pages_high + (zone->pages_high *
+				    priority / DEF_PRIORITY);
+
+			if (!zone_watermark_ok(zone, order, watermark,
 					       end_zone, 0))
 				all_zones_ok = 0;
 			zone->temp_priority = priority;
@@ -1358,7 +1376,7 @@
 		.may_swap = 0,
 		.swap_cluster_max = nr_pages,
 		.may_writepage = 1,
-		.swappiness = vm_swappiness,
+		.mapped = vm_mapped,
 	};
 
 	delay_swap_prefetch();
@@ -1405,7 +1423,7 @@
 		/* Force reclaiming mapped pages in the passes #3 and #4 */
 		if (pass > 2) {
 			sc.may_swap = 1;
-			sc.swappiness = 100;
+			sc.mapped = 0;
 		}
 
 		for (prio = DEF_PRIORITY; prio >= 0; prio--) {
@@ -1550,7 +1568,7 @@
 		.swap_cluster_max = max_t(unsigned long, nr_pages,
 					SWAP_CLUSTER_MAX),
 		.gfp_mask = gfp_mask,
-		.swappiness = vm_swappiness,
+		.mapped = vm_mapped,
 	};
 
 	disable_swap_token();
diff -urN oldtree/scripts/kconfig/confdata.c newtree/scripts/kconfig/confdata.c
--- oldtree/scripts/kconfig/confdata.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/scripts/kconfig/confdata.c	2006-07-12 19:03:01.000000000 -0400
@@ -421,10 +421,11 @@
 
 	fprintf(out, _("#\n"
 		       "# Automatically generated make config: don't edit\n"
-		       "# Linux kernel version: %s\n"
+		       "# Linux kernel version: %s \"%s\"\n"
 		       "%s%s"
 		       "#\n"),
 		     sym_get_string_value(sym),
+		     getenv("NAME"),
 		     use_timestamp ? "# " : "",
 		     use_timestamp ? ctime(&now) : "");
 
@@ -673,17 +674,17 @@
 	time(&now);
 	fprintf(out, "#\n"
 		     "# Automatically generated make config: don't edit\n"
-		     "# Linux kernel version: %s\n"
+		     "# Linux kernel version: %s \"%s\"\n"
 		     "# %s"
 		     "#\n",
-		     sym_get_string_value(sym), ctime(&now));
+		     sym_get_string_value(sym), getenv("NAME"), ctime(&now));
 	fprintf(out_h, "/*\n"
 		       " * Automatically generated C config: don't edit\n"
-		       " * Linux kernel version: %s\n"
+		       " * Linux kernel version: %s \"%s\"\n"
 		       " * %s"
 		       " */\n"
 		       "#define AUTOCONF_INCLUDED\n",
-		       sym_get_string_value(sym), ctime(&now));
+		       sym_get_string_value(sym), getenv("NAME"), ctime(&now));
 
 	for_all_symbols(i, sym) {
 		sym_calc_value(sym);
diff -urN oldtree/scripts/kconfig/gconf.c newtree/scripts/kconfig/gconf.c
--- oldtree/scripts/kconfig/gconf.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/scripts/kconfig/gconf.c	2006-07-12 19:03:01.000000000 -0400
@@ -267,8 +267,8 @@
 					  /*"style", PANGO_STYLE_OBLIQUE, */
 					  NULL);
 
-	sprintf(title, _("Linux Kernel v%s Configuration"),
-		getenv("KERNELVERSION"));
+	sprintf(title, _("Linux Kernel v%s \"%s\" Configuration"),
+		getenv("KERNELVERSION"), getenv("NAME"));
 	gtk_window_set_title(GTK_WINDOW(main_wnd), title);
 
 	gtk_widget_show(main_wnd);
diff -urN oldtree/scripts/kconfig/mconf.c newtree/scripts/kconfig/mconf.c
--- oldtree/scripts/kconfig/mconf.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/scripts/kconfig/mconf.c	2006-07-12 19:03:01.000000000 -0400
@@ -1053,8 +1053,8 @@
 
 	sym = sym_lookup("KERNELVERSION", 0);
 	sym_calc_value(sym);
-	sprintf(menu_backtitle, _("Linux Kernel v%s Configuration"),
-		sym_get_string_value(sym));
+	sprintf(menu_backtitle, _("Linux Kernel v%s \"%s\" Configuration"),
+		sym_get_string_value(sym), getenv("NAME"));
 
 	mode = getenv("MENUCONFIG_MODE");
 	if (mode) {
diff -urN oldtree/scripts/kconfig/qconf.cc newtree/scripts/kconfig/qconf.cc
--- oldtree/scripts/kconfig/qconf.cc	2006-07-05 10:06:57.000000000 -0400
+++ newtree/scripts/kconfig/qconf.cc	2006-07-12 19:03:01.000000000 -0400
@@ -1431,14 +1431,14 @@
 	if (s.isNull())
 		return;
 	if (conf_read(QFile::encodeName(s)))
-		QMessageBox::information(this, "qconf", "Unable to load configuration!");
+		QMessageBox::information(this, "QConf Error", "Unable to load configuration!");
 	ConfigView::updateListAll();
 }
 
 void ConfigMainWindow::saveConfig(void)
 {
 	if (conf_write(NULL))
-		QMessageBox::information(this, "qconf", "Unable to save configuration!");
+		QMessageBox::information(this, "QConf Error", "Unable to save configuration!");
 }
 
 void ConfigMainWindow::saveConfigAs(void)
@@ -1447,7 +1447,7 @@
 	if (s.isNull())
 		return;
 	if (conf_write(QFile::encodeName(s)))
-		QMessageBox::information(this, "qconf", "Unable to save configuration!");
+		QMessageBox::information(this, "QConf Error", "Unable to save configuration!");
 }
 
 void ConfigMainWindow::searchConfig(void)
@@ -1588,7 +1588,7 @@
 		e->accept();
 		return;
 	}
-	QMessageBox mb("qconf", "Save configuration?", QMessageBox::Warning,
+	QMessageBox mb("Save .config?", "Save configuration?", QMessageBox::Warning,
 			QMessageBox::Yes | QMessageBox::Default, QMessageBox::No, QMessageBox::Cancel | QMessageBox::Escape);
 	mb.setButtonText(QMessageBox::Yes, "&Save Changes");
 	mb.setButtonText(QMessageBox::No, "&Discard Changes");
@@ -1607,7 +1607,8 @@
 
 void ConfigMainWindow::showIntro(void)
 {
-	static char str[] = "Welcome to the qconf graphical kernel configuration tool for Linux.\n\n"
+	static char str[1000];
+	sprintf(str, "Welcome to the qconf graphical kernel configuration tool for Linux %s \"%s\".\n\n"
 		"For each option, a blank box indicates the feature is disabled, a check\n"
 		"indicates it is enabled, and a dot indicates that it is to be compiled\n"
 		"as a module.  Clicking on the box will cycle through the three states.\n\n"
@@ -1617,9 +1618,10 @@
 		"options must be enabled to support the option you are interested in, you can\n"
 		"still view the help of a grayed-out option.\n\n"
 		"Toggling Show Debug Info under the Options menu will show the dependencies,\n"
-		"which you can then match by examining other options.\n\n";
+		"which you can then match by examining other options.\n\n",
+		getenv("KERNELRELEASE"), getenv("NAME"));
 
-	QMessageBox::information(this, "qconf", str);
+	QMessageBox::information(this, "Introduction", str);
 }
 
 void ConfigMainWindow::showAbout(void)
@@ -1627,7 +1629,7 @@
 	static char str[] = "qconf is Copyright (C) 2002 Roman Zippel <zippel@linux-m68k.org>.\n\n"
 		"Bug reports and feature request can also be entered at http://bugzilla.kernel.org/\n";
 
-	QMessageBox::information(this, "qconf", str);
+	QMessageBox::information(this, "About", str);
 }
 
 void ConfigMainWindow::saveSettings(void)
@@ -1685,7 +1687,12 @@
 {
 	ConfigMainWindow* v;
 	const char *name;
-
+	static char title[100];
+	
+	sprintf(title,"Linux Kernel %s \"%s\" Configuration",
+		getenv("KERNELRELEASE"), getenv("NAME")
+	);
+	
 	bindtextdomain(PACKAGE, LOCALEDIR);
 	textdomain(PACKAGE);
 
@@ -1716,6 +1723,7 @@
 	configSettings->beginGroup("/kconfig/qconf");
 	v = new ConfigMainWindow();
 
+	v->setCaption(title);
 	//zconfdump(stdout);
 	configApp->setMainWidget(v);
 	configApp->connect(configApp, SIGNAL(lastWindowClosed()), SLOT(quit()));
diff -urN oldtree/scripts/kconfig/zconf.tab.c_shipped newtree/scripts/kconfig/zconf.tab.c_shipped
--- oldtree/scripts/kconfig/zconf.tab.c_shipped	2006-07-05 10:06:57.000000000 -0400
+++ newtree/scripts/kconfig/zconf.tab.c_shipped	2006-07-12 19:03:01.000000000 -0400
@@ -2107,15 +2107,19 @@
 {
 	struct symbol *sym;
 	int i;
+	static char title[200];
 
 	zconf_initscan(name);
+	
+	sprintf(title,"Linux Kernel %s \"%s\" Configuration",
+		getenv("KERNELRELEASE"), getenv("NAME"));
 
 	sym_init();
 	menu_init();
 	modules_sym = sym_lookup(NULL, 0);
 	modules_sym->type = S_BOOLEAN;
 	modules_sym->flags |= SYMBOL_AUTO;
-	rootmenu.prompt = menu_add_prompt(P_MENU, "Linux Kernel Configuration", NULL);
+	rootmenu.prompt = menu_add_prompt(P_MENU, title, NULL);
 
 #if YYDEBUG
 	if (getenv("ZCONF_DEBUG"))
diff -urN oldtree/security/Kconfig newtree/security/Kconfig
--- oldtree/security/Kconfig	2006-07-05 10:06:57.000000000 -0400
+++ newtree/security/Kconfig	2006-07-12 19:02:04.000000000 -0400
@@ -105,6 +105,17 @@
 
 	  If you are unsure how to answer this question, answer N.
 
+config SECURITY_REALTIME
+	tristate "Realtime Capabilities"
+	depends on SECURITY && SECURITY_CAPABILITIES!=y
+	default n
+	help
+	  This module selectively grants realtime privileges
+	  controlled by parameters set at load time or via files in
+	  /sys/module/realtime/parameters.
+
+	  If you are unsure how to answer this question, answer N.
+
 source security/selinux/Kconfig
 
 endmenu
diff -urN oldtree/security/Makefile newtree/security/Makefile
--- oldtree/security/Makefile	2006-07-05 10:06:57.000000000 -0400
+++ newtree/security/Makefile	2006-07-12 19:02:04.000000000 -0400
@@ -17,3 +17,4 @@
 obj-$(CONFIG_SECURITY_CAPABILITIES)	+= commoncap.o capability.o
 obj-$(CONFIG_SECURITY_ROOTPLUG)		+= commoncap.o root_plug.o
 obj-$(CONFIG_SECURITY_SECLVL)		+= seclvl.o
+obj-$(CONFIG_SECURITY_REALTIME)		+= commoncap.o realtime.o
diff -urN oldtree/security/realtime.c newtree/security/realtime.c
--- oldtree/security/realtime.c	1969-12-31 19:00:00.000000000 -0500
+++ newtree/security/realtime.c	2006-07-12 19:02:04.000000000 -0400
@@ -0,0 +1,147 @@
+/*
+ * Realtime Capabilities Linux Security Module
+ *
+ *  Copyright (C) 2003 Torben Hohn
+ *  Copyright (C) 2003, 2004 Jack O'Quin
+ *
+ *	This program is free software; you can redistribute it and/or modify
+ *	it under the terms of the GNU General Public License as published by
+ *	the Free Software Foundation; either version 2 of the License, or
+ *	(at your option) any later version.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/security.h>
+
+#define RT_LSM "Realtime LSM "		/* syslog module name prefix */
+#define RT_ERR "Realtime: "		/* syslog error message prefix */
+
+#include <linux/vermagic.h>
+MODULE_INFO(vermagic,VERMAGIC_STRING);
+
+/* module parameters
+ *
+ *  These values could change at any time due to some process writing
+ *  a new value in /sys/module/realtime/parameters.  This is OK,
+ *  because each is referenced only once in each function call.
+ *  Nothing depends on parameters having the same value every time.
+ */
+
+/* if TRUE, any process is realtime */
+static int rt_any;
+module_param_named(any, rt_any, int, 0644);
+MODULE_PARM_DESC(any, " grant realtime privileges to any process.");
+
+/* realtime group id, or NO_GROUP */
+static int rt_gid = -1;
+module_param_named(gid, rt_gid, int, 0644);
+MODULE_PARM_DESC(gid, " the group ID with access to realtime privileges.");
+
+/* enable mlock() privileges */
+static int rt_mlock = 1;
+module_param_named(mlock, rt_mlock, int, 0644);
+MODULE_PARM_DESC(mlock, " enable memory locking privileges.");
+
+/* helper function for testing group membership */
+static inline int gid_ok(int gid)
+{
+	if (gid == -1)
+		return 0;
+
+	if (gid == current->gid)
+		return 1;
+
+	return in_egroup_p(gid);
+}
+
+static void realtime_bprm_apply_creds(struct linux_binprm *bprm, int unsafe)
+{
+	cap_bprm_apply_creds(bprm, unsafe);
+
+	/*  If a non-zero `any' parameter was specified, we grant
+	 *  realtime privileges to every process.  If the `gid'
+	 *  parameter was specified and it matches the group id of the
+	 *  executable, of the current process or any supplementary
+	 *  groups, we grant realtime capabilites.
+	 */
+
+	if (rt_any || gid_ok(rt_gid)) {
+		cap_raise(current->cap_effective, CAP_SYS_NICE);
+		if (rt_mlock) {
+			cap_raise(current->cap_effective, CAP_IPC_LOCK);
+			cap_raise(current->cap_effective, CAP_SYS_RESOURCE);
+		}
+	}
+}
+
+static struct security_operations capability_ops = {
+	.ptrace =			cap_ptrace,
+	.capget =			cap_capget,
+	.capset_check =			cap_capset_check,
+	.capset_set =			cap_capset_set,
+	.capable =			cap_capable,
+	.netlink_send =			cap_netlink_send,
+	.netlink_recv =			cap_netlink_recv,
+	.bprm_apply_creds =		realtime_bprm_apply_creds,
+	.bprm_set_security =		cap_bprm_set_security,
+	.bprm_secureexec =		cap_bprm_secureexec,
+	.task_post_setuid =		cap_task_post_setuid,
+	.task_reparent_to_init =	cap_task_reparent_to_init,
+	.syslog =                       cap_syslog,
+	.vm_enough_memory =             cap_vm_enough_memory,
+};
+
+#define MY_NAME __stringify(KBUILD_MODNAME)
+
+static int secondary;	/* flag to keep track of how we were registered */
+
+static int __init realtime_init(void)
+{
+	/* register ourselves with the security framework */
+	if (register_security(&capability_ops)) {
+
+		/* try registering with primary module */
+		if (mod_reg_security(MY_NAME, &capability_ops)) {
+			printk(KERN_INFO RT_ERR "Failure registering "
+			       "capabilities with primary security module.\n");
+			printk(KERN_INFO RT_ERR "Is kernel configured "
+			       "with CONFIG_SECURITY_CAPABILITIES=m?\n");
+			return -EINVAL;
+		}
+		secondary = 1;
+	}
+
+	if (rt_any)
+		printk(KERN_INFO RT_LSM
+		       "initialized (all groups, mlock=%d)\n", rt_mlock);
+	else if (rt_gid == -1)
+		printk(KERN_INFO RT_LSM
+		       "initialized (no groups, mlock=%d)\n", rt_mlock);
+	else
+		printk(KERN_INFO RT_LSM
+		       "initialized (group %d, mlock=%d)\n", rt_gid, rt_mlock);
+		
+	return 0;
+}
+
+static void __exit realtime_exit(void)
+{
+	/* remove ourselves from the security framework */
+	if (secondary) {
+		if (mod_unreg_security(MY_NAME, &capability_ops))
+			printk(KERN_INFO RT_ERR "Failure unregistering "
+				"capabilities with primary module.\n");
+
+	} else if (unregister_security(&capability_ops)) {
+		printk(KERN_INFO RT_ERR
+		       "Failure unregistering capabilities with the kernel\n");
+	}
+	printk(KERN_INFO "Realtime Capability LSM exiting\n");
+}
+
+late_initcall(realtime_init);
+module_exit(realtime_exit);
+
+MODULE_DESCRIPTION("Realtime Capabilities Security Module");
+MODULE_LICENSE("GPL");
diff -urN oldtree/usr/kinit/fstype/fstype.c newtree/usr/kinit/fstype/fstype.c
--- oldtree/usr/kinit/fstype/fstype.c	2006-07-05 10:06:57.000000000 -0400
+++ newtree/usr/kinit/fstype/fstype.c	2006-07-12 19:01:48.000000000 -0400
@@ -31,6 +31,7 @@
 #include "xfs_sb.h"
 #include "luks_fs.h"
 #include "lvm2_sb.h"
+#include "squashfs_fs.h"
 
 /*
  * Slightly cleaned up version of jfs_superblock to
@@ -221,6 +222,17 @@
 	return 0;
 }
 
+static int squashfs_image(const void *buf, unsigned long long *bytes)
+{
+        const struct squashfs_super *sb = (const struct squashfs_super *)buf;
+
+        if (sb->s_magic == SQUASHFS_MAGIC) {
+                        *bytes = 0;
+                return 1;
+        }
+        return 0;
+}
+
 struct imagetype {
 	off_t block;
 	const char name[12];
@@ -240,6 +252,7 @@
 static struct imagetype images[] = {
 	{0, "gzip", gzip_image},
 	{0, "cramfs", cramfs_image},
+        {0, "squashfs", squashfs_image},
 	{0, "romfs", romfs_image},
 	{0, "xfs", xfs_image},
 	{0, "luks", luks_image},
diff -urN oldtree/usr/kinit/fstype/squashfs_fs.h newtree/usr/kinit/fstype/squashfs_fs.h
--- oldtree/usr/kinit/fstype/squashfs_fs.h	1969-12-31 19:00:00.000000000 -0500
+++ newtree/usr/kinit/fstype/squashfs_fs.h	2006-07-12 19:01:48.000000000 -0400
@@ -0,0 +1,907 @@
+#ifndef SQUASHFS_FS
+#define SQUASHFS_FS
+
+/*
+ * Squashfs
+ *
+ * Copyright (c) 2002, 2003, 2004, 2005, 2006
+ * Phillip Lougher <phillip@lougher.org.uk>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2,
+ * or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * squashfs_fs.h
+ */
+
+#ifndef CONFIG_SQUASHFS_2_0_COMPATIBILITY
+#define CONFIG_SQUASHFS_2_0_COMPATIBILITY
+#endif
+
+#ifdef	CONFIG_SQUASHFS_VMALLOC
+#define SQUASHFS_ALLOC(a)		vmalloc(a)
+#define SQUASHFS_FREE(a)		vfree(a)
+#else
+#define SQUASHFS_ALLOC(a)		kmalloc(a, GFP_KERNEL)
+#define SQUASHFS_FREE(a)		kfree(a)
+#endif
+#define SQUASHFS_CACHED_FRAGMENTS	CONFIG_SQUASHFS_FRAGMENT_CACHE_SIZE	
+#define SQUASHFS_MAJOR			3
+#define SQUASHFS_MINOR			0
+#define SQUASHFS_MAGIC			0x73717368
+#define SQUASHFS_MAGIC_SWAP		0x68737173
+#define SQUASHFS_START			0
+
+/* size of metadata (inode and directory) blocks */
+#define SQUASHFS_METADATA_SIZE		8192
+#define SQUASHFS_METADATA_LOG		13
+
+/* default size of data blocks */
+#define SQUASHFS_FILE_SIZE		65536
+#define SQUASHFS_FILE_LOG		16
+
+#define SQUASHFS_FILE_MAX_SIZE		65536
+
+/* Max number of uids and gids */
+#define SQUASHFS_UIDS			256
+#define SQUASHFS_GUIDS			255
+
+/* Max length of filename (not 255) */
+#define SQUASHFS_NAME_LEN		256
+
+#define SQUASHFS_INVALID		((long long) 0xffffffffffff)
+#define SQUASHFS_INVALID_FRAG		((unsigned int) 0xffffffff)
+#define SQUASHFS_INVALID_BLK		((long long) -1)
+#define SQUASHFS_USED_BLK		((long long) -2)
+
+/* Filesystem flags */
+#define SQUASHFS_NOI			0
+#define SQUASHFS_NOD			1
+#define SQUASHFS_CHECK			2
+#define SQUASHFS_NOF			3
+#define SQUASHFS_NO_FRAG		4
+#define SQUASHFS_ALWAYS_FRAG		5
+#define SQUASHFS_DUPLICATE		6
+
+#define SQUASHFS_BIT(flag, bit)		((flag >> bit) & 1)
+
+#define SQUASHFS_UNCOMPRESSED_INODES(flags)	SQUASHFS_BIT(flags, \
+						SQUASHFS_NOI)
+
+#define SQUASHFS_UNCOMPRESSED_DATA(flags)	SQUASHFS_BIT(flags, \
+						SQUASHFS_NOD)
+
+#define SQUASHFS_UNCOMPRESSED_FRAGMENTS(flags)	SQUASHFS_BIT(flags, \
+						SQUASHFS_NOF)
+
+#define SQUASHFS_NO_FRAGMENTS(flags)		SQUASHFS_BIT(flags, \
+						SQUASHFS_NO_FRAG)
+
+#define SQUASHFS_ALWAYS_FRAGMENTS(flags)	SQUASHFS_BIT(flags, \
+						SQUASHFS_ALWAYS_FRAG)
+
+#define SQUASHFS_DUPLICATES(flags)		SQUASHFS_BIT(flags, \
+						SQUASHFS_DUPLICATE)
+
+#define SQUASHFS_CHECK_DATA(flags)		SQUASHFS_BIT(flags, \
+						SQUASHFS_CHECK)
+
+#define SQUASHFS_MKFLAGS(noi, nod, check_data, nof, no_frag, always_frag, \
+		duplicate_checking)	(noi | (nod << 1) | (check_data << 2) \
+		| (nof << 3) | (no_frag << 4) | (always_frag << 5) | \
+		(duplicate_checking << 6))
+
+/* Max number of types and file types */
+#define SQUASHFS_DIR_TYPE		1
+#define SQUASHFS_FILE_TYPE		2
+#define SQUASHFS_SYMLINK_TYPE		3
+#define SQUASHFS_BLKDEV_TYPE		4
+#define SQUASHFS_CHRDEV_TYPE		5
+#define SQUASHFS_FIFO_TYPE		6
+#define SQUASHFS_SOCKET_TYPE		7
+#define SQUASHFS_LDIR_TYPE		8
+#define SQUASHFS_LREG_TYPE		9
+
+/* 1.0 filesystem type definitions */
+#define SQUASHFS_TYPES			5
+#define SQUASHFS_IPC_TYPE		0
+
+/* Flag whether block is compressed or uncompressed, bit is set if block is
+ * uncompressed */
+#define SQUASHFS_COMPRESSED_BIT		(1 << 15)
+
+#define SQUASHFS_COMPRESSED_SIZE(B)	(((B) & ~SQUASHFS_COMPRESSED_BIT) ? \
+		(B) & ~SQUASHFS_COMPRESSED_BIT :  SQUASHFS_COMPRESSED_BIT)
+
+#define SQUASHFS_COMPRESSED(B)		(!((B) & SQUASHFS_COMPRESSED_BIT))
+
+#define SQUASHFS_COMPRESSED_BIT_BLOCK		(1 << 24)
+
+#define SQUASHFS_COMPRESSED_SIZE_BLOCK(B)	(((B) & \
+	~SQUASHFS_COMPRESSED_BIT_BLOCK) ? (B) & \
+	~SQUASHFS_COMPRESSED_BIT_BLOCK : SQUASHFS_COMPRESSED_BIT_BLOCK)
+
+#define SQUASHFS_COMPRESSED_BLOCK(B)	(!((B) & SQUASHFS_COMPRESSED_BIT_BLOCK))
+
+/*
+ * Inode number ops.  Inodes consist of a compressed block number, and an
+ * uncompressed  offset within that block
+ */
+#define SQUASHFS_INODE_BLK(a)		((unsigned int) ((a) >> 16))
+
+#define SQUASHFS_INODE_OFFSET(a)	((unsigned int) ((a) & 0xffff))
+
+#define SQUASHFS_MKINODE(A, B)		((squashfs_inode_t)(((squashfs_inode_t) (A)\
+					<< 16) + (B)))
+
+/* Compute 32 bit VFS inode number from squashfs inode number */
+#define SQUASHFS_MK_VFS_INODE(a, b)	((unsigned int) (((a) << 8) + \
+					((b) >> 2) + 1))
+/* XXX */
+
+/* Translate between VFS mode and squashfs mode */
+#define SQUASHFS_MODE(a)		((a) & 0xfff)
+
+/* fragment and fragment table defines */
+#define SQUASHFS_FRAGMENT_BYTES(A)	(A * sizeof(struct squashfs_fragment_entry))
+
+#define SQUASHFS_FRAGMENT_INDEX(A)	(SQUASHFS_FRAGMENT_BYTES(A) / \
+					SQUASHFS_METADATA_SIZE)
+
+#define SQUASHFS_FRAGMENT_INDEX_OFFSET(A)	(SQUASHFS_FRAGMENT_BYTES(A) % \
+						SQUASHFS_METADATA_SIZE)
+
+#define SQUASHFS_FRAGMENT_INDEXES(A)	((SQUASHFS_FRAGMENT_BYTES(A) + \
+					SQUASHFS_METADATA_SIZE - 1) / \
+					SQUASHFS_METADATA_SIZE)
+
+#define SQUASHFS_FRAGMENT_INDEX_BYTES(A)	(SQUASHFS_FRAGMENT_INDEXES(A) *\
+						sizeof(long long))
+
+/* cached data constants for filesystem */
+#define SQUASHFS_CACHED_BLKS		8
+
+#define SQUASHFS_MAX_FILE_SIZE_LOG	64
+
+#define SQUASHFS_MAX_FILE_SIZE		((long long) 1 << \
+					(SQUASHFS_MAX_FILE_SIZE_LOG - 2))
+
+#define SQUASHFS_MARKER_BYTE		0xff
+
+/* meta index cache */
+#define SQUASHFS_META_INDEXES	(SQUASHFS_METADATA_SIZE / sizeof(unsigned int))
+#define SQUASHFS_META_ENTRIES	31
+#define SQUASHFS_META_NUMBER	8
+#define SQUASHFS_SLOTS		4
+
+struct meta_entry {
+	long long		data_block;
+	unsigned int		index_block;
+	unsigned short		offset;
+	unsigned short		pad;
+};
+
+struct meta_index {
+	unsigned int		inode_number;
+	unsigned int		offset;
+	unsigned short		entries;
+	unsigned short		skip;
+	unsigned short		locked;
+	unsigned short		pad;
+	struct meta_entry	meta_entry[SQUASHFS_META_ENTRIES];
+};
+
+
+/*
+ * definitions for structures on disk
+ */
+
+typedef long long		squashfs_block_t;
+typedef long long		squashfs_inode_t;
+
+struct squashfs_super {
+	unsigned int		s_magic;
+	unsigned int		inodes;
+	unsigned int		bytes_used_2;
+	unsigned int		uid_start_2;
+	unsigned int		guid_start_2;
+	unsigned int		inode_table_start_2;
+	unsigned int		directory_table_start_2;
+	unsigned int		s_major:16;
+	unsigned int		s_minor:16;
+	unsigned int		block_size_1:16;
+	unsigned int		block_log:16;
+	unsigned int		flags:8;
+	unsigned int		no_uids:8;
+	unsigned int		no_guids:8;
+	unsigned int		mkfs_time /* time of filesystem creation */;
+	squashfs_inode_t	root_inode;
+	unsigned int		block_size;
+	unsigned int		fragments;
+	unsigned int		fragment_table_start_2;
+	long long		bytes_used;
+	long long		uid_start;
+	long long		guid_start;
+	long long		inode_table_start;
+	long long		directory_table_start;
+	long long		fragment_table_start;
+	long long		unused;
+} __attribute__ ((packed));
+
+struct squashfs_dir_index {
+	unsigned int		index;
+	unsigned int		start_block;
+	unsigned char		size;
+	unsigned char		name[0];
+} __attribute__ ((packed));
+
+#define SQUASHFS_BASE_INODE_HEADER		\
+	unsigned int		inode_type:4;	\
+	unsigned int		mode:12;	\
+	unsigned int		uid:8;		\
+	unsigned int		guid:8;		\
+	unsigned int		mtime;		\
+	unsigned int 		inode_number;
+
+struct squashfs_base_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+} __attribute__ ((packed));
+
+struct squashfs_ipc_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+	unsigned int		nlink;
+} __attribute__ ((packed));
+
+struct squashfs_dev_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+	unsigned int		nlink;
+	unsigned short		rdev;
+} __attribute__ ((packed));
+	
+struct squashfs_symlink_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+	unsigned int		nlink;
+	unsigned short		symlink_size;
+	char			symlink[0];
+} __attribute__ ((packed));
+
+struct squashfs_reg_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+	squashfs_block_t	start_block;
+	unsigned int		fragment;
+	unsigned int		offset;
+	unsigned int		file_size;
+	unsigned short		block_list[0];
+} __attribute__ ((packed));
+
+struct squashfs_lreg_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+	unsigned int		nlink;
+	squashfs_block_t	start_block;
+	unsigned int		fragment;
+	unsigned int		offset;
+	long long		file_size;
+	unsigned short		block_list[0];
+} __attribute__ ((packed));
+
+struct squashfs_dir_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+	unsigned int		nlink;
+	unsigned int		file_size:19;
+	unsigned int		offset:13;
+	unsigned int		start_block;
+	unsigned int		parent_inode;
+} __attribute__  ((packed));
+
+struct squashfs_ldir_inode_header {
+	SQUASHFS_BASE_INODE_HEADER;
+	unsigned int		nlink;
+	unsigned int		file_size:27;
+	unsigned int		offset:13;
+	unsigned int		start_block;
+	unsigned int		i_count:16;
+	unsigned int		parent_inode;
+	struct squashfs_dir_index	index[0];
+} __attribute__  ((packed));
+
+union squashfs_inode_header {
+	struct squashfs_base_inode_header	base;
+	struct squashfs_dev_inode_header	dev;
+	struct squashfs_symlink_inode_header	symlink;
+	struct squashfs_reg_inode_header	reg;
+	struct squashfs_lreg_inode_header	lreg;
+	struct squashfs_dir_inode_header	dir;
+	struct squashfs_ldir_inode_header	ldir;
+	struct squashfs_ipc_inode_header	ipc;
+};
+	
+struct squashfs_dir_entry {
+	unsigned int		offset:13;
+	unsigned int		type:3;
+	unsigned int		size:8;
+	int			inode_number:16;
+	char			name[0];
+} __attribute__ ((packed));
+
+struct squashfs_dir_header {
+	unsigned int		count:8;
+	unsigned int		start_block;
+	unsigned int		inode_number;
+} __attribute__ ((packed));
+
+struct squashfs_fragment_entry {
+	long long		start_block;
+	unsigned int		size;
+	unsigned int		unused;
+} __attribute__ ((packed));
+
+/*
+ * macros to convert each packed bitfield structure from little endian to big
+ * endian and vice versa.  These are needed when creating or using a filesystem
+ * on a machine with different byte ordering to the target architecture.
+ *
+ */
+
+#define SQUASHFS_SWAP_START \
+	int bits;\
+	int b_pos;\
+	unsigned long long val;\
+	unsigned char *s;\
+	unsigned char *d;
+
+#define SQUASHFS_SWAP_SUPER_BLOCK(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_super));\
+	SQUASHFS_SWAP((s)->s_magic, d, 0, 32);\
+	SQUASHFS_SWAP((s)->inodes, d, 32, 32);\
+	SQUASHFS_SWAP((s)->bytes_used_2, d, 64, 32);\
+	SQUASHFS_SWAP((s)->uid_start_2, d, 96, 32);\
+	SQUASHFS_SWAP((s)->guid_start_2, d, 128, 32);\
+	SQUASHFS_SWAP((s)->inode_table_start_2, d, 160, 32);\
+	SQUASHFS_SWAP((s)->directory_table_start_2, d, 192, 32);\
+	SQUASHFS_SWAP((s)->s_major, d, 224, 16);\
+	SQUASHFS_SWAP((s)->s_minor, d, 240, 16);\
+	SQUASHFS_SWAP((s)->block_size_1, d, 256, 16);\
+	SQUASHFS_SWAP((s)->block_log, d, 272, 16);\
+	SQUASHFS_SWAP((s)->flags, d, 288, 8);\
+	SQUASHFS_SWAP((s)->no_uids, d, 296, 8);\
+	SQUASHFS_SWAP((s)->no_guids, d, 304, 8);\
+	SQUASHFS_SWAP((s)->mkfs_time, d, 312, 32);\
+	SQUASHFS_SWAP((s)->root_inode, d, 344, 64);\
+	SQUASHFS_SWAP((s)->block_size, d, 408, 32);\
+	SQUASHFS_SWAP((s)->fragments, d, 440, 32);\
+	SQUASHFS_SWAP((s)->fragment_table_start_2, d, 472, 32);\
+	SQUASHFS_SWAP((s)->bytes_used, d, 504, 64);\
+	SQUASHFS_SWAP((s)->uid_start, d, 568, 64);\
+	SQUASHFS_SWAP((s)->guid_start, d, 632, 64);\
+	SQUASHFS_SWAP((s)->inode_table_start, d, 696, 64);\
+	SQUASHFS_SWAP((s)->directory_table_start, d, 760, 64);\
+	SQUASHFS_SWAP((s)->fragment_table_start, d, 824, 64);\
+	SQUASHFS_SWAP((s)->unused, d, 888, 64);\
+}
+
+#define SQUASHFS_SWAP_BASE_INODE_CORE(s, d, n)\
+	SQUASHFS_MEMSET(s, d, n);\
+	SQUASHFS_SWAP((s)->inode_type, d, 0, 4);\
+	SQUASHFS_SWAP((s)->mode, d, 4, 12);\
+	SQUASHFS_SWAP((s)->uid, d, 16, 8);\
+	SQUASHFS_SWAP((s)->guid, d, 24, 8);\
+	SQUASHFS_SWAP((s)->mtime, d, 32, 32);\
+	SQUASHFS_SWAP((s)->inode_number, d, 64, 32);
+
+#define SQUASHFS_SWAP_BASE_INODE_HEADER(s, d, n) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, n)\
+}
+
+#define SQUASHFS_SWAP_IPC_INODE_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, \
+			sizeof(struct squashfs_ipc_inode_header))\
+	SQUASHFS_SWAP((s)->nlink, d, 96, 32);\
+}
+
+#define SQUASHFS_SWAP_DEV_INODE_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, \
+			sizeof(struct squashfs_dev_inode_header)); \
+	SQUASHFS_SWAP((s)->nlink, d, 96, 32);\
+	SQUASHFS_SWAP((s)->rdev, d, 128, 16);\
+}
+
+#define SQUASHFS_SWAP_SYMLINK_INODE_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, \
+			sizeof(struct squashfs_symlink_inode_header));\
+	SQUASHFS_SWAP((s)->nlink, d, 96, 32);\
+	SQUASHFS_SWAP((s)->symlink_size, d, 128, 16);\
+}
+
+#define SQUASHFS_SWAP_REG_INODE_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, \
+			sizeof(struct squashfs_reg_inode_header));\
+	SQUASHFS_SWAP((s)->start_block, d, 96, 64);\
+	SQUASHFS_SWAP((s)->fragment, d, 160, 32);\
+	SQUASHFS_SWAP((s)->offset, d, 192, 32);\
+	SQUASHFS_SWAP((s)->file_size, d, 224, 32);\
+}
+
+#define SQUASHFS_SWAP_LREG_INODE_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, \
+			sizeof(struct squashfs_lreg_inode_header));\
+	SQUASHFS_SWAP((s)->nlink, d, 96, 32);\
+	SQUASHFS_SWAP((s)->start_block, d, 128, 64);\
+	SQUASHFS_SWAP((s)->fragment, d, 192, 32);\
+	SQUASHFS_SWAP((s)->offset, d, 224, 32);\
+	SQUASHFS_SWAP((s)->file_size, d, 256, 64);\
+}
+
+#define SQUASHFS_SWAP_DIR_INODE_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, \
+			sizeof(struct squashfs_dir_inode_header));\
+	SQUASHFS_SWAP((s)->nlink, d, 96, 32);\
+	SQUASHFS_SWAP((s)->file_size, d, 128, 19);\
+	SQUASHFS_SWAP((s)->offset, d, 147, 13);\
+	SQUASHFS_SWAP((s)->start_block, d, 160, 32);\
+	SQUASHFS_SWAP((s)->parent_inode, d, 192, 32);\
+}
+
+#define SQUASHFS_SWAP_LDIR_INODE_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE(s, d, \
+			sizeof(struct squashfs_ldir_inode_header));\
+	SQUASHFS_SWAP((s)->nlink, d, 96, 32);\
+	SQUASHFS_SWAP((s)->file_size, d, 128, 27);\
+	SQUASHFS_SWAP((s)->offset, d, 155, 13);\
+	SQUASHFS_SWAP((s)->start_block, d, 168, 32);\
+	SQUASHFS_SWAP((s)->i_count, d, 200, 16);\
+	SQUASHFS_SWAP((s)->parent_inode, d, 216, 32);\
+}
+
+#define SQUASHFS_SWAP_DIR_INDEX(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_dir_index));\
+	SQUASHFS_SWAP((s)->index, d, 0, 32);\
+	SQUASHFS_SWAP((s)->start_block, d, 32, 32);\
+	SQUASHFS_SWAP((s)->size, d, 64, 8);\
+}
+
+#define SQUASHFS_SWAP_DIR_HEADER(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_dir_header));\
+	SQUASHFS_SWAP((s)->count, d, 0, 8);\
+	SQUASHFS_SWAP((s)->start_block, d, 8, 32);\
+	SQUASHFS_SWAP((s)->inode_number, d, 40, 32);\
+}
+
+#define SQUASHFS_SWAP_DIR_ENTRY(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_dir_entry));\
+	SQUASHFS_SWAP((s)->offset, d, 0, 13);\
+	SQUASHFS_SWAP((s)->type, d, 13, 3);\
+	SQUASHFS_SWAP((s)->size, d, 16, 8);\
+	SQUASHFS_SWAP((s)->inode_number, d, 24, 16);\
+}
+
+#define SQUASHFS_SWAP_FRAGMENT_ENTRY(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_fragment_entry));\
+	SQUASHFS_SWAP((s)->start_block, d, 0, 64);\
+	SQUASHFS_SWAP((s)->size, d, 64, 32);\
+}
+
+#define SQUASHFS_SWAP_SHORTS(s, d, n) {\
+	int entry;\
+	int bit_position;\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, n * 2);\
+	for(entry = 0, bit_position = 0; entry < n; entry++, bit_position += \
+			16)\
+		SQUASHFS_SWAP(s[entry], d, bit_position, 16);\
+}
+
+#define SQUASHFS_SWAP_INTS(s, d, n) {\
+	int entry;\
+	int bit_position;\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, n * 4);\
+	for(entry = 0, bit_position = 0; entry < n; entry++, bit_position += \
+			32)\
+		SQUASHFS_SWAP(s[entry], d, bit_position, 32);\
+}
+
+#define SQUASHFS_SWAP_LONG_LONGS(s, d, n) {\
+	int entry;\
+	int bit_position;\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, n * 8);\
+	for(entry = 0, bit_position = 0; entry < n; entry++, bit_position += \
+			64)\
+		SQUASHFS_SWAP(s[entry], d, bit_position, 64);\
+}
+
+#define SQUASHFS_SWAP_DATA(s, d, n, bits) {\
+	int entry;\
+	int bit_position;\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, n * bits / 8);\
+	for(entry = 0, bit_position = 0; entry < n; entry++, bit_position += \
+			bits)\
+		SQUASHFS_SWAP(s[entry], d, bit_position, bits);\
+}
+
+#define SQUASHFS_SWAP_FRAGMENT_INDEXES(s, d, n) SQUASHFS_SWAP_LONG_LONGS(s, d, n)
+
+#ifdef CONFIG_SQUASHFS_1_0_COMPATIBILITY
+
+struct squashfs_base_inode_header_1 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:4; /* index into uid table */
+	unsigned int		guid:4; /* index into guid table */
+} __attribute__ ((packed));
+
+struct squashfs_ipc_inode_header_1 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:4; /* index into uid table */
+	unsigned int		guid:4; /* index into guid table */
+	unsigned int		type:4;
+	unsigned int		offset:4;
+} __attribute__ ((packed));
+
+struct squashfs_dev_inode_header_1 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:4; /* index into uid table */
+	unsigned int		guid:4; /* index into guid table */
+	unsigned short		rdev;
+} __attribute__ ((packed));
+	
+struct squashfs_symlink_inode_header_1 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:4; /* index into uid table */
+	unsigned int		guid:4; /* index into guid table */
+	unsigned short		symlink_size;
+	char			symlink[0];
+} __attribute__ ((packed));
+
+struct squashfs_reg_inode_header_1 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:4; /* index into uid table */
+	unsigned int		guid:4; /* index into guid table */
+	unsigned int		mtime;
+	unsigned int		start_block;
+	unsigned int		file_size:32;
+	unsigned short		block_list[0];
+} __attribute__ ((packed));
+
+struct squashfs_dir_inode_header_1 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:4; /* index into uid table */
+	unsigned int		guid:4; /* index into guid table */
+	unsigned int		file_size:19;
+	unsigned int		offset:13;
+	unsigned int		mtime;
+	unsigned int		start_block:24;
+} __attribute__  ((packed));
+
+#define SQUASHFS_SWAP_BASE_INODE_CORE_1(s, d, n) \
+	SQUASHFS_MEMSET(s, d, n);\
+	SQUASHFS_SWAP((s)->inode_type, d, 0, 4);\
+	SQUASHFS_SWAP((s)->mode, d, 4, 12);\
+	SQUASHFS_SWAP((s)->uid, d, 16, 4);\
+	SQUASHFS_SWAP((s)->guid, d, 20, 4);
+
+#define SQUASHFS_SWAP_BASE_INODE_HEADER_1(s, d, n) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_1(s, d, n)\
+}
+
+#define SQUASHFS_SWAP_IPC_INODE_HEADER_1(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_1(s, d, \
+			sizeof(struct squashfs_ipc_inode_header_1));\
+	SQUASHFS_SWAP((s)->type, d, 24, 4);\
+	SQUASHFS_SWAP((s)->offset, d, 28, 4);\
+}
+
+#define SQUASHFS_SWAP_DEV_INODE_HEADER_1(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_1(s, d, \
+			sizeof(struct squashfs_dev_inode_header_1));\
+	SQUASHFS_SWAP((s)->rdev, d, 24, 16);\
+}
+
+#define SQUASHFS_SWAP_SYMLINK_INODE_HEADER_1(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_1(s, d, \
+			sizeof(struct squashfs_symlink_inode_header_1));\
+	SQUASHFS_SWAP((s)->symlink_size, d, 24, 16);\
+}
+
+#define SQUASHFS_SWAP_REG_INODE_HEADER_1(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_1(s, d, \
+			sizeof(struct squashfs_reg_inode_header_1));\
+	SQUASHFS_SWAP((s)->mtime, d, 24, 32);\
+	SQUASHFS_SWAP((s)->start_block, d, 56, 32);\
+	SQUASHFS_SWAP((s)->file_size, d, 88, 32);\
+}
+
+#define SQUASHFS_SWAP_DIR_INODE_HEADER_1(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_1(s, d, \
+			sizeof(struct squashfs_dir_inode_header_1));\
+	SQUASHFS_SWAP((s)->file_size, d, 24, 19);\
+	SQUASHFS_SWAP((s)->offset, d, 43, 13);\
+	SQUASHFS_SWAP((s)->mtime, d, 56, 32);\
+	SQUASHFS_SWAP((s)->start_block, d, 88, 24);\
+}
+
+#endif
+
+#ifdef CONFIG_SQUASHFS_2_0_COMPATIBILITY
+
+struct squashfs_dir_index_2 {
+	unsigned int		index:27;
+	unsigned int		start_block:29;
+	unsigned char		size;
+	unsigned char		name[0];
+} __attribute__ ((packed));
+
+struct squashfs_base_inode_header_2 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:8; /* index into uid table */
+	unsigned int		guid:8; /* index into guid table */
+} __attribute__ ((packed));
+
+struct squashfs_ipc_inode_header_2 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:8; /* index into uid table */
+	unsigned int		guid:8; /* index into guid table */
+} __attribute__ ((packed));
+
+struct squashfs_dev_inode_header_2 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:8; /* index into uid table */
+	unsigned int		guid:8; /* index into guid table */
+	unsigned short		rdev;
+} __attribute__ ((packed));
+	
+struct squashfs_symlink_inode_header_2 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:8; /* index into uid table */
+	unsigned int		guid:8; /* index into guid table */
+	unsigned short		symlink_size;
+	char			symlink[0];
+} __attribute__ ((packed));
+
+struct squashfs_reg_inode_header_2 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:8; /* index into uid table */
+	unsigned int		guid:8; /* index into guid table */
+	unsigned int		mtime;
+	unsigned int		start_block;
+	unsigned int		fragment;
+	unsigned int		offset;
+	unsigned int		file_size:32;
+	unsigned short		block_list[0];
+} __attribute__ ((packed));
+
+struct squashfs_dir_inode_header_2 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:8; /* index into uid table */
+	unsigned int		guid:8; /* index into guid table */
+	unsigned int		file_size:19;
+	unsigned int		offset:13;
+	unsigned int		mtime;
+	unsigned int		start_block:24;
+} __attribute__  ((packed));
+
+struct squashfs_ldir_inode_header_2 {
+	unsigned int		inode_type:4;
+	unsigned int		mode:12; /* protection */
+	unsigned int		uid:8; /* index into uid table */
+	unsigned int		guid:8; /* index into guid table */
+	unsigned int		file_size:27;
+	unsigned int		offset:13;
+	unsigned int		mtime;
+	unsigned int		start_block:24;
+	unsigned int		i_count:16;
+	struct squashfs_dir_index_2	index[0];
+} __attribute__  ((packed));
+
+union squashfs_inode_header_2 {
+	struct squashfs_base_inode_header_2	base;
+	struct squashfs_dev_inode_header_2	dev;
+	struct squashfs_symlink_inode_header_2	symlink;
+	struct squashfs_reg_inode_header_2	reg;
+	struct squashfs_dir_inode_header_2	dir;
+	struct squashfs_ldir_inode_header_2	ldir;
+	struct squashfs_ipc_inode_header_2	ipc;
+};
+	
+struct squashfs_dir_header_2 {
+	unsigned int		count:8;
+	unsigned int		start_block:24;
+} __attribute__ ((packed));
+
+struct squashfs_dir_entry_2 {
+	unsigned int		offset:13;
+	unsigned int		type:3;
+	unsigned int		size:8;
+	char			name[0];
+} __attribute__ ((packed));
+
+struct squashfs_fragment_entry_2 {
+	unsigned int		start_block;
+	unsigned int		size;
+} __attribute__ ((packed));
+
+#define SQUASHFS_SWAP_BASE_INODE_CORE_2(s, d, n)\
+	SQUASHFS_MEMSET(s, d, n);\
+	SQUASHFS_SWAP((s)->inode_type, d, 0, 4);\
+	SQUASHFS_SWAP((s)->mode, d, 4, 12);\
+	SQUASHFS_SWAP((s)->uid, d, 16, 8);\
+	SQUASHFS_SWAP((s)->guid, d, 24, 8);\
+
+#define SQUASHFS_SWAP_BASE_INODE_HEADER_2(s, d, n) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_2(s, d, n)\
+}
+
+#define SQUASHFS_SWAP_IPC_INODE_HEADER_2(s, d) \
+	SQUASHFS_SWAP_BASE_INODE_HEADER_2(s, d, sizeof(struct squashfs_ipc_inode_header_2))
+
+#define SQUASHFS_SWAP_DEV_INODE_HEADER_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_2(s, d, \
+			sizeof(struct squashfs_dev_inode_header_2)); \
+	SQUASHFS_SWAP((s)->rdev, d, 32, 16);\
+}
+
+#define SQUASHFS_SWAP_SYMLINK_INODE_HEADER_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_2(s, d, \
+			sizeof(struct squashfs_symlink_inode_header_2));\
+	SQUASHFS_SWAP((s)->symlink_size, d, 32, 16);\
+}
+
+#define SQUASHFS_SWAP_REG_INODE_HEADER_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_2(s, d, \
+			sizeof(struct squashfs_reg_inode_header_2));\
+	SQUASHFS_SWAP((s)->mtime, d, 32, 32);\
+	SQUASHFS_SWAP((s)->start_block, d, 64, 32);\
+	SQUASHFS_SWAP((s)->fragment, d, 96, 32);\
+	SQUASHFS_SWAP((s)->offset, d, 128, 32);\
+	SQUASHFS_SWAP((s)->file_size, d, 160, 32);\
+}
+
+#define SQUASHFS_SWAP_DIR_INODE_HEADER_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_2(s, d, \
+			sizeof(struct squashfs_dir_inode_header_2));\
+	SQUASHFS_SWAP((s)->file_size, d, 32, 19);\
+	SQUASHFS_SWAP((s)->offset, d, 51, 13);\
+	SQUASHFS_SWAP((s)->mtime, d, 64, 32);\
+	SQUASHFS_SWAP((s)->start_block, d, 96, 24);\
+}
+
+#define SQUASHFS_SWAP_LDIR_INODE_HEADER_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_SWAP_BASE_INODE_CORE_2(s, d, \
+			sizeof(struct squashfs_ldir_inode_header_2));\
+	SQUASHFS_SWAP((s)->file_size, d, 32, 27);\
+	SQUASHFS_SWAP((s)->offset, d, 59, 13);\
+	SQUASHFS_SWAP((s)->mtime, d, 72, 32);\
+	SQUASHFS_SWAP((s)->start_block, d, 104, 24);\
+	SQUASHFS_SWAP((s)->i_count, d, 128, 16);\
+}
+
+#define SQUASHFS_SWAP_DIR_INDEX_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_dir_index_2));\
+	SQUASHFS_SWAP((s)->index, d, 0, 27);\
+	SQUASHFS_SWAP((s)->start_block, d, 27, 29);\
+	SQUASHFS_SWAP((s)->size, d, 56, 8);\
+}
+#define SQUASHFS_SWAP_DIR_HEADER_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_dir_header_2));\
+	SQUASHFS_SWAP((s)->count, d, 0, 8);\
+	SQUASHFS_SWAP((s)->start_block, d, 8, 24);\
+}
+
+#define SQUASHFS_SWAP_DIR_ENTRY_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_dir_entry_2));\
+	SQUASHFS_SWAP((s)->offset, d, 0, 13);\
+	SQUASHFS_SWAP((s)->type, d, 13, 3);\
+	SQUASHFS_SWAP((s)->size, d, 16, 8);\
+}
+
+#define SQUASHFS_SWAP_FRAGMENT_ENTRY_2(s, d) {\
+	SQUASHFS_SWAP_START\
+	SQUASHFS_MEMSET(s, d, sizeof(struct squashfs_fragment_entry_2));\
+	SQUASHFS_SWAP((s)->start_block, d, 0, 32);\
+	SQUASHFS_SWAP((s)->size, d, 32, 32);\
+}
+
+#define SQUASHFS_SWAP_FRAGMENT_INDEXES_2(s, d, n) SQUASHFS_SWAP_INTS(s, d, n)
+
+/* fragment and fragment table defines */
+#define SQUASHFS_FRAGMENT_BYTES_2(A)	(A * sizeof(struct squashfs_fragment_entry_2))
+
+#define SQUASHFS_FRAGMENT_INDEX_2(A)	(SQUASHFS_FRAGMENT_BYTES_2(A) / \
+					SQUASHFS_METADATA_SIZE)
+
+#define SQUASHFS_FRAGMENT_INDEX_OFFSET_2(A)	(SQUASHFS_FRAGMENT_BYTES_2(A) % \
+						SQUASHFS_METADATA_SIZE)
+
+#define SQUASHFS_FRAGMENT_INDEXES_2(A)	((SQUASHFS_FRAGMENT_BYTES_2(A) + \
+					SQUASHFS_METADATA_SIZE - 1) / \
+					SQUASHFS_METADATA_SIZE)
+
+#define SQUASHFS_FRAGMENT_INDEX_BYTES_2(A)	(SQUASHFS_FRAGMENT_INDEXES_2(A) *\
+						sizeof(int))
+
+#endif
+
+#ifdef __KERNEL__
+
+/*
+ * macros used to swap each structure entry, taking into account
+ * bitfields and different bitfield placing conventions on differing
+ * architectures
+ */
+
+#include <asm/byteorder.h>
+
+#ifdef __BIG_ENDIAN
+	/* convert from little endian to big endian */
+#define SQUASHFS_SWAP(value, p, pos, tbits) _SQUASHFS_SWAP(value, p, pos, \
+		tbits, b_pos)
+#else
+	/* convert from big endian to little endian */ 
+#define SQUASHFS_SWAP(value, p, pos, tbits) _SQUASHFS_SWAP(value, p, pos, \
+		tbits, 64 - tbits - b_pos)
+#endif
+
+#define _SQUASHFS_SWAP(value, p, pos, tbits, SHIFT) {\
+	b_pos = pos % 8;\
+	val = 0;\
+	s = (unsigned char *)p + (pos / 8);\
+	d = ((unsigned char *) &val) + 7;\
+	for(bits = 0; bits < (tbits + b_pos); bits += 8) \
+		*d-- = *s++;\
+	value = (val >> (SHIFT))/* & ((1 << tbits) - 1)*/;\
+}
+
+#define SQUASHFS_MEMSET(s, d, n)	memset(s, 0, n);
+
+#endif
+#endif
