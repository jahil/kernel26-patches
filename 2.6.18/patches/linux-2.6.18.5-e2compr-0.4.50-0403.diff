diff -pruN linux-2.6.18.5.org/.config linux-2.6.18.5/.config
--- linux-2.6.18.5.org/.config	2007-03-29 11:44:06.000000000 -0700
+++ linux-2.6.18.5/.config	2007-02-17 10:32:04.000000000 -0800
@@ -2545,6 +2545,39 @@ CONFIG_INTEL_IOATDMA=m
 CONFIG_EXT2_FS=y
 # CONFIG_EXT2_FS_XATTR is not set
 # CONFIG_EXT2_FS_XIP is not set
+CONFIG_EXT2_COMPRESS=y
+
+#
+# Ext2 file compression options
+#
+# CONFIG_EXT2_HAVE_LZO is not set
+# CONFIG_EXT2_HAVE_LZV1 is not set
+# CONFIG_EXT2_HAVE_LZRW3A is not set
+CONFIG_EXT2_HAVE_GZIP=y
+# CONFIG_EXT2_HAVE_BZIP2 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_DEFER is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_LZO is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_LZV1 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_LZRW3A is not set
+CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP=y
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_BZIP2 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP1 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP2 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP3 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP4 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP5 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP6 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP7 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP8 is not set
+CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP9=y
+CONFIG_EXT2_COMPR_X86_CODE=y
+CONFIG_EXT2_SEPARATE_WORK_AREAS=y
+CONFIG_GZ_HACK=y
+# CONFIG_EXT2_VERIFY_COMPRESSION is not set
+# CONFIG_EXT2_DEFAULT_CLUSTER_BITS_2 is not set
+# CONFIG_EXT2_DEFAULT_CLUSTER_BITS_3 is not set
+# CONFIG_EXT2_DEFAULT_CLUSTER_BITS_4 is not set
+CONFIG_EXT2_DEFAULT_CLUSTER_BITS_5=y
 CONFIG_EXT3_FS=y
 # CONFIG_EXT3_FS_XATTR is not set
 CONFIG_JBD=y
diff -pruN linux-2.6.18.5.org/Documentation/filesystems/e2compress.txt linux-2.6.18.5/Documentation/filesystems/e2compress.txt
--- linux-2.6.18.5.org/Documentation/filesystems/e2compress.txt	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/Documentation/filesystems/e2compress.txt	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,117 @@
+Transparent compression for ext2 filesystem
+===========================================
+
+What this document is.
+----------------------
+This document is intended for explaining how e2compress has been implented/ported
+in kernel 2.4. It also give a status of current work. You need to have e2compress
+knowledge (i.e. to know how e2compress works, from a general point of view)
+
+What this document is not.
+--------------------------
+This document is not a full explaination of how e2compress work. For this,
+there are other documents such as fs/ext2/Readme.e2compr file for the technical
+point of view and user manual can be found at <http://e2compr.sourceforge.net/>.
+This site is also a place were you will find many information about e2compress
+development for kernel 2.4, tools, manuals and so on.
+
+
+Introduction
+============
+
+This is a first adaptation of e2compress for kernel 2.4. The work has been done
+by Alcatel (Alcatel Business Systems - R&D) at Illkirch. It has been started
+from the latest patch provided by Peter Moulder for kernel 2.2,
+i.e. e2compr-0.4.39-patch-2.2.18.
+It is full compatible with previous version.
+Here after you will first find some explainations about the choices mades for
+the development, and then the status of current work from functionnal point of
+view.
+
+
+Development
+===========
+
+As for previous patches, most interesting happens when reading in ext2_readpage
+and when writing in ext2_writepage and ext2_file_write.
+In fact, in 2.2 kernel, compression occures on cluster of blocks. So when reading
+or writing a part of a file, we first have to compute the cluster on which I/O
+occures, then we have to get every buffers of the cluster, uncompress the data if
+needed, then reading/writing happens "as for normal files".
+In 2.4 kernels, I/O occures through page cache: i.e. when reading/writing to a
+part of the file, first the corresponding page is get, we then get the needed
+buffers, which point to the page; this means that for keeping same work as for 2.2,
+we have to use the notion of cluster of page. For getting every buffers of a cluster,
+we first get every pages of the cluster, then get buffers of every pages...
+
+So, things happens as follow:
+
+ext2_readpage
+-------------
+
+If data corresponding to the page are in a compressed cluster, this functions perfoms
+more works: instead of reading one page, it reads the whole "cluster of pages".
+In fact, anyway, we have to read all compressed buffer. Once we have got all buffers
+of the cluster, uncompressed (at least a part of) the data, and located the part of
+the uncompressed data which correspond to the requested page, there is not any more 
+lot of work for also reading (i.e. doing some memcpy) other pages belonging to this
+cluster.
+So, the first reading of the first page of the cluster is quite longer, but then,
+every pages of the cluster are uptodate in the cache.
+
+ext2_writepage
+--------------
+An overhead has been added for pages belonging to a compressed cluster.
+In fact, if cluster is still compressed on the disk, we can't directly write the
+page (which contains uncompressed data) in the middle of a compressed cluster.
+So, we first have to uncompress the whole cluster on the disk, then we can write the
+new data of the dirty page(s).
+
+ext2_file_write
+---------------
+This replaces `generic_file_write' when e2compress option is activated.
+It is a copy of `generic_file_write'. The main difference is that instead of looping
+page by page in `generic_file_write', we loops on cluster of page.
+In each loop:
+	* we compute the cluster on which beginning of data (to be written) belongs to.
+	* then, we get all pages of the cluster.
+	* If cluster is a compressed one, we read all pages, and uncompress it.
+	  Otherwise, we perfoms a `prepare_write' (as in generic_file_write).
+	* We copy the data on each page from user space,
+	* Call `commit_write' on dirty pages.
+	* When reaching end of cluster, we compress it. (As in 2.2)
+
+Note: Another implentation could have been to keep generic_file_write, and add an overhead
+to `ext2_prepare_write' and `ext2_commit_write'; on the first access to a page of a compressed
+cluster, whole cluster will be uncompressed (i.e. all pages of the cluster will be read and
+uncompressed in `ext2_prepare_write') and when commiting the last page of the cluster,
+compression occures...
+
+ext2_open_file
+--------------
+In 2.4.16 kernel, this function has been added for treating the case of files opened for
+"direct IO". Direct IO is not supported on compressed file. So opening a file by this way
+is forbidden.
+
+Other places in ext2
+--------------------
+Other changes occures as in 2.2 for managing the compression flags of files and specific
+`COMPRESSED_BLK_ADDR' address for compressed blocks.
+So please, refer to existing documentation for 2.2 about this topic.
+
+Status
+======
+Today (middle of december 2001), e2compress on kernel 2.4.16 has been tested on i386
+architecture, is used with success by tens of people in the department from some weeks.
+It is full fonctionnal on ix86, full compatible with 2.2 version of e2compress.
+It should work on other architecture, but has NOT been tested.
+Please, note the following:
+	* No performance tests have been done.
+	* I don't proclaim that code is optimized (and it is probably not, but I hope that
+	  "gurus" will not find it too bad)
+So, I think I can say that there is no known "big" bug or "blocking" bug.
+
+Some strange things has been observed in very limit case, i.e. when memory is overloaded.
+
+
+As usual, this e2compress comes without warranty, use it at your won risk, etc...
diff -pruN linux-2.6.18.5.org/Documentation/filesystems/ext2_compression linux-2.6.18.5/Documentation/filesystems/ext2_compression
--- linux-2.6.18.5.org/Documentation/filesystems/ext2_compression	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/Documentation/filesystems/ext2_compression	2007-03-06 11:11:24.000000000 -0800
@@ -0,0 +1,42 @@
+Release notes
+=============
+
+(These are sorted by version so that you don't have to reread things.
+However, all of them are still important, or else I'd have removed
+them.)
+
+
+New with 0.4.5
+--------------
+
+Adapted the 2.6.10 kernel patch to  2.6.18.5 (patched Vector Linux kernel).
+NOTE: Post 2.6.10 kernel changed from a semaphore in the inode data structure
+to a mutex. See fs/ext2/ChangeLog.e2compr for more details. Added ifdef code
+to force real-time write of inode data to disk in support of mounting USB 
+thumbdrives as root file system. TLL Mar 6, 2007.
+
+
+New with 0.4.3
+--------------
+
+If you get many many compression error messages in your kernel system
+logs, then it is probably an error in the error checking rather than
+in the de/compression algorithms.
+
+
+New with 0.4.0
+--------------
+
+Starting with e2compr-0.4, you must reply `y' to CONFIG_EXPERIMENTAL
+if you wish to be asked for ext2 compression.  (This is in readiness
+for inclusion into the standard kernel.)
+
+Don't rely on `chattr +X' (or the corresponding ioctl).  That
+interface is broken.  It doesn't guarantee exclusivity for any kernel
+after 2.1.42, so I've mostly removed support for it here.  (I still
+keep it partially available for diagnostic purposes.)  A replacement
+interface will probably arrive later.  If you wish to implement it
+yourself, write to me <pjm@bofh.asn.au> and I'll send you my
+plans for how to implement it.  (I don't mind how you implement it,
+but I can at least inform you of the various race conditions etc. to
+avoid.)
diff -pruN linux-2.6.18.5.org/MAINTAINERS linux-2.6.18.5/MAINTAINERS
--- linux-2.6.18.5.org/MAINTAINERS	2006-12-04 13:34:15.000000000 -0800
+++ linux-2.6.18.5/MAINTAINERS	2007-01-23 11:37:02.000000000 -0800
@@ -929,6 +929,13 @@ W:	http://linuxtv.org/
 T:	git kernel.org:/pub/scm/linux/kernel/git/mchehab/v4l-dvb.git
 S:	Maintained
 
+E2COMPR (ext2 compression patches)
+P:	Bodo Thiesen
+P:	Paul Whittaker
+M:	bothie@users.sourceforge.net
+M:	whitpa@users.sourceforge.net
+S:	Odd Fixes
+
 EATA-DMA SCSI DRIVER
 P:	Michael Neuffer
 L:	linux-eata@i-connect.net, linux-scsi@vger.kernel.org
diff -pruN linux-2.6.18.5.org/arch/i386/defconfig linux-2.6.18.5/arch/i386/defconfig
--- linux-2.6.18.5.org/arch/i386/defconfig	2006-12-04 13:32:23.000000000 -0800
+++ linux-2.6.18.5/arch/i386/defconfig	2007-01-27 10:40:14.000000000 -0800
@@ -1363,6 +1363,35 @@ CONFIG_USB_STORAGE=y
 CONFIG_EXT2_FS=y
 # CONFIG_EXT2_FS_XATTR is not set
 # CONFIG_EXT2_FS_XIP is not set
+CONFIG_EXT2_COMPRESS=n
+CONFIG_EXT2_HAVE_LZO=y
+CONFIG_EXT2_HAVE_LZV1=y
+CONFIG_EXT2_HAVE_LZRW3A=y
+CONFIG_EXT2_HAVE_GZIP=y
+CONFIG_EXT2_HAVE_BZIP2=y
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_DEFER is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_LZO is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_LZV1 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_LZRW3A is not set
+CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP=y
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_BZIP2 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP1 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP2 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP3 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP4 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP5 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP6 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP7 is not set
+# CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP8 is not set
+CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP9=y
+# CONFIG_EXT2_DEFAULT_CLUSTER_BITS_2 is not set
+# CONFIG_EXT2_DEFAULT_CLUSTER_BITS_3 is not set
+# CONFIG_EXT2_DEFAULT_CLUSTER_BITS_4 is not set
+CONFIG_EXT2_DEFAULT_CLUSTER_BITS_5=y
+# CONFIG_EXT2_COMPR_X86_CODE is not set
+CONFIG_EXT2_SEPARATE_WORK_AREAS=y
+CONFIG_GZ_HACK=y
+CONFIG_EXT2_VERIFY_COMPRESSION=y
 # CONFIG_EXT3_FS is not set
 # CONFIG_REISERFS_FS is not set
 # CONFIG_JFS_FS is not set
diff -pruN linux-2.6.18.5.org/fs/Kconfig linux-2.6.18.5/fs/Kconfig
--- linux-2.6.18.5.org/fs/Kconfig	2006-12-04 13:32:03.000000000 -0800
+++ linux-2.6.18.5/fs/Kconfig	2007-01-23 11:37:02.000000000 -0800
@@ -68,6 +68,301 @@ config FS_XIP
 	depends on EXT2_FS_XIP
 	default y
 
+config EXT2_COMPRESS
+	bool "Ext2 file compression (DANGEROUS)"
+	depends on EXT2_FS && EXPERIMENTAL
+	help
+	  Ext2 file compression allows transparent compression of files on an
+	  ext2 filesystem.  Transparent compression means that files are
+	  stored on the disk in a compressed format but they are automatically
+	  decompressed as they are read in and compressed when written out.
+	  The user is in control of how and which files are compressed, using
+	  the `chattr' utility (see chattr(1)).  For the sake of safety,
+	  administrative data (superblock, inodes, directories, etc.) are not
+	  compressed.
+
+	  Compression is very useful if you're short on disk space, and
+	  provides a better option than having lots of .gz files around.
+	  For more information, see <http://e2compr.sourceforge.net/>.
+
+	  You _need_ to have the special e2compr version of e2fsck to be able
+	  to make use of this.
+
+	  If you say Y, you will be asked which compression algorithms you wish
+	  to include.  Gzip is a good all-round algorithm, as its 1..9 parameter
+	  allows a good range of speed/compression trade-off.  Other noteworthy
+	  algorithms are LZV, which caters better to the faster/less compressing
+	  end of the scale, and bzip, which caters slightly better to the more
+	  compressing but slower end of the scale.
+
+	  Ext2 compression is still experimental, so unless you know you need
+	  it, you'd better say N.
+
+menu "Ext2 file compression options"
+	depends on EXT2_COMPRESS
+
+config EXT2_HAVE_LZO
+	tristate "LZO1X_1 algorithm"
+	depends on EXT2_COMPRESS && EXT2_FS
+	help
+	  Enable this if you want the Lempel-Ziv-Oberhumer (LZO) algorithm
+	  to be available for compression or decompression.  (It is actually
+	  the lzo1x_1 algorithm.)  If it is not enabled, you will not be able
+	  to read any file already stored in this format.
+
+	  This algorithm is the fastest at decompressing, but is slower than
+	  LZV1 at compressing.  LZO compresses smaller than LZV1.
+
+	  If unsure, say Y.
+
+	  If you want to compile this algorithm as a module ( = code which
+	  can be inserted in and removed from the running kernel whenever
+	  you want), say M here and read Documentation/modules.txt.  The module
+	  will be called ext2-compr-lzo.ko.  Be aware however that the default
+	  compression algorithm cannot be compiled as a module.
+
+config EXT2_HAVE_LZV1
+	tristate "LZV1 algorithm"
+	depends on EXT2_COMPRESS && EXT2_FS
+	help
+	  Enable this if you want the Lempel-Ziv-Vogt (LZV) algorithm to be
+	  available for compression or decompression.  If it is not enabled,
+	  you will not be able to read any file already stored in this format.
+
+	  This algorithm tends to be the fastest of the available algorithms
+	  (except that LZO is faster at decompressing), but tends not to
+	  compress as well as the others.  If unsure, say Y.
+
+	  If you want to compile this algorithm as a module ( = code which
+	  can be inserted in and removed from the running kernel whenever
+	  you want), say M here and read Documentation/modules.txt.  The module
+	  will be called ext2-compr-lzv1.ko.  Be aware however that the default
+	  compression algorithm cannot be compiled as a module.
+
+config EXT2_HAVE_LZRW3A
+	tristate "LZRW3A algorithm"
+	depends on EXT2_COMPRESS && EXT2_FS
+	help
+	  Enable this if you want the LZRW3A algorithm to be available for
+	  compression or decompression.  If it is not enabled, you will not be
+	  able to read any file already stored in this format.
+
+	  LZRW3A tends to be between GZIP and LZV1 in both speed and
+	  compactness.  If unsure, say Y.
+
+	  If you want to compile this algorithm as a module ( = code which
+	  can be inserted in and removed from the running kernel whenever
+	  you want), say M here and read Documentation/modules.txt.  The module
+	  will be called ext2-compr-lzrw3a.ko.  Be aware however that the
+	  default compression algorithm cannot be compiled as a module.
+
+config EXT2_HAVE_GZIP
+	tristate "GZIP algorithm"
+	depends on EXT2_COMPRESS && EXT2_FS
+	default EXT2_FS
+	help
+	  Enable this if you want the GZIP algorithm to be available for
+	  compression or decompression.  If it is not enabled, you will not be
+	  able to read any file already stored in this format.
+
+	  This is a "good value" compression algorithm, because the 1..9
+	  parameter allows a wide range of compression speeds and compression
+	  amounts.  If unsure, say Y.
+
+	  If you want to compile this algorithm as a module ( = code which
+	  can be inserted in and removed from the running kernel whenever
+	  you want), say M here and read Documentation/modules.txt.  The module
+	  will be called ext2-compr-gzip.ko.  Be aware however that the
+	  default compression algorithm cannot be compiled as a module.
+
+config EXT2_HAVE_BZIP2
+	tristate "BZIP2 algorithm"
+	depends on EXT2_COMPRESS && EXT2_FS
+	help
+	  Enable this if you want the BZIP2 algorithm to be available for
+	  compression or decompression.  If it is not enabled, you will not be
+	  able to read any file already stored in this format.
+
+	  This algorithm is related to, but is not the same as, the algorithm
+	  used by the `bzip2' program.  Compared to gzip9, it tends to compress
+	  10-15% smaller for text files, but to about the same as gzip9 for
+	  binaries.  It is 5 times slower at compressing than gzip9, and 50%
+	  slower at decompressing.  Enabling this will enlarge your kernel by
+	  about 33kB.  If unsure, say Y.
+
+	  If you want to compile this algorithm as a module ( = code which
+	  can be inserted in and removed from the running kernel whenever
+	  you want), say M here and read Documentation/modules.txt.  The module
+	  will be called ext2-compr-bzip2.ko.  Be aware however that the
+	  default compression algorithm cannot be compiled as a module.
+
+choice
+	depends on EXT2_COMPRESS
+	prompt "Default compression algorithm"
+	default EXT2_DEFAULT_COMPR_METHOD_DEFER if !EXT2_HAVE_GZIP
+	default EXT2_DEFAULT_COMPR_METHOD_GZIP if EXT2_HAVE_GZIP
+	help
+	  This is the compression algorithm that is used if you do not specify
+	  any other when you set the compressed attribute of a file or
+	  directory.  The algorithm that you choose must be one of those
+	  enabled in the previous questions, and not a module.  For the
+	  relative merit of each see the help of the individual algorithms
+	  above.
+
+	  If the GZIP algorithm is eligible the default is GZIP8, which tends
+	  to produce relatively compression at the cost of being moderately
+	  slow.  Otherwise the default is deferred compression (a dummy
+	  algorithm that prepares for later compression without actually
+	  compressing anything).
+
+config EXT2_DEFAULT_COMPR_METHOD_DEFER
+	bool "None (defer compression)"
+
+config EXT2_DEFAULT_COMPR_METHOD_LZO
+	bool "LZO1X_1 algorithm" if EXT2_HAVE_LZO = y
+
+config EXT2_DEFAULT_COMPR_METHOD_LZV1
+	bool "LZV1 algorithm" if EXT2_HAVE_LZV1 = y
+
+config EXT2_DEFAULT_COMPR_METHOD_LZRW3A
+	bool "LZRW3A algorithm" if EXT2_HAVE_LZRW3A = y
+
+config EXT2_DEFAULT_COMPR_METHOD_GZIP
+	bool "GZIP algorithm" if EXT2_HAVE_GZIP = y
+
+config EXT2_DEFAULT_COMPR_METHOD_BZIP2
+	bool "BZIP2 algorithm" if EXT2_HAVE_BZIP2 = y
+
+endchoice
+
+choice
+	depends on EXT2_DEFAULT_COMPR_METHOD_GZIP
+	prompt "Gzip parameter for default compression method"
+	default EXT2_DEFAULT_COMPR_METHOD_GZIP8
+	help
+	  You have selected `gzip' as your default compression algorithm, but
+	  I need to know whether to use `gzip -1', `gzip -9', or somewhere
+	  in between.  gzip1 is the least compressing but fastest; gzip9 is the
+	  most compressing and slowest; and the numbers in between have
+	  characteristics in between (though not on a linear scale; see
+	  fs/ext2/gzip/deflate.c if you're interested).
+	  If unsure, say `8'.
+
+config EXT2_DEFAULT_COMPR_METHOD_GZIP1
+	bool "1"
+config EXT2_DEFAULT_COMPR_METHOD_GZIP2
+	bool "2"
+config EXT2_DEFAULT_COMPR_METHOD_GZIP3
+	bool "3"
+config EXT2_DEFAULT_COMPR_METHOD_GZIP4
+	bool "4"
+config EXT2_DEFAULT_COMPR_METHOD_GZIP5
+	bool "5"
+config EXT2_DEFAULT_COMPR_METHOD_GZIP6
+	bool "6"
+config EXT2_DEFAULT_COMPR_METHOD_GZIP7
+	bool "7"
+config EXT2_DEFAULT_COMPR_METHOD_GZIP8
+	bool "8"
+config EXT2_DEFAULT_COMPR_METHOD_GZIP9
+	bool "9"
+
+endchoice
+
+config EXT2_COMPR_X86_CODE
+	bool "Assembly versions of compression routines"
+	depends on EXT2_COMPRESS && X86
+	default y
+	help
+	  For x86 kernels there are some assembler language versions of the
+	  compression routines for the lzv1 and gzip algorithms.
+	  These apparently run "much faster" than the C ones.  There isn't any
+	  reason to say N here unless you've found a problem with them (which
+	  you should of course report).  If unsure, say Y.
+
+config EXT2_SEPARATE_WORK_AREAS
+	bool "Extra work area for compression"
+	depends on EXT2_COMPRESS
+	default y
+	help
+	  This is an experimental feature aimed at improving response times in
+	  the case where most of your binaries are compressed and you are
+	  relatively short of memory (so there's lots of paging), and there is
+	  some compression going on.  Without enabling this option in this
+	  case, none of the compressed binaries are able to page in until the
+	  (slow) compression has finished.
+
+	  By enabling this option, compression and decompression happen in
+	  separate work areas.  Since decompression is generally faster than
+	  compression, this results in a shorter wait and hopefully better
+	  response times.  The downside is that the extra work area costs about
+	  300KB of kernel memory, which of course tends to _increase_ response
+	  times, and decrease throughput.  (In future releases, the memory cost
+	  can be decreased.)
+
+	  If you wish to compress most of your binaries, and response time is
+	  important to you, and you have enough memory that 300KB will not be
+	  missed much, then say `y'.  That said, I'd encourage most existing
+	  e2compr users to say `y' for now, and tell me whether you notice a
+	  difference.
+
+config GZ_HACK
+	bool "Exclude .gz files from automatic compression"
+	depends on EXT2_COMPRESS
+	default y
+	help
+	  If you say Y here, then files created with names ending in `.gz' or
+	  `.?gz' or `.bz2' don't inherit the `c' ("compress") attribute from
+	  their parent directory.  (However, you can still do `chattr +c FILE'
+	  if you want to try to compress it anyway.)  This means that you
+	  don't waste CPU time trying to compress a file that probably can't
+	  be compressed.  See fs/ext2/namei.c if you want to add other rules.
+	  If you have any aesthetic sensibilities then you will say N here
+	  and try to implement something better.  Most people will say Y here.
+
+config EXT2_VERIFY_COMPRESSION
+	bool "Verify compression"
+	depends on EXT2_COMPRESS
+	default y
+	help
+	  Every time we compress a cluster, immediately decompress the data
+	  and compare with original.  This takes a bit more time, but is
+	  probably worth it for the safety.  Remember, a rare bug was
+	  discovered in the gzip algorithm after it had been in world-wide use
+	  for years.  If unsure, say Y.
+
+choice
+	depends on EXT2_COMPRESS
+        prompt "Default cluster size (in blocks, usually 1KB each)"
+        default EXT2_DEFAULT_CLUSTER_BITS_5
+	help
+	  To make random access to compressed files reasonably fast the files
+	  are compressed in clusters.  By default, the clusters will be of the
+	  size defined here but there is a modified version of the chattr
+	  utility that can set the cluster size for each file independently.
+	  Large clusters usually result in better compression at the cost of
+	  being slower.
+
+	  Note that the answer to this question is specified in filesystem
+	  blocks rather than in kilobytes, though most filesystems have 1KB
+	  blocks anyway.  (If you have a filesystem with large blocks then
+	  you should know it, but if you want to check then "tune2fs -l
+	  /dev/xxx | grep size".)  The default is 32 blocks which is the
+	  slowest setting but gives the best compression.
+
+config EXT2_DEFAULT_CLUSTER_BITS_2
+	bool "4"
+config EXT2_DEFAULT_CLUSTER_BITS_3
+	bool "8"
+config EXT2_DEFAULT_CLUSTER_BITS_4
+	bool "16"
+config EXT2_DEFAULT_CLUSTER_BITS_5
+	bool "32"
+
+endchoice
+
+endmenu
+
 config EXT3_FS
 	tristate "Ext3 journalling file system support"
 	select JBD
diff -pruN linux-2.6.18.5.org/fs/Makefile linux-2.6.18.5/fs/Makefile
--- linux-2.6.18.5.org/fs/Makefile	2006-12-04 13:32:01.000000000 -0800
+++ linux-2.6.18.5/fs/Makefile	2007-01-23 11:37:02.000000000 -0800
@@ -51,6 +51,11 @@ obj-y				+= devpts/
 
 obj-$(CONFIG_PROFILING)		+= dcookies.o
  
+ifneq ($(CONFIG_EXT2_FS),n)
+ifneq ($(CONFIG_EXT2_FS),)
+obj-m				+= ext2/
+endif
+endif
 # Do not add any filesystems before this line
 obj-$(CONFIG_REISERFS_FS)	+= reiserfs/
 obj-$(CONFIG_EXT3_FS)		+= ext3/ # Before ext2 so root fs can be ext3
diff -pruN linux-2.6.18.5.org/fs/ext2/ChangeLog.e2compr linux-2.6.18.5/fs/ext2/ChangeLog.e2compr
--- linux-2.6.18.5.org/fs/ext2/ChangeLog.e2compr	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/ChangeLog.e2compr	2007-04-02 00:47:12.000000000 -0700
@@ -0,0 +1,631 @@
+25 Mar 2007
+
+	Terry Loveall <loveall@iinet.com>
+
+	* compress.c:
+	  bug fix: ext2_count_blocks: init head_bh for each iteration.
+	  bug fix: ext2_count_blocks: add set clen=ulen for uncompressable clusters.
+	  bug fix: ext2_compress_cluster: replacement and inlining of an 
+	   invalidate_inode_buffers function to keep root filesystem changes
+	   uptodate on disk (prevents umounting root file system to update).
+	  warning fix: ext2_compress_cluster: various variables initialized.
+	  ext2_compress_cluster: removed #ifdef NDEBUG
+	  bug fix: ext2_compress_cluster: defined maxclus, calculate and set for:
+	  bug fix: ext2_compress_cluster: set i_size for uncompressed clusters.
+	  ext2_cleanup_compressed_inode: changed error message to indicate 'Z'
+	   flag was caused by trying to un/compress already open file.
+	  bug fix: cp to compr dir: Truncate uncompressed files to their
+	   uncompressed length, i.e. force kernel to update inode and sb
+
+	* inode.c:
+	  bug fix: ext2_get_block: restored changed: loop to bforget
+
+	* ioctl.c:
+	  ext2_ioctl: scrubbed 'B' flag on file uncompress.
+
+28 Feb 2005
+
+	Yabo Ding <bobfree_cn@yahoo.com.cn>,<yding@wyse.com>
+
+	* Corrected page unlocking in inode.c.
+
+19 Feb 2005
+
+	Paul Whittaker <whitpa@users.sourceforge.net>
+
+	* Added corrections le32_to_cpu in critical areas of compress.c
+	* Optimized function exit code in inode.c.
+
+26 Jan 2005
+
+	Paul Whittaker <whitpa@users.sourceforge.net>
+
+	* ../Kconfig: Added EXT2_FS dependency for algorithms (such that they
+	  behave like dep_tristate under the old system).  Partially fixed a
+	  problem with GZIP not being set as the default if eligible.  Updated
+	  the help for most items.
+
+	* compress.c: commented out quota-disabled assertion, as it applies too
+	  generally.
+
+	* ../../arch/i386/defconfig: corrected missing and obsolete default
+	  options.
+
+	* ../../include/linux/assert.h, ../../include/linux/ext2_fs_c.h,
+	  debug.h, ioctl.c, lzo/{Makefile,lzoconf.h}: for tidiness, moved
+	  assert.h into this directory, and amended other files as required.
+	  <linux/assert.h> is an obsolete Linux-2.2-ism; this really ought to
+	  be fixed properly at some stage.
+
+	* compress.c: update for 2.6.10: buffer_insert_list() no longer exists,
+	  instantiated the body of this function inline instead.
+
+	* file.c: update for 2.6.10: current->rlim is now current->signal->rlim.
+
+	* super.c, inode.c: update for 2.6.10: ext2_put_inode had been removed,
+	  so I had to reinstate it (for e2compr use only).
+
+30 Sep 2004
+
+	Paul Whittaker <whitpa@users.sourceforge.net>
+
+	* compress.c: fixed LZRW3A availability mistake in
+	  ext2_algorithm_table.
+
+	* new-method-howto.c: updated.
+
+29 Sep 2004
+
+	Paul Whittaker <whitpa@users.sourceforge.net>
+
+	* ioctl.c: for completeness, added atomic_read to a commented-out
+	  i_count access.
+
+	* Readme.e2compr: minor corrections.
+
+02 Aug 2004
+
+	Paul Whittaker <whitpa@users.sourceforge.net>
+
+	* lzrw3a/lzrw3a.c: added (UBYTE *) casts to avoid compiler warnings.
+
+01 May 2004
+
+	Paul Whittaker <whitpa@users.sourceforge.net>
+
+	* inode.c: fix from Guy Bilodeau <guy.bilodeau@colubris.com>: zero the
+	  epg array after failed ext2_get_cluster_extra_pages() before exiting
+	  ext2_readpage().
+
+	* compress.c: fixes from Guy Bilodeau <guy.bilodeau@colubris.com>:
+	  in ext2_get_cluster_pages() and ext2_get_cluster_extra_pages() npg
+	  must be int, otherwise "while (--npg >= 0)" is never true; NULL-test
+	  epg[npg] in ext2_get_cluster_extra_pages() before attempting page
+	  release.
+
+25 Mar 2004
+
+Version 0.4.44 released.
+
+	Paul Whittaker <whitpa@users.sourceforge.net>
+
+	* Makefile, {bzip2,gzip,lzo,lzrw3a,lzv1}/Makefile: changed target names
+	  to ext2-compr-{bzip2,gzip,lzo,lzwr3a,lzv1}.  This both ensures that
+	  the revised automatic module loading scheme works (see below), and
+	  removes any possible ambiguity as to an algorithm module's purpose.
+
+	* bzip/, lzrw/, lzv/: renamed to bzip2, lzrw3a and lzv1 respectively,
+	  for consistency with kernel config defines and algorithm names in
+	  compress.c.
+
+29 Feb 2004
+
+	* compress.c, inode.c, ioctl.c: changed code preceding request_module()
+	  to request the algorithm by name rather than number.  The
+	  modules.conf aliases are therefore no longer required (removed
+	  Modules.e2compr).
+
+22 Feb 2004
+
+	Paul Whittaker <whitpa@users.sourceforge.net>
+
+	* ext2_fs_c.h, compress.c: fixed broken trace_e2c() calls, associated
+	  with EXT2_COMPR_REPORT instead of EXT2_COMPR_DEBUG.
+
+17 Feb 2004
+
+	Paul Whittaker <whitpa@users.sourceforge.net>
+
+	* ioctl.c: added S_ISREG safety checks to EXT2_IOC_SETCOMPRMETHOD ioctl
+	  to prevent inadvertent marking of directories with EXT2_DIRTY_FL and
+	  EXT2_CLEANUP_FL, or attempts to decompress them.
+
+22 Jan 2004
+
+	Paul Whittaker <whitpa@users.sourceforge.net>
+
+	* various: reinstated the LZRW3A algorithm using code from an old 2.2
+	  kernel patch, and updated e2compr-lzrw3a.c with module author,
+	  description and license info.
+
+	  LZRW3A was removed due to concerns with patent encumbrances, however
+	  LZRW3 is still used in the Linux ftape driver, and public opinion is
+	  that LZW-derivatives of this sort ought to be OK to use in GPL code.
+
+	* compress.c: allow for the case where a WA is legitimately NULL when
+	  ext2_register_compression_module() is called.  This can happen!
+
+	* ext2_fs_c.h: CONFIG_EXT2_DEFAULT_COMPR_METHOD_NONE is never defined,
+	  should be CONFIG_EXT2_DEFAULT_COMPR_METHOD_DEFER.
+
+19 Jan 2004
+
+Version 0.4.43a released
+
+	Paul Whittaker <whitpa@users.sourceforge.net>
+
+	* lzo/lzoconf.h, lzo/lzo_conf.h: minimal modifications to allow
+	  compilation with -nostdinc.  This code is a mess!
+
+	* inode.c: fixed a broken EXT2_COMPR_REPORT_VERBOSE printk.
+
+	* compress.c: moved n_clusters assignment ahead of its first use.
+
+	* compress.c, file.c, iname.c: use atomic_read() to read i_count.
+
+	* gzip/Makefile, gzip/deflate.c: Simplified #ifs such that assembly
+	  routines are used in a wider range of circumstances; assume user has
+	  at least a 586 if CONFIG_EXT2_COMPR_X86_CODE is set, and assume it
+	  is a 686 or better if CONFIG_X86_F00F_WORKS_OK=y.
+
+	* Configure.help: various updates, removed entry for LZRW3A algorithm.
+
+	* bzip/init.c,gzip/e2compr_gzip.c,lzo/e2compr_lzo.c,lzv/e2compr_lzv.c:
+	  Add module info.
+
+	* bzip/lib_bzip_e.c: Removed superfluous ##s.
+
+	* inode.c: e2compr additions to module info.
+
+	* ioctl.c: change obsolete refs to fsuser() to capable(CAP_FOWNER).
+
+	* compress.c: change EXT2_COMPR_DEBUG to EXT2_COMPR_REPORT_VERBOSE
+
+	* compress.c: integrate compression verification fixes from
+	  Ulrich Holeschak <UHoleschak@bihl-wiedemann.de>.
+
+	* Readme.e2compr, e2compress.txt, ext2_compression: updated URLs and
+	  contact details.
+
+	* project name reverted to "e2compr", relocated to sourceforge.net.
+	  CHANGES.e2compress integrated into this file.
+
+15 Mar 2003
+	Paul Whittaker <whitpa@users.sourceforge.net>
+
+	* compress.c, inode.c: updated for page wait queues (2.4.20 kernel).
+
+Changes from 0.4.42 to 0.4.43 :
+===============================
+	Pierre Peiffer <pierre.peiffer@sxb.bsf.alcatel.fr>
+	Denis Richard <denis.richard@sxb.bsf.alcatel.fr>
+
+	- Deadlock correction when compressing cluster and sync pages.
+	- Management of MAPPED and DIRTY buffer flags.
+	- Test of block number in compression.
+	- Test if page is dirty to allocate buffer on device when compressing.
+	- When decompress cluster use the size of the cluster and not the size
+	  of the file, because it can be called from vmtruncate (the size of the 
+	  file has already changed).
+	- Management of working area lock.
+	- When (de)compress file containing holes, the data must be moved and not
+	  only the block number.
+	- New function ext2_decompress_pages() to allocate blocks for a cluster
+	  already (block) decompressed. It is now called in ext2_decompress_cluster()
+	  and not only in ext2_file_write().
+
+Changes from 0.4.41 to 0.4.42 :
+===============================
+	Pierre Peiffer <pierre.peiffer@sxb.bsf.alcatel.fr>
+	Denis Richard <denis.richard@sxb.bsf.alcatel.fr>
+
+	- Delete the i_blocks field decrementation (Thanks to Peter Wächtler).
+	- Clear dirty bit of buffers not in compressed area, after compression.
+	- Unlock pages before sync of inode, after compression.
+	- Change parameters (OSYNC_METADATA|OSYNC_DATA) of generic_osync_inode()
+	  calls to write data inode.
+	- Ext2_readpage() returns an error code.
+	- Allocation of working area even when readonly mount.
+	- Clear dirty bit of buffers after uncompress in ext2_readpage.
+	- Unlock page after free buffers in error case in ext2_readpage.
+
+Version 0.4.41 is the first version of e2compress patch for kernel 2.4.16
+=========================================================================
+	Pierre Peiffer <pierre.peiffer@sxb.bsf.alcatel.fr>
+	Denis Richard <denis.richard@sxb.bsf.alcatel.fr>
+
+1998-12-16  Peter Moulder  <reiter@netspace.net.au>
+
+	* compress.c (ext2_register_compression_module): Unlock working
+ 	area before we exit, even if ENOMEM.  Set ext2_rd_wa_size
+ 	correctly if decompression needs > compression needs.  (Never
+ 	happens with current algorithms, but in the process I simplified
+ 	the code a little.)
+	(ext2_decompress_inode): Moved assertion to before blocking calls,
+ 	and added a non-asserted test (of the same condition) after the
+ 	blocking calls.
+	(ext2_cleanup_compressed_inode): Repeat some tests after some
+	blocking calls (down, DQUOT_INIT).
+
+	* ext2_fs_c.h: Remove last remnants of `ifndef EXT2_HOLEMAP'
+	code.
+	(ext2_offset_is_clu_boundary): Tiny efficiency tweek.
+
+1998-11-30  Peter Moulder  <reiter@netspace.net.au>
+
+	* compress.c (ext2_compress_cluster, ext2_decompress_cluster): 
+	* ioctl.c (ext2_ioctl): 
+	* file.c (ext2_readpage):
+	Make request_module() call dependent on CONFIG_KMOD.
+
+1998-11-22  Peter Moulder  <pjm@opensource.captech.com>
+
+	* compress.c (ext2_mark_algorithm_use): Adjust s_rev_level when 
+	adding compression to the superblock.
+
+1998-11-01  Jan Rêkorajski  <baggins@hunter.mimuw.edu.pl>
+
+	* Merged with patch 0.4.21
+	* Updated defconfigs for all architectures
+	* Cleared some forgotten code
+
+1998-10-31  Jan Rêkorajski  <baggins@hunter.mimuw.edu.pl>
+
+	* Made yesterday statement false :)
+	I followed Peter's sugestion to make working area pointer
+	a parameter to de/compression functions.  Now there is
+	no problem selecting both CONFIG_EXT2_SEPARATE_WORK_AREAS and
+	CONFIG_EXT2_VERIFY_COMPRESSION.
+
+1998-10-30  Jan Rêkorajski  <baggins@hunter.mimuw.edu.pl>
+
+	* Hello, my name is Dexter, and I'm boy genius ;)
+
+	* Ha, nailed The Bug, and squashed it :)
+	The problem with separate work areas was due to
+	compression verification.  It's clear that
+	CONFIG_EXT2_SEPARATE_WORK_AREAS and CONFIG_EXT2_VERIFY_COMPRESSION
+	are mutually exclusive.  Now, only fix the *config stuff
+	and I'm done with "known bug" hunting.
+
+1998-10-28  Jan Rêkorajski  <baggins@hunter.mimuw.edu.pl>
+
+	* OK, working area pointers separated. Didn't help :(
+	Oopsed while testing.  Seems that "critical bug" was
+	something more...
+
+	* Fixed a bug in ext2_register_compression_module,
+	I forgot to put locks around memory reallocation.
+
+1998-10-26 (later that day ;)  Jan Rêkorajski  <baggins@hunter.mimuw.edu.pl>
+
+	* more separate areas work...
+	Now I know what is that pjm's "critical bug", it's fundametal
+	design screw up.  (Structures do _NOT_ change their size magically,
+	sizeof is a const, so using ext2_wa[0/1] was a BAD idea)
+	Only 6 more (ext2_wa != NULL) lines to go...
+
+1998-10-26  Jan Rêkorajski  <baggins@hunter.mimuw.edu.pl>
+
+	* patch against 2.1.126
+
+	* started fixing separate work areas problems (a.k.a. races ;)
+	Every algorithm uses separate variables to store pointers to
+	read/write work areas.  Now they can compress and decompress
+	in the same time.
+
+	Apparently, it will save us a lot of memory :)
+	See this:
+
+	ALG	C MEM	D MEM
+	---------------------
+	bzip2	~150K	~150K
+	gzip	 200K	  64K
+	lzo	~128K	   0K
+	lzrw	 ~16K	 ~16K
+	lzv	  32K	   0K
+	---------------------
+	MAX	 200K	 150K
+
+	So, in worst case with separate work areas we save ~50K _kernel_,
+	_unswappable_ memory :)
+
+1998-11-01  Peter Moulder  <reiter@netspace.net.au>
+
+	* Misc small changes.
+
+1998-10-18  Jan Rêkorajski  <baggins@hunter.mimuw.edu.pl>
+
+	* compress.c: fix "off by one" maxlen in ex2_compress_cluster
+
+	* modularized bzip2, gzip, lzo, lzv, lzrw
+		- added ext2_compress_(un)register_module
+		  see new-method-howto.c or lzv/e2compr_lzv.c
+	* updated gzip to zlib 1.1.3 (was 0.93!)
+	* fixed lzo - yes it works now :)
+	
+1998-06-30  Peter Moulder  <reiter@netspace.net.au>
+
+	* binfmt_elf.c (elf_core_dump): 
+	* binfmt_aout.c (do_aout_core_dump):
+	Fix deadlock.  OK, make that 0.4.15.
+
+	* Release e2compr-0.4.14-patch-2.1.107.
+
+	* truncate.c, binfmt_aout.c: resolve with 2.1.107.
+
+	* truncate.c: Don't test feature flag; EXT2_COMPRESSED_BLKADDR
+	is proof enough.
+
+1998-06-15  Peter Moulder  <reiter@netspace.net.au>
+
+	* compress.c: Reset i_next_alloc_block and i_next_alloc goal
+	to zero after compressing a cluster.
+
+1998-06-11  Peter Moulder  <reiter@netspace.net.au>
+
+	* compress.c: Fix [stupid] race condition.  Added many diagnostic
+	checks, too, which I haven't yet removed.
+
+1998-06-10  Peter Moulder  <reiter@netspace.net.au>
+
+	* ext2_fs_c.h: Add aliases for some #definitions.
+
+1998-06-09  Peter Moulder  <reiter@netspace.net.au>
+
+	* file.c (ext2_readpage): Allow for page sizes bigger than 8KB,
+	for certain ARM processors.
+
+	* ioctl.c (ext2_ioctl): Remove spurious ext2_warning about
+ 	"COMPRBLK w/o COMPR".
+
+1998-06-06  Peter Moulder  <reiter@netspace.net.au>
+
+	* inode.c (block_getblk): Moved COMPRESSED_BLKADDR macro to
+	ext2_fs_c.h, and removed the inode parameter.  We no longer
+	consult the superblock to check for e2compr support; if the block
+	address is 0xffffffff then we say it's compressed.
+
+1998-05-21  Peter Moulder  <reiter@netspace.net.au>
+
+	* compress.c (ext2_count_blocks): Do more consistency checks on
+	cluster head.
+	(ext2_compress_cluster): Improve information display for changing
+	b_blockno problem.  Check return code of ext2_get_key().
+
+1998-05-12  Peter Moulder  <reiter@netspace.net.au>
+
+	* compress.c (ext2_cleanup_compressed_inode,
+ 	ext2_decompress_inode): Use DQUOT_INIT() for 2.1.101+ kernels.
+
+1998-03-07  Peter Moulder  <pjm@bofh.asn.au>
+
+	* compress.c, lzo/, ext2_fs_c.h: Add lzo algorithm.  (Patch sent
+	in by Chris Old <cold@irdeto.com>.)
+
+1998-03-06  Peter Moulder  <pjm@bofh.asn.au>
+
+	* compress.c, file.c (ext2_readpage), ext2_fs_c.h: Support
+	CONFIG_EXT2_SEPARATE_WORK_AREAS.
+
+	* Release 0.4.2.
+
+1998-03-02  Peter Moulder  <pjm@bofh.asn.au>
+
+	* ioctl.c (ext2_ioctl: EXT2_IOC_SETFLAGS): Properly clear
+	EXT2_DIRTY_FL if we drop EXT2_COMPR_FL.
+
+1998-02-12  Peter Moulder  <pjm@bofh.asn.au>
+
+	* compress.c (ext2_decompress_inode,
+	ext2_cleanup_compressed_inode): Initialise quotas.
+
+1998-01-24  Peter Moulder  <pjm@bofh.asn.au>
+
+	* compress.c (ext2_get_cluster_blocks): Number of blocks to fetch
+ 	shouldn't be calculated from i_size; just use clu_nblocks.
+	(i_size is short when truncate() is called.)  I suspect that this
+ 	change should also apply to 0.3.6.
+
+1998-01-09  Peter Moulder  <pjm@bofh.asn.au>
+
+	CHANGES FOR 0.4.0.  (Various files.)
+
+	* Get rid of ,compressed cluster` bitmaps.  Store
+	EXT2_COMPRESSED_BLKADDR (new constant, currently 0xffffffff) in
+	last blockno of cluster to mark as compressed.
+
+	* First cluster is no longer in general the same size as other
+ 	clusters of a file.  See ext2_first_cluster_nblocks() and
+ 	surrounding macros in ext2_fs.h.
+
+	* Blocknos stored contiguously at the beginning of the cluster
+ 	rather than having holes in places where uncompressed data has
+ 	holes.
+
+	* Mark incompatibilities in s_feature_incompat and
+	s_algorithm_usage_bitmap (new field).
+
+	* Always use i_sem, never I_LOCK.  (This change is overdue, and
+	should probably be back-ported.)
+
+	* ioctl.c: Allow EXT2_ECOMPR_FL to be either raised from userspace (not
+ 	just cleared).
+
+	* ioctl.c: Don't ,raise EXT2_DIRTY_FL just because of toggling of
+	EXT2_NOCOMPR_FL`.
+
+	* ext2_fs.h, ioctl.h: New ioctl EXT2_IOC_RECOGNIZE_COMPRESSED to
+	replace EXT2_IOC_SETCLUSTERBIT.  EXT2_IOC_CLRCLUSTERBIT has been
+	removed.
+
+	* Have ext2_ioctl() decompress the file immediately if
+	EXT2_COMPR_FL is cleared from a regular file.  Report the error if
+	the request couldn't be carried out.
+
+	* Use two-level compression method table.  ext2_algorithm_table
+ 	keeps the `name', `avail', `init', `compress', `decompress'
+ 	fields, while there is a new ext2_method_table with two fields: an
+ 	index into ext2_algorithm_table, and an argument to the
+ 	de/compressor.  See compress.c.
+
+	* Writes are compressed as soon as the end of a cluster is
+ 	reached, instead of waiting until close().
+
+	* EXT2_IOC_SETCOMPRMETHOD doesn't fail for unknown compression
+	method.  However, it writes 1 or 0 to *arg depending on whether or
+	not (respectively) the requested method is known to the kernel.
+
+	* fs/Config.in: CONFIG_EXT2_COMPRESS question only asked if
+	CONFIG_EXPERIMENTAL=y.
+
+	* arch/*/defconfig: Default answer for CONFIG_EXT2_COMPRESS
+	changed to `n'.
+
+	* Do checksum on _compressed_ data not uncompressed.  The intent
+ 	is to prevent data corruption from skrogging kernel memory or
+ 	causing illegal memory references.  The `reserved_2' field has been
+	removed from the cluster head and replaced with `checksum'.
+
+	* Removed `bitmap' and `reserved0' from cluster head and replaced
+	with `holemap' and `holemap_nbytes'.  holemap[] is a bitmap of
+	where to put holes.  (It is declared as a zero-length array of
+	unsigned char.)  Unlike the former `bitmap' field, we don't
+	include holes at the end of the cluster, a `1' represents a hole
+	instead of a `0' in `bitmap', and of course it is variable-length.
+	This paves the way for longer clusters.  However, I haven't yet
+	increased the length of clen or ulen, which would be necessary for
+	clusters longer than 64KB, and desirable (no 0==64K hack) even for
+	64KB clusters.
+
+	* New methods `never' and `auto'.  `auto' is largely
+	unimplemented (see compress.c).
+
+	* Gratuitous name changes.  Many things using word `uncompress'
+ 	changed to `decompress'.  (`Decompressed' always means `having
+ 	been decompressed', whereas `uncompressed' can also (and usually
+ 	does) mean `not having been compressed'.)
+  	ext2_put_compressed_cluster() split into
+ 	ext2_cleanup_compressed_cluster() and ext2_decompress_inode().
+  	head->len becomes head->ulen.  CONFIG_EXT2_USE_* becomes
+ 	s/USE/HAVE/.  EXT2_COMPR_DEBUG* become EXT2_COMPR_REPORT*.  Maybe
+ 	some other things.
+
+Tue Dec  9 19:53:12 1997  Peter Moulder  <pjm@bofh.asn.au>
+
+	* ioctl.c (ext2_ioctl): Don't ,try to prevent 4KB clusters on
+	machines with 8KB pages`.
+
+Mon Dec  8 17:48:19 1997  Peter Moulder  <pjm@bofh.asn.au>
+
+	* namei.c (ext2_create): implement CONFIG_GZ_HACK
+
+	* compress.c (ext2_lock_wa): call signal_pending() instead of
+	testing the bitmaps.  (Necessary for 2.1.68 POSIX.1b signal
+	handling.)
+
+	* file.c (ext2_readpage): Attempt to correct rare bug for
+	machines with 8KB (or larger) pages.
+
+Mon Dec  8 16:31:38 1997  Peter Moulder  <pjm@bofh.asn.au>
+
+	* inode.c, ext2_fs.h: Add support for a couple of e2compr-
+	reserved inodes.  (This change will probably be reversed,
+	but it's harmless for now.)
+
+Fri Aug  1 15:10:48 1997  Peter Moulder  <pjm@bofh.asn.au>
+
+	* compress.c (ext2_unlock_wa): Call schedule() if other processes
+	are waiting for the working area.
+
+Sat Jul 26 08:28:08 1997  Peter Moulder  <pjm@bofh.asn.au>
+
+	* Makefile: e2compr files were getting compiled even if
+ 	CONFIG_EXT2_COMPRESS wasn't defined.  Makefile now tests for this
+ 	being defined.
+
+	* acl.c: Not even root gets write access to a file with a
+ 	compression error.
+
+	* compress.c (ext2_get_cluster_bit): Reverse change of Apr 21,
+	i.e. return any positive number instead of converting to 1.
+	(Apr 21 change was needlessly cautious.)
+
+	* inode.c:
+	* compress.c:
+	* file.c: 
+	Access to the cluster head does endian conversion.
+	Access to inode has endian conversion in 2.1 but not in 2.0 --
+	just like the standard kernel.
+	* ext2_byteorder.h: new file.  (Only needed in 2.0.)
+	
+	* file.c (ext2_readpage): Rewrite.  Handles errors better.  Also
+ 	takes better care of working area lock.
+
+	* compress.c:
+	* file.c:
+	* inode.c:
+	* truncate.c:
+	Change printk level to KERN_DEBUG for things surrounded by
+	EXT2_COMPR_REPORT*.
+
+	* inode.c: Allow bmap() if EXT2_NOCOMPR_FL set.
+
+	* inode.c: Don't take high bit of i_flags into compr_meth.  (The
+	high bit is reserved for ext2 lib.)  Also, don't write it
+	from compr_meth (though the result should be the same: the
+	bit is set to zero).
+
+	* ioctl.c (ext2_ioctl): Call verify_area in a few places
+	where previously we didn't.
+
+	* super.c (ext2_setup_super): The debug string on mount didn't seem
+	to be printing, so I changed `%p' to `%lx' (plus an appropriate cast)
+	to see if it would change things.
+
+	* compress.c (ext2_uncompress_cluster, ext2_uncompress_blocks): In
+ 	the consistency checks, consider the head size as well as
+ 	compr_len in calculation.
+
+	* gzip/deflate.c (deflate_fast, deflate_slow):
+	* lzv/lzv1.c:
+	Yield CPU every fifth of a second (ifdef __KERNEL__).
+
+	* include/linux/ext2_fs.h: Change definition of EXT2_ECOMPR_FL
+ 	from 0x0080 to 0x0800 to avoid clash with EXT2_NOATIME_FL in 2.1.
+  	* Change name EXT2_MAX_CLUSTER_SIZE to EXT2_MAX_CLUSTER_BLOCKS.
+
+Tue Apr 29 12:00:00 1997  Peter Moulder  <pjm@bofh.asn.au>
+
+	* ioctl.c (ext2_ioctl): Call invalidate_inode_pages() if
+	EXT2_NOCOMPR_FL is toggled.
+
+Mon Apr 21 15:50:16 1997  Peter Moulder  <pjm@bofh.asn.au>
+
+	* ext2_fs.h (ext2_cluster_is_compressed): If
+	ext2_get_cluster_bit() returns error then pass the error back.
+	* compress.c (ext2_get_cluster_bit): Return 1 rather than any +ve num.
+	ext2_cluster_is_compressed() is the only caller.
+
+	* compress.c (ext2_free_key,ext2_get_key,ext2_next_key): Make static.
+
+Thu Jan 16 12:00:00 1997  Peter Moulder  <pjm@bofh.asn.au>
+
+	* file.c (ext2_mmap): Previously we would deny mmap access if
+	there were an error or if compression were disabled (NOCOMPR).  
+	These checks have both been removed.
+	It was wrong to disallow mmap access to a file with NOCOMPR,
+	but it's more questionable whether or not we should allow
+	compression to a file with ECOMPR.
+
+This file is not very accurate, particularly the older changes.
diff -pruN linux-2.6.18.5.org/fs/ext2/ChangeLog.e2compr-26port linux-2.6.18.5/fs/ext2/ChangeLog.e2compr-26port
--- linux-2.6.18.5.org/fs/ext2/ChangeLog.e2compr-26port	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/ChangeLog.e2compr-26port	2007-04-01 21:17:14.000000000 -0700
@@ -0,0 +1,120 @@
+06 Mar 2007
+
+	Terry Loveall <loveall@iinet.com>
+
+	* adapted linux-2.6.10-e2compr-0.4.45-alpha0126.diff to 2.6.18.5 kernel
+
+	* replaced most instances of down/up(inode->i_sem) with 
+	  lock/unlock(inode->i_mutex). For exception see file.c, below.
+
+	* made various printk regularizations to uniquely identify each printk
+	  instance. Inserted missing KERN_DEBUG and KERN_WARNING.
+
+	* compress.c:
+	  bug fix: ext2_count_blocks: init head_bh for each iteration.
+	  bug fix: ext2_count_blocks: add set clen=ulen for uncompressable clusters.
+	  bug fix: ext2_compress_cluster: replacement and inlining of an 
+	   invalidate_inode_buffers function to keep root filesystem changes
+	   uptodate on disk (prevents umounting root file system to update).
+	  warning fix: ext2_compress_cluster: various variables initialized.
+	  ext2_compress_cluster: removed #ifdef NDEBUG
+	  bug fix: ext2_compress_cluster: defined maxclus, calculate and set for:
+	  bug fix: ext2_compress_cluster: set filesize for uncompressed clusters.
+	  ext2_cleanup_compressed_inode: changed error message to indicate 'Z'
+	   flag was caused by trying to un/compress already open file.
+	  bug fix: cp to compr dir: Truncate uncompressed files to their
+	   uncompressed length, i.e. force kernel to update inode and sb
+
+	* file.c:
+	  removed file->f_error code since f_error no longer in file struct.
+	  ext2_file_write: changed down/up i_sem to down_read/up_read i_alloc_sem
+
+	* inode.c:
+	  bug fix: ext2_get_block: restored changed: loop to bforget
+
+	* ioctl.c:
+	  ext2_ioctl: scrubbed 'B' flag on file uncompress.
+
+	* match[56]86.S:
+	  made code dependent on #ifdef CONFIG_REGPARM to compile with either
+	   register variable or stack variable parameter passing.
+
+28 Feb 2005
+
+	Yabo Ding <bobfree_cn@yahoo.com.cn>,<yding@wyse.com>
+
+	* Corrected page unlocking in inode.c.
+
+19 Feb 2005
+
+	Paul Whittaker <whitpa@users.sourceforge.net>
+
+	* Added corrections le32_to_cpu in critical areas of compress.c
+	* Optimized function exit code in inode.c.
+
+24 Aug 2004
+Yabo Ding <bobfree_cn@yahoo.com.cn>,<yding@wyse.com>
+
+  compress.c
+*  ext2_decompress_pages()
+     The old code cannot reread data from disk to a changed buffers data pointer in 2.6.x.
+     So, I copy memory data(decompressed) to a temporary buffer;
+     Then reread data(compressed) from disk, and copy to head;
+     Then copy back the memory data from temporary buffer.
+     It seems clumsy, but it works well.
+*  ext2_compress_cluster()
+     Force write to disk.
+
+  inode.c
+*  ext2_writepage()
+     Delete old code. All directly call block_write_full_page() function.
+
+* ../Kconfig
+    Change e2compr config as a submenu config
+
+04 Aug 2004
+
+Paul Whittaker <whitpa@users.sourceforge.net>
+
+* compress.c: replaced mark_buffer_dirty(x,y) with mark_buffer_dirty(x).  I'm
+  still not at all sure that this is sufficient.
+
+03 Aug 2004
+
+Paul Whittaker <whitpa@users.sourceforge.net>
+
+* ../../include/linux/ext2_fs_c.h: added missing prototypes for ext2_iLZRW3A(),
+  ext2_iLZRW3A(), ext2_rLZRW3A().
+
+02 Aug 2004
+
+Paul Whittaker <whitpa@users.sourceforge.net>
+
+* ../../mm/page_alloc.c: added EXPORT_SYMBOL(__pagevec_free).
+
+* ../../include/linux/pagemap.h, ../../mm/filemap.c: removed inline from
+  __grab_cache_page() declarations, added EXPORT_SYMBOL(__grab_cache_page).
+
+* ../../include/linux/mm.h, ../../mm/filemap.c: removed inline from
+  page_waitqueue() declarations, added EXPORT_SYMBOL(page_waitqueue).
+
+* bzip2/{lib_bzip_d,lib_bzip_e}.c, {gzip,lzo,lzrw3a,lzv1}/e2compr*.c:
+  replaced MOD_INC_USE_COUNT and MOD_DEC_USE_COUNT with try_module_get()
+  and module_put() to avoid deprecation and safety warnings.
+
+* lzrw3a/lzrw3a.c: added (UBYTE *) casts to avoid compiler warnings.
+
+* compress.c, inode.c: incorporated Yabo's changes, correcting mistakes in
+  ext2_readpages() in inode.c.
+
+* removed printks for ext2_discard_prealloc from file.c and inode.c (not
+  needed now that this problem has been resolved).
+
+2.6.5 -> 2.6.7 updates:
+
+* ../../mm/filemap.c: rewrote CONFIG_EXT2_COMPRESS hunk for 2.6.7.
+
+* compress.c, file.c: use mapping_mapped(), since mapping->i_mmap has changed
+  and mapping->i_mmap_shared no longer exists.
+
+* inode.c: page->count becomes page->_count.
diff -pruN linux-2.6.18.5.org/fs/ext2/Makefile linux-2.6.18.5/fs/ext2/Makefile
--- linux-2.6.18.5.org/fs/ext2/Makefile	2006-12-04 13:31:58.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/Makefile	2007-01-24 10:24:25.000000000 -0800
@@ -2,10 +2,22 @@
 # Makefile for the linux ext2-filesystem routines.
 #
 
+ifeq ($(CONFIG_EXT2_COMPRESS),y)
+
+obj-$(CONFIG_EXT2_HAVE_BZIP2)		+= bzip2/
+obj-$(CONFIG_EXT2_HAVE_GZIP)		+= gzip/
+obj-$(CONFIG_EXT2_HAVE_LZO)		+= lzo/
+obj-$(CONFIG_EXT2_HAVE_LZV1)		+= lzv1/
+obj-$(CONFIG_EXT2_HAVE_LZRW3A)		+= lzrw3a/
+
+COMPRESS_STUFF := adler32.o compress.o none.o compress_syms.o \
+		  $($(obj-y):%/=%/ext2-compr-%.o)
+endif
+
 obj-$(CONFIG_EXT2_FS) += ext2.o
 
 ext2-y := balloc.o dir.o file.o fsync.o ialloc.o inode.o \
-	  ioctl.o namei.o super.o symlink.o
+ 	  ioctl.o namei.o super.o symlink.o $(COMPRESS_STUFF)
 
 ext2-$(CONFIG_EXT2_FS_XATTR)	 += xattr.o xattr_user.o xattr_trusted.o
 ext2-$(CONFIG_EXT2_FS_POSIX_ACL) += acl.o
diff -pruN linux-2.6.18.5.org/fs/ext2/Readme.e2compr linux-2.6.18.5/fs/ext2/Readme.e2compr
--- linux-2.6.18.5.org/fs/ext2/Readme.e2compr	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/Readme.e2compr	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,511 @@
+
+  0. Introduction
+  ~~~~~~~~~~~~~~~
+
+This file gives some technical information on e2compr and how it's
+implemented.  
+
+More general information on e2compr can be found at
+http://e2compr.sourceforge.net/.
+
+The first couple of sections of this document are written for those
+who have no interest in the source code but just want to know enough
+to be able to predict and understand e2compr behaviour and its
+implications.
+
+Section 3 describes the e2compr-specific ext2 attributes for a file
+(i.e. chattr things).
+
+Section 4 describes the e2compr ioctls from the point of view of a
+user-mode C programmer.
+
+Section 5 gives more detail about the file format on disk.
+
+Section 6 gives details on what's written where, i.e. a map of e2compr
+code in the kernel.
+
+
+Authorship: section 2 is written mainly by Antoine; the remainder is
+written by Peter.
+
+Questions should be sent to the e2compr mailing list,
+e2compr-misc@lists.sourceforge.net, or to the current maintainers,
+bothie@users.sourceforge.net and whitpa@users.sourceforge.net.
+
+
+  1. The idea
+  ~~~~~~~~~~~
+
+See section `E2compr implementation' in the main e2compr texinfo
+documentation for an introduction to how e2compr works.  (Type
+`info "(e2compr)Implementation"' at the shell prompt.)  It was
+originally written as part of the file you're now reading.
+
+
+  2. More details
+  ~~~~~~~~~~~~~~~
+
+Every compressed file stores its cluster size in the inode structure
+(in the ext2 attribute flags field).
+This (the cluster size) is the most important information: when
+knowing the cluster size, we can convert a block number into a cluster
+number, get the cluster the block belongs to, and then get the block.
+The inode's flags field also keeps the algorithm that is used to compress data
+written to the file.
+
+(The algorithm that was used to compress a given
+cluster is stored in the cluster head near the beginning of the
+compressed data.  This may differ from the current algorithm
+identified in the inode, which is only used to determine which
+algorithm to use at the time clusters are written.)
+
+The algorithm id and the cluster size are stored in the i_flags field
+(thus reducing the number of possible flags).  We also create some new
+flags: the COMPRBLK flags tells if there is at least one compressed
+cluster in the file, the ECOMPR flag indicates that an error (related
+to compression) occurred while reading from or writing to this file.
+If it is set, the file becomes read-only.  (In previous releases, you
+were denied even read access to the file unless you set the NOCOMPR
+flag.  There might be some benefit in returning to the old behaviour
+if decompressing erroneous data can cause an OOPS, but I think it
+would be better to correct the decompressors.  Others may disagree,
+pointing out that it costs CPU time to check for incorrect data.)
+
+Beside the information stored into the inode, each cluster holds some
+data.  Here is the cluster_head structure for e2compr-0.4:
+
+struct ext2_cluster_head {
+  __u16 magic;		/* == EXT2_COMPRESS_MAGIC_04X. */
+  __u8  method;		/* compression method id. */
+  __u8  holemap_nbytes;	/* length of holemap[] array */
+  __u32 checksum;	/* adler32 checksum.  Checksum covers all fields
+			   below this one, and the compressed data. */
+  __u32 ulen;		/* size of uncompressed data */
+  __u32 clen;		/* size of compressed data (excluding cluster head) */
+  __u8  holemap[0];     /* bitmap describing where to put holes. */
+};
+
+The `magic' field is a magic number.  It is used to detect filesystem
+corruption, and can also be used for data recovery purposes.  (The
+e2compress program for e2compr-0.3 does this.)
+
+The `checksum' field contains an Adler-32 checksum on the fields below
+it in the struct and the compressed data.  Its purpose is to protect
+us from buffer overruns caused by corrupted data.
+
+The `ulen' field says how many bytes are stored in the cluster, when
+uncompressed.
+
+The `clen' field says how many bytes are held in the cluster, when
+compressed.
+
+The `method'
+field identifies the algorithm that was used to compress the cluster
+(this id will be used to uncompress the cluster, not the one stored
+into the inode that will be used only to compress a new cluster).
+
+The variable-length `holemap' array says where to put hole blocks when
+decompressing data.  The `holemap_nbytes' field gives the length of
+this array.  Iff holemap_nbytes is zero then there are no holes (other
+than at the end of the cluster, as determined by ulen versus cluster
+size).
+
+The compressed data immediately follows the holemap array (with no
+padding before it).
+
+
+Compressing a cluster is done in the following way:  We first get every
+block in the cluster and compute the bitmap.  We then compress the
+non-hole data, and store back the compressed data into the existing
+blocks.  Unused blocks are then freed.
+
+Decompressing a cluster is done in the following way:  We get the
+cluster head and retrieve the bitmap.  Missing blocks are allocated and
+put where the bitmap says, and then compressed data is decompressed and
+stored back into the blocks.
+
+
+Reading from a compressed cluster is really easy: get the blocks,
+decompress them into a working area, and get the bytes we want from
+the working area.  Writing to a compressed cluster is done by first
+decompressing the cluster, and then write to it, as if it were a
+normal file.  The file is then marked so that the cluster will be
+recompressed later.  [pjm: Do we decompress the cluster even if it's
+to be entirely written over?]
+
+In the current version, compression really occurs only when the inode
+is put (which in turn only occurs when no processes have the file
+open).  This may change.
+
+
+  3. Ext2 file attributes
+  ~~~~~~~~~~~~~~~~~~~~~~~
+
+Attribute     Lsattr  Meaning
+~~~~~~~~~     ~~~~~~  ~~~~~~~
+EXT2_SECRM_FL	   s  Secure deletion (not yet implemented)
+EXT2_UNRM_FL	   u  Undelete-able.  (Not yet implemented.)
+EXT2_COMPR_FL	   c  Future writes to this file should be compressed.
+                      (Clearing this flag decompresses the file if it
+		      is a regular file and there is space to do so;
+		      see the e2compr FAQ for details.)
+EXT2_SYNC_FL	   S  Synchronous updates.  (As far as I know, this is
+                      not yet fully implemented.)
+EXT2_IMMUTABLE_FL  i  Immutable file.
+EXT2_APPEND_FL	   a  Writes to file may only append.
+EXT2_NODUMP_FL	   d  Not a candidate for backup with dump(8).
+EXT2_NOATIME_FL    A  No access time updates.
+EXT2_DIRTY_FL	   Z  De/compression is yet to happen.  Read the
+                      source for exact meaning.
+EXT2_COMPRBLK_FL   B  File contains one or more compressed clusters.
+EXT2_NOCOMPR_FL    X  Access raw compressed data.  This isn't really
+		      supported at the moment; user-space access is
+		      yet to be worked out for 0.4.
+EXT2_ECOMPR_FL	   E  Compression error associated with this file
+EXT2_BTREE_FL      I  B-tree indexed directory (seemingly not yet implemented)
+EXT2_RESERVED_FL   -  (reserved for ext2 lib)
+
+See the chattr(1) man page for more verbose descriptions of the
+non-e2compr flags.
+
+
+  4. Ioctls available
+  ~~~~~~~~~~~~~~~~~~~
+
+  In brief
+  ~~~~~~~~
+
+Action             Ioctl                    To kernel	 From kernel
+~~~~~~             ~~~~~                    ~~~~~~~~~    ~~~~~~~~~~~
+Get cluster bit    EXT2_IOC_GETCLUSTERBIT   Cluster num  1 or 0 (cmp,uncmp)
+Recognize compressed                        Cluster num  -
+                   EXT2_IOC_RECOGNIZE_COMPRESSED
+Get algorithm      EXT2_IOC_GETCOMPRMETHOD  -		 Id
+Set algorithm      EXT2_IOC_SETCOMPRMETHOD  Id		 -
+Get cluster size   EXT2_IOC_GETCLUSTERSIZE  -		 Cluster size
+Set cluster size   EXT2_IOC_SETCLUSTERSIZE  Cluster size -
+Get attributes     EXT2_IOC_GETFLAGS	    -		 Flags
+Set attributes     EXT2_IOC_SETFLAGS	    Flags	 -
+Get block size     FIGETBSZ		    -		 Block size
+
+#include <linux/ext2_fs.h> to use any of these ioctls, except FIGETBSZ,
+which requires <linux/fs.h>.
+
+To find out what errors can be returned by these ioctls, read
+fs/ext2/ioctl.c (for all of the above ioctls except FIGETBSZ) or
+fs/ioctl.c (for FIGETBSZ).
+
+
+  Setting or testing a cluster bit
+  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+[Note: user-space access to compression details are yet to be worked out,
+so this section may not be accurate.]
+
+EXT2_IOC_GETCLUSTERBIT sets *arg to 1 if the specified cluster (0 for first
+cluster, 1 for second, etc.) is stored in compressed form.
+
+To make the kernel consider a certain cluster to be compressed (after
+you've done the compression yourself, in user space), use
+EXT2_IOC_RECOGNIZE_COMPRESSED.  This ioctl checks the validity of the
+cluster's data, then marks it as compressed (if valid).  This ioctl
+requires special priveleges, because if the compressed data is not
+valid then it may be possible to crash the system (due to buffer
+overruns).
+
+
+  Setting or getting the compression algorithm
+  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+EXT2_IOC_SETCOMPRMETHOD sets the default compression method (stored in
+the inode).  This is the compression method that is used for future
+writes.  In the current version of e2compr [accurate at 0.4.36], this
+does not cause a change to how
+existing clusters are stored, except when the compression method
+changes from `none' to something else, in which case the kernel
+attempts to compress ,all currently-uncompressed clusters` using the
+new algorithm.  It is an error to use this ioctl on a file without the
+compressed attribute.
+
+EXT2_IOC_GETCOMPRMETHOD sets *arg to the current compression method.
+
+In either case, Id is one of: EXT2_DEFER_METH, EXT2_LZV1_METH,
+EXT2_AUTO_METH, EXT2_NEVER_METH, EXT2_BZIP2_METH, EXT2_LZO1X_1_METH,
+EXT2_LZRW3A_METH (deprecated), EXT2_GZIP1_METH, EXT2_GZIP2_METH, ...,
+EXT2_GZIP9_METH.
+
+
+  Setting or getting the cluster size
+  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+EXT2_IOC_SETCLUSTERSIZE sets the cluster size to the value of *arg.
+This ioctl fails if there are already compressed clusters in the file
+(as determined by checking the EXT2_COMPRBLK_FL attribute).
+
+EXT2_IOC_GETCLUSTERSIZE sets *arg to the current cluster size.
+Surprisingly, this ioctl succeeds even if the EXT2_COMPR_FL attribute
+is clear.  (Maybe this will change in future, since the result is
+meaningless.) 
+
+In either case, the size is one of {4, 8, 16, 32}, and represents the
+number of blocks per cluster.  To convert to or from a number of
+bytes, use the FIGETBSZ ioctl.
+
+
+  Setting or getting the ext2 file attributes
+  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+
+These ioctls (EXT2_IOC_GETFLAGS and EXT2_IOC_SETFLAGS) are not
+e2compr-specific, but some attributes are e2compr-specific.
+
+*arg consists of the set of attributes for that file OR'ed together.
+E.g. a value of (EXT2_COMPR_FL | EXT2_COMPRBLK_FL | EXT2_NODUMP_FL)
+for a regular file means that the file contains one or more compressed
+clusters, and should not be backed up when using dump(8).
+
+See section 3 for a description of the various attributes.
+
+Note that although the compression method and cluster size are
+physically stored in the flags field on disk this information is
+masked out (i.e. set to zero) for GETFLAGS if the kernel has e2compr compiled in.
+If the kernel does not have e2compr compiled in, then this information
+is not masked out.  See section 5 for how the cluster size and
+compression method is stored if you wish to work with ,kernels without
+e2compr`.
+
+
+  Getting the block size
+  ~~~~~~~~~~~~~~~~~~~~~~
+
+This ioctl (FIGETBSZ) is not e2compr-specific, but is useful in
+interpreting a cluster size (which is specified as a number of blocks
+rather than bytes or kilobytes).
+
+*arg is set to the block size (in bytes) of the file.  For ext2 files,
+this is one of {1024,2048,4096}.  It is the same value for all files
+on the same filesystem.
+
+You must #include <linux/fs.h> to use this ioctl (unlike the rest of
+the ioctls listed here, which require <linux/ext2_fs.h>).
+
+
+  5. File format
+  ~~~~~~~~~~~~~~
+
+A note on byte ordering.  All current versions of the kernel and
+e2compr write to disk in little-endian format, so the 16-bit number
+`0x8EC7' would be written as a 0xC7 byte followed by a 0x8E byte.
+Unless you want to know the most general rule for byte ordering, you
+can skip to the `Inode' heading.
+
+In kernel 2.0, the ext2 fs is written to disk in the native byte
+ordering.  On x86 machines, this means little endian; most other
+architectures are big-endian (so the same 16-bit number would be
+written as an 0x8E byte followed by 0xC7).
+
+On kernel 2.1 and later, the ext2 fs (including e2compr data) is
+written in little-endian order regardless of the host architecture.
+
+
+  5.1. Inode
+  ~~~~~~~~~~
+
+fs/inode.c controls the reading and writing of inode information
+to/from disk; consult this file (functions ext2_read_inode(),
+ext2_update_inode() and/or ext2_write_inode()) for any detail omitted
+from this section.
+
+The physical structure of an inode is struct ext2_inode (defined in
+include/linux/ext2_fs.h).
+
+
+The i_flags member contains the ext2 file attributes, as well as
+cluster size and compression method.  
+
+The normal flags are stored in the low 23 bits.  Only the low 12 bits
+are defined at present, including 4 flags introduced by the e2compr
+patch.  See ext2_fs.h for the flag meanings (search for
+EXT2_SECRM_FL).
+
+Bits 23 through 25 hold the cluster size, or more precisely the log2 of
+the number of filesystem blocks per cluster (excluding the first cluster;
+see ext2_first_cluster_nblocks in include/linux/ext2_fs_c.h).
+
+Bits 26 through 30 store the compression method.  See the definitions
+for EXT2_LZV1_METH etc. in ext2_fs_c.h for the interpretation.
+
+Bit 31 is reserved for ext2 lib (which means that programs like e2fsck
+store things there during its operation but it isn't used by the
+kernel).
+
+
+  Data blocks
+  ~~~~~~~~~~~
+
+Uncompressed clusters are stored just as they would be without
+e2compr.  So if there are no compressed clusters then the file
+is stored identically to any other file.
+
+
+If a cluster is compressed, then the first non-hole block starts with
+a `cluster head', as defined in struct ext2_cluster_head in ext2_fs.h.
+
+The magic number (i.e. the value of the `magic' field) is 0x8ec7.
+`method' holds one of EXT2_LZV1_ID and the like.  `reserved_0'
+contains zero.  `ubitmap' describes where the uncompressed data goes.
+(Recall that when we compress a cluster, we only compress the data
+from non-hole blocks, so we need to know where the holes and non-holes
+go when we decompress the data.)  A `0' bit means a hole and a `1' bit
+means a data block; bit 0 refers to the first block, b1 the second,
+and so on.
+
+
+The block positions within the file where the compressed data is held
+is a subset of where the uncompressed data would be held.  Further, if the
+uncompressed data occupies u non-hole blocks and this compresses to c
+blocks, then the compressed data occupies the first c non-hole blocks
+of the file (and the remainder are freed).
+
+[This paragraph is an expansion of the preceeding: if you understood
+the preceeding paragraph then skip this one.]  Consider an array
+cblock[] where cblock[0] holds the block number on disk (or 0 to
+represent a hole) of the first block of a certain cluster of a file,
+cblock[1] the second, and so on.  (If you are familiar with the bmap
+array or the format of first-level indirect blocks, then cblock[] is a
+section of that array.)  Suppose that the cluster size of this file is
+16 blocks.  Suppose too that, when uncompressed, blocks 0, 1, 5 and 6
+of the cluster are holes but the other 12 blocks (2,3,4,7,8,...,15)
+contain data.  (Thus the bitmap is 0x0000ff9c.)  Now if we compress this 
+cluster to just 5 blocks, then cblock[0], [1], [5] and [6] will continue 
+to be holes, ,the positions of the compressed data blocks` are stored in 
+cblock[2], cblock[3], [4], [7] and [8], the blocks referenced by 
+cblock[9] through cblock[15] are freed, and cblock[9] through cblock[15] 
+are set to zero.
+
+
+  6. What's coded where
+  ~~~~~~~~~~~~~~~~~~~~~
+
+File names in this section are relative to linux/fs/ext2, except for
+ext2_fs.h which is in linux/include/linux.
+
+Most of the action happens in compress.c; though note that a few
+small, commonly-used routines are written as inline functions in
+ext2_fs.h.
+
+ext2_readpage() and ext2_mmap() are in file.c.  ext2_file_write() is
+also there.
+
+Routines to read/write the inode from/to disk are in inode.c.
+
+super.c contains some e2compr initialisation code (such as allocating
+the e2compr work area).
+
+All ioctl handling is in ioctl.c.
+
+acl.c is where we deny open() access in a couple of situations (if the
+EXT2_NOCOMPR_FL is set and another process has the file open; and we
+deny write access to a file with EXT2_ECOMPR_FL set).
+
+ialloc.c contains code in ext2_new_inode() for newly-created files to
+inherit compression attributes from the directory in which they're
+created.
+
+truncate.c handles truncation, i.e. zeroing any part of the cluster
+bitmap that's been truncated, and decompressing the final cluster (but
+marking dirty so that we try to recompress it on file close) if the
+new size is part-way through a compressed cluster, so that zeroing
+over the truncated data works.
+
+linux/include/linux/ext2_fs_i.h has the definition of the
+ext2-specific parts of the in-memory inode.  (The on-disk inode is
+defined in ext2_fs.h.)
+
+linux/mm/filemap.c is also interesting, though there's no
+e2compr-specific code there.  Similarly linux/include/linux/mm.h and
+linux/include/linux/fs.h.
+
+generic_readpage() is in linux/fs/buffer.c.  Also all buffer handling.
+
+
+The cleanup scheme
+~~~~~~~~~~~~~~~~~~
+
+inode->u.ext2_i.i_compr_flags has only a single bit defined:
+EXT2_CLEANUP_FL.  This bit gets set to 1 to indicate that
+ext2_cleanup_compressed_inode() needs to be called.  
+
+There is a related flag stored on disk as well as in memory:
+EXT2_DIRTY_FL of i_flags.  If ext2_cleanup_compressed_inode() couldn't
+finish it's job (e.g. due to I/O error) then it clears EXT2_CLEANUP_FL
+of i_compr_flags, but leaves EXT2_DIRTY_FL high.
+
+In ext2_read_inode(), if EXT2_DIRTY_FL is high then EXT2_CLEANUP_FL is
+raised, in the hope that ,whatever was preventing
+ext2_cleanup_compressed_inode() from finishing` is now past.
+
+Except for ext2_read_inode() as noted above, everything that raises
+EXT2_CLEANUP_FL (i.e. ext2_write_file(), ext2_ioctl() and
+ext2_truncate()) also raises EXT2_DIRTY_FL.
+
+Nothing lowers either EXT2_CLEANUP_FL or EXT2_DIRTY_FL except
+ext2_cleanup_compressed_inode() (and one or both of new_inode and
+delete_inode routines).
+
+
+One feels that at least one of these cleanup flags ought to
+disappear.  The main use of the persistent EXT2_DIRTY_FL is where the
+user does `chattr -c' in order to decompress the file, but there isn't
+enough space on the device to do this.  We can get rid of this problem
+by having ext2_ioctl() call ext2_cleanup_compressed_inode()
+try to
+
+
+Notes on a few variables
+~~~~~~~~~~~~~~~~~~~~~~~~
+
+Don't confuse the inode->i_dirt flag with (inode->u.ext2_i.i_flags &
+EXT2_DIRTY_FL).  See section `The cleanup scheme' above for a
+description of EXT2_DIRTY_FL.
+
+
+inode->u.ext2_i.i_clu_nblocks,
+inode->u.ext2_i.i_log2_clu_nblocks:
+
+i_clu_nblocks is always equal to ,1 << i_clu_nblocks` (except during a
+couple of cycles while they're being changed; I haven't consciously
+tried to avoid problems for SMP machines in this respect).
+
+i_clu_nblocks is the number of blocks per cluster for this inode.
+
+Old information: these variables were previously called
+`i_cluster_bits' and `i_cluster_size'.  They were in an array:
+
+inode->u.ext2_i.i_cluster_bits[2], 
+inode->u.ext2_i.i_cluster_size[2]: 
+
+I believe the reason these were declared as an array was for the case
+where someone changes the cluster size of a file that was already
+compressed.  (Reason for this belief: All readers of these fields use
+[0].  On creation (ialloc), read_inode, and `chattr +c' (where
+previously uncompressed), both [0] and [1] are updated.  On change
+(IOC_SET_CLUSTERSIZE), only [0] is updated.)  Since ,changing cluster
+size of an already-compressed file` isn't implemented, I've renamed
+them and made them scalars rather than arrays.
+
+
+inode->u.ext2_i.i_flags: When the e2compr patch is applied, this
+variable only holds the low 24 bits of the on-disk i_flags field.
+(Without the e2compr patch applied, all 32 bits are available.  An
+interesting side effect of this is that user programs can access the
+compression algorithm and cluster size on kernels without e2compr
+patch by using the EXT2_IOC_GETFLAGS, EXT2_IOC_SETFLAGS ioctls.)
+
+
+inode->u.ext2_i.i_compr_method: Holds the compression method
+identifier.  Starting from e2compr-0.4.0, this is different from an
+algorithm identifier: an example of a method is gzip9; the
+corresponding algorithm is gzip.  See compress.c for where
+ext2_method_table and ext2_algorithm_table are defined.  ext2_fs.h has
+some enumerations for addressing these tables (search for
+`EXT2_NONE_METH' and `EXT2_NONE_ALG').
diff -pruN linux-2.6.18.5.org/fs/ext2/adler32.c linux-2.6.18.5/fs/ext2/adler32.c
--- linux-2.6.18.5.org/fs/ext2/adler32.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/adler32.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,43 @@
+/* adler32.c -- compute the Adler-32 checksum of a data stream
+ * Copyright (C) 1995-1998 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* @(#) $Id$ */
+
+#define BASE 65521L /* largest prime smaller than 65536 */
+#define NMAX 5552
+/* NMAX is the largest n such that 255n(n+1)/2 + (n+1)(BASE-1) <= 2^32-1 */
+
+#define DO1(buf,i)  {s1 += buf[i]; s2 += s1;}
+#define DO2(buf,i)  DO1(buf,i); DO1(buf,i+1);
+#define DO4(buf,i)  DO2(buf,i); DO2(buf,i+2);
+#define DO8(buf,i)  DO4(buf,i); DO4(buf,i+4);
+#define DO16(buf)   DO8(buf,0); DO8(buf,8);
+
+/* ========================================================================= */
+unsigned long ext2_adler32(unsigned long adler, const unsigned char *buf, unsigned int len)
+{
+    unsigned long s1 = adler & 0xffff;
+    unsigned long s2 = (adler >> 16) & 0xffff;
+    int k;
+
+    if (buf == 0) return 1L;
+
+    while (len > 0) {
+        k = len < NMAX ? len : NMAX;
+        len -= k;
+        while (k >= 16) {
+            DO16(buf);
+	    buf += 16;
+            k -= 16;
+        }
+        if (k != 0) do {
+            s1 += *buf++;
+	    s2 += s1;
+        } while (--k);
+        s1 %= BASE;
+        s2 %= BASE;
+    }
+    return (s2 << 16) | s1;
+}
diff -pruN linux-2.6.18.5.org/fs/ext2/assert.h linux-2.6.18.5/fs/ext2/assert.h
--- linux-2.6.18.5.org/fs/ext2/assert.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/assert.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,73 @@
+/* Assertion facility for the kernel.
+
+   todo: Provide some way of releasing resources when an assertion
+   fails.
+
+   --- Taken from /usr/include/assert.h.  Some things (such as ANSI /
+   SysV conformance) have been removed.  <sys/cdefs.h> macros have
+   been inlined. --- */
+
+/* Copyright (C) 1991, 1992, 1994, 1995 Free Software Foundation, Inc.
+This file (assert.h) is part of the GNU C Library.
+
+The GNU C Library is free software; you can redistribute it and/or
+modify it under the terms of the GNU Library General Public License as
+published by the Free Software Foundation; either version 2 of the
+License, or (at your option) any later version.
+
+The GNU C Library is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+Library General Public License for more details.
+
+You should have received a copy of the GNU Library General Public
+License along with the GNU C Library; see the file COPYING.LIB.  If
+not, write to the, 1992 Free Software Foundation, Inc., 675 Mass Ave,
+Cambridge, MA 02139, USA.  */
+
+#ifdef	_ASSERT_H
+# undef	_ASSERT_H
+# undef	assert
+#else
+/* This prints an "Assertion failed" message and aborts.  */
+static void __assert_fail (__const char *__assertion,
+			   __const char *__file,
+			   unsigned int __line,
+			   __const char *__function)
+     __attribute__ ((/*__noreturn__, */__unused__));
+
+static void
+__assert_fail(char const *assertion,
+	      char const *file,
+	      unsigned line,
+	      char const *function) 
+{
+	printk(KERN_CRIT "%s:%u: %s: Assertion `%s' failed.\n",
+	       file, line, function, assertion);
+	/* pjm: I don't want this in released code:
+           *(volatile int *)0 = 0; */
+}
+#endif /* !ASSERT_H */
+
+#define	_ASSERT_H	1
+
+#ifdef	NDEBUG
+# define assert(expr)		((void) 0)
+#else /* Not NDEBUG.  */
+
+# define assert(expr)							      \
+  ((void) ((expr) ||							      \
+	   (__assert_fail (#expr,					      \
+			   __FILE__, __LINE__, __ASSERT_FUNCTION), 0)))
+
+
+/* Version 2.4 and later of GCC define a magical variable `__PRETTY_FUNCTION__'
+   which contains the name of the function currently being defined.
+   This is broken in G++ before version 2.6.  */
+# if (!defined (__GNUC__) || __GNUC__ < 2 || \
+     __GNUC_MINOR__ < (defined (__cplusplus) ? 6 : 4))
+#  define __ASSERT_FUNCTION	((__const char *) 0)
+# else
+#  define __ASSERT_FUNCTION	__PRETTY_FUNCTION__
+# endif
+#endif /* NDEBUG.  */
diff -pruN linux-2.6.18.5.org/fs/ext2/balloc.c linux-2.6.18.5/fs/ext2/balloc.c
--- linux-2.6.18.5.org/fs/ext2/balloc.c	2006-12-04 13:31:58.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/balloc.c	2007-01-24 10:09:26.000000000 -0800
@@ -11,8 +11,17 @@
  *        David S. Miller (davem@caip.rutgers.edu), 1995
  */
 
-#include "ext2.h"
+/*
+ *  Copyright (C) 2001 Alcatel Business Systems - R&D Illkirch
+ *    (transparent compression code)
+ *  Pierre Peiffer (pierre.peiffer@sxb.bsf.alcatel.fr) - Denis Richard (denis.
+richard@sxb.bsf.alcatel.fr)
+ *  Adapted from patch e2compr-0.4.39-patch-2.2.18 .
+ */
+
 #include <linux/quotaops.h>
+#include <linux/ext2_fs_c.h>
+#include "debug.h"
 #include <linux/sched.h>
 #include <linux/buffer_head.h>
 #include <linux/capability.h>
@@ -189,6 +198,12 @@ void ext2_free_blocks (struct inode * in
 	struct ext2_super_block * es = sbi->s_es;
 	unsigned freed = 0, group_freed;
 
+#ifdef CONFIG_EXT2_COMPRESS
+	assert((block != EXT2_COMPRESSED_BLKADDR)
+		|| !S_ISREG(inode->i_mode)
+		|| !(EXT2_SB(sb)->s_es->s_feature_incompat
+			& cpu_to_le32(EXT2_FEATURE_INCOMPAT_COMPRESSION)));
+#endif
 	if (block < le32_to_cpu(es->s_first_data_block) ||
 	    block + count < block ||
 	    block + count > le32_to_cpu(es->s_blocks_count)) {
diff -pruN linux-2.6.18.5.org/fs/ext2/bzip2/ChangeLog linux-2.6.18.5/fs/ext2/bzip2/ChangeLog
--- linux-2.6.18.5.org/fs/ext2/bzip2/ChangeLog	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/bzip2/ChangeLog	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,24 @@
+Sun Jan  4 19:58:56 1998  Me  <sewardj@muraroa>
+
+	* lib_bzip_e.c (sortBlock): Added SRC-124 Sec Q6a/Q6b
+	optimisation for strings of identical chars.  Makes a
+	big difference for files with sections of repeated chars.
+
+	* Made version 0.02
+
+Sat Jan  3 20:18:29 1998  Me  <sewardj@muraroa>
+
+	* lib_bzip_d.c: Added a couple of scheduling points
+
+	* lib_bzip_e.c (sortBucket): Changed stackL/R to UInt16.  Added
+	work-limit and scheduling checks to improve worst-case
+	responsiveness.
+	
+	(sendMTFValues): Changed N_GROUPS to 2, and got rid of
+	the selector MTF mechanism.  Fixed bug in prediction of compressed
+	data size.
+
+Mon Dec 29 13:57:51 1997  Me  <sewardj@muraroa>
+
+	* Created version 0.00
+
diff -pruN linux-2.6.18.5.org/fs/ext2/bzip2/Makefile linux-2.6.18.5/fs/ext2/bzip2/Makefile
--- linux-2.6.18.5.org/fs/ext2/bzip2/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/bzip2/Makefile	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,11 @@
+#
+# Makefile for the linux compression routines.
+#
+# Note! Dependencies are done automagically by 'make dep', which also
+# removes any old dependencies. DON'T put your own dependencies here
+# unless it's something special (ie not a .c file).
+#
+# Note 2! The CFLAGS definitions are now in the main makefile...
+
+obj-$(CONFIG_EXT2_HAVE_BZIP2) := ext2-compr-bzip2.o
+ext2-compr-bzip2-y := lib_bzip_huffman.o lib_bzip_e.o lib_bzip_d.o init.o
diff -pruN linux-2.6.18.5.org/fs/ext2/bzip2/Makefile.lib linux-2.6.18.5/fs/ext2/bzip2/Makefile.lib
--- linux-2.6.18.5.org/fs/ext2/bzip2/Makefile.lib	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/bzip2/Makefile.lib	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,30 @@
+
+CC=gcc
+CFLAGS=-m486 -Wall -Winline -O2 -fomit-frame-pointer -funroll-loops
+
+#CC=lcc -Dinline=
+#CFLAGS= -A
+
+#CC=gcc -g
+#CFLAGS= -Wall -Wshadow
+
+OBJS= lib_bzip_huffman.o  \
+      lib_bzip_e.o        \
+      lib_bzip_d.o        \
+      init.o
+
+all: lib testdriver.o
+	$(CC) $(CFLAGS) -o testdriver testdriver.o -L. -lthelib
+
+clean: 
+	rm -f *.o libthelib.a testdriver
+
+lib: $(OBJS) lib_bzip.h lib_bzip_private.h
+	rm -f libthelib.a
+	ar clq libthelib.a $(OBJS)
+
+.c.o: $*.o lib_bzip.h lib_bzip_private.h
+	$(CC) $(CFLAGS) -c $*.c -o $*.o
+
+tarfile:
+	tar cvf lib_bzip-0.02.tar *.c *.h Makefile ChangeLog TODO NOTES
diff -pruN linux-2.6.18.5.org/fs/ext2/bzip2/NOTES linux-2.6.18.5/fs/ext2/bzip2/NOTES
--- linux-2.6.18.5.org/fs/ext2/bzip2/NOTES	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/bzip2/NOTES	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,17 @@
+
+bzip_compressBlock does check that the output will fit into
+the specified space, but only after creating about 1k of
+output first.  So e2compr must ensure that at least 1k
+of output space is available before calling bzip_compressBlock.
+
+bzip_decompressBlock *ignores* the output space indication
+passed by e2compr, and just assumes that N_BLOCK bytes
+of space are available to decompress into.  Does this matter?
+It also ignores the input buffer size indication, since 
+it the end-of-block is detected by interpreting the compressed
+bitstream.
+
+Compression of very repetitive blocks can be quite slow.  
+Blocks which compress too slowly are heavily randomised;
+this limits the max compression ratio to about 15:1.  Most
+blocks are unaffected, tho.
diff -pruN linux-2.6.18.5.org/fs/ext2/bzip2/TODO linux-2.6.18.5/fs/ext2/bzip2/TODO
--- linux-2.6.18.5.org/fs/ext2/bzip2/TODO	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/bzip2/TODO	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,27 @@
+
+Test thoroughly
+
+   -- does it work for very small blocks (down to 1 byte?)
+      (Should do)
+
+      pjm: Incidentally, I've inserted a check `if (ulen == 0) return 0;'
+      as I don't think that the ulen==0 case would work correctly.
+
+      ulen =< 1024 shouldn't be an issue for e2compr, though: the 
+      compressor shouldn't even be called in this case.  When ext2fs 
+      supports fragments, this may change.  However, I suspect that the 
+      smallest possible allocation unit will even then be quite large.
+
+   -- does it always reliably ensure that bzip_compressBlock
+      returns 0 if the compressed data won't fit into the
+      specified space?  (Should do)
+
+      pjm: It doesn't.
+
+
+Check that all stack frames have insignificant size
+(I think so)
+pjm: I haven't found anything looking like a problem,
+     but I haven't been very thorough.
+
+Properly measure compress/decompress speeds 
diff -pruN linux-2.6.18.5.org/fs/ext2/bzip2/init.c linux-2.6.18.5/fs/ext2/bzip2/init.c
--- linux-2.6.18.5.org/fs/ext2/bzip2/init.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/bzip2/init.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,60 @@
+#include "lib_bzip.h"
+#include "lib_bzip_private.h"
+#include <linux/fs.h>
+#include <linux/ext2_fs_c.h>
+#ifdef MODULE
+# include <linux/module.h>
+MODULE_AUTHOR("Julian Seward");
+MODULE_DESCRIPTION("Bzip2 algorithm for EXT2 file compression");
+MODULE_LICENSE("GPL");
+#endif
+
+#define MAX(a,b) ((a)>(b)?(a):(b))
+
+#if 0
+void *bzip2_work_area;
+#endif
+
+size_t bzip_init(int action)
+{
+	switch (action) {
+		case EXT2_ALG_INIT_COMPRESS:
+			return MAX(sizeof(Lib_Bzip_Encode_Storage_Ty),
+					sizeof(Lib_Bzip_Decode_Storage_Ty));
+		case EXT2_ALG_INIT_DECOMPRESS:
+			return sizeof(Lib_Bzip_Decode_Storage_Ty);
+		default:
+			return 0;
+	}
+   /* sizeof(encoding space) == 147268,
+      sizeof(decoding space) == 144180
+      (though this may have changed slightly by the time you read
+      this comment). */
+}
+
+
+#ifdef MODULE
+
+int init_module(void)
+{
+	struct ext2_algorithm bzip_alg;
+
+	bzip_alg.name = NULL;
+	bzip_alg.avail = 1;
+	bzip_alg.init = ext2_iBZIP2;
+	bzip_alg.compress = ext2_wBZIP2;
+	bzip_alg.decompress = ext2_rBZIP2;
+
+	return ext2_register_compression_module(EXT2_BZIP2_ALG, 
+			MAX(sizeof(Lib_Bzip_Encode_Storage_Ty),
+				sizeof(Lib_Bzip_Decode_Storage_Ty)),
+			sizeof(Lib_Bzip_Decode_Storage_Ty),
+			&bzip_alg);
+}
+
+void cleanup_module(void)
+{
+	ext2_unregister_compression_module(EXT2_BZIP2_ALG);
+}
+
+#endif
diff -pruN linux-2.6.18.5.org/fs/ext2/bzip2/lib_bzip.h linux-2.6.18.5/fs/ext2/bzip2/lib_bzip.h
--- linux-2.6.18.5.org/fs/ext2/bzip2/lib_bzip.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/bzip2/lib_bzip.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,32 @@
+#ifdef __KERNEL__
+# include <linux/types.h>
+#else
+# include <stdio.h> /* size_t */
+#endif
+
+#define bzip_compressBlock ext2_wBZIP2
+#define bzip_uncompressBlock ext2_rBZIP2
+#define bzip_init ext2_iBZIP2
+
+extern
+size_t 
+bzip_compressBlock ( unsigned char *in_buf, 
+                     unsigned char *out_buf,
+		     void *heap,
+                     size_t num_in_buf,
+                     size_t num_out_buf,
+                     int param );
+
+extern
+size_t 
+bzip_uncompressBlock ( unsigned char *in_buf, 
+                       unsigned char *out_buf,
+		       void *heap,
+                       size_t num_in_buf,
+                       size_t num_out_buf,
+                       int param );
+
+
+extern
+size_t 
+bzip_init ( int action );
diff -pruN linux-2.6.18.5.org/fs/ext2/bzip2/lib_bzip_d.c linux-2.6.18.5/fs/ext2/bzip2/lib_bzip_d.c
--- linux-2.6.18.5.org/fs/ext2/bzip2/lib_bzip_d.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/bzip2/lib_bzip_d.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,427 @@
+
+/*-------------------------------------------------------------*/
+/*-- bzip-like decompression of 32KB blocks     lib_bzip_d.c --*/
+/*-------------------------------------------------------------*/
+
+/*--
+  This file is part of lib_bzip, a block-sorting compression 
+  library designed to compress 32kbyte blocks, for on-the-fly
+  disk compression/decompression.
+
+  Version 0.02, 4-Jan-1998
+
+  Copyright (C) 1996, 1997, 1998 by Julian Seward.
+     Guildford, Surrey, UK
+     email: jseward@acm.org
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of the GNU General Public License as published by
+  the Free Software Foundation; either version 2 of the License, or
+  (at your option) any later version.
+
+  This program is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with this program; if not, write to the Free Software
+  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+--*/
+
+
+#include "lib_bzip_private.h"
+#include "lib_bzip.h"
+
+#ifdef __KERNEL__
+# include <linux/linkage.h>
+# include <asm/param.h>
+# include <linux/string.h>
+# include <linux/sched.h>
+# define __NO_VERSION__
+# include <linux/module.h>
+
+static unsigned long next_brk = 0;
+extern unsigned long volatile jiffies;
+
+/* Time slice of a fifth of a second. */
+# define MAYBE_SCHEDULE do {            \
+   if (jiffies > next_brk) {            \
+      schedule();                       \
+      next_brk = jiffies + HZ / 5;      \
+   }} while(0)
+
+#else
+
+# include <string.h>
+# define MAYBE_SCHEDULE do { } while(0)
+
+#endif
+
+
+/*---------------------------------------------*/
+/*--
+   Bit stream reading stuff.
+--*/
+
+static void 
+bsInit_d ( Lib_Bzip_Decode_Storage_Ty *dst )
+{
+   dst->inctr = 0;
+   dst->bsLive = 0;
+   dst->bsBuff = 0;
+}
+
+
+#define bsNEEDR(nz)                                   \
+do {	                                              \
+   while (dst->bsLive < nz) {                          \
+      Int32 zzi = (Int32)(dst->inbuff[dst->inctr++]);   \
+      dst->bsBuff = (dst->bsBuff << 8) | (zzi & 0xffL); \
+      dst->bsLive += 8;                                \
+   }                                                  \
+} while(0)
+
+
+static inline
+UInt32 
+bsR ( Lib_Bzip_Decode_Storage_Ty *dst, Int32 n )
+{
+   UInt32 v;
+   bsNEEDR ( n );
+   v = (dst->bsBuff >> (dst->bsLive-n)) & ((1 << n)-1);
+   dst->bsLive -= n;
+   return v;
+}
+
+/*---------------------------------------------*/
+static void 
+makeMaps_d ( Lib_Bzip_Decode_Storage_Ty *dst )
+{
+   Int32 i;
+   dst->nInUse = 0;
+   for (i = 0; i < 256; i++)
+      if (dst->inUse[i]) {
+         dst->seqToUnseq[dst->nInUse] = i;
+         dst->nInUse++;
+      }
+}
+
+
+/*---------------------------------------------*/
+static void 
+recvDecodingTables ( Lib_Bzip_Decode_Storage_Ty *dst )
+{
+   Int32  i, j, t, nSelectors, alphaSize;
+   Int32  minLen, maxLen;
+   Bool inUse16[16];
+
+   dst->origPtr = bsR ( dst, 16 );
+
+   /*--- Receive the mapping table ---*/
+   for (i = 0; i < 16; i++)
+      if (bsR(dst, 1) == 1) 
+         inUse16[i] = True; else 
+         inUse16[i] = False;
+
+   for (i = 0; i < 256; i++) dst->inUse[i] = False;
+
+   for (i = 0; i < 16; i++)
+      if (inUse16[i])
+         for (j = 0; j < 16; j++)
+            if (bsR(dst, 1) == 1) dst->inUse[i * 16 + j] = True;
+
+   makeMaps_d (dst);
+   alphaSize = dst->nInUse + RUNBASE;
+
+   /*--- Now the selectors ---*/
+   nSelectors = bsR ( dst, 10 );
+   for (i = 0; i < nSelectors; i++)
+      dst->selector[i] = bsR(dst, 1);
+
+   /*--- Now the coding tables ---*/
+   for (t = 0; t < N_GROUPS; t++) {
+      Int32 curr = bsR ( dst, 5 );
+      for (i = 0; i < alphaSize; i++) {
+         while (bsR(dst, 1) == 1) {
+            if (bsR(dst, 1) == 0) curr++; else curr--;
+         }
+         dst->len[t][i] = curr;
+      }
+   }
+
+   /*--- Create the Huffman decoding tables ---*/
+   for (t = 0; t < N_GROUPS; t++) {
+      minLen = 32;
+      maxLen = 0;
+      for (i = 0; i < alphaSize; i++) {
+         if (dst->len[t][i] > maxLen) maxLen = dst->len[t][i];
+         if (dst->len[t][i] < minLen) minLen = dst->len[t][i];
+      }
+      hbCreateDecodeTables ( 
+         &(dst->limit[t][0]), &(dst->base[t][0]),
+         &(dst->perm[t][0]), &(dst->len[t][0]),
+         minLen, maxLen, alphaSize
+      );
+      dst->minLens[t] = minLen;
+   }
+}
+
+
+/*---------------------------------------------*/
+#define GET_MTF_VAL(lval)                 \
+do {                                      \
+   Int32 zn, zvec, zj;                    \
+   if (groupPos == 0) {                   \
+      groupNo++;                          \
+      groupPos = G_SIZE;                  \
+      gSel = dst->selector[groupNo];       \
+      gMinlen = dst->minLens[gSel];        \
+      gLimit = &(dst->limit[gSel][0]);     \
+      gPerm = &(dst->perm[gSel][0]);       \
+      gBase = &(dst->base[gSel][0]);       \
+   }                                      \
+   groupPos--;                            \
+   zn = gMinlen; zvec = bsR ( dst, zn );       \
+   while (zvec > gLimit[zn]) {            \
+      zn++; zj = bsR ( dst, 1 );               \
+      zvec = (zvec << 1) | zj;            \
+   };                                     \
+   lval = gPerm[zvec - gBase[zn]];        \
+} while(0)
+
+
+/*---------------------------------------------*/
+static void 
+decodeBlock ( Lib_Bzip_Decode_Storage_Ty *dst,
+              UInt32* cc, 
+              UChar*  mtfa, 
+              Int32*  mtfbase )
+{
+   Int32  nextSym;
+   Int32  EOB, groupNo, groupPos, gSel;
+   Int32  gMinlen = 0;
+   Int32* gLimit = NULL;
+   Int32* gBase = NULL;
+   Int32* gPerm = NULL;
+
+   Int32 lastR, op;
+
+   /*--
+      Now passed in from caller, so as to keep stack
+      frame small.
+   UInt32 cc[256];
+   UChar  mtfa   [MTFA_SIZE];
+   Int32  mtfbase[256 / MTFL_SIZE];
+   --*/
+
+   MAYBE_SCHEDULE;
+
+   recvDecodingTables (dst);
+
+   EOB      = dst->nInUse + RUNBASE - 1;
+   groupNo  = -1;
+   groupPos = 0;
+
+   memset(cc, 0, 256 * sizeof(*cc));
+
+   //-- MTF init
+   {
+      unsigned i, j, k;
+
+      k = MTFA_SIZE-1;
+      for (i = 256 / MTFL_SIZE; i-- > 0;) {
+         for (j = MTFL_SIZE; j-- > 0;) {
+            mtfa[k] = (UChar)(i * MTFL_SIZE + j);
+            k--;
+         }
+         mtfbase[i] = k + 1;
+      }
+   }
+   //-- end MTF init
+
+   lastR = -1;
+
+   MAYBE_SCHEDULE;
+
+   GET_MTF_VAL(nextSym);
+
+   while (True) {
+
+      if (nextSym < RUNBASE) {
+         UChar ch;
+         Int32 sum = -1;
+         Int32 N = 1;
+         do {
+            if (nextSym == RUNA) sum = sum + (0+1) * N;
+            else if (nextSym == RUNB) sum = sum + (1+1) * N;
+            N = N * RUNBASE;
+            GET_MTF_VAL(nextSym);
+         }
+            while (nextSym < RUNBASE);
+
+         sum++;
+         ch = dst->seqToUnseq[ mtfa[mtfbase[0]] ];
+
+         while (sum > 0) {
+            lastR++;
+            dst->tvec[lastR] = (ch << 24) | cc[ch];
+            cc[ch]++;
+            sum--;
+         };
+
+         continue;
+
+      } else if (nextSym == EOB) {
+
+         break;
+
+      } else {
+
+         UChar tmp, ch;
+         lastR++;
+
+         //-- tmp = MTF ( nextSym-(RUNBASE-1) );
+         {
+            Int32 i, j, k, n, p, lno, off;
+            n = (Int32)(nextSym - (RUNBASE-1) );
+
+            if (n < MTFL_SIZE) { // avoid general-case expense
+               p = mtfbase[0];
+               tmp = mtfa[p+n];
+               while (n > 3) {
+                  Int32 z = p+n;
+                  mtfa[(z)  ] = mtfa[(z)-1];
+                  mtfa[(z)-1] = mtfa[(z)-2];
+                  mtfa[(z)-2] = mtfa[(z)-3];
+                  mtfa[(z)-3] = mtfa[(z)-4];
+                  n -= 4;
+               }
+               while (n > 0) { mtfa[(p+n)] = mtfa[(p+n)-1]; n--; };
+               mtfa[p] = tmp;
+            } else { // general case
+               lno = n / MTFL_SIZE;
+               off = n % MTFL_SIZE;
+               p = mtfbase[lno] + off;
+               tmp = mtfa[p];
+               while (p > mtfbase[lno]) { mtfa[p] = mtfa[p-1]; p--; };
+               mtfbase[lno]++;
+               while (lno > 0) {
+                  mtfbase[lno]--;
+                  mtfa[mtfbase[lno]] = mtfa[mtfbase[lno-1] + MTFL_SIZE - 1];
+                  lno--;
+               }
+               mtfbase[0]--;
+               mtfa[mtfbase[0]] = tmp;
+               if (mtfbase[0] == 0) {
+                  k = MTFA_SIZE-1;
+                  for (i = 256 / MTFL_SIZE-1; i >= 0; i--) {
+                     for (j = MTFL_SIZE-1; j >= 0; j--) {
+                        mtfa[k] = mtfa[mtfbase[i] + j];
+                        k--;
+                     }
+                     mtfbase[i] = k + 1;
+                  }
+	       }
+            }
+         }
+         //-- end tmp = MTF ( nextSym-(RUNBASE-1) );
+
+         ch = dst->seqToUnseq[tmp];
+         dst->tvec[lastR] = (ch << 24) | cc[ch]; 
+         cc[ch]++;
+
+         GET_MTF_VAL(nextSym);
+         continue;
+      }
+   }
+
+   MAYBE_SCHEDULE;
+
+   /* */
+   {
+      UInt32 k;
+      unsigned i;
+      Int32 sum;
+
+      k = dst->origPtr;
+      sum = 0;
+      for (i = 0; i < 256; i++) {
+	 sum += cc[i];
+	 cc[i] = sum - cc[i];
+      }
+
+      op = lastR+1;
+      dst->nblock = op;
+      while (op > 0) {
+	 UInt32 u;
+	 UChar v;
+	 u = dst->tvec[k]; 
+	 v = u >> 24; 
+	 k = (u & 0x00ffffff) + cc[v];
+	 op--; dst->outbuff[op] = v;
+      }
+   }
+}
+
+
+/*---------------------------------------------*/
+static void 
+unrandomiseBlock ( Lib_Bzip_Decode_Storage_Ty *dst )
+{
+   Int32 i;
+   RAND_DECLS;
+
+   for (i = 0; i < dst->nblock; i++) {
+      RAND_UPD_MASK;
+      dst->outbuff[i] ^= RAND_MASK;
+   }
+}
+
+/*---------------------------------------------*/
+size_t 
+bzip_uncompressBlock ( unsigned char *in_buf, 
+		       unsigned char *out_buf,
+		       void *heap,
+		       size_t num_in_buf, /* unused */
+		       size_t num_out_buf, /* unused */
+		       int param ) /* unused */
+{
+   Lib_Bzip_Decode_Storage_Ty *dst;
+
+   if (!try_module_get(THIS_MODULE))
+      return 0;
+
+#if 0
+   dst = (Lib_Bzip_Decode_Storage_Ty *) bzip2_work_area_D;
+#else
+   dst = (Lib_Bzip_Decode_Storage_Ty *) heap;
+#endif
+   dst->inbuff = in_buf;
+   dst->outbuff = out_buf;
+
+   bsInit_d (dst);
+
+   if (bsR ( dst, 8 ) != 0xa2) {
+      module_put(THIS_MODULE);
+      return 0;
+   }
+
+   dst->randomised = False;
+   if (bsR(dst, 1) == 1) dst->randomised = True;
+
+   decodeBlock ( dst,
+                 &(dst->cc_tmp[0]),
+		 &(dst->mtfa_tmp[0]),
+		 &(dst->mtfbase_tmp[0]) );
+
+   if (dst->randomised) unrandomiseBlock(dst);
+
+   module_put(THIS_MODULE);;
+   return dst->nblock;
+}
+
+
+/*-------------------------------------------------------------*/
+/*-- end                                        lib_bzip_d.c --*/
+/*-------------------------------------------------------------*/
diff -pruN linux-2.6.18.5.org/fs/ext2/bzip2/lib_bzip_e.c linux-2.6.18.5/fs/ext2/bzip2/lib_bzip_e.c
--- linux-2.6.18.5.org/fs/ext2/bzip2/lib_bzip_e.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/bzip2/lib_bzip_e.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,973 @@
+
+/*-------------------------------------------------------------*/
+/*-- bzip-like compression of 32k blocks        lib_bzip_e.c --*/
+/*-------------------------------------------------------------*/
+
+/*--
+  This file is part of lib_bzip, a block-sorting compression 
+  library designed to compress 32kbyte blocks, for on-the-fly
+  disk compression/decompression.
+
+  Version 0.02, 4-Jan-1998
+
+  Copyright (C) 1996, 1997, 1998 by Julian Seward.
+     Guildford, Surrey, UK
+     email: jseward@acm.org
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of the GNU General Public License as published by
+  the Free Software Foundation; either version 2 of the License, or
+  (at your option) any later version.
+
+  This program is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with this program; if not, write to the Free Software
+  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+--*/
+
+
+#include "lib_bzip_private.h"
+#include "lib_bzip.h"
+
+
+/*---------------------------------------------*/
+/*--
+   Compressed block format:
+
+      UChar 0xa2      -- version 0.02 of this lib
+      
+      (a single bit indicating whether or not this
+       block was randomised)
+
+      UInt16 origPtr  -- rotation posn of original
+
+      (table indicating which chars are in use)
+
+      (Huffman code length tables, one for each
+       of N_GROUPS)
+
+      (selector table)
+
+      (codes for the MTF values proper)
+
+--*/
+
+#ifdef __KERNEL__
+# include <linux/kernel.h>
+# include <linux/linkage.h>
+# include <linux/sched.h>
+# include <asm/param.h>
+# define __NO_VERSION__
+# include <linux/module.h>
+
+static unsigned long next_brk = 0;
+extern unsigned long volatile jiffies;
+
+/* Time slice of a fifth of a second. */
+# define MAYBE_SCHEDULE                 \
+   if (jiffies > next_brk) {            \
+      schedule();                       \
+      next_brk = jiffies + HZ / 5;      \
+   }
+
+# define e_verbosity 0  /* EXT2_BZIP2_VERBOSITY */
+# define do_status_printf(_v, _a...) printk(KERN_DEBUG "bzip2: " _a)
+# define status_printf(_v, _a...) do { if (e_verbosity >= (_v)) printk(KERN_DEBUG "bzip2: " _a); } while(0)
+#else
+
+# define MAYBE_SCHEDULE /*--*/
+# define e_verbosity est->verb
+# define do_status_printf(_v, _a...) fprintf(stderr, "" _a)
+# define status_printf(_v, _a...) do { if (e_verbosity >= (_v)) fprintf(stderr, "" _a); } while(0)
+#endif
+
+/*---------------------------------------------*/
+/*-- 
+   Bit stream writing stuff (boring but necessary).
+--*/
+
+static void 
+bsInit_e ( Lib_Bzip_Encode_Storage_Ty *est )
+{
+   est->outctr = 0;
+   est->bsLive = 0;
+   est->bsBuff = 0;
+}
+
+
+#define bs_MAKE_SPACE                         \
+do {                                          \
+   while (est->bsLive >= 8) {                 \
+      est->outbuff[est->outctr++]             \
+         = (UChar)(est->bsBuff >> 24);        \
+      est->bsBuff <<= 8;                      \
+      est->bsLive -= 8;                       \
+   }                                          \
+} while(0)
+
+
+static inline
+void 
+bsW ( Lib_Bzip_Encode_Storage_Ty *est, Int32 n, UInt32 v )
+{
+   bs_MAKE_SPACE;
+   est->bsBuff |= (v << (32 - est->bsLive - n));
+   est->bsLive += n;
+}
+
+
+static void 
+bsFlush_e ( Lib_Bzip_Encode_Storage_Ty *est )
+{
+   while (est->bsLive > 0) {
+      est->outbuff[est->outctr++]
+         = (UChar)(est->bsBuff >> 24);
+      est->bsBuff <<= 8;
+      est->bsLive -= 8;
+   }
+}
+
+
+/*---------------------------------------------*/
+static inline
+Bool 
+fullGt ( Lib_Bzip_Encode_Storage_Ty *est, Int32 i1, Int32 i2 )
+{
+   UInt16 s1, s2;
+   Int32 k;
+
+   if (i1 == i2) return False;
+
+   s1 = est->block[i1]; s2 = est->block[i2];
+   if (s1 != s2) return (s1 > s2);
+   i1 += 2; i2 += 2;
+
+   s1 = est->block[i1]; s2 = est->block[i2];
+   if (s1 != s2) return (s1 > s2);
+   i1 += 2; i2 += 2;
+
+   s1 = est->block[i1]; s2 = est->block[i2];
+   if (s1 != s2) return (s1 > s2);
+   i1 += 2; i2 += 2;
+
+   k = est->nblock + 16;
+
+   do {
+
+      s1 = est->block[i1]; s2 = est->block[i2];
+      if (s1 != s2) return (s1 > s2);
+      i1 += 2; i2 += 2;
+
+      s1 = est->block[i1]; s2 = est->block[i2];
+      if (s1 != s2) return (s1 > s2);
+      i1 += 2; i2 += 2;
+
+      s1 = est->block[i1]; s2 = est->block[i2];
+      if (s1 != s2) return (s1 > s2);
+      i1 += 2; i2 += 2;
+
+      if (i1 >= (Int32) est->nblock)
+         i1 -= est->nblock;
+      if (i2 >= (Int32) est->nblock)
+         i2 -= est->nblock;
+
+      k -= 6;
+      est->workDone ++;
+   }
+      while (k >= 0);
+
+   return False;
+}
+
+
+/*---------------------------------------------*/
+/*  This qsort is derived from Weiss' book 
+    "Data Structures and Algorithm Analysis in C",
+    Section 7.7.
+*/
+
+#define ISORT_BELOW 10
+
+#define SWAP(za,zb)                                               \
+do { Int32 zl = (za); Int32 zr = (zb);                            \
+     UInt16 zt = est->zptr[zl]; est->zptr[zl] = est->zptr[zr];    \
+     est->zptr[zr] = zt;                                          \
+   } while(0)
+
+static void 
+sortBucket ( Lib_Bzip_Encode_Storage_Ty *est, Int32 left, Int32 right )
+{
+   UInt16 v;
+   Int32 pivot;
+   Int32 i, j;
+   Int32 wuC;
+
+   UInt16 stackL[20];
+   UInt16 stackR[20];
+
+   Int32 sp = 0;
+   Int32 wuL = left;
+   Int32 wuR = right;
+
+   if (left >= right) return;
+
+   while (True) {
+
+      /*-- 
+         At the beginning of this loop, wuL and wuR hold the
+         bounds of the next work-unit.  First, though, check
+         that we are not out of CPU time.
+      --*/
+      MAYBE_SCHEDULE;
+
+      if (wuR - wuL > ISORT_BELOW) {
+
+         /*--
+            a large Work Unit; partition-exchange 
+         --*/
+         wuC = (wuL + wuR) >> 1;
+         if (fullGt ( est, 1+(Int32)est->zptr[wuL], 1+(Int32)est->zptr[wuC] ))
+            SWAP ( wuL, wuC );
+         if (fullGt ( est, 1+(Int32)est->zptr[wuL], 1+(Int32)est->zptr[wuR] ))
+            SWAP ( wuL, wuR );
+         if (fullGt ( est, 1+(Int32)est->zptr[wuC], 1+(Int32)est->zptr[wuR] ))
+            SWAP ( wuC, wuR );
+
+         SWAP ( wuC, wuR-1 );
+         pivot = 1 + (Int32)est->zptr[wuR-1];
+
+         i = wuL;
+         j = wuR - 1;
+         for (;;) {
+            do i++; while (fullGt ( est, pivot, 1+(Int32)est->zptr[i] ));
+            do j--; while (fullGt ( est, 1+(Int32)est->zptr[j], pivot ));
+            if (i < j) SWAP ( i, j ); else break;
+            if (!est->randomised && est->workDone > est->workLimit) return;
+         }
+         SWAP ( i, wuR-1 );
+
+         if ((i - wuL) > (wuR - i)) {
+            stackL[sp] = wuL; stackR[sp] = i-1; sp++; wuL = i+1;
+         } else {
+            stackL[sp] = i+1; stackR[sp] = wuR; sp++; wuR = i-1;
+         }
+
+      } else {
+
+         /*--
+            a small Work-Unit; insertion-sort it
+         --*/                
+         for (i = wuL + 1; i <= wuR; i++) {
+            v = est->zptr[i];
+            j = i;
+            while ( fullGt ( est, 1+(Int32)est->zptr[j-1], 1+(Int32)v )) {
+               est->zptr[j] = est->zptr[j-1];
+               j = j - 1;
+               if (j <= wuL) break;
+            }
+            est->zptr[j] = v;
+            if (!est->randomised && est->workDone > est->workLimit) return;
+         }
+         if (sp == 0) return;
+         sp--; wuL = stackL[sp]; wuR = stackR[sp];
+
+      } /* if this is a small work-unit */
+   }
+}
+
+#undef RC
+#undef SWAP
+#undef ISORT_BELOW
+
+
+
+
+/*---------------------------------------------*/
+#define BIGFREQ(zz) (est->ftab[(zz)+1] - est->ftab[(zz)])
+
+#define FREQ(zz)    (est->ftab[(zz)*3 +3] - est->ftab[(zz)*3 +0])
+#define FREQ_LT(zz) (est->ftab[(zz)*3 +1] - est->ftab[(zz)*3 +0])
+#define FREQ_EQ(zz) (est->ftab[(zz)*3 +2] - est->ftab[(zz)*3 +1])
+#define FREQ_GT(zz) (est->ftab[(zz)*3 +3] - est->ftab[(zz)*3 +2])
+
+#define TIMES3(zz) ((zz)+(zz)+(zz))
+
+static void 
+sortBlock ( Lib_Bzip_Encode_Storage_Ty *est )
+{
+   Int32 j, k, totalOrdered, bucket;
+   unsigned i;
+   UInt16 s, sLo, sHi;
+   Int32 ftabNo;
+
+   MAYBE_SCHEDULE;
+
+   status_printf(3, "      bucket sort ...\n");
+
+   // stripe the block, and set up overshoot area
+
+   if (est->nblock == 1) {
+      est->block[0] &= 0xff00;
+      est->block[0] = (est->block[0] >> 8) | est->block[0];
+   } else {
+      for (i = 0; i < est->nblock - 1; i++) {
+         est->block[i] &= 0xff00;
+         est->block[i] |= (est->block[i+1] >> 8);
+      }
+      est->block[est->nblock-1] &= 0xff00;
+      est->block[est->nblock-1] |= (est->block[0] >> 8);
+   }
+
+   for (i = 0; i < N_OVERSHOOT; i++)
+      est->block[i + est->nblock] = est->block[i % est->nblock];
+
+
+   // set up freq table
+
+   for (i = 0; i < 769; i++) est->ftab[i] = 0;
+
+   for (i = 0; i < est->nblock; i++) {
+      s = est->block[i];
+      sHi = s >> 8;
+      sLo = s & 0xFF;
+      if (sHi > sLo) ftabNo = TIMES3(sHi) + 0; else
+      if (sHi < sLo) ftabNo = TIMES3(sHi) + 2; else
+                     ftabNo = TIMES3(sHi) + 1;
+      est->ftab[ftabNo]++;
+   }
+
+   for (i = 1; i < 769; i++) est->ftab[i] += est->ftab[i-1];
+
+   for (i = 0; i < est->nblock; i++) {
+      s = est->block[i];
+      sHi = s >> 8;
+      sLo = s & 0xFF;
+      if (sHi > sLo) ftabNo = TIMES3(sHi) + 0; else
+      if (sHi < sLo) ftabNo = TIMES3(sHi) + 2; else
+                     ftabNo = TIMES3(sHi) + 1;
+      j = est->ftab[ftabNo] - 1;
+      est->ftab[ftabNo] = j;
+      est->zptr[j] = i;
+   }
+
+   for (i = 0; i < 256; i++)
+      est->inUse[i] = (FREQ(i) > 0);
+   for (i = 0; i < 256; i++)
+      est->rorder[i] = i;
+
+   /* */
+   {
+      Int32 vv;
+      Int32 h = 1;
+      do h = 3 * h + 1; while (h <= 256);
+      do {
+         h = h / 3;
+         for (i = h; i <= 255; i++) {
+            vv = est->rorder[i];
+            j = i;
+            while ( FREQ(est->rorder[j-h]) > FREQ(vv) ) {
+               est->rorder[j] = est->rorder[j-h];
+               j = j - h;
+               if (j < (Int32) h) break;
+            }
+            est->rorder[j] = vv;
+         }
+      } while (h != 1);
+   }
+
+   totalOrdered = 0;
+   for (i = 0; i < 256; i++) {
+      MAYBE_SCHEDULE;
+
+      bucket = est->rorder[i];
+
+      if (FREQ_LT(bucket) > 1) {
+	 status_printf ( 3, "      sort 0x%2x <  size %-4d   ", 
+			 bucket, FREQ_LT(bucket) );
+         sortBucket ( est,
+                      est->ftab[TIMES3(bucket) +0], 
+                      est->ftab[TIMES3(bucket) +1]-1 );
+         totalOrdered += FREQ_LT(bucket);
+         status_printf ( 3, "done %d\n", totalOrdered );
+         if (!est->randomised && est->workDone > est->workLimit) return;
+      }
+
+      if (FREQ_GT(bucket) > 1) {
+	 status_printf ( 3, "      sort 0x%2x >  size %-4d   ", 
+			 bucket, FREQ_GT(bucket) );
+         sortBucket ( est,
+                      est->ftab[TIMES3(bucket) +2], 
+                      est->ftab[TIMES3(bucket) +3]-1 );
+         totalOrdered += FREQ_LT(bucket);
+         status_printf ( 3, "done %d\n", totalOrdered );
+         if (!est->randomised && est->workDone > est->workLimit) return;
+      }
+
+      if (FREQ_EQ(bucket) > 1) {
+         Int32 put0, get0, put1, get1;
+         UChar c1;
+         Int32 sbn = TIMES3(bucket) + 1;
+         Int32 lo = est->ftab[sbn];
+         Int32 hi = est->ftab[sbn+1] - 1;
+         UChar ssc = (UChar)bucket;
+
+	 status_printf ( 3, "      copy 0x%2x =  size %-4d   ", 
+			 bucket, FREQ_EQ(bucket) );
+
+         put0 = lo;
+         get0 = est->ftab[TIMES3(bucket)];
+         put1 = hi;
+         get1 = est->ftab[TIMES3(bucket)+3] - 1;
+         while (get0 < put0) {
+            j = est->zptr[get0]-1; if (j < 0) j += est->nblock;
+            c1 = (UChar)(est->block[j] >> 8);
+            if (c1 == ssc) { est->zptr[put0] = j; put0++; };
+            get0++;
+         }
+         while (get1 > put1) {
+            j = est->zptr[get1]-1; if (j < 0) j += est->nblock;
+            c1 = (UChar)(est->block[j] >> 8);
+            if (c1 == ssc) { est->zptr[put1] = j; put1--; };
+            get1--;
+         }
+         totalOrdered += FREQ_EQ(bucket);
+         status_printf ( 3, "done %d\n", totalOrdered );
+      }
+
+      if (i < 255 && FREQ(bucket) < 256) {
+         Int32 loLim = est->ftab[TIMES3(bucket)];
+         Int32 hiLim = est->ftab[TIMES3(bucket) + 3];
+         for (j = 0, k = loLim;   k < hiLim;   j++, k++) {
+            UInt16 a2upd = est->zptr[k];
+            est->block[a2upd] = (est->block[a2upd] & 0xff00) | ((UInt16)j);
+            if (a2upd < N_OVERSHOOT)
+               est->block[a2upd + est->nblock] 
+                  = (est->block[a2upd + est->nblock] & 0xff00) | ((UInt16)j);
+         }
+      }
+
+   }
+}
+
+#undef FREQ
+#undef FREQ_LT
+#undef FREQ_EQ
+#undef FREQ_GT
+#undef TIMES3
+
+
+/*---------------------------------------------*/
+static void makeMaps_e ( Lib_Bzip_Encode_Storage_Ty *est )
+{
+   unsigned i;
+
+   est->nInUse = 0;
+   for (i = 0; i < 256; i++)
+      if (est->inUse[i]) {
+         est->unseqToSeq[i] = est->nInUse;
+         est->nInUse++;
+      }
+}
+
+
+
+/*---------------------------------------------*/
+static void 
+generateMTFValues ( Lib_Bzip_Encode_Storage_Ty *est )
+{
+   UChar   yy[256];
+   unsigned i;
+   Int32   j;
+   UChar   tmp;
+   UChar   tmp2;
+   Int32   zPend;
+   Int32   wr;
+   Int32   EOB;
+
+   MAYBE_SCHEDULE;
+
+   makeMaps_e (est);
+   EOB = est->nInUse + RUNBASE - 1;
+
+   for (i = 0; (Int32) i <= EOB; i++) est->mtfFreq[i] = 0;
+
+   wr = 0;
+   zPend = 0;
+   for (i = 0; (Int32) i < est->nInUse; i++)
+      yy[i] = (UChar) i;
+
+   for (i = 0; i < est->nblock; i++) {
+      UChar ll_i;
+
+      j = est->zptr[i] - 1; if (j < 0) j = est->nblock-1;      
+      ll_i = est->unseqToSeq[est->block[j] >> 8];
+
+      j = 0;
+      tmp = yy[j];
+      while ( ll_i != tmp ) {
+         j++;
+         tmp2 = tmp;
+         tmp = yy[j];
+         yy[j] = tmp2;
+      };
+      yy[0] = tmp;
+
+      if (j == 0) {
+         zPend++;
+      } else {
+         if (zPend > 0) {
+            zPend--;
+            while (True) {
+               switch (zPend % RUNBASE) {
+                  case 0: est->zptr[wr] = RUNA; wr++; est->mtfFreq[RUNA]++; break;
+                  case 1: est->zptr[wr] = RUNB; wr++; est->mtfFreq[RUNB]++; break;
+               };
+               if (zPend < RUNBASE) break;
+               zPend = (zPend - RUNBASE) / RUNBASE;
+            };
+            zPend = 0;
+         }
+         est->zptr[wr] = j+RUNBASE-1; wr++; est->mtfFreq[j+RUNBASE-1]++;
+      }
+   }
+
+   if (zPend > 0) {
+      zPend--;
+      while (True) {
+         switch (zPend % RUNBASE) {
+            case 0:  est->zptr[wr] = RUNA; wr++; est->mtfFreq[RUNA]++; break;
+            case 1:  est->zptr[wr] = RUNB; wr++; est->mtfFreq[RUNB]++; break;
+         };
+         if (zPend < RUNBASE) break;
+         zPend = (zPend - RUNBASE) / RUNBASE;
+      };
+   }
+
+   est->zptr[wr] = EOB; wr++; est->mtfFreq[EOB]++;
+
+   est->nMTF = wr;
+}
+
+
+/*---------------------------------------------*/
+#define LESSER_ICOST  0
+#define GREATER_ICOST 15
+
+static Bool
+sendMTFValues ( Lib_Bzip_Encode_Storage_Ty *est )
+{
+   Int32 v, t, i, j, gs, ge, totc, bt, bc, iter;
+   Int32 alphaSize, selCtr;
+   UInt31 nSelectors;
+   Int32 nBytes;
+
+   /*--
+   UChar  len [N_GROUPS][MAX_ALPHA_SIZE];
+   is a global since the decoder also needs it.
+
+   UInt32 code[N_GROUPS][MAX_ALPHA_SIZE];
+   Int32  rfreq[N_GROUPS][MAX_ALPHA_SIZE];
+   are also globals only used in this proc.
+   Made global to keep stack frame size small.
+   --*/
+
+   UInt16 cost[N_GROUPS];
+   Int32  fave[N_GROUPS];
+
+   MAYBE_SCHEDULE;
+
+   status_printf ( 3, 
+                "      %u in block, %d after MTF & 1-2 coding, %u+2 syms in use\n", 
+                est->nblock, est->nMTF, est->nInUse );
+
+   alphaSize = est->nInUse + RUNBASE;
+   for (t = 0; t < N_GROUPS; t++)
+      for (v = 0; v < alphaSize; v++)
+         est->len[t][v] = GREATER_ICOST;
+
+   /*--- Generate an initial set of coding tables ---*/
+   { 
+      Int32 nPart, remF, tFreq, aFreq;
+
+      nPart = N_GROUPS;
+      remF  = est->nMTF;
+      gs = 0;
+      while (nPart > 0) {
+         tFreq = remF / nPart;
+         ge = gs-1;
+         aFreq = 0;
+         while (aFreq < tFreq && ge < alphaSize-1) {
+            ge++;
+            aFreq += est->mtfFreq[ge];
+         }
+
+         if (ge > gs 
+             && nPart != N_GROUPS && nPart != 1 
+             && ((N_GROUPS-nPart) % 2 == 1)) {
+            aFreq -= est->mtfFreq[ge];
+            ge--;
+         }
+#ifndef __KERNEL__  /* floating point (todo) */
+         status_printf ( 3, "      initial group %d, [%d .. %d], has %d syms (%4.1f%%)\n",
+			 nPart, gs, ge, aFreq, 
+			 (100.0 * (float)aFreq) / (float)est->nMTF );
+#endif
+         for (v = 0; v < alphaSize; v++)
+            if (v >= gs && v <= ge) 
+               est->len[nPart-1][v] = LESSER_ICOST; else
+               est->len[nPart-1][v] = GREATER_ICOST;
+ 
+         nPart--;
+         gs = ge+1;
+         remF -= aFreq;
+      }
+   }
+
+   /*--- 
+      Iterate up to N_ITERS times to improve the tables.
+   ---*/
+   for (iter = 0; iter < N_ITERS; iter++) {
+
+      MAYBE_SCHEDULE;
+
+      for (t = 0; t < N_GROUPS; t++) fave[t] = 0;
+
+      for (t = 0; t < N_GROUPS; t++)
+         for (v = 0; v < alphaSize; v++)
+            est->rfreq[t][v] = 0;
+
+      nSelectors = 0;
+      totc = 0;
+      gs = 0;
+      while (True) {
+
+         /*--- Set group start & end marks. --*/
+         if (gs >= est->nMTF) break;
+         ge = gs + G_SIZE - 1; 
+         if (ge >= est->nMTF) ge = est->nMTF-1;
+
+         /*-- 
+            Calculate the cost of this group as coded
+            by each of the coding tables.
+         --*/
+         for (t = 0; t < N_GROUPS; t++)
+	    cost[t] = 0;
+
+	 /* */
+         {
+            register UInt16 cost0, cost1;
+
+#if N_GROUPS != 2
+# error "bad assumption here"
+#endif
+            cost0 = cost1 = 0;
+            for (i = gs; i <= ge; i++) { 
+               UInt16 icv = est->zptr[i];
+               cost0 += est->len[0][icv];
+               cost1 += est->len[1][icv];
+            }
+            cost[0] = cost0; cost[1] = cost1;
+         }
+ 
+         /*-- 
+            Find the coding table which is best for this group,
+            and record its identity in the selector table.
+         --*/
+         bc = 999999999; bt = -1;
+         for (t = 0; t < N_GROUPS; t++)
+            if (cost[t] < bc) { bc = cost[t]; bt = t; };
+         totc += bc;
+         fave[bt]++;
+         est->selector[nSelectors] = bt;
+         nSelectors++;
+
+         /*-- 
+            Increment the symbol frequencies for the selected table.
+          --*/
+         for (i = gs; i <= ge; i++)
+            est->rfreq[bt][ est->zptr[i] ]++;
+
+         gs = ge+1;
+      }
+
+#ifndef __KERNEL__  /* too hard... todo */
+      if (e_verbosity >= 3) {
+         status_printf ( 3, "      pass %d: size is %d, grp uses are ", 
+			 iter+1, totc/8 );
+         for (t = 0; t < N_GROUPS; t++)
+            status_printf ( 3, "%d ", fave[t] );
+         fprintf ( stderr, "\n" );
+      }
+#endif
+
+      /*--
+        Recompute the tables based on the accumulated frequencies.
+      --*/
+      for (t = 0; t < N_GROUPS; t++)
+         hbMakeCodeLengths ( 
+            &(est->len[t][0]), &(est->rfreq[t][0]), alphaSize, 20,
+            &(est->heap_tmp[0]), &(est->weight_tmp[0]), &(est->parent_tmp[0])
+         );
+   }
+
+   /*--- Assign actual codes for the tables. --*/
+   for (t = 0; t < N_GROUPS; t++) {
+      unsigned minLen = 32;
+      unsigned maxLen = 0;
+
+      for (i = 0; i < alphaSize; i++) {
+         if (est->len[t][i] > maxLen)
+	    maxLen = est->len[t][i];
+         if (est->len[t][i] < minLen)
+	    minLen = est->len[t][i];
+      }
+      hbAssignCodes ( &(est->code[t][0]), &(est->len[t][0]), 
+                      minLen, maxLen, alphaSize );
+   }
+
+   /*--- Transmit the mapping table. ---*/
+   { 
+      Bool inUse16[16];
+
+      for (i = 0; i < 16; i++) {
+          inUse16[i] = 0;
+          for (j = 0; j < 16; j++)
+             if (est->inUse[i * 16 + j])
+		inUse16[i] = 1;
+      }
+     
+      nBytes = est->outctr;
+      for (i = 0; i < 16; i++)
+	 bsW(est, 1, inUse16[i]);
+
+      for (i = 0; i < 16; i++)
+         if (inUse16[i]) {
+            for (j = 0; j < 16; j++)
+               if (est->inUse[i * 16 + j]) 
+                  { bsW(est, 1,1); }
+	       else 
+                  { bsW(est, 1,0); };
+         }
+      status_printf ( 3, " bytes: mapping %d, ", est->outctr-nBytes );
+   }
+
+   /*--- Now the selectors. ---*/
+   nBytes = est->outctr;
+   bsW ( est, 10, nSelectors );
+   for (i = 0; i < (Int32) nSelectors; i++)
+      bsW(est, 1, (est->selector[i] != 0));
+
+   status_printf ( 3, "selectors %d, ", est->outctr-nBytes );
+
+   /*--- Now the coding tables. ---*/
+   nBytes = est->outctr;
+
+   for (t = 0; t < N_GROUPS; t++) {
+      unsigned curr = est->len[t][0];
+      bsW ( est, 5, curr );
+      for (i = 0; i < alphaSize; i++) {
+         while (curr < est->len[t][i]) { bsW(est, 2,2); curr++; /* 2 -> binary 10 */ };
+         while (curr > est->len[t][i]) { bsW(est, 2,3); curr--; /* 3 -> binary 11 */ };
+         bsW ( est, 1, 0 );
+      }
+   }
+
+   status_printf ( 3, "code lengths %d, ", est->outctr - nBytes );
+
+
+   /*--
+      Compute the total size of output, and check whether 
+      it will fit into the caller's output buffer, and, 
+      if not, give up now.
+   --*/
+   {
+      Int32 totalBits = 8 * est->outctr;
+
+      selCtr = 0;
+      gs = 0;
+      while (True) {
+         UChar* ltab;
+         if (gs >= est->nMTF) break;
+         ge = gs + G_SIZE - 1; 
+         if (ge >= est->nMTF) ge = est->nMTF-1;
+         ltab = &(est->len[est->selector[selCtr]][0]);
+         for (i = gs; i <= ge; i++)
+            totalBits += ltab[est->zptr[i]];
+         gs = ge+1;
+         selCtr++;
+      }
+      /*-- allow for worst-case flushing of bit buffer --*/
+      totalBits += 32;
+      /*-- convert to bytes --*/
+      totalBits = (totalBits + 7) / 8;
+      est->predictedSize = totalBits;
+      if (totalBits > est->totalSizeAllowed) return False;
+   }
+
+
+   /*--- And finally, the block data proper ---*/
+   nBytes = est->outctr;
+   selCtr = 0;
+   gs = 0;
+   while (True) {
+      UChar*  ltab;
+      UInt32* ctab;
+
+      if (gs >= est->nMTF) break;
+      ge = gs + G_SIZE - 1; 
+      if (ge >= est->nMTF) ge = est->nMTF-1;
+      ltab = &(est->len [est->selector[selCtr]][0]);
+      ctab = &(est->code[est->selector[selCtr]][0]);
+      for (i = gs; i <= ge; i++) {
+         UInt16 zptr_i = est->zptr[i];
+         bsW ( est, ltab[zptr_i], ctab[zptr_i] );
+      }
+      gs = ge+1;
+      selCtr++;
+   }
+
+   status_printf ( 3, "codes %d\n", est->outctr-nBytes );
+
+   return True;
+}
+
+
+/*---------------------------------------------*/
+Int16 const rNums[64] = {    
+   619, 720, 127, 481, 931, 816, 813, 233, 566, 247, 
+   985, 724, 205, 454, 863, 491, 741, 242, 949, 214, 
+   733, 859, 335, 708, 621, 574, 73, 654, 730, 472, 
+   419, 436, 278, 496, 867, 210, 399, 680, 480, 51, 
+   878, 465, 811, 169, 869, 675, 611, 697, 867, 561, 
+   862, 687, 507, 283, 482, 129, 807, 591, 733, 623, 
+   150, 238, 59, 379
+};
+
+
+static void 
+randomiseBlock ( Lib_Bzip_Encode_Storage_Ty *est )
+{
+   unsigned i;
+   RAND_DECLS;
+
+   for (i = 0; i < est->nblock; i++) {
+      RAND_UPD_MASK;
+      est->block[i] ^= RAND_MASK << 8;
+   }
+}
+
+
+/*---------------------------------------------*/
+size_t 
+bzip_compressBlock ( unsigned char *in_buf, 
+		     unsigned char *out_buf,
+		     void *heap,
+		     size_t num_in_buf,
+		     size_t num_out_buf,
+		     int param /* unused */ )
+{
+   size_t i;
+   Bool fitsOk;
+   Lib_Bzip_Encode_Storage_Ty *est;
+
+   if (num_in_buf == 0)
+      return 0; /* Some functions (e.g. sortBlock() don't like nblock to equal 0. */
+
+   if (!try_module_get(THIS_MODULE))
+      return 0;
+
+#if 0
+   est = (Lib_Bzip_Encode_Storage_Ty *) bzip2_work_area_C;
+#else
+   est = (Lib_Bzip_Encode_Storage_Ty *) heap;
+#endif
+
+   est->totalSizeAllowed = num_out_buf;
+   est->verb             = 0;
+   est->nblock           = num_in_buf;
+
+   // make est->block a copy of in_buf <<\'d 8
+
+   for (i = 0; i < num_in_buf; i++)
+      est->block[i] = ((UInt16)in_buf[i]) << 8;
+
+   // try to sort it
+   est->randomised = False;
+   est->workLimit  = 250 * est->nblock;
+   est->workDone   = 0;
+   sortBlock (est);
+#ifndef __KERNEL__  /* floating point (todo) */
+   status_printf ( 2, "      %d work, %u nblock, %5.2f ratio\n",
+		   est->workDone, est->nblock,
+		   (float)est->workDone /
+		   (float)(est->nblock==0 ? 1 : est->nblock) );
+#endif
+   // if sorting failed because of too much work, 
+   // randomise and try again (this can\'t fail).
+
+   if (est->workDone > est->workLimit) {
+      status_printf ( 2, "      sorting aborted; randomising block\n" );
+      randomiseBlock (est);
+      est->randomised = True;
+      est->workLimit = 0;
+      est->workDone = 0;
+      sortBlock (est);
+#ifndef __KERNEL__  /* floating point (todo) */
+      status_printf ( 3, "      %d work, %d nblock, %5.2f ratio\n",
+		      est->workDone, est->nblock,
+		      (float)est->workDone /
+		      (float)(est->nblock==0 ? 1 : est->nblock) );
+#endif
+   }
+
+   // find the posn of the original string
+
+   i = 0;
+   while (i < est->nblock && est->zptr[i] != 0)
+      i++;
+
+   est->outbuff = out_buf;
+   bsInit_e (est);
+   bsW ( est, 8, 0xa2 );  /* version number */
+
+   if (est->randomised) bsW(est, 1,1); else bsW(est, 1,0);
+
+   bsW ( est, 16, i );
+
+   generateMTFValues (est);
+
+   fitsOk = sendMTFValues (est);
+   if (!fitsOk) {
+      module_put(THIS_MODULE);
+      return 0;
+   }
+
+   if (est->outctr > est->predictedSize) {
+#ifdef __KERNEL__
+      printk ( KERN_ERR
+	       "bzip2 panic: predicted size (%d) less than actual (%d)\n",
+	       est->predictedSize, est->outctr );
+      module_put(THIS_MODULE);
+      return 0;
+#else
+      fprintf ( stderr, "panic: predicted size is too small\n" );
+      fprintf ( stderr, "       predicted %d, actual %d\n", 
+                       est->predictedSize, est->outctr );
+      exit(1);
+#endif
+   }
+
+   bsFlush_e (est);
+
+   module_put(THIS_MODULE);
+   return est->outctr;
+}
+
+
+/*-------------------------------------------------------------*/
+/*-- end                                        lib_bzip_e.c --*/
+/*-------------------------------------------------------------*/
diff -pruN linux-2.6.18.5.org/fs/ext2/bzip2/lib_bzip_huffman.c linux-2.6.18.5/fs/ext2/bzip2/lib_bzip_huffman.c
--- linux-2.6.18.5.org/fs/ext2/bzip2/lib_bzip_huffman.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/bzip2/lib_bzip_huffman.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,210 @@
+
+/*-------------------------------------------------------------*/
+/*-- Huffman coding low-level stuff                          --*/
+/*--                                      lib_bzip_huffman.c --*/
+/*-------------------------------------------------------------*/
+
+/*--
+  This file is part of lib_bzip, a block-sorting compression 
+  library designed to compress 32kbyte blocks, for on-the-fly
+  disk compression/decompression.
+
+  Version 0.02, 4-Jan-1998
+
+  Copyright (C) 1996, 1997, 1998 by Julian Seward.
+     Guildford, Surrey, UK
+     email: jseward@acm.org
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of the GNU General Public License as published by
+  the Free Software Foundation; either version 2 of the License, or
+  (at your option) any later version.
+
+  This program is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with this program; if not, write to the Free Software
+  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+--*/
+
+
+#include "lib_bzip_private.h"
+
+
+/*---------------------------------------------------*/
+/*--- Huffman coding low-level stuff              ---*/
+/*---------------------------------------------------*/
+
+#define WEIGHTOF(zz0)  ((zz0) & 0xffffff00U)
+#define DEPTHOF(zz1)   ((zz1) & 0x000000ffU)
+#define MYMAX(zz2,zz3) ((zz2) > (zz3) ? (zz2) : (zz3))
+
+#define ADDWEIGHTS(zw1,zw2)                           \
+   (WEIGHTOF(zw1)+WEIGHTOF(zw2)) |                    \
+   (1 + MYMAX(DEPTHOF(zw1),DEPTHOF(zw2)))
+
+#define UPHEAP(z)                                     \
+do {                                                  \
+   Int32 zz, tmp;                                     \
+   zz = z; tmp = heap[zz];                            \
+   while (weight[tmp] < weight[heap[zz >> 1]]) {      \
+      heap[zz] = heap[zz >> 1];                       \
+      zz >>= 1;                                       \
+   }                                                  \
+   heap[zz] = tmp;                                    \
+} while(0)
+
+#define DOWNHEAP(z)                                   \
+do {                                                  \
+   Int32 zz, yy, tmp;                                 \
+   zz = z; tmp = heap[zz];                            \
+   while (True) {                                     \
+      yy = zz << 1;                                   \
+      if (yy > nHeap) break;                          \
+      if (yy < nHeap &&                               \
+          weight[heap[yy+1]] < weight[heap[yy]])      \
+         yy++;                                        \
+      if (weight[tmp] < weight[heap[yy]]) break;      \
+      heap[zz] = heap[yy];                            \
+      zz = yy;                                        \
+   }                                                  \
+   heap[zz] = tmp;                                    \
+} while(0)
+
+
+/*---------------------------------------------*/
+void hbMakeCodeLengths ( UChar* len, 
+                         Int32* freq,
+                         Int32  alphaSize,
+                         Int32  maxLen,
+                         // temporaries
+                         Int32* heap,
+                         Int32* weight,
+                         Int32* parent)
+{
+   /*--
+      Nodes and heap entries run from 1.  Entry 0
+      for both the heap and nodes is a sentinel.
+   --*/
+   Int32 nNodes, nHeap, n1, n2, i, j, k;
+   Bool  tooLong;
+
+   /*--
+      Now passed in from caller, so as to keep stack
+      frame small.
+   Int32 heap   [ MAX_ALPHA_SIZE + 2 ];
+   Int32 weight [ MAX_ALPHA_SIZE * 2 ];
+   Int32 parent [ MAX_ALPHA_SIZE * 2 ]; 
+   --*/
+
+   for (i = 0; i < alphaSize; i++)
+      weight[i+1] = (freq[i] == 0 ? 1 : freq[i]) << 8;
+
+   while (True) {
+
+      nNodes = alphaSize;
+      nHeap = 0;
+
+      heap[0] = 0;
+      weight[0] = 0;
+      parent[0] = -2;
+
+      for (i = 1; i <= alphaSize; i++) {
+         parent[i] = -1;
+         nHeap++;
+         heap[nHeap] = i;
+         UPHEAP(nHeap);
+      }
+   
+      while (nHeap > 1) {
+         n1 = heap[1]; heap[1] = heap[nHeap]; nHeap--; DOWNHEAP(1);
+         n2 = heap[1]; heap[1] = heap[nHeap]; nHeap--; DOWNHEAP(1);
+         nNodes++;
+         parent[n1] = parent[n2] = nNodes;
+         weight[nNodes] = ADDWEIGHTS(weight[n1], weight[n2]);
+         parent[nNodes] = -1;
+         nHeap++;
+         heap[nHeap] = nNodes;
+         UPHEAP(nHeap);
+      }
+
+      tooLong = False;
+      for (i = 1; i <= alphaSize; i++) {
+         j = 0;
+         k = i;
+         while (parent[k] >= 0) { k = parent[k]; j++; }
+         len[i-1] = j;
+         if (j > maxLen) tooLong = True;
+      }
+      
+      if (! tooLong) break;
+
+      for (i = 1; i < alphaSize; i++) {
+         j = weight[i] >> 8;
+         j = 1 + (j / 2);
+         weight[i] = j << 8;
+      }
+   }
+}
+
+
+/*---------------------------------------------*/
+void hbAssignCodes ( UInt32 *code,
+                     UChar *length,
+                     unsigned minLen,
+                     unsigned maxLen,
+                     Int32 alphaSize )
+{
+   Int32 i;
+   UInt32 vec;
+   unsigned n;
+
+   vec = 0;
+   for (n = minLen; n <= maxLen; n++) {
+      for (i = 0; i < alphaSize; i++)
+         if (length[i] == n) { code[i] = vec; vec++; };
+      vec <<= 1;
+   }
+}
+
+
+/*---------------------------------------------*/
+void hbCreateDecodeTables ( Int32 *limit,
+                            Int32 *base,
+                            Int32 *perm,
+                            UChar *length,
+                            Int32 minLen,
+                            Int32 maxLen,
+                            Int32 alphaSize )
+{
+   Int32 pp, i, j, vec;
+
+   pp = 0;
+   for (i = minLen; i <= maxLen; i++)
+      for (j = 0; j < alphaSize; j++)
+         if (length[j] == i) { perm[pp] = j; pp++; };
+
+   for (i = 0; i < MAX_CODE_LEN; i++) base[i] = 0;
+   for (i = 0; i < alphaSize; i++) base[length[i]+1]++;
+
+   for (i = 1; i < MAX_CODE_LEN; i++) base[i] += base[i-1];
+
+   for (i = 0; i < MAX_CODE_LEN; i++) limit[i] = 0;
+   vec = 0;
+
+   for (i = minLen; i <= maxLen; i++) {
+      vec += (base[i+1] - base[i]);
+      limit[i] = vec-1;
+      vec <<= 1;
+   }
+   for (i = minLen + 1; i <= maxLen; i++)
+      base[i] = ((limit[i-1] + 1) << 1) - base[i];
+}
+
+
+/*-------------------------------------------------------------*/
+/*-- end                                  lib_bzip_huffman.c --*/
+/*-------------------------------------------------------------*/
diff -pruN linux-2.6.18.5.org/fs/ext2/bzip2/lib_bzip_private.h linux-2.6.18.5/fs/ext2/bzip2/lib_bzip_private.h
--- linux-2.6.18.5.org/fs/ext2/bzip2/lib_bzip_private.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/bzip2/lib_bzip_private.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,253 @@
+
+/*-------------------------------------------------------------*/
+/*-- private header for lib_bzip          lib_bzip_private.h --*/
+/*-------------------------------------------------------------*/
+
+/*--
+  This file is part of lib_bzip, a block-sorting compression 
+  library designed to compress 32kbyte blocks, for on-the-fly
+  disk compression/decompression.
+
+  Version 0.02, 3-Jan-1998
+
+  Copyright (C) 1996, 1997, 1998 by Julian Seward.
+     Guildford, Surrey, UK
+     email: jseward@acm.org
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of the GNU General Public License as published by
+  the Free Software Foundation; either version 2 of the License, or
+  (at your option) any later version.
+
+  This program is distributed in the hope that it will be useful,
+  but WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+  GNU General Public License for more details.
+
+  You should have received a copy of the GNU General Public License
+  along with this program; if not, write to the Free Software
+  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+--*/
+
+
+#ifndef LIB_BZIP_PRIVATE_H
+#define LIB_BZIP_PRIVATE_H
+
+
+#ifndef __KERNEL__
+# include <stdlib.h>
+# include <stdio.h>
+/* unused: #include <assert.h> */
+#endif
+
+
+/*---------------------------------------------*/
+// useful types
+
+typedef char            Char;
+typedef unsigned char   Bool;
+typedef unsigned char   UChar;
+typedef int             Int32;
+typedef unsigned int    UInt32;
+/* UInt31 is used for things that can safely be cast to either UInt32
+   or Int32 */
+typedef unsigned int    UInt31;
+typedef short           Int16;
+typedef unsigned short  UInt16;
+
+#define True  ((Bool)1)
+#define False ((Bool)0)
+
+
+
+/*---------------------------------------------*/
+// global constants
+
+// max block size, 
+// and max pointer denormalisation when sorting
+#define N_BLOCK     32768
+#define N_OVERSHOOT (1U+12U)
+
+// parameters for run-len coding of zeroes
+#define RUNBASE 2
+#define RUNA 0
+#define RUNB 1
+
+// parameters for the back end
+#define MAX_ALPHA_SIZE (256 + RUNBASE)
+#define MAX_SELECTORS (2 + (N_BLOCK / G_SIZE))
+#define MAX_CODE_LEN 23
+
+// Don\'t change N_GROUPS; doing so will make the
+// back end totally not work (not even slightly!)
+// sendMTFValues() and the compressed file format
+// now assume that N_GROUPS == 2.
+#define N_GROUPS 2
+#define G_SIZE   50
+#define N_ITERS  4
+
+
+
+/*---------------------------------------------*/
+// a type containing all storage needed for compression
+typedef
+   struct {
+      // verbosity of debugging output
+      Int32 verb;
+
+      // block size, block, pointers to block
+      unsigned nblock;
+      UInt16 block[N_BLOCK + N_OVERSHOOT];
+      UInt16 zptr [N_BLOCK];
+
+      // freq table and running order, both for sorting
+      Int32  ftab  [769];
+      Int32  rorder[256];
+
+      // regarding block randomisation during sorting
+      Bool  randomised;
+      Int32 workDone;
+      Int32 workLimit;
+
+      // a record of which chars are used in block,
+      // and a mapping into contiguous values 0 .. nInUse-1
+      Bool  inUse[256];
+      Int32 nInUse;
+      UChar unseqToSeq[256];
+
+      // to do with MTF values
+      Int32 mtfFreq[MAX_ALPHA_SIZE];
+      Int32 nMTF;
+
+      // to do with calculating Huffman coding tables
+      UChar  len  [N_GROUPS][MAX_ALPHA_SIZE];
+      UInt32 code [N_GROUPS][MAX_ALPHA_SIZE];
+      Int32  rfreq[N_GROUPS][MAX_ALPHA_SIZE];
+      UChar  selector   [MAX_SELECTORS];
+
+      // temporaries used only for calculating
+      // Huffman code lengths
+      Int32 heap_tmp   [ MAX_ALPHA_SIZE + 2 ];
+      Int32 weight_tmp [ MAX_ALPHA_SIZE * 2 ];
+      Int32 parent_tmp [ MAX_ALPHA_SIZE * 2 ]; 
+
+      // bit stream buffering
+      UInt32 bsBuff;
+      Int32  bsLive;
+      Int32  outctr;
+      UChar* outbuff;
+      Int32  totalSizeAllowed;
+      Int32  predictedSize;  // only for debugging
+   }
+   Lib_Bzip_Encode_Storage_Ty;
+
+
+/*---------------------------------------------*/
+// constants for the fast MTF decoder
+#define MTFA_SIZE 4096
+#define MTFL_SIZE 16
+
+
+// a type containing all storage needed for decompression
+typedef
+   struct {
+      Bool   inUse[256];
+      UInt16 origPtr;
+      Int32  nInUse;
+      UChar  seqToUnseq[256];
+      Bool   randomised;
+
+      // temporaries used only for MTF decoding
+      UInt32 cc_tmp      [256];
+      UChar  mtfa_tmp    [MTFA_SIZE];
+      Int32  mtfbase_tmp [256 / MTFL_SIZE];
+
+      // the selector tables
+      UChar  selector   [MAX_SELECTORS];
+ 
+      // huffman decoding tables
+      UChar len    [N_GROUPS][MAX_ALPHA_SIZE];
+      Int32 limit  [N_GROUPS][MAX_ALPHA_SIZE];
+      Int32 base   [N_GROUPS][MAX_ALPHA_SIZE];
+      Int32 perm   [N_GROUPS][MAX_ALPHA_SIZE];
+      Int32 minLens[N_GROUPS];
+
+      // for the inverse BWT
+      UInt32 tvec[N_BLOCK];
+
+      // bit stream buffering
+      UInt32 bsBuff;
+      Int32  bsLive;
+      Int32  inctr;
+
+      // buffers, and size of uncompressed block
+      UChar* inbuff;
+      UChar* outbuff;
+      Int32  nblock;
+   }
+   Lib_Bzip_Decode_Storage_Ty;
+
+
+/*---------------------------------------------*/
+// Huffman coding helper functions
+
+extern 
+void 
+hbMakeCodeLengths ( UChar* len, 
+                    Int32* freq,
+                    Int32  alphaSize,
+                    Int32  maxLen,
+                    Int32* heap,
+                    Int32* weight,
+                    Int32* parent );
+
+extern 
+void 
+hbAssignCodes ( UInt32 *code,
+                UChar *length,
+                unsigned minLen,
+                unsigned maxLen,
+                Int32 alphaSize );
+
+extern 
+void 
+hbCreateDecodeTables ( Int32 *limit,
+                       Int32 *base,
+                       Int32 *perm,
+                       UChar *length,
+                       Int32 minLen,
+                       Int32 maxLen,
+                       Int32 alphaSize );
+
+
+/*---------------------------------------------*/
+// stuff for randomising blocks
+
+// numbers themselves are in lib_bzip_e.c
+#define rNums bzip2_rNums /* don't pollute name space */
+extern Int16 const rNums[64];
+
+#define RAND_DECLS                                \
+   Int16 rNToGo = 0;                              \
+   Int16 rTPos  = 0;                              \
+   Int16 rMask  = 1;
+
+#define RAND_MASK ((rNToGo == 1) ? rMask : 0)
+
+#define RAND_UPD_MASK                             \
+   if (rNToGo == 0) {                             \
+      rNToGo = rNums[rTPos] / 10;                 \
+      rTPos++; if (rTPos == 64) rTPos = 0;        \
+      rMask++; if (rMask == 256) rMask = 1;       \
+   }                                              \
+   rNToGo--;
+
+#if 0
+extern void *bzip2_work_area;
+#endif
+
+#endif
+
+/*-------------------------------------------------------------*/
+/*-- end                                  lib_bzip_private.h --*/
+/*-------------------------------------------------------------*/
diff -pruN linux-2.6.18.5.org/fs/ext2/compress.c linux-2.6.18.5/fs/ext2/compress.c
--- linux-2.6.18.5.org/fs/ext2/compress.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/compress.c	2007-04-01 21:03:26.000000000 -0700
@@ -0,0 +1,3581 @@
+/*
+ *  linux/fs/ext2/compress.c
+ *
+ *  Copyright (C) 1995  Antoine Dumesnil de Maricourt (dumesnil@etca.fr) 
+ *      (transparent compression code)
+ */
+
+/*
+ *  Copyright (C) 2001 Alcatel Business Systems - R&D Illkirch FRANCE
+ *
+ *  	Transparent compression code for 2.4 kernel.
+ *
+ *  Denis Richard (denis.richard@sxb.bsf.alcatel.fr)
+ *  Pierre Peiffer (pierre.peiffer@sxb.bsf.alcatel.fr)
+ *
+ *  Adapted from patch e2compr-0.4.39-patch-2.2.18 .
+ */
+
+#include <asm/segment.h>
+#include <asm/system.h>
+
+#include <linux/errno.h>
+#include <linux/fs.h>
+#include "debug.h"
+#include <linux/ext2_fs.h>
+#include <linux/ext2_fs_c.h>
+#include <linux/fcntl.h>
+#include <linux/sched.h>
+#include <linux/stat.h>
+#include <linux/buffer_head.h>
+#include <linux/string.h>
+#include <linux/config.h>
+#include <linux/kernel.h>
+#include <linux/quotaops.h>
+#include <linux/kmod.h>
+#include <linux/vmalloc.h>
+#include <linux/swap.h>
+#include <linux/slab.h>
+#include <linux/pagemap.h>
+
+#define MIN(a,b) ((a) < (b) ? (a) : (b))
+
+/* Working area to compress or decompress data.  This is a single
+   shared resource.  At present it is even shared between processors.
+   See fs/ext2/super.c for de/allocation.
+ 
+   I wouldn't mind having one working area dedicated to reads
+   (i.e. decompression).  The problem that would solve is that things
+   can be very jerky if most of your hard disk is compressed because
+   whenever an executable or something pages in from disk, it has to
+   wait for any compression to finish; compression can be slow.
+
+   Alternative solutions would be to get caching-to-swap working, or
+   just making sure that the compressing process(es) set themselves to
+   low priority when waiting on the working area and normal (or high)
+   priority once they've got it.
+
+   Another idea would be for compression to be preemptible by
+   decompression (unless compressing to a device very short of space).
+   This idea is at its best if compression happens during the idle
+   thread, where nothing has to wait for it.
+ 
+ */
+
+struct ext2_wa_S *ext2_rd_wa = NULL;
+#ifdef CONFIG_EXT2_SEPARATE_WORK_AREAS
+struct ext2_wa_S *ext2_wr_wa = NULL;
+#endif
+
+/* !!!   This is the size of _heap_ only   !!! */
+size_t ext2_rd_wa_size = 0;
+#ifdef CONFIG_EXT2_SEPARATE_WORK_AREAS
+size_t ext2_wr_wa_size = 0;
+#endif
+
+/* Use of this isn't fully implemented at time of writing (we update
+   it but never consult it), but the idea is to get around a problem
+   with readpage where it can decompress a cluster several times if
+   there are several pages to a cluster (which is the usual case).
+   The cost of the optimisation is: (1) this struct in memory and the
+   minor CPU cost of updating it, and (2) there's extra latency in
+   accessing the first page, because we decompress the whole cluster
+   rather than just enough to fill the page (as we previously did). */
+
+/* What's the best way of identifying a cluster?  Here we use the
+   three-tiered <inode->i_rdev, inode->i_ino, clusternum>. */
+struct ext2_wa_contents_S ext2_rd_wa_ucontents;
+#ifdef CONFIG_EXT2_SEPARATE_WORK_AREAS
+struct ext2_wa_contents_S ext2_wr_wa_ucontents;
+#endif
+
+/* Lock to prevent concurrent use of the working area. */
+static int ext2_wa_lock[EXT2_N_WA] = {0};
+# ifdef CONFIG_EXT2_SEPARATE_WORK_AREAS
+
+#  if EXT2_N_WA != 2
+#   error EXT2_N_WA constant has wrong value
+#  endif
+static wait_queue_head_t ext2_wa_wait[EXT2_N_WA] = {__WAIT_QUEUE_HEAD_INITIALIZER(ext2_wa_wait[0]), __WAIT_QUEUE_HEAD_INITIALIZER(ext2_wa_wait[1])};
+
+# else
+
+#  if EXT2_N_WA != 1
+#   error EXT2_N_WA constant has wrong value
+#  endif
+static wait_queue_head_t ext2_wa_wait[EXT2_N_WA] = {__WAIT_QUEUE_HEAD_INITIALIZER(ext2_wa_wait[0])};
+
+# endif
+
+
+/*
+ *    Algorithm and method tables
+ */
+
+struct ext2_algorithm ext2_algorithm_table[] =
+{
+  /* Note: all algorithms must have the `name' field filled in.
+     This is used to autoload algorithm modules (ext2-compr-%s), and
+     in kernel printk. */
+  /* N.B. Do not renumber these algorithms!  (To do so is to change
+     the binary format.)  It's OK for `none' and `undef' to be
+     renumbered, though. */
+
+  /* Fields:
+         name; available; routines for:
+	                 init,  compress,   decompress. */
+#ifdef  CONFIG_EXT2_HAVE_LZV1
+	{"lzv1", 1, ext2_iLZV1, ext2_wLZV1, ext2_rLZV1},
+#else
+	{"lzv1", 0, ext2_iNONE, ext2_wNONE, ext2_rNONE},
+#endif
+
+#ifdef  CONFIG_EXT2_HAVE_LZRW3A
+	{"lzrw3a", 1, ext2_iLZRW3A, ext2_wLZRW3A, ext2_rLZRW3A},
+#else
+	{"lzrw3a", 0, ext2_iNONE, ext2_wNONE, ext2_rNONE},
+#endif
+
+#ifdef  CONFIG_EXT2_HAVE_GZIP
+	{"gzip", 1, ext2_iGZIP, ext2_wGZIP, ext2_rGZIP},
+#else
+	{"gzip", 0, ext2_iNONE, ext2_wNONE, ext2_rNONE},
+#endif
+
+
+#ifdef  CONFIG_EXT2_HAVE_BZIP2
+	{"bzip2", 1, ext2_iBZIP2, ext2_wBZIP2, ext2_rBZIP2},
+#else
+	{"bzip2", 0, ext2_iNONE, ext2_wNONE, ext2_rNONE},
+#endif
+
+
+#ifdef  CONFIG_EXT2_HAVE_LZO
+	{"lzo", 1, ext2_iLZO, ext2_wLZO, ext2_rLZO},
+#else
+	{"lzo", 0, ext2_iNONE, ext2_wNONE, ext2_rNONE},
+#endif
+
+
+	{"none", 1, ext2_iNONE, ext2_wNONE, ext2_rNONE},
+
+	/* This "algorithm" is for unused entries in the method table.
+	   It differs from EXT2_NONE_ALG in that it is considered
+	   unavailable, whereas `none' is always available. */
+	{"undef", 0, ext2_iNONE, ext2_wNONE, ext2_rNONE}
+};
+/* Note: EXT2_N_ALGORITHMS can't be increased beyond 16 without
+   changing the width of the s_algorithms_used field in the in-memory
+   superblock.  The on-disk s_algorithms_used field is 32 bits long.
+   (This is in a state of flux.  Currently (1998-02-05) there is no
+   distinction: we always use the s_es copy. */
+
+
+/* The size of this table must be 32 to prevent Oopsen from 
+   invalid data.  We index this from 5 bits of i_flags, so
+   the size is (1 << 5) == 32. */
+struct ext2_method ext2_method_table[32] =
+{
+	/* Fields: algorithm id, algorithm argument. */
+	{EXT2_LZV1_ALG, 0},
+	{EXT2_NONE_ALG, 0},   /* 1: auto */
+	{EXT2_NONE_ALG, 0},   /* 2: defer */
+	{EXT2_NONE_ALG, 0},   /* 3: never */
+	{EXT2_BZIP2_ALG, 0},  /* 4: bzip2 */
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_LZRW3A_ALG, 0}, /* 8: lzrw3a */
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_LZO_ALG, 0},    /* 10: lzo1x_1 */
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_GZIP_ALG, 1},   /* 16 */
+	{EXT2_GZIP_ALG, 2},
+	{EXT2_GZIP_ALG, 3},
+	{EXT2_GZIP_ALG, 4},
+	{EXT2_GZIP_ALG, 5},
+	{EXT2_GZIP_ALG, 6},
+	{EXT2_GZIP_ALG, 7},
+	{EXT2_GZIP_ALG, 8},
+	{EXT2_GZIP_ALG, 9},
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_UNDEF_ALG, 0},
+	{EXT2_UNDEF_ALG, 0}
+};
+
+/* Returns 0 on success, -ENOMEM on failure i.e. if working area
+ * is smaller than what module needs and we failed to allocate bigger one
+ */
+int ext2_register_compression_module(unsigned alg, size_t compress_memory,
+		size_t decompress_memory,
+		struct ext2_algorithm *new_algorithm)
+{
+	if (alg >= EXT2_N_ALGORITHMS) {
+		printk(KERN_ERR
+		       "EXT2-fs: trying to load invalid algorithm %u\n",
+		       alg);
+		return -EINVAL;
+	}
+	if (EXT2_ALGORITHMS_BUILTIN & (1 << alg)) {
+		printk(KERN_ERR
+		       "EXT2-fs: trying to load built-in algorithm %u!\n",
+		       alg);
+		return -EINVAL;
+	}
+
+	printk(KERN_INFO "EXT2-fs: loading compression algorithm %u\n", alg);
+
+#ifdef CONFIG_EXT2_SEPARATE_WORK_AREAS
+	if (ext2_wr_wa_size < compress_memory) {
+		struct ext2_wa_S *new_ext2_wa;
+		size_t size;
+
+		printk(KERN_DEBUG
+			"EXT2-fs: allocating more memory for algorithm %u\n", alg);
+
+		if (ext2_wr_wa != NULL && ext2_lock_wr_wa() == 0)
+			return -EINTR; /* Maybe EBUSY would be better? */
+		size = compress_memory + 2 * EXT2_MAX_CLUSTER_BYTES;
+		new_ext2_wa = vmalloc(size);
+		if (new_ext2_wa == NULL) {
+			printk (KERN_WARNING
+				"EXT2-fs: can't allocate working area "
+				"for algorithm %u.\n", alg);
+			ext2_unlock_wr_wa();
+			return -ENOMEM;
+		}
+# ifdef EXT2_COMPR_REPORT
+		printk (KERN_INFO "EXT2-fs: wr_wa=%p--%p (%u)\n",
+			new_ext2_wa, (char *)new_ext2_wa + size, size);
+# endif
+		vfree(ext2_wr_wa);
+		ext2_wr_wa = new_ext2_wa;
+		ext2_wr_wa_size = compress_memory;
+		ext2_unlock_wr_wa();
+	}
+	if (ext2_rd_wa_size < decompress_memory) {
+		struct ext2_wa_S *new_ext2_wa;
+		size_t size;
+
+		printk(KERN_INFO "EXT2-fs: allocating more memory for algorithm %u\n", alg);
+
+		if (ext2_rd_wa != NULL && ext2_lock_rd_wa() == 0)
+			return -EINTR; /* Maybe EBUSY would be better? */
+		size = decompress_memory + 2 * EXT2_MAX_CLUSTER_BYTES;
+		new_ext2_wa = vmalloc(size);
+		if (new_ext2_wa == NULL) {
+			printk (KERN_WARNING
+				"EXT2-fs: can't allocate working area "
+				"for algorithm %u.\n", alg);
+			ext2_unlock_rd_wa();
+			return -ENOMEM;
+		}
+# ifdef EXT2_COMPR_REPORT
+		printk (KERN_INFO "EXT2-fs: rd_wa=%p--%p (%u)\n",
+			new_ext2_wa, (char *)new_ext2_wa + size, size);
+# endif
+		vfree(ext2_rd_wa);
+		ext2_rd_wa = new_ext2_wa;
+		ext2_rd_wa_size = decompress_memory;
+		ext2_unlock_rd_wa();
+	}
+#else
+	if (compress_memory < decompress_memory)
+		compress_memory = decompress_memory;
+	if (ext2_rd_wa_size < compress_memory) {
+		struct ext2_wa_S *new_ext2_wa;
+		size_t size;
+
+		printk(KERN_INFO "EXT2-fs: allocating more memory for algorithm %u\n", alg);
+
+		if (ext2_rd_wa != NULL && ext2_lock_rd_wa() == 0)
+			return -EINTR; /* Maybe EBUSY would be better? */
+		size = compress_memory + 2 * EXT2_MAX_CLUSTER_BYTES;
+		new_ext2_wa = vmalloc(size);
+		if (new_ext2_wa == NULL) {
+			printk (KERN_WARNING
+				"EXT2-fs: can't allocate working area "
+				"for algorithm %u.\n", alg);
+			ext2_unlock_rd_wa();
+			return -ENOMEM;
+		}
+# ifdef EXT2_COMPR_REPORT
+		printk (KERN_INFO "EXT2-fs: wa=%p--%p (%u)\n",
+			new_ext2_wa, (char *)new_ext2_wa + size, size);
+# endif
+		vfree(ext2_rd_wa);
+		ext2_rd_wa = new_ext2_wa;
+		ext2_rd_wa_size = compress_memory;
+		ext2_unlock_rd_wa();
+	}
+#endif
+	ext2_algorithm_table[alg].avail = 1;
+	ext2_algorithm_table[alg].init = new_algorithm->init;
+	ext2_algorithm_table[alg].compress = new_algorithm->compress;
+	ext2_algorithm_table[alg].decompress = new_algorithm->decompress;
+	printk(KERN_INFO "EXT2-fs: compression algorithm %u loaded\n", alg);
+
+	return 0;
+}
+
+void ext2_unregister_compression_module(unsigned alg)
+{
+	if (alg >= EXT2_N_ALGORITHMS)
+		printk(KERN_ERR
+		       "EXT2-fs: trying to unload invalid algorithm %u\n",
+		       alg);
+	else if (EXT2_ALGORITHMS_BUILTIN & (1 << alg))
+		printk(KERN_ERR
+		       "EXT2-fs: trying to unload non-module algorithm %u\n",
+		       alg);
+	else {
+		ext2_algorithm_table[alg].avail = 0;
+		ext2_algorithm_table[alg].init = ext2_iNONE;
+		ext2_algorithm_table[alg].compress = ext2_wNONE;
+		ext2_algorithm_table[alg].decompress = ext2_rNONE;
+		/* TODO: I guess we ought to shrink the compression heap here if possible. */
+		printk(KERN_INFO
+		       "EXT2-fs: compression algorithm %u unloaded\n", alg);
+	}
+}
+
+/* Returns 1 on success, 0 on "failure" (see code).
+
+   It is the caller's responsibility to check that the working area is
+   allocated. */
+int ext2_lock_wa(unsigned id)
+{
+	assert (id < EXT2_N_WA);
+	switch (id) {
+		case 0:
+			if (ext2_rd_wa == NULL) {
+				printk(KERN_WARNING
+					"ext2_lock_rd_wa: working area not allocated.\n");
+				return 0;
+			}
+			break;
+#ifdef CONFIG_EXT2_SEPARATE_WORK_AREAS
+		case 1:
+			if (ext2_wr_wa == NULL) {
+				printk(KERN_WARNING
+					"ext2_lock_wr_wa: working area not allocated.\n");
+				return 0;
+			}
+			break;
+#endif
+		default:
+			printk(KERN_WARNING
+				"ext2_lock_rd_wa: wrong working area id.\n");
+			return 0;
+	}
+
+	while (ext2_wa_lock[id]) {
+#if 1
+		/* pjm 1998-02-19: I'm putting this back on trial (see
+		   comment in #else).  What's different is that
+		   callers check for ext2_wa==NULL themselves, and
+		   return EINTR instead of EBUSY when ext2_lock_wa
+		   returns zero. */
+		interruptible_sleep_on(&ext2_wa_wait[id]);
+#else
+		/* Better not to be interruptible: this will avoid
+		   some 'File busy' when processes get the sleep
+		   signal and are woken up later.  We could however
+		   avoid that if the access to the working area was
+		   made atomic (it is no longer because of IND_BITMAP
+		   being defined, and now (as of 0.3.5) because of
+		   schedule()).
+
+		   pjm 1998-01-13: I think `EBUSY' is meant rather
+		   than `File busy'. */
+		sleep_on(&ext2_wa_wait[id]);
+#endif
+		if (signal_pending(current))
+			return 0;
+	}
+	ext2_wa_lock[id] = 1;
+#ifdef EXT2_COMPR_REPORT_WA
+	printk(KERN_DEBUG "ext2: process %u locks wa %u\n", current->pid, id);
+#endif
+	return 1;
+}
+
+
+/* ,Version of ext2_lock_wa` for ext2_readpage, because I don't know
+   what to do with signals arriving during ext2_readpage(). */
+void ext2_lock_rd_wa_uninterruptible(void)
+{
+	assert (ext2_rd_wa != NULL);
+	while (ext2_wa_lock[ext2_rd_wa_ix])
+		sleep_on(&ext2_wa_wait[ext2_rd_wa_ix]);
+	ext2_wa_lock[ext2_rd_wa_ix] = 1;
+#ifdef EXT2_COMPR_REPORT_WA
+	printk(KERN_DEBUG
+		"ext2: process %u locks wa %u\n", current->pid, ext2_rd_wa_ix);
+#endif
+}
+
+
+void ext2_unlock_wa(unsigned id)
+{
+	assert(id < EXT2_N_WA);
+	ext2_wa_lock[id] = 0;
+
+#ifdef EXT2_COMPR_REPORT_WA
+	printk(KERN_DEBUG
+		"ext2: process %u unlocks wa %u\n", current->pid, id);
+#endif
+
+	if (waitqueue_active(&ext2_wa_wait[id])) {
+		wake_up(&ext2_wa_wait[id]);
+		schedule();
+		/* pjm 1997-08-01: responsiveness can suffer if we
+		   call schedule() only when the working area is
+		   locked. */
+ 	}
+}
+
+
+static void ext2_mark_algorithm_use(struct inode *inode,
+				    unsigned alg)
+{
+	struct ext2_sb_info *sbi = EXT2_SB(inode->i_sb);
+
+	/* Hopefully, lock_super() isn't needed here, as we don't
+           block in the critical region.  True? */
+	assert (alg < EXT2_N_ALGORITHMS);
+	if (sbi->s_es->s_feature_incompat
+	    & cpu_to_le32(EXT2_FEATURE_INCOMPAT_COMPRESSION)) {		
+		sbi->s_es->s_algorithm_usage_bitmap
+			|= cpu_to_le32(1 << alg);
+	} else {
+		struct ext2_super_block *es = sbi->s_es;
+
+		es->s_algorithm_usage_bitmap
+			= cpu_to_le32(1 << alg);
+		es->s_feature_incompat
+			|= cpu_to_le32(EXT2_FEATURE_INCOMPAT_COMPRESSION);
+		if (es->s_rev_level < EXT2_DYNAMIC_REV) {
+			/* Raise the filesystem revision level to
+                           EXT2_DYNAMIC_REV so that s_feature_incompat
+                           is honoured (except in ancient kernels /
+                           e2fsprogs).  We must also initialize two
+                           other dynamic-rev fields.  The remaining
+                           fields are assumed to be already correct
+                           (e.g. still zeroed). */
+			es->s_rev_level = cpu_to_le32(EXT2_DYNAMIC_REV);
+			es->s_first_ino = cpu_to_le32(EXT2_GOOD_OLD_FIRST_INO);
+			es->s_inode_size = cpu_to_le16(EXT2_GOOD_OLD_INODE_SIZE);
+		}
+	}
+	mark_buffer_dirty(sbi->s_sbh);
+}
+
+
+/* Displays an error message if algorithm ,alg` is not marked in use,
+   and then marks it in use. */
+static void ext2_ensure_algorithm_use(struct inode *inode, unsigned alg)
+{
+	assert (alg < EXT2_N_ALGORITHMS);
+
+	if (!(EXT2_SB(inode->i_sb)->s_es->s_algorithm_usage_bitmap
+	      & cpu_to_le32(1 << alg))) {
+		ext2_warning(inode->i_sb,
+			     "algorithm usage bitmap",
+			     "algorithm %s not marked used in inode %lu",
+			     ext2_algorithm_table[alg].name,
+			     inode->i_ino);
+		ext2_mark_algorithm_use(inode, alg);
+	}
+}
+
+int ext2_get_cluster_pages(struct inode *inode, u32 cluster,
+			    struct page *pg[], struct page *page, int compr)
+{
+	int nbpg, npg;
+	u32 page0;  /* = position within file (not position within fs). */
+	u32 idx = 0;
+	struct page *cached_page;
+	struct pagevec lru_pvec;
+
+	cached_page = NULL;
+	pagevec_init(&lru_pvec, 0);
+
+	page0 = ext2_cluster_page0(inode, cluster);
+	nbpg = ext2_cluster_npages(inode, cluster);
+  
+	if (compr && (((page0+nbpg)<<PAGE_CACHE_SHIFT) > inode->i_size))
+		nbpg = ((inode->i_size-1) >> PAGE_CACHE_SHIFT) - page0 + 1;
+  
+	trace_e2c("ext2_get_cluster_pages: page0=%d, nbpg=%d page=%ld\n",
+	    page0, nbpg, ((page != NULL) ? page->index : 0));
+	for (npg = 0; npg < nbpg; npg++) {
+		if ((page == NULL) || ((page0 + npg) != page->index)) {
+			pg[npg] = __grab_cache_page(inode->i_mapping,
+			    page0+npg, &cached_page, &lru_pvec);
+			if (!pg[npg])
+				goto error;
+		} else {
+			pg[npg] = page;
+		}
+		if (!page_has_buffers(pg[npg]))
+			create_empty_buffers(pg[npg], inode->i_sb->s_blocksize,
+			0);
+	}
+	for (idx = npg; idx < EXT2_MAX_CLUSTER_PAGES; idx++)
+		pg[idx]=NULL;
+
+	if (cached_page) 
+		page_cache_release(cached_page);
+	pagevec_lru_add(&lru_pvec);
+	pagevec_free(&lru_pvec);
+	return(npg);
+error:
+	if (cached_page) 
+		page_cache_release(cached_page);
+	pagevec_lru_add(&lru_pvec);
+	pagevec_free(&lru_pvec);
+	while (--npg >= 0) {
+		if ((page == NULL) || ((page0 + npg) !=  page->index)) {
+			unlock_page(pg[npg]);
+		page_cache_release(pg[npg]);
+		}
+		pg[npg]=NULL;
+	}
+	trace_e2c("ext2_get_cluster_pages: error no page\n");
+	return(-ENOMEM);
+}
+
+
+int ext2_get_cluster_extra_pages(struct inode *inode, u32 cluster,
+				 struct page *pg[], struct page *epg[])
+{
+  struct page *page;
+  int nbpg, npg;
+  unsigned long flags;
+
+  nbpg = ext2_cluster_npages(inode, cluster);  
+  for (npg = 0; npg < nbpg; npg++)
+    {
+      if (pg[npg] == NULL)
+	break;
+      if (PageUptodate(pg[npg])) {
+	page = page_cache_alloc(inode->i_mapping);
+	if (!page)
+	  {
+	    goto error;
+	  }
+	flags = page->flags & ~((1 << PG_uptodate) | (1 << PG_error) | (1 << PG_dirty) | (1 << PG_referenced) | (1 << PG_arch_1));
+	page->flags = flags | (1 << PG_locked);
+	page->index = pg[npg]->index;
+/* 	page_cache_get(page); */
+	if (!page_has_buffers(page))
+	  create_empty_buffers(page, inode->i_sb->s_blocksize, 0);
+	epg[npg] = page;
+	trace_e2c("ext2_get_cluster_extra_pages: allocated page idx=%ld\n", pg[npg]->index);
+      } else {
+	epg[npg] = NULL;
+      }
+    }
+  return(npg);
+ error:
+  while (--npg >= 0)
+    if (epg[npg]) {
+      try_to_free_buffers(epg[npg]);
+      unlock_page(epg[npg]);
+      assert(page_count(epg[npg]) == 1);
+      page_cache_release(epg[npg]);
+    }
+  trace_e2c("ext2_get_cluster_extra_pages: error no page\n");
+  return(-ENOMEM);
+  
+}
+
+
+/* Read every block in the cluster.  The blocks are stored in the bh
+   array, which must be big enough.
+
+   Return the number of block contained in the cluster, or -errno if an
+   error occured.  The buffers should be released by the caller
+   (unless an error occurred).
+ 
+   The inode must be locked, otherwise it is possible that we return
+   some out of date blocks.
+ 
+   Called by :
+ 
+         ext2_decompress_cluster()      [i_sem]
+         ext2_compress_cluster()        [i_sem]
+         ext2_readpage()      		[i_sem] */
+
+int ext2_get_cluster_blocks(struct inode *inode, u32 cluster,
+			    struct buffer_head *bh[], struct page *pg[], struct page *epg[],
+			    int compr)
+{
+	struct buffer_head *br[EXT2_MAX_CLUSTER_BLOCKS];
+	int nreq, nbh=0, npg, i;
+	u32 clu_nblocks;
+	int err;
+	const int blocks = PAGE_CACHE_SIZE >> inode->i_sb->s_blocksize_bits;
+
+	assert (atomic_read(&inode->i_mutex.count) <= 0); /* i.e. mutex_lock */
+
+	/*
+	 *  Request full cluster.
+	 */
+	{
+		u32 endblk;
+		u32 block;  /* = position within file (not position within fs). */
+		u32 nbpg;
+		u32 page0;  /* = position within file (not position within fs). */
+		u32 idx;
+
+		block = ext2_cluster_block0(inode, cluster);
+		clu_nblocks = ext2_cluster_nblocks(inode, cluster);
+		/* impl: Don't shorten endblk for i_size.  The
+		   remaining blocks should be NULL anyway, except in
+		   the case when called from ext2_decompress_cluster
+		   from ext2_truncate, in which case i_size is short
+		   and we _want_ to get all of the blocks. */
+		endblk = block + clu_nblocks;
+
+		page0 = ext2_cluster_page0(inode, cluster);
+		nbpg = ext2_cluster_npages(inode, cluster);
+		
+		if (compr && (((page0+nbpg) << PAGE_CACHE_SHIFT) > inode->i_size)) {
+		  nbpg = ((inode->i_size-1) >> PAGE_CACHE_SHIFT) - page0 + 1;
+		  endblk = block + (nbpg << (PAGE_CACHE_SHIFT - inode->i_sb->s_blocksize_bits));
+		}
+
+		idx = page0 << (PAGE_CACHE_SHIFT - inode->i_sb->s_blocksize_bits);
+		trace_e2c("ext2_get_cluster_blocks: page0=%d, nbpg=%d\n", page0, nbpg);
+		for (npg = 0; npg < nbpg; npg++)
+		  {
+		    struct buffer_head *buffer;
+
+		    if ((epg != NULL) && (epg[npg] != NULL))
+		      buffer = page_buffers(epg[npg]);
+		    else 
+		      buffer = page_buffers(pg[npg]);
+		    for (i=0; i<blocks && (block+nbh)<endblk ; buffer = buffer->b_this_page, i++)
+		      {
+			if (idx == (block+nbh))
+			  {
+			    bh[nbh]=buffer;
+			    nbh++;
+			  }
+			idx++;
+		      }
+		  }
+		trace_e2c("ext2_get_cluster_blocks: get every pages and %d buffers\n", nbh);
+
+		for (nbh = 0, nreq = 0; block < endblk; nbh++) {
+		        assert(bh[nbh] != NULL);
+			bh[nbh]->b_blocknr=0;
+			clear_bit(BH_Mapped, &bh[nbh]->b_state);
+  		        err=ext2_get_block(inode, block++, bh[nbh], (PageDirty(bh[nbh]->b_page) ? 1 : 0));
+
+			if (bh[nbh]->b_blocknr != 0) {
+				if (!buffer_uptodate(bh[nbh])
+				    /* TODO: Do we need this
+                                       `!buffer_locked' test? */
+				    && !buffer_locked(bh[nbh])
+				    && !PageDirty(bh[nbh]->b_page))
+					br[nreq++] = bh[nbh];
+			} else if ((err != 0)
+				   && (err != -EFBIG))
+				/* impl: for some unknown reason,
+				   ext2_getblk() returns -EFBIG if
+				   !create and there's a hole. ==> not roght any more in 2.4*/
+				goto error;
+		}
+		for (i = nbh; i < EXT2_MAX_CLUSTER_BLOCKS; i++) {
+		  bh[i] = NULL;
+		}
+	}
+
+	trace_e2c("ext2_get_cluster_blocks: nreq=%d for cluster=%d\n", nreq, cluster);
+
+	if (nreq > 0)
+		ll_rw_block(READ, nreq, br);
+
+	/*
+	 *  Adjust nbh if we have some null blocks at end of cluster.
+	 */
+	while ((nbh != 0) && (bh[nbh - 1]->b_blocknr == 0))
+		nbh--;
+
+	/*
+	 *  Wait for blocks.
+	 */
+	err = -EIO;
+	for (i = 0; i < nbh; i++)
+		  if ((!PageDirty(bh[i]->b_page)) && (bh[i]->b_blocknr != 0)) {
+			wait_on_buffer(bh[i]);
+			if (!buffer_uptodate(bh[i])) {	/* Read error ??? */
+			  trace_e2c("ext2_get_cluster_blocks: wait_on_buffer error (blocknr=%ld)\n", bh[i]->b_blocknr);
+			  goto error;
+			}
+		}
+	assert (nbh <= EXT2_MAX_CLU_NBLOCKS);
+	return nbh;
+
+ error:
+	return err;
+}
+
+
+/* Iterations over block in the inode are done with a generic
+   iteration key mechanism.  We need one method to convert a block
+   number into a new key, one method to iterate (i.e., increment the
+   key) and one method to free the key.  The code could be shared with
+   truncate.c, as this mechanism is very general.
+ 
+   This code assumes tht nobody else can read or write the file
+   between ext2_get_key() and ext2_free_key(), so callers need to have
+   i_sem (which they all do anyway). */
+
+/* TODO: Get all of the bkey routines to return -errno instead of
+   true/false. */
+/* TODO: The bkey routines currently assume tht address blocks are
+   allocated even if all contained addresses are NULL, but this is not
+   true.  Make sure tht we differentiate between NULL block and error,
+   and then fix up ext2_set_key_blkaddr() and anything else (including
+   the pack/unpack routines). */
+struct ext2_bkey {
+	int level;
+	u32 block;
+	struct inode *inode;
+	int off[4];
+	u32 *ptr[4];
+	struct buffer_head *ibh[4];
+};
+
+
+/*
+ *    Method to convert a block number into a key.
+ *
+ *    Returns 1 on success, 0 on failure.  You may safely, but need
+ *    not, free the key even if ext2_get_key() fails. 
+ */
+static int ext2_get_key(struct ext2_bkey *key, struct inode *inode, u32 block)
+{
+	int x, level;
+	int addr_per_block = EXT2_ADDR_PER_BLOCK(inode->i_sb);
+
+	/*
+	 *  The first step can be viewed as translating the
+	 *    original block number in a special base (powers
+	 *    of addr_per_block).
+	 */
+
+	key->block = block;
+
+	key->off[0] = key->off[1] = key->off[2] = key->off[3] = 0;
+	key->ibh[0] = key->ibh[1] = key->ibh[2] = key->ibh[3] = NULL;
+	key->ptr[0] = key->ptr[1] = key->ptr[2] = key->ptr[3] = NULL;
+
+	if (block >= EXT2_NDIR_BLOCKS) {
+		block -= EXT2_NDIR_BLOCKS;
+
+		if (block >= addr_per_block) {
+			block -= addr_per_block;
+
+			if (block >= addr_per_block * addr_per_block) {
+				block -= addr_per_block * addr_per_block;
+
+				key->off[0] = EXT2_TIND_BLOCK;
+				key->off[1] = (block / (addr_per_block * addr_per_block));
+				key->off[2] = (block % (addr_per_block * addr_per_block)) / addr_per_block;
+				key->off[3] = (block % addr_per_block);
+				level = 3;
+			} else {
+				key->off[0] = EXT2_DIND_BLOCK;
+				key->off[1] = block / addr_per_block;
+				key->off[2] = block % addr_per_block;
+				level = 2;
+			}
+		} else {
+			key->off[0] = EXT2_IND_BLOCK;
+			key->off[1] = block;
+			level = 1;
+		}
+	} else {
+		key->off[0] = block;
+		level = 0;
+	}
+
+	/*
+	 *  In the second step, we load the needed buffers.
+	 */
+
+	key->level = level;
+	key->inode = inode;
+
+	key->ptr[0] = (u32 *) (&(EXT2_I(inode)->i_data));
+
+	for (x = 1; x <= level; x++) {
+		u32 *ptr;
+
+		ptr = key->ptr[x - 1];
+		if (ptr == NULL)
+			break;
+/* Paul Whittaker tweak 19 Feb 2005 */
+		block = le32_to_cpu(ptr[key->off[x - 1]]);
+		if (x - 1 != 0)
+			block = le32_to_cpu(block);
+		if ((key->ibh[x] = __bread(inode->i_sb->s_bdev,
+					 block,
+					 inode->i_sb->s_blocksize))
+		    == NULL)
+			goto error;
+		key->ptr[x] = (u32 *) (key->ibh[x]->b_data);
+	}
+
+	return 1;
+ error:
+	for (; x != 0; x--)
+		if (key->ibh[x] != NULL)
+			brelse(key->ibh[x]);
+	return 0;
+}
+
+
+/*
+ *    Find the block for a given key.  Return 0 if there
+ *      is no block for this key.
+ */
+static inline u32 ext2_get_key_blkaddr(struct ext2_bkey *key)
+{
+/* Paul Whittaker tweak 19 Feb 2005 */
+	if (key->ptr[key->level] == NULL)
+		return 0;
+	return le32_to_cpu(key->ptr[key->level][key->off[key->level]]);
+}
+
+
+/*
+ *    Change the block for a given key.  Return 0 on success,
+ *      -errno on failure.
+ */
+static inline int ext2_set_key_blkaddr(struct ext2_bkey *key, u32 blkaddr)
+{
+	char bdn[BDEVNAME_SIZE];
+
+	if (key->ptr[key->level] == NULL) {
+		/* The reason that this "can't happen" is that this
+		   routine is only used to shuffle block numbers or by
+		   free_cluster_blocks.  Cluster sizes are such that
+		   clusters can't straddle address blocks.  So the
+		   indirect block address can't be zero.  AFAIK, ptr
+		   can only be NULL on error or on null indirect block
+		   address.  Hmm, come to think of it, I think there
+		   are still some callers that don't check for errors
+		   from ext2_get_key(), so this still can happen until
+		   those are fixed up. */
+		printk(KERN_ERR
+		       "ext2_set_key_blkaddr: can't happen: NULL parent.  "
+		       "dev=%s, ino=%lu, level=%u.\n",
+		       bdevname(key->inode->i_sb->s_bdev, bdn),
+		       key->inode->i_ino,
+		       key->level);
+		return -ENOSYS;
+	}
+/* Paul Whittaker tweak 19 Feb 2005 */
+	key->ptr[key->level][key->off[key->level]] = le32_to_cpu(blkaddr);
+	if (key->level > 0)
+		mark_buffer_dirty(key->ibh[key->level]);
+	return 0;
+}
+
+
+/*
+ *    Increment the key.  Returns 0 if we go beyond the limits,
+ *      1 otherwise.
+ *
+ *    Precondition: -key->off[level] <= incr < addr_per_block.
+ */
+static int ext2_next_key(struct ext2_bkey *key, int incr)
+{
+	int addr_per_block = EXT2_ADDR_PER_BLOCK(key->inode->i_sb);
+	int x, level = key->level;
+	u32 tmp;
+
+	/*
+	 *  Increment the key. This is done in two step: first
+	 *    adjust the off array, then reload buffers that should
+	 *    be reloaded (we assume level > 0).
+	 */
+
+	assert (key->off[level] >= -incr);
+	assert (incr < addr_per_block);
+	key->block += incr;
+	key->off[level] += incr;
+
+	/*
+	 *  First step: should be thought as the propagation
+	 *    of a carry.
+	 */
+
+	if (level == 0) {
+		if (key->off[0] >= EXT2_NDIR_BLOCKS) {
+			key->off[1] = key->off[0] - EXT2_NDIR_BLOCKS;
+			key->off[0] = EXT2_IND_BLOCK;
+			level = 1;
+		}
+		x = 0;
+	} else {
+		for (x = level; x > 0; x--) {
+			if (key->off[x] >= addr_per_block) {
+				key->off[x] -= addr_per_block;
+				key->off[x - 1]++;
+
+				if (x == 1) {
+					if (++level < 4) {
+						key->off[level] = key->off[level - 1];
+						key->off[level - 1] = 0;
+					} else
+						return 0;
+				}
+			} else
+				break;
+		}
+	}
+
+	/*
+	 *  Second step: reload the buffers that have changed.
+	 */
+
+	key->level = level;
+
+	while (x++ < level) {
+		if (key->ibh[x] != NULL) {
+			if (IS_SYNC(key->inode) && buffer_dirty(key->ibh[x])) {
+				ll_rw_block(WRITE, 1, &(key->ibh[x]));
+				wait_on_buffer(key->ibh[x]);
+			}
+			brelse(key->ibh[x]);
+		}
+/* Paul Whittaker tweak 19 Feb 2005 */
+		if ((key->ptr[x - 1] != NULL)
+		    && ((tmp = le32_to_cpu(key->ptr[x - 1][key->off[x - 1]])) != 0)) {
+			if ((key->ibh[x] = __bread(key->inode->i_sb->s_bdev,
+						 tmp,
+						 key->inode->i_sb->s_blocksize))
+			    != NULL)
+				key->ptr[x] = (u32 *) (key->ibh[x]->b_data);
+			else
+				key->ptr[x] = NULL;
+		} else {
+			key->ibh[x] = NULL;
+			key->ptr[x] = NULL;
+		}
+	}
+
+	return 1;
+}
+
+
+/* Method to free the key: just release buffers.
+
+   Returns 0 on success, -errno on error.
+*/
+
+static int ext2_free_key(struct ext2_bkey *key)
+{
+	int x, n;
+	struct buffer_head *bh[4];
+
+	for (x = 0, n = 0; x <= key->level; x++) {
+		if (key->ibh[x] != NULL) {
+			if (IS_SYNC(key->inode) && buffer_dirty(key->ibh[x]))
+				bh[n++] = key->ibh[x];
+			else
+				brelse(key->ibh[x]);
+		}
+	}
+
+	if (n > 0) {
+		ll_rw_block(WRITE, n, bh);
+
+		while (n-- > 0) {
+			wait_on_buffer(bh[n]);
+			/* TODO: Check for error. */
+			brelse(bh[n]);
+		}
+	}
+	return 0;
+}
+
+
+/* Returns positive if specified cluster is compressed,
+   zero if not,
+   -errno if an error occurred.
+
+   If you need the result to be accurate, then down i_sem before
+   calling this, and don't raise i_sem until after you've used the
+   result. */
+int
+ext2_cluster_is_compressed_fn(struct inode *inode, unsigned cluster)
+{
+	unsigned block = (ext2_cluster_block0(inode, cluster)
+			  + ext2_cluster_nblocks(inode, cluster)
+			  - 1);
+	struct ext2_bkey key;
+	int result;
+
+	/* impl: Not all callers of ext2_cluster_is_compressed_fn() have
+           i_sem down.  Of course it is impossible to guarantee
+           up-to-date information for such callers (someone may
+           compress or decompress between when we check and when they
+           use the information), so hopefully it won't matter if the
+           information we return is slightly inaccurate (e.g. because
+           someone is de/compressing the cluster while we check). */
+	if (!ext2_get_key(&key, inode, block))
+		return -EIO;
+
+	result = (ext2_get_key_blkaddr(&key) == EXT2_COMPRESSED_BLKADDR);
+	ext2_free_key(&key);
+	return result;
+}
+
+
+/* Support for the GETCOMPRRATIO ioctl() call.  We calculate how many
+   blocks the file would hold if it weren't compressed.  This requires
+   reading the cluster head for every compressed cluster.
+
+   Returns either -EAGAIN or the number of blocks that the file would
+   take up if uncompressed.  */
+int ext2_count_blocks(struct inode *inode)
+{
+	struct buffer_head *head_bh;
+	int count;
+	int cluster;
+	struct ext2_bkey key;
+	u32 end_blknr;
+
+	if (!(EXT2_I(inode)->i_flags & EXT2_COMPRBLK_FL))
+		return inode->i_blocks;
+
+	mutex_lock(&inode->i_mutex);
+	end_blknr = ROUNDUP_RSHIFT(inode->i_size,
+				   inode->i_sb->s_blocksize_bits);
+
+	/* inode->i_blocks is stored in units of 512-byte blocks.  It's
+	   more convenient for us to work in units of s_blocksize. */
+	{
+		u32 shift = inode->i_sb->s_blocksize_bits - 9;
+
+		count = inode->i_blocks;
+		if (count & ((1 << shift) - 1))
+			ext2_warning(inode->i_sb,
+				     "ext2_count_blocks",
+				     "i_blocks not multiple of blocksize");
+		count >>= shift;
+	}
+
+	cluster = 0;
+	if (!ext2_get_key(&key, inode, 0)) {
+		count = -EIO;
+		goto out;
+	}
+	while (key.block < end_blknr) {
+		u32 head_blkaddr = ext2_get_key_blkaddr(&key);
+
+		/* bug fix: init head_bh for each iteration TLL 2/21/07 */
+		head_bh = NULL;
+		if (head_blkaddr == EXT2_COMPRESSED_BLKADDR) {
+			count = -EXT2_ECOMPR;
+			break;
+		}
+		if (!ext2_next_key(&key,
+				   ext2_cluster_nblocks(inode, cluster) - 1))
+			break;
+		if (ext2_get_key_blkaddr(&key) == EXT2_COMPRESSED_BLKADDR) {
+			struct ext2_cluster_head *head;
+
+			if (head_blkaddr == 0) {
+				count = -EXT2_ECOMPR;
+				break;
+			}
+			head_bh = __getblk(inode->i_sb->s_bdev,
+					 head_blkaddr,
+					 inode->i_sb->s_blocksize);
+			if (head_bh == NULL) {
+				/* Hmm, EAGAIN or EIO? */
+				count = -EAGAIN;
+				break;
+			}
+			if (!buffer_uptodate(head_bh))
+				ll_rw_block(READ, 1, &head_bh);
+
+			wait_on_buffer(head_bh);
+			head = (struct ext2_cluster_head *) head_bh->b_data;
+			if ((head->magic != cpu_to_le16(EXT2_COMPRESS_MAGIC_04X))
+			    || (le32_to_cpu(head->ulen) > EXT2_MAX_CLUSTER_BYTES)
+			    || (head->holemap_nbytes > 4)
+			    || (le32_to_cpu(head->clen) > le32_to_cpu(head->ulen))) {
+				count = -EXT2_ECOMPR;
+				break;
+			}
+			assert (sizeof(struct ext2_cluster_head) == 16);
+			count += (ROUNDUP_RSHIFT(le32_to_cpu(head->ulen),
+						 inode->i_sb->s_blocksize_bits)
+				  - ROUNDUP_RSHIFT((le32_to_cpu(head->clen)
+						    + sizeof (struct ext2_cluster_head)
+						    + head->holemap_nbytes),
+						   inode->i_sb->s_blocksize_bits));
+			brelse(head_bh);
+			head_bh = NULL;
+		}
+		if (!ext2_next_key(&key, 1))
+			break;
+		cluster++;
+	}
+	ext2_free_key(&key);
+	if (head_bh != NULL)
+		brelse(head_bh);
+ out:
+	mutex_unlock(&inode->i_mutex);
+	if (count == -EXT2_ECOMPR) {
+		ext2_warning(inode->i_sb,
+			     "ext2_count_blocks",
+			     "invalid compressed cluster %u of inode %lu",
+			     cluster, inode->i_ino);
+		EXT2_I(inode)->i_flags |= EXT2_ECOMPR_FL;
+	}
+
+	/* The count should be in units of 512 (i.e. 1 << 9) bytes. */
+	if (count >= 0)
+		count <<= inode->i_sb->s_blocksize_bits - 9;
+	return count;
+}
+
+
+/* Decompress some blocks previously obtained from a cluster.
+   Decompressed data is stored in ext2_rd_wa.u.  Buffer heads in the bh
+   array are packed together at the begining of the array.  The ulen
+   argument is an indication of how many bytes the caller wants to
+   obtain, excluding holes.  (This can be less than head->ulen, as in the
+   case of readpage.)  No hole processing is done; we don't even look at
+   head->holemap.
+
+   Note the semantic difference between this and
+   ext2_decompress_cluster(): the latter decompresses a cluster _and
+   stores it as such_, whereas ext2_decompress_blocks() just
+   decompresses the contents of the blocks into ext2_rd_wa.u.
+
+   The working area is supposed to be available and locked.
+ 
+   Returns a negative value on failure, the number of bytes
+   decompressed otherwise.
+ 
+   Called by :
+ 
+         ext2_decompress_cluster ()    [sem down]
+         ext2_readpage () [sem down, but only ifndef EXT2_LOCK_BUFFERS] */
+
+/* TODO: ext2_decompress_blocks() scribbles in ext2_rd_wa.c.
+   Check callers to make sure this isn't a problem. */
+size_t
+ext2_decompress_blocks(struct inode * inode,
+		       struct buffer_head ** bh,
+		       int nblk,
+		       size_t ulen)
+{
+	struct ext2_cluster_head *head;
+	int count, src_ix, x;
+	unsigned char *dst;
+	unsigned meth, alg;
+	char bdn[BDEVNAME_SIZE];
+
+	assert (atomic_read(&inode->i_mutex.count) <= 0); /* i.e. mutex_lock */
+
+	assert (ext2_rd_wa_lock != 0);
+
+	/*
+	   We pack the buffer together before (and must take care
+	   not to duplicate the buffer heads in the array).
+
+	   pjm 1998-01-09: Starting from e2compr-0.4.0, they should
+	   already be packed together in the blkaddr array.  TODO:
+	   Insert appropriate assert() statements checking tht this is
+	   the case.  TODO: Check that callers have bh[] packed. */
+
+	trace_e2c("ext2_decompress_blocks: nblk=%d\n", nblk);
+	for (src_ix = 0, x = 0; src_ix < nblk; src_ix++)
+		if ((bh[src_ix] != NULL) && (bh[src_ix]->b_blocknr != 0)) {
+			if (x < src_ix) {
+				ext2_warning(inode->i_sb, "bad buffer table", "inode = %lu", inode->i_ino);
+				goto error;
+			}
+			x++;
+		}
+	nblk = x;
+	trace_e2c("ext2_decompress_blocks (2): nblk=%d\n", nblk);
+	if (nblk == 0) {
+		ext2_warning(inode->i_sb, "no block in cluster", "inode = %lu", inode->i_ino);
+		goto error;
+	}
+	head = (struct ext2_cluster_head *) (bh[0]->b_data);
+
+	/*
+	 *  Do some consistency checks.
+	 */
+
+	if (head->magic != cpu_to_le16(EXT2_COMPRESS_MAGIC_04X)) {
+		ext2_warning(inode->i_sb,
+			     "bad magic number",
+			     "inode = %lu, magic = %#04x",
+			     inode->i_ino,
+			     le16_to_cpu(head->magic));
+		goto error;
+	}
+
+#if EXT2_GRAIN_SIZE & (EXT2_GRAIN_SIZE - 1)
+# error "This code assumes EXT2_GRAIN_SIZE to be a power of two."
+#endif
+	/* The macro also assumes that _a > 0, _b > 0. */
+#define ROUNDUP_GE(_a, _b, _d) (   (  ((_a) - 1) \
+				    | ((_d) - 1)) \
+				>= (  ((_b) - 1) \
+				    | ((_d) - 1)))
+	if ((le32_to_cpu(head->ulen) > EXT2_MAX_CLUSTER_BYTES)
+	    || (head->clen == 0)
+	    || ROUNDUP_GE(le32_to_cpu(head->clen)
+			  + head->holemap_nbytes
+			  + sizeof(struct ext2_cluster_head),
+			  le32_to_cpu(head->ulen),
+			  EXT2_GRAIN_SIZE)) {
+		ext2_warning(inode->i_sb,
+			     "invalid cluster len",
+			     "inode = %lu, len = %u:%u",
+			     inode->i_ino,
+			     le32_to_cpu(head->clen),
+			     le32_to_cpu(head->ulen));
+		goto error;
+	}
+#undef ROUNDUP_GE
+
+	/* TODO: Test for `nblk != 1 + ...' instead of the current
+	   one-sided test.  However, first look at callers, and make
+	   sure that they handle the situation properly (e.g. freeing
+	   unneeded blocks) and tht they always pass a correct
+	   value for nblk. */
+	if (nblk <= ((le32_to_cpu(head->clen)
+		      + head->holemap_nbytes
+		      + sizeof(struct ext2_cluster_head)
+		      - 1)
+		     / bh[0]->b_size)) {
+	        int i;
+		ext2_warning(inode->i_sb,
+			     "missing blocks",
+			     "inode = %lu, blocks = %d/%u",
+			     inode->i_ino,
+			     nblk,
+			     ((le32_to_cpu(head->clen)
+			       + head->holemap_nbytes
+			       + sizeof(struct ext2_cluster_head)
+			       - 1)
+			      / bh[0]->b_size) + 1);
+		printk("i_size=%d\n", (int)inode->i_size);
+		for (i=0; i<12; i++)
+		  printk("i_data[%d]=%d\n", i, EXT2_I(inode)->i_data[i]);
+		printk("cluster_head (sizeof head=%d):\n\tmagic=0x%4x\n\tmethod=%d\n\tholemap_nbytes=%d\n\tulen=%d\n\tclen=%d\n\tbh->b_size=%d\n",
+		       sizeof(struct ext2_cluster_head),
+		       head->magic,
+		       (int) head->method,
+		       (int) head->holemap_nbytes,
+		       head->ulen,
+		       head->clen,
+		       bh[0]->b_size);
+		goto error;
+	}
+
+	/* We must invalidate the cache before scribbling over ext2_rd_wa.u. */
+	memset (&ext2_rd_wa_ucontents, 0xff, sizeof(struct ext2_wa_contents_S));
+
+	/* I moved it here in case we need to load a module that
+	 * needs more heap that is currently allocated.
+	 * In such case "init_module" for that algorithm forces
+	 * re-allocation of ext2_wa. It should be safe here b/c the
+	 * first reference to ext2_wa comes just after and we have
+	 * locked ext2_wa before.
+	 *
+	 * FIXME: Totally separate working areas for reading and writing.
+	 * 	Jan Rêkorajski
+	 */
+	meth = head->method; /* only a byte, so no swabbing needed. */
+	if (meth >= EXT2_N_METHODS) {
+		ext2_warning(inode->i_sb,
+			     "illegal method id",
+			     "inode = %lu, id = %u",
+			     inode->i_ino, meth);
+		goto error;
+	}
+	alg = ext2_method_table[meth].alg;
+	/* jmr 1998-10-28 This is a Bad Idea to put it here
+	if (!ext2_algorithm_table[alg].avail) {
+		sprintf(str, "ext2-compr-%s", ext2_algorithm_table[alg].name);
+		request_module(str);
+	}
+	*/
+	
+	/*
+	 *  Adjust the length if too many bytes are requested.
+	 *
+	 *    TODO: Traiter les bitmaps ici, et non plus au niveau de    
+	 *          l'appelant. Faire un petit cache en memorisant le    
+	 *          numero du dernier noeud decompresse et du dernier    
+	 *          cluster. Le pb, c'est qu'on ne peut pas savoir si    
+	 *          les blocs ont ete liberes et realloue entre temps    
+	 *          -> il faut etre prevenu pour invalider le buffer.    
+	 *
+	 *          pjm fixme tr: Take care of the bitmaps here,
+	 *          instead of by the caller as we currently do.  Keep
+	 *          a small cache that holds the number of the
+	 *          previous <inode, cluster> to have been
+	 *          decompressed.  The problem is that we have no way
+	 *          of knowing whether the blocks have been freed and
+	 *          reallocated in the meantime / since last time ->
+	 *          we must be informed so that we can invalidate the
+	 *          buffer.  */
+	if (ulen > le32_to_cpu(head->ulen)) {
+		memset(ext2_rd_wa->u + le32_to_cpu(head->ulen),
+		       0,
+		       ulen - le32_to_cpu(head->ulen));
+		ulen = le32_to_cpu(head->ulen);
+
+		assert((bh[0]->b_size & (bh[nblk - 1]->b_size - 1)) == 0);
+		if (((le32_to_cpu(head->clen)
+		      + head->holemap_nbytes
+		      + sizeof(struct ext2_cluster_head)
+		      - 1)
+		     | (bh[0]->b_size - 1))
+		    >= ((ulen - 1) | (bh[0]->b_size - 1))) {
+			printk(KERN_WARNING
+			       "ext2_decompress_blocks: "
+			       "ulen (=%u) or clen (=%u) wrong "
+			       "in dev %s, inode %lu.\n",
+			       ulen, le32_to_cpu(head->clen),
+			       bdevname(inode->i_sb->s_bdev, bdn),
+			       inode->i_ino);
+			goto error;
+		}
+	}
+
+	/*
+	 *  Now, decompress data.
+	 */
+	/* TODO: Is this (ulen == 0) possible? */
+	if (ulen == 0) 
+		return 0;
+
+	assert(ext2_rd_wa_lock != 0);
+	for (x = 0, dst = ext2_rd_wa->c; x < nblk; dst += bh[x++]->b_size)
+		memcpy(dst, bh[x]->b_data, bh[x]->b_size);
+
+	/* TODO: Check checksum. */
+
+	if (!ext2_algorithm_table[alg].avail) {
+		ext2_warning(inode->i_sb,
+			     "ext2_decompress_blocks",
+			     "algorithm `%s' not available for inode %lu",
+			     ext2_algorithm_table[alg].name,
+			     inode->i_ino);
+		ext2_mark_algorithm_use(inode, alg);
+		goto error;
+	}
+
+	/* */
+	{
+		struct ext2_cluster_head *wa1head
+			= (struct ext2_cluster_head *) ext2_rd_wa->c;
+		unsigned clen = le32_to_cpu(wa1head->clen);
+
+		if (wa1head->checksum != cpu_to_le32(
+			ext2_adler32 (le32_to_cpu(*(u32*)ext2_rd_wa->c),
+				      ext2_rd_wa->c + 8,
+				      (sizeof(struct ext2_cluster_head) - 8
+				       + head->holemap_nbytes + clen)))) {
+			ext2_warning(inode->i_sb,
+			     "ext2_decompress_blocks",
+			     "corrupted compressed data in inode %lu",
+			     inode->i_ino);
+			goto error;
+		}
+	}
+
+	count = ext2_algorithm_table[alg].decompress(
+		ext2_rd_wa->c + sizeof(struct ext2_cluster_head) + head->holemap_nbytes,
+		ext2_rd_wa->u,
+		ext2_rd_wa->heap,
+		le32_to_cpu(head->clen),
+		ulen,
+		ext2_method_table[meth].xarg);
+
+	/* If we got fewer than ulen bytes, there is a problem, since
+	   we corrected the ulen value before decompressing.  Note
+	   that it's OK for count to exceed ulen, because ulen can be
+	   less than head->ulen. */
+	if ((count < ulen) || (count > le32_to_cpu(head->ulen))) {
+		ext2_warning(inode->i_sb,
+			     "corrupted compressed data",
+			     "inode = %lu, count = %u of %u (%u/%u)",
+			     inode->i_ino, count, ulen,
+			     le32_to_cpu(head->clen),
+			     le32_to_cpu(head->ulen));
+		goto error;
+	}
+	ext2_ensure_algorithm_use(inode, alg);
+	return count;
+
+      error:
+
+	/* Raise the ECOMPR flag for this file.  What this means is
+	   that the file cannot be written to, and can only be read if
+	   the user raises the NOCOMPR flag.
+
+	   pjm 1997-01-16: I've changed it so that files with ECOMPR
+	   still have read permission, so user can still read the rest
+	   of the file but get an I/O error (errno = EXT2_ECOMPR) when
+	   they try to access anything from this cluster. */
+
+	EXT2_I(inode)->i_flags |= EXT2_ECOMPR_FL;
+
+	inode->i_ctime = CURRENT_TIME;
+	mark_inode_dirty_sync(inode);
+	/* pjm 1998-02-21: We used to do `memset(ext2_rd_wa.u, 0, ulen)'
+	   here because once upon a time the user could sometimes see
+	   buf contents.  I believe that this can never happen any
+	   more. */
+	return -EXT2_ECOMPR;
+}
+
+
+/* ext2_calc_free_ix: Calculates the position of the C_NBLK'th non-hole
+   block; equals C_NBLK plus the number of holes in the first CALC_FREE_IX()
+   block positions of the cluster.
+
+   pre: 1 =< c_nblk < EXT2_MAX_CLUSTER_BLOCKS,
+        Number of 1 bits in ,ubitmap` > ,c_nblk`.
+   post: c_nblk =< calc_free_ix() < EXT2_MAX_CLUSTER_BLOCKS
+
+   Called by:
+       ext2_decompress_cluster()
+       ext2_file_write()
+
+   TODO: Have ext2_compress_cluster() call this.
+   */
+unsigned ext2_calc_free_ix (unsigned holemap_nbytes, u8 const *holemap, unsigned c_nblk)
+{
+	unsigned i;
+
+	assert (1 <= c_nblk);
+	assert (c_nblk < EXT2_MAX_CLUSTER_BLOCKS);
+	for (i = 0; (i < holemap_nbytes * 8) && (c_nblk > 0);) {
+		assert (i < EXT2_MAX_CLUSTER_BLOCKS - 1);
+		if ((holemap[i >> 3] & (1 << (i & 7))) == 0)
+			c_nblk--;
+		i++;
+	}
+	i += c_nblk;
+	assert (i < EXT2_MAX_CLUSTER_BLOCKS);
+	return i;
+}
+
+
+/* ext2_unpack_blkaddrs(): Prepare the blkaddr[] array for
+   decompression by moving non-hole blocks to their proper positions
+   (according to ubitmap) and zeroing any other blocks.
+
+   Returns 0 on success, -errno on error.
+
+   Note: We assume tht blkaddr[i] won't change under us forall
+   clu_block0 =< i < clu_block0 + clu_nblocks.  Holding i_sem should
+   guarantee this.
+
+   Called by:
+       ext2_decompress_cluster()
+       ext2_file_write() */
+int
+ext2_unpack_blkaddrs(struct inode *inode,
+		     struct buffer_head	*bh[],
+		     int mmcp,
+		     unsigned holemap_nbytes,
+		     u8 const *holemap,
+		     unsigned c_nblk,
+		     unsigned free_ix,
+		     unsigned clu_block0,
+		     unsigned clu_nblocks)
+{
+	struct ext2_bkey key;
+	u32 *blkaddr;
+	unsigned si, di;
+	
+	assert (clu_nblocks <= EXT2_MAX_CLUSTER_BLOCKS);
+	assert (1 <= c_nblk);
+	assert (c_nblk <= free_ix);
+	assert (free_ix < EXT2_MAX_CLUSTER_BLOCKS);
+	if (!ext2_get_key(&key, inode, clu_block0))
+		return -EIO;
+
+	if (key.ptr[key.level] == NULL) {
+		/* TODO: Call ext2_error(). */
+		ext2_free_key(&key);
+		return -EIO;
+	}
+
+	/* impl: Note tht we're relying on clusters not straddling
+	   address block boundaries. */
+	blkaddr = &key.ptr[key.level][key.off[key.level]];
+	memset(blkaddr + free_ix,
+	       0,
+	       sizeof(*blkaddr) * (clu_nblocks - free_ix));
+	si = c_nblk;
+	for (di = free_ix; di > si; ) {
+		--di;
+		if (((di >> 3) < holemap_nbytes)
+		    && (holemap[di >> 3] & (1 << (di & 7)))) {
+			blkaddr[di] = 0;
+			bh[di]->b_blocknr = 0;
+			clear_bit(BH_Mapped, &bh[di]->b_state);
+		} else {
+		  	if (si == 0) {
+			  break;
+			}
+			blkaddr[di] = blkaddr[--si];
+			assert(bh[di]->b_blocknr == 0);
+			assert(bh[si]->b_blocknr != 0);
+			assert(buffer_mapped(bh[si]));
+			trace_e2c("unpack: di=%d sts=0x%x si=%d blk=%ld sts=0x%x\n", di, (int)bh[di]->b_state, si, bh[si]->b_blocknr, (int)bh[si]->b_state);
+			bh[di]->b_blocknr = bh[si]->b_blocknr;
+			set_bit(BH_Mapped, &bh[di]->b_state);
+			bh[si]->b_blocknr = 0;
+			clear_bit(BH_Mapped, &bh[si]->b_state);
+			set_bit(BH_Uptodate, &bh[di]->b_state);
+			if (mmcp) {
+			  memcpy(bh[di]->b_data, bh[si]->b_data, inode->i_sb->s_blocksize);
+			}
+		}
+	}
+	if (key.level > 0)
+		mark_buffer_dirty(key.ibh[key.level]);
+	return ext2_free_key(&key);
+}
+
+
+/*
+ *    Decompress one cluster.  If already compressed, the cluster
+ *      is decompressed in place, and the compress bitmap is updated.
+ *
+ *      Returns the size of decompressed data on success, a negative
+ *      value in case of failure, or 0 if the cluster was not compressed.
+ *
+ *      The inode is supposed to be writable.
+ *
+ *      Called by :
+ *
+ *        ext2_decompress_inode()      [sem down]
+ *        ext2_file_write()            [sem down]
+ *        trunc_bitmap()               [sem down]
+ */
+int ext2_decompress_cluster(struct inode *inode, u32 cluster)
+{
+	struct buffer_head *bh[EXT2_MAX_CLUSTER_BLOCKS];
+	struct buffer_head *bhc[EXT2_MAX_CLUSTER_BLOCKS];
+	struct page *pg[EXT2_MAX_CLUSTER_PAGES], *epg[EXT2_MAX_CLUSTER_PAGES];
+	int result, nbh;
+	unsigned npg, c_nblk;
+	struct ext2_cluster_head *head;
+	int i=0;
+	unsigned free_ix, clu_block0, clu_nblocks;
+	int d_npg = -1;	/* number of decompressed page	*/
+	unsigned long	allpagesuptodate = 1;
+
+	assert (atomic_read(&inode->i_mutex.count) <= 0); /* i.e. mutex_lock */
+	if (ext2_rd_wa == NULL)
+		return -EBUSY;
+
+	/*
+	  Get blocks from cluster.
+	  Assign to variables head, ubitmap, clu_block0, clu_nblocks.
+	  Shuffle blkaddr[] array and write zero to holes.
+	  Allocate new blocks.
+	  Get the working area.
+	  Decompress.
+	  Copy to bh[]->b_data (marking buffers uptodate and dirty).
+	  Release working area.
+	  Release bh[].
+	  */
+
+	nbh = 0;
+	npg = ext2_cluster_npages(inode, cluster);
+ 	result =  ext2_get_cluster_pages(inode, cluster, pg, NULL, 0);
+	if (result <= 0) {
+	  for (i = 0; i < npg; i++)
+	    epg[i] = NULL;
+	  goto out;
+	}
+	for (i = 0; i < npg; i++) {
+	  if ((pg[i]->index <= ((inode->i_size - 1) >> PAGE_CACHE_SHIFT)) &&
+	      !PageUptodate(pg[i])) {
+	    allpagesuptodate = 0;
+	  }
+	}
+	if (allpagesuptodate) {
+	  result = ext2_decompress_pages(inode, cluster, pg);
+	  if (result != 0) {
+	    for (i = 0; i < npg; i++)
+	      epg[i] = NULL;
+	    goto out;
+	  }
+	}
+	result =  ext2_get_cluster_extra_pages(inode, cluster, pg, epg);
+	if (result <= 0) {
+	  goto out;
+	}
+ 	result = ext2_get_cluster_blocks(inode, cluster, bh, pg, epg, 0);
+	if (result <= 0) {
+	  goto out;
+	}
+	nbh = c_nblk = result;
+#ifdef EXT2_COMPR_REPORT_VERBOSE
+	  {
+	    int j;
+	    printk("ext2_decompress_cluster: buffers get for cluster=%d, inode=%ld, size=%d nbh=%d\n", cluster, inode->i_ino, (int)inode->i_size, nbh);
+	    for (j=0; j<nbh; j++)
+	      {
+		if (bh[j])
+		  {
+		    printk("0buffer_head[%d]: blocknr=%lu, addr=%p ", j, (unsigned long) bh[j]->b_blocknr, bh[j]);
+		    if (bh[j]->b_page)
+		      printk("0:[page->index=%ld]\n", bh[j]->b_page->index);
+		    else
+		      printk("[No page]\n");
+		  }
+		else
+		  printk("buffer_head[%d] is NULL\n", j);
+	      }
+	    while ((j < EXT2_MAX_CLUSTER_BLOCKS) && (bh[j] != NULL) && bh[j]->b_blocknr) /*Add by Yabo Ding*/
+	      {
+		printk("buffer_head[%d] is free but not NULL: blocknr=%lu, addr=%p\n", j, (unsigned long) bh[j]->b_blocknr, bh[j]);
+		j++;
+	      }
+	  }
+#endif
+	for (i = 0; i < nbh; i++)
+		assert(bh[i]->b_blocknr != 0);
+	head = (struct ext2_cluster_head *) bh[0]->b_data;
+	if (head->magic != cpu_to_le16(EXT2_COMPRESS_MAGIC_04X)) {
+		ext2_warning(inode->i_sb,
+			     "ext2_decompress_cluster: bad magic number",
+			     "inode = %lu, magic = %#04x",
+			     inode->i_ino,
+			     le16_to_cpu(head->magic));
+		EXT2_I(inode)->i_flags |= EXT2_ECOMPR_FL;
+		result = -EXT2_ECOMPR;
+		goto out;
+	}
+	if (le32_to_cpu(head->ulen) - (c_nblk << inode->i_sb->s_blocksize_bits) <= 0) {
+		ext2_error(inode->i_sb, "ext2_decompress_cluster",
+			   "ulen too small for c_nblk.  ulen=%u, c_nblk=%u, bs=%lu",
+			   le32_to_cpu(head->ulen), c_nblk, inode->i_sb->s_blocksize);
+		EXT2_I(inode)->i_flags |= EXT2_ECOMPR_FL;
+		result = -EXT2_ECOMPR;
+		goto out;
+	}
+	free_ix = ext2_calc_free_ix(head->holemap_nbytes, (u8 const *)(&head[1]), c_nblk);
+	clu_block0 = ext2_cluster_block0(inode, cluster);
+	clu_nblocks = ext2_cluster_nblocks(inode, cluster);
+	ext2_unpack_blkaddrs(inode, bh, 1,
+			     head->holemap_nbytes, (u8 const *) (&head[1]),
+			     c_nblk, free_ix, clu_block0, clu_nblocks);
+
+	/* Allocate the extra blocks needed. */
+	{
+		int data_left = le32_to_cpu(head->ulen);
+
+		data_left -= c_nblk << inode->i_sb->s_blocksize_bits;
+		assert (data_left > 0);
+		for (i = free_ix; i < clu_nblocks; i++)
+			if (((i >> 3) >= head->holemap_nbytes)
+			    || !(head->holemap[i >> 3] & (1 << (i & 7)))) {
+				result = ext2_get_block(inode,
+							clu_block0 + i,
+							bh[i],
+							1 /* create */);
+				if (bh[i]->b_blocknr == 0)
+					goto out;
+				d_npg = (i >> (PAGE_CACHE_SHIFT - inode->i_sb->s_blocksize_bits)) + 1;
+				nbh++;
+				data_left -= inode->i_sb->s_blocksize;
+				if (data_left <= 0)
+					break;
+			}
+	}
+
+	/* jmr 1998-10-28 Hope this is the last time I'm moving this code.
+	 * Module loading must be done _before_ we lock wa, just think what
+	 * can happen if we reallocate wa when somebody else uses it...
+	 */
+	{
+		unsigned meth, alg;
+
+		meth = head->method; /* only a byte, so no swabbing needed. */
+		if (meth >= EXT2_N_METHODS) {
+			ext2_warning(inode->i_sb,
+				     "illegal method id",
+				     "inode = %lu, id = %u",
+				     inode->i_ino, meth);
+			result = -EXT2_ECOMPR;
+			goto out;
+		}
+
+#ifdef CONFIG_KMOD
+		alg = ext2_method_table[meth].alg;
+		if (!ext2_algorithm_table[alg].avail) {
+			char str[32];
+
+			sprintf(str, "ext2-compr-%s", ext2_algorithm_table[alg].name);
+			request_module(str);
+		}
+#endif
+	}
+
+
+	result = -EINTR;
+	if (ext2_lock_rd_wa() == 0)
+		goto out;
+
+#ifdef EXT2_COMPR_REPORT_WA
+	printk(KERN_DEBUG "pid %d locks wa\n", current->pid);
+#endif
+
+	/*
+	 *  Then, decompress and copy back data.
+	 */
+
+	{
+	  int	ic;
+
+	  for (ic = 0, i = 0; i < clu_nblocks; i++) {
+	    if (bh[i]->b_blocknr != 0) {
+	      bhc[ic] = bh[i];
+	      ic++;
+	      if (ic == c_nblk) {
+		break;
+	      }
+	    }
+	  }
+	}
+	result = ext2_decompress_blocks(inode, bhc, c_nblk, 
+					le32_to_cpu(head->ulen));
+	if (result != (int) le32_to_cpu(head->ulen))
+	{
+		if (result >= 0) {
+			/* I think this is impossible, as
+			   ext2_decompress_blocks() checks against
+			   head->ulen. */
+			printk(KERN_WARNING "Unexpected return value %d "
+			       "from ext2_decompress_blocks()\n", result);
+			result = -EXT2_ECOMPR;
+		}
+
+		ext2_unlock_rd_wa();
+#ifdef EXT2_COMPR_REPORT_WA
+		printk(KERN_DEBUG "pid %d unlocks wa\n", current->pid);
+#endif
+		goto out;
+	}
+	assert (ext2_rd_wa_ucontents.ino == ~0ul);
+	ext2_rd_wa_ucontents.ino = inode->i_ino;
+	ext2_rd_wa_ucontents.dev = inode->i_rdev;
+	ext2_rd_wa_ucontents.cluster = cluster;
+
+#ifdef EXT2_COMPR_REPORT_ALGORITHMS
+	printk(KERN_DEBUG "ext2: %04x:%lu: cluster %d+%d [%d] "
+	       "decompressed into %d bytes\n",
+	       inode->i_rdev,
+	       inode->i_ino,
+	       clu_block0,
+	       clu_nblocks,
+	       c_nblk, result);
+#endif
+
+	/* Copy back decompressed data. */
+	{
+		int count = result;
+		unsigned char const *src;
+		int c, p;
+		int cbh;
+		int			n;	/* block index in page	*/
+		struct buffer_head	*bp;
+		unsigned addr0, b_start, b_end;
+
+		assert(count > 0);
+		if (d_npg == -1) {
+		  d_npg = ((count - 1) >> PAGE_CACHE_SHIFT) + 1;
+		}
+		trace_e2c("ext2_decompress_cluster: cnt=%d free_ix=%d d_npg=%d nbh=%d\n",
+		    count, free_ix, d_npg, nbh);
+		result = -EXT2_ECOMPR;
+		src = ext2_rd_wa->u;
+		cbh = 0;
+		for (c = 0; c < clu_nblocks; c++) {
+			if (bh[c]->b_blocknr == 0) {
+			  trace_e2c("\t clear buf %d sts=0x%x\n", c,
+				(int)bh[c]->b_state);
+			  memset(bh[c]->b_data, 0, inode->i_sb->s_blocksize);
+			  continue;
+			}
+			if (cbh >= (nbh -1)) {
+			  break;
+			}
+			if (count < inode->i_sb->s_blocksize) {
+				ext2_unlock_rd_wa();
+				goto out;
+			}
+			cbh++;
+			count -= inode->i_sb->s_blocksize;
+			p = c >> (PAGE_CACHE_SHIFT - inode->i_sb->s_blocksize_bits);
+			if (!PageUptodate(pg[p])) {
+			  addr0 = (clu_block0 << inode->i_sb->s_blocksize_bits);
+			  b_start = addr0 + (c << inode->i_sb->s_blocksize_bits);
+			  b_end = b_start + inode->i_sb->s_blocksize;
+			  trace_e2c("\t[%d] sts=0x%x e=%d s=%d sz=%d\n", c,
+				    (int)bh[c]->b_state, b_end, b_start, (int)inode->i_size);
+			  if (b_end <= inode->i_size) {
+			    /* Block is before end of file, copy data */
+			    memcpy(bh[c]->b_data, src, inode->i_sb->s_blocksize);
+			  } else if (b_start < inode->i_size) {
+			    /* Block contains end of file, copy to end */
+			    memcpy(bh[c]->b_data, src, inode->i_size - b_start);
+			  }
+			  set_buffer_uptodate(bh[c]);
+			  mark_buffer_dirty(bh[c]);
+/*			  mark_buffer_dirty_inode(bh[c], inode); */
+			} else {
+			  n = c & ((PAGE_CACHE_SIZE - 1) >> inode->i_sb->s_blocksize_bits);
+			  bp = page_buffers(pg[p]);
+			  for (i = 0; i < n; i++) {
+			    bp = bp->b_this_page;
+			  }
+			  result = ext2_get_block(inode, clu_block0 + c, bp, 0);
+			  if (bp->b_blocknr == 0) {
+			    ext2_unlock_rd_wa();
+			    goto out;
+			  }
+			  assert(bp->b_blocknr == bh[c]->b_blocknr);
+			}
+			src += inode->i_sb->s_blocksize;
+		}
+		if (count > inode->i_sb->s_blocksize) {
+			ext2_unlock_rd_wa();
+			goto out;
+		}
+		p = c >> (PAGE_CACHE_SHIFT - inode->i_sb->s_blocksize_bits);
+		if (!PageUptodate(pg[p])) {
+		  addr0 = (clu_block0 << inode->i_sb->s_blocksize_bits);
+		  b_start = addr0 + (c << inode->i_sb->s_blocksize_bits);
+		  trace_e2c("\t[%d] sts=0x%x c=%d s=%d sz=%d\n", c,
+			    (int)bh[c]->b_state, count, b_start, (int)inode->i_size);
+		  if (b_start >= inode->i_size) {
+		    memset(bh[c]->b_data, 0, inode->i_sb->s_blocksize);
+		  } else {
+		    if ((inode->i_size - b_start) < count) {
+		      memcpy(bh[c]->b_data, src, inode->i_size - b_start);
+		      memset(bh[c]->b_data + (inode->i_size - b_start), 0, count - (inode->i_size - b_start));
+		    } else {
+		      memcpy(bh[c]->b_data, src, count);
+		    }
+		  }
+		  set_buffer_uptodate(bh[c]);
+		  mark_buffer_dirty(bh[c]);
+/*		  mark_buffer_dirty_inode(bh[c], inode); */
+		} else {
+		  n = c & ((PAGE_CACHE_SIZE - 1) >> inode->i_sb->s_blocksize_bits);
+		  bp = page_buffers(pg[p]);
+		  for (i = 0; i < n; i++) {
+		    bp = bp->b_this_page;
+		  }
+		  result = ext2_get_block(inode, clu_block0 + c, bp, 0);
+		  if (bp->b_blocknr == 0) {
+		    ext2_unlock_rd_wa();
+		    goto out;
+		  }
+		  assert(bp->b_blocknr == bh[c]->b_blocknr);
+		}
+		result = (nbh - 1) * inode->i_sb->s_blocksize + count;
+	}
+
+	for (i = 0; i < npg; i++)
+	  {
+	    if (pg[i] == NULL)
+	      break;
+	    if (i < d_npg)
+	      SetPageUptodate(pg[i]);
+	  }
+
+	inode->i_ctime = CURRENT_TIME;
+	mark_inode_dirty_sync(inode);
+	/* If needed, EXT2_DIRTY_FL is raised by the caller. */
+
+	ext2_unlock_rd_wa();
+#ifdef EXT2_COMPR_REPORT_WA
+	printk(KERN_DEBUG "pid %d unlocks wa\n", current->pid);
+#endif
+
+#if 0
+	/* TODO: SYNC */
+	if (IS_SYNC(inode)) {
+	  generic_osync_inode(inode, inode->i_mapping, OSYNC_METADATA|OSYNC_DATA);
+	}
+#endif
+	assert (result >= 0);
+
+      out:
+	for (i = 0; i < npg; i++)
+	  {
+	    if (pg[i] == NULL)
+	      break;
+	    unlock_page(pg[i]);
+
+	    page_cache_release(pg[i]);
+	    if (epg[i] != NULL) 
+	      {
+		try_to_free_buffers(epg[i]);
+		unlock_page(epg[i]);
+ 		assert(page_count(epg[i]) == 1);
+		page_cache_release(epg[i]);		
+	      }
+	  }
+	
+	/*
+	 *  Release buffers, don't forget to unlock the locked ones.
+	 *  pjm 1998-01-14: TODO: Locked ones?
+	 */
+	assert (nbh >= 0);
+	assert (nbh <= EXT2_MAX_CLUSTER_BLOCKS);
+	return result;
+}
+
+
+/*
+ * Function to decompress the pages of a cluster.
+ *
+ *	Allocate buffers to pages what are not mapped on the device.
+ *
+ *      Returns the size of decompressed data on success, a negative
+ *      value in case of failure, or 0 if some pages are not uptodate.
+ *
+ *      The inode is supposed to be writable.
+ *	All the pages must be UPTODATE, 
+ */
+int ext2_decompress_pages(struct inode *inode, u32 cluster, struct page *pg[])
+{
+  struct ext2_cluster_head	*head;
+  struct buffer_head		*bh0;
+  struct buffer_head		*bh[EXT2_MAX_CLUSTER_BLOCKS];
+  unsigned			nbh, c_nblk;
+  unsigned			free_ix, clu_block0, clu_nblocks;
+  int				i, pagesPerCluster, data_left, size = 0;
+  long				status = 0;
+  char * dp;
+  
+  /* First, get cluster_head (For this, we need to re-read the first block of
+     the cluster, without overwriting the data of the page the buffer point to... */
+  /* This suppose that cluster are aligned with PAGE_SIZE... To be improved */
+  
+  /* Changed by Yabo Ding<bobfree_cn@yahoo.com.cn>,<yding@wyse.com>
+     The old code cannot reread data from disk to a changed buffers data pointer in 2.6.x.
+     So, I copy memory data(decompressed) to a temporary buffer;
+     Then reread data(compressed) from disk, and copy to head;
+     Then copy back the memory data from temporary buffer.
+     It seems clumsy, but it works well.
+  */
+  bh0 = page_buffers(pg[0]);
+  head = (struct ext2_cluster_head *)kmalloc(bh0->b_size, GFP_KERNEL);
+  if (head == NULL) {
+    ext2_warning(inode->i_sb, "no more memory", "inode = %lu", inode->i_ino);
+    return(-EIO);
+  }
+  dp = kmalloc(bh0->b_size, GFP_KERNEL);
+  if (dp == NULL) {
+    ext2_warning(inode->i_sb, "no more memory", "inode = %lu", inode->i_ino);
+    kfree(head);    
+    return(-EIO);
+  }
+  memcpy(dp, bh0->b_data, bh0->b_size);
+  clear_bit(BH_Uptodate, &bh0->b_state);
+  if (!buffer_mapped(bh0)) {
+    status = ext2_get_block(inode, ext2_cluster_block0(inode, cluster), bh0, 0);
+    if (bh0->b_blocknr == 0) {
+      trace_e2c("ext2_decompress_pages: ext2_get_block error %ld (cluster = %u)\n", status, cluster);
+      kfree(head);
+      memcpy(bh0->b_data, dp, bh0->b_size);
+      kfree(dp);
+      status = -EIO;
+      goto out;
+    }
+  }
+  ll_rw_block(READ, 1, &bh0);
+  wait_on_buffer(bh0);
+  if (!buffer_uptodate(bh0)) {	/* Read error ??? */
+    trace_e2c("ext2_decompress_pages: IO error (cluster = %u)\n", cluster);
+    kfree(head);
+    memcpy(bh0->b_data, dp, bh0->b_size);
+    kfree(dp);
+    status = -EIO;
+    goto out;
+  }
+  /* This suppose that cluster are aligned with PAGE_SIZE... To be improved 
+  bh0->b_data = page_address(pg[0]);                                    */
+  memcpy((char *)head, bh0->b_data, bh0->b_size);
+  memcpy(bh0->b_data, dp, bh0->b_size);
+  kfree(dp);  
+
+  if (head->magic != cpu_to_le16(EXT2_COMPRESS_MAGIC_04X)) {
+    ext2_warning(inode->i_sb, "ext2_decompress_pages: bad magic number", "inode = %lu, magic = %#04x",
+		 inode->i_ino, le16_to_cpu(head->magic));
+    kfree(head);
+    status = -EIO;
+    goto out;
+  }
+  trace_e2c("ext2_decompress_pages: clt=%d i=%ld head=0x%x\n", cluster, inode->i_ino, (unsigned) head);
+
+  /* Now, try to do the same as in ext2_decompress_cluster for moving/allocating blocks */
+  nbh=0;
+  pagesPerCluster = ext2_cluster_npages(inode, cluster);
+  for (i = 0; i < pagesPerCluster && pg[i]; i++) {
+    assert(PageLocked(pg[i]));
+    if (!(PageUptodate(pg[i]))) {
+      kfree(head);
+      return(0);
+    }
+  }
+  for (i = 0; i < pagesPerCluster && pg[i]; i++) {
+    struct buffer_head* bhead, *bhx;
+    int idx = 0;
+
+    /* assert(PageUptodate(pg[i])); with ftruncate() can be false */
+    if (!page_has_buffers(pg[i])) {
+      create_empty_buffers(pg[i], inode->i_sb->s_blocksize, 0);
+    }
+    bhead = page_buffers(pg[i]);
+    for (bhx = bhead; bhx != bhead || !idx; bhx = bhx->b_this_page) {
+      idx++;
+      bh[nbh] = bhx;
+      nbh++;
+    }
+  }
+
+  while ((nbh != 0) && (bh[nbh - 1]->b_blocknr == 0))
+    --nbh;
+
+  c_nblk = nbh;
+
+  free_ix = ext2_calc_free_ix(head->holemap_nbytes, (u8 const *)(&head[1]), c_nblk);
+  clu_block0 = ext2_cluster_block0(inode, cluster);
+  clu_nblocks = ext2_cluster_nblocks(inode, cluster);
+  ext2_unpack_blkaddrs(inode, bh, 0, head->holemap_nbytes, (u8 const *) (&head[1]),
+		       c_nblk, free_ix, clu_block0, clu_nblocks);
+
+  /* Allocate the extra blocks needed. */
+  data_left = size = le32_to_cpu(head->ulen);
+
+  data_left -= c_nblk << inode->i_sb->s_blocksize_bits;
+  assert (data_left > 0);
+  for (i = 0; i < free_ix; i++) {
+    if (bh[i]->b_blocknr != 0) {
+      trace_e2c("\t [%d] blk=%ld sts=0x%x\n", i, bh[i]->b_blocknr, (int)bh[i]->b_state);
+      mark_buffer_dirty(bh[i]);
+/*      mark_buffer_dirty_inode(bh[i], inode); */
+    }
+  }
+  for (i = free_ix; i < clu_nblocks; i++) {
+    if (((i >> 3) >= head->holemap_nbytes)
+	|| !(head->holemap[i >> 3] & (1 << (i & 7)))) {
+      status = ext2_get_block(inode, clu_block0 + i, bh[i], 1 /* create */);
+      if (status || bh[i]->b_blocknr == 0) {
+	status = -EIO;
+	goto out;
+      }
+      trace_e2c("\t [%d] blk=%ld sts=0x%x\n", i, bh[i]->b_blocknr, (int)bh[i]->b_state);
+      set_bit(BH_Uptodate, &bh[i]->b_state);
+      mark_buffer_dirty(bh[i]);
+/*      mark_buffer_dirty_inode(bh[i], inode); */
+      nbh++;
+      data_left -= inode->i_sb->s_blocksize;
+      if (data_left <= 0)
+	break;
+    }
+  }
+ out:
+  kfree(head);
+  return(status ? status : size);
+}
+
+
+#if 0 /* old code */
+/* TODO: Compare this code with the above; differences may
+   indicate bugs. */
+frob() {
+	int s_nblk, c_nblk, result, n, err;
+	u32 block, clu_nblocks;
+	struct ext2_cluster_head *head = NULL;
+	u32 bitmap;
+	unsigned char *src;
+
+	if ((blocks = s_nblk = ext2_get_cluster_blocks(inode, cluster, bh)) <= 0)
+		return s_nblk;
+
+	/*
+	 *  Check if somebody decompressed the cluster while we were
+	 *    waiting for it.  After that, we know that the cluster is
+	 *    compressed, and we have a lock over the buffers.  We can
+	 *    thus safely go.
+	 *
+	 *  pjm 1998-01-09: But if i_sem is down then no-one should be able
+	 *  to compress beneath us...
+	 */
+	result = ext2_cluster_is_compressed(inode, cluster);
+	if (result <= 0) {
+		if (result == 0)
+			ext2_warning(inode->i_sb, "ext2_decompress_cluster",
+				     "Concurrency assumption violated.  Inode %d.\n",
+				     inode->i_ino);
+		goto out;
+	}
+
+	/*
+	 *  Find cluster head.
+	 */
+# if 0 /* Old code, just in case we go back on our decision to pack blkaddrs
+	 together. */
+	for (n = 0; n < s_nblk; n++)
+		if (bh[n] != NULL) {
+			head = ((struct ext2_cluster_head *) (bh[n]->b_data));
+			break;
+		}
+# else /* new code */
+	assert(bh[0] != NULL);
+	head = ((struct ext2_cluster_head *) (bh[0]->b_data));
+# endif /* new code */
+
+	ubitmap = le32_to_cpu(head->ubitmap);
+	clu_block0 = ext2_cluster_block0(inode, cluster);
+	clu_nblocks = ext2_cluster_nblocks(inode, cluster);
+
+	/* First, reshuffle the compressed blocks to the positions
+	   specified by head->ubitmap and zero the remaining block
+	   numbers.  (Remember, holes in non-compressed clusters are
+	   stored as zero, not EXT2_COMPRESSED_BLKADDR as stored at
+	   the moment.)  This implies marking the cluster as
+	   uncompressed. */
+	{
+		unsigned free_ix;
+		ext2_bkey key;
+		u32 *ptr;
+		unsigned i, c;
+
+		/* effic: s/s_nblk/c_nblk/g. */
+		c_nblk = 0;
+		for (i = 0; i < s_nblk; i++)
+			if (bh[i] != NULL)
+				c_nblk++;
+		assert (c_nblk == s_nblk);
+
+		/* impl: The reason we choose to work backwards is so that if
+		   the inode (or an indirect block) is sync'ed when we're half
+		   way through, then recovery is slightly nicer (because all
+		   of our blocks are still marked as belonging to us, even if
+		   some appear twice; compared to the alternative where some
+		   don't appear at all).  If it's impossible for the block
+		   numbers to be synced (for example, because we hold i_sem,
+		   or if we later decide to take I_LOCK), then it would be
+		   sligtly easier if this were done the forwards way. */
+		result = -EIO;
+		if (!ext2_get_key(&key, inode, ext2_cluster_block0(inode, cluster)))
+			goto out;
+		/* This is nasty, but efficient.  We get a key to the
+		   _first_ block of the cluster, and then let ptr[]
+		   become the array of block numbers for the cluster.
+		   We can do this because the cluster sizes of the
+		   file are calculated such that no cluster straddles
+		   an indirect block boundary. */
+		ptr = key->ptr[key->level] + key->off[key->level];
+		free_ix = calc_free_ix(le32_to_cpu(head->ubitmap), c_nblk);
+		memset(ptr + free_ix, 0, (clu_nblocks - free_ix) * sizeof(*ptr));
+		c = c_nblk;
+		for (i = free_ix; i != 0;) {
+			if (ubitmap & (1 << --i))
+				ptr[i] = bh[--c]->b_blocknr;
+		}
+		assert (c == 0);
+	}
+
+	/* fixme: ici: up to here: check nblk references. */
+
+	/* Allocate the new blocks.  We do this now, in order to be
+	   able to copy the decompressed data as soon as we get it.
+	   We reuse the old blocks, too.
+
+	   Allocate the blocks directly after the compressed blocks:
+	   we will shuffle them around (according to bitmap) later.
+	   Hmm, alternatively, we could shuffle the first ,c_nblk`
+	   blocks to fill up the first ,free_ix` positions and shove 0
+	   into the remaining block pointers, and then do the
+	   allocations.
+
+	   TODO: If an error occurs when allocating new blocks, we should
+	           free the ones we've already allocated before returning.  */
+
+	/* TODO: tenir compte de i_size a cause de truncate ... 
+
+	   (fixme pjm tr) Keep note of i_size because of truncate.
+
+	   pjm 1998-01-09: truncate() shouldn't be called for the inode
+	   because we hold i_sem.
+	*/
+	{
+		unsigned c, u;
+
+		c = c_nblk;
+		for (u = free_ix; u < clu_nblocks; u++) {
+			if (bitmap & (1 << u)) {
+				bh[c] = ext2_getblk(inode, clu_block0 + u, 1, &err);
+				if (bh[c] == NULL) {
+					result = err;
+					blocks = c;
+					goto out;
+				}
+			}
+		}
+	}
+
+	/*
+	 *  Try to get the working area.
+	 */
+	if (ext2_lock_rd_wa() == 0) {
+		result = -EINTR;
+		goto out;
+	}
+
+# ifdef EXT2_COMPR_REPORT_WA
+	printk(KERN_DEBUG "pid %d locks wa\n", current->pid);
+# endif
+
+	/*
+	 *  Then, decompress and copy back data.
+	 */
+
+	result = ext2_decompress_blocks(inode, bh, c_nblk, 
+					le32_to_cpu(head->ulen));
+	if (result == (int) le32_to_cpu(head->ulen)) {
+		int count = result;
+
+		ext2_rd_wa_ucontents.ino = inode->i_ino;
+		ext2_rd_wa_ucontents.dev = inode->i_rdev;
+		ext2_rd_wa_ucontents.cluster = cluster;
+
+# ifdef EXT2_COMPR_REPORT_ALGORITHMS
+		printk(KERN_DEBUG "ext2: %lu: cluster %d+%d [%d] "
+		       "decompressed into %d bytes (bitmap = %x)\n",
+		       inode->i_ino,
+		       ext2_cluster_block0(inode, cluster),
+		       blocks,
+		       nblk, count, (unsigned) bitmap);
+# endif
+
+		/*
+		 *        Copy back decompressed data.
+		 */
+
+		src = ext2_rd_wa->u;
+		for (c = 0;
+		     (c < nbh) && (count > 0);
+		     c++) {
+			if (count >= inode->i_sb->s_blocksize)
+				memcpy(bh[c]->b_data,
+				       src,
+				       inode->i_sb->s_blocksize);
+			else {
+				memcpy(bh[c]->b_data,
+				       src,
+				       count);
+				memset(bh[c]->b_data + count,
+				       0,
+				       inode->i_sb->s_blocksize - count);
+				/* pjm 1997-02-03: We zero the
+				   rest for the sake of
+				   mmap(). */
+			}
+			set_buffer_uptodate(bh[c]);
+			mark_buffer_dirty(bh[c]);
+			src += inode->i_sb->s_blocksize;
+			count -= inode->i_sb->s_blocksize;
+		}
+		/* If needed, the dirty
+		   bit is maintained by the caller.  The inode is
+		   marked dirty or synced by ext2_set_cluster_flag().  (fixme)*/
+
+
+		/*
+		 *        Release working area.
+		 */
+		ext2_unlock_rd_wa();
+# ifdef EXT2_COMPR_REPORT_WA
+		printk(KERN_DEBUG "pid %d unlocks wa\n", current->pid);
+# endif
+
+		/* TODO: SYNC */
+# if 0
+		if (IS_SYNC(inode)) {
+			ll_rw_block(WRITE, nbh, bh);
+			for (c = 0; c < nbh; c++)
+				if (bh[c])
+					wait_on_buffer(bh[c]);
+		}
+# endif
+
+		nblk = EXT2_I(inode)->i_clu_nblocks; /* fixme */
+	} else {
+		if (result >= 0) {
+			printk(KERN_ERR "Unexpected return value %d from ext2_decompress_blocks()\n", result);
+			result = -EXT2_ECOMPR;
+		}
+
+		ext2_unlock_rd_wa();
+# ifdef EXT2_COMPR_REPORT_WA
+		printk(KERN_DEBUG "pid %d unlocks wa\n", current->pid);
+# endif
+	}
+
+	/*
+	 *  Release buffers, don't forget to unlock the locked ones.
+	 */
+
+      out:			/* TODO: Take another look at this. */
+	/* fixme: Look at `goto out' places and check what variable we
+	   should use in place of `blocks' here. */
+	while (nbh != 0)
+		if (bh[--nbh] != NULL)
+			brelse(bh[nbh]);
+
+	return result;
+}
+#endif /* code to be deleted once comparison has been made. */
+
+
+/* Decompress every cluster that is still compressed.
+   We stop and return -ENOSPC if we run out of space on device.
+
+   The caller needs to check for EXT2_COMPRBLK_FL before calling.
+
+   Returns 0 on success, -errno on failure.
+
+   Called by ext2_ioctl(). */
+int
+ext2_decompress_inode(struct inode *inode)
+{
+	u32 cluster;
+	u32 n_clusters;
+	int err;
+	struct ext2_inode_info *ei = EXT2_I(inode);
+
+	/* TODO: We could try again to allocate it. */
+	if (ext2_rd_wa == NULL)
+		return -EBUSY;
+
+	assert (ei->i_flags & EXT2_COMPRBLK_FL);
+
+	/* Quotas aren't otherwise kept if file is opened O_RDONLY. */
+	DQUOT_INIT(inode);
+
+	mutex_lock(&inode->i_mutex);
+	assert (atomic_read(&inode->i_mutex.count) <= 0); /* i.e. mutex_lock */
+	err = 0;
+	/* This test can succeed because down() (and I think DQUOT_INIT) can block. */
+	if (!(ei->i_flags & EXT2_COMPRBLK_FL))
+		goto out;
+
+	n_clusters = ext2_n_clusters(inode);
+	for (cluster = 0;
+	     cluster < n_clusters;
+	     cluster++) {
+		err = ext2_cluster_is_compressed_fn(inode, cluster);
+		if (err > 0) {
+			err = ext2_decompress_cluster(inode, cluster);
+			/* If we later get an error, we'll need to recompress. */
+			ei->i_flags |= EXT2_DIRTY_FL;
+			ei->i_compr_flags |= EXT2_CLEANUP_FL;
+		}
+		if (err < 0)
+			goto error;
+	}
+	assert (err >= 0);
+	err = 0;
+	ei->i_flags &= ~(EXT2_COMPRBLK_FL | EXT2_DIRTY_FL);
+	ei->i_compr_flags &= ~EXT2_CLEANUP_FL;
+ error:
+	inode->i_ctime = CURRENT_TIME;
+	mark_inode_dirty_sync(inode);
+ out:
+	mutex_unlock(&inode->i_mutex);
+	return err;
+}
+
+
+/*
+   TODO: SECRM_FL
+
+   TODO: Avant de liberer les blocs, regarder si le compteur
+   est a 1, et marquer le noeud si ce n'est pas le cas
+   (pour preparer la recompression immediate).        
+
+   pjm fixme translation.
+   "Before freeing the blocks, check if the counter is 1, 
+   and mark the inode if not (in order to prepare for
+   immediate recompression)." */
+
+/* This is called by ext2_compress_cluster to free the blocks now
+   available due to compression.  We free ,nb` blocks beginning with
+   block ,block`.  We set the address of each freed block to
+   EXT2_COMPRESSED_BLKADDR, thus marking the cluster as compressed.
+   N.B. It is up to the caller to adjust i_blocks. */
+
+/* TODO: ext2_truncate() is much more careful than this routine.
+   (E.g. it checks for bh->b_count > 1, and checks for things changing
+   underneath it.  It also calls bforget instead of brelse if it's
+   going to free it.)  Why?  Maybe we should copy it. */
+
+/* effic: Reduce the number of calls to ext2_free_block() the way
+   ext2_trunc_direct() does. */
+
+/* fixme: I think tht we do indeed need to check if buffers are held by
+   somebody else before freeing them. */
+static int ext2_free_cluster_blocks(struct inode *inode, u32 block, unsigned nb)
+{
+	u32 tmp;
+	struct ext2_bkey key;
+	int err;
+
+/*
+ * whitpa 04 Oct 2004: although it may be true that using e2compr in
+ * conjunction with quotas is a Bad Idea, having quotas enabled for other
+ * filesystems doesn't necessarily mean that the quota feature will actually be
+ * used in this one, so many people find the following assertion very annoying.
+ * I have therefore disabled it.
+ */
+/*	assert (!inode->i_sb->dq_op || (inode->i_flags & S_QUOTA)); */
+	if (!nb)
+		return 0;
+	if (nb > EXT2_MAX_CLU_NBLOCKS) {
+		assert ((int)nb >= 0);
+		assert (nb <= EXT2_MAX_CLU_NBLOCKS);
+		return -EDOM;
+	}
+	assert (((block + nb) & 3) == 0);
+	if (!ext2_get_key(&key, inode, block))
+		return -EIO;
+
+	while (nb-- > 0) {
+		tmp = ext2_get_key_blkaddr(&key);
+		err = ext2_set_key_blkaddr(&key, EXT2_COMPRESSED_BLKADDR);
+		if (err)
+			goto out;
+		if (tmp != 0) {
+			assert(tmp != EXT2_COMPRESSED_BLKADDR);
+#ifdef EXT2_COMPR_REPORT_ALLOC
+			printk(KERN_DEBUG "ext2: free %d = (%d) %d:%d:%d:%d : %d\n",
+			       key.block,
+			       key.level,
+			       key.off[0], key.off[1], key.off[2], key.off[3],
+			       tmp);
+#endif
+			ext2_free_blocks(inode, tmp, 1);
+		}
+		if (!ext2_next_key(&key, 1))
+			break;
+	}
+	err = 0;
+ out:
+	ext2_free_key(&key);
+	return err;
+}
+
+
+#if 0 /* Some possible replacement code for ext2_free_cluster_blocks(). */
+/* This does the main freeing work.  Taken from trunc_direct().
+   Returns non-zero iff we need to repeat.  Unlike
+   ext2_free_cluster_blocks(), i_blocks must be updated here. */
+/* TODO: Walk through this (including the key_ routines) and
+   ext2_truncate and check that they do exactly the same thing (wrt
+   races). */
+static int
+frob (struct inode *inode, u32 block0, u32 nblk)
+{
+	u32 * p;
+	int i, tmp;
+	struct buffer_head * bh;
+	unsigned long block_to_free = 0;
+	unsigned long free_count = 0;
+	int retry = 0;
+	int dblocks = inode->i_sb->s_blocksize / 512;
+
+
+	for (i = block0 ; i < block0 + nblk ; i++) {
+		p = EXT2_I(inode)->i_data + i;
+		tmp = *p;
+		if (!tmp)
+			continue;
+		bh = get_hash_table (inode->i_rdev, tmp,
+				     inode->i_sb->s_blocksize);
+		if ((bh && bh->b_count != 1) || tmp != *p) {
+			retry = 1;
+			brelse (bh);
+			continue;
+		}
+		*p = 0;
+		inode->i_blocks -= blocks;
+		mark_inode_dirty_sync(inode);
+		bforget(bh);
+		if (free_count == 0) {
+			block_to_free = tmp;
+			free_count++;
+		} else if (free_count > 0 && block_to_free == tmp - free_count)
+			free_count++;
+		else {
+			ext2_free_blocks (inode, block_to_free, free_count);
+			block_to_free = tmp;
+			free_count = 1;
+		}
+	}
+	if (free_count > 0)
+		ext2_free_blocks (inode, block_to_free, free_count);
+	return retry;
+}
+/* Free the blocks associated with a particular area of a file.
+   `truncate' is of course the wrong word (in that we don't do
+   anything to the blocks after the specified area) but the code is
+   essentially the same as ext2_truncate() -- in fact, I hope that
+   ext2_truncate() will be changed to call this routine. */
+static int
+ext2_truncate_region(struct inode *inode,
+		     u32 block0,
+		     u32 nblk)
+{
+	while (1) {
+
+		if (!retry)
+			break;
+		if (IS_SYNC(inode) && (inode->i_state & I_DIRTY))
+			ext2_sync_inode (inode);
+		current->counter = 0;
+		schedule ();
+	}
+
+}
+#endif /* replacement for ext2_free_cluster_blocks() */
+
+
+#ifndef NDEBUG
+static unsigned count_bits(unsigned char *p, unsigned nb)
+{
+	u32 x = le32_to_cpu(*(u32 *)p);
+	unsigned n = 0;
+
+	assert (nb <= 4);
+	if (nb != 4)
+		x &= (1 << (nb * 8)) - 1;
+	while (x) {
+		x &= (x - 1);
+		n++;
+	}
+	return n;
+}
+
+#endif /* !NDEBUG */
+
+/*
+ * __remove_compr_assoc_queue is used in invalidate_inode_buffers
+ * replacement code for ext2_compress_cluster(). TLL 02/21/07
+ * Yeah, it is duplicate code, but using it does not require
+ * patching fs/buffer.c/__remove_assoc_queue to export it.
+ * The buffer's backing address_space's private_lock must be held.
+ */
+static inline void __remove_compr_assoc_queue(struct buffer_head *bh)
+{
+	list_del_init(&bh->b_assoc_buffers);
+}
+
+/* Compress one cluster.  If the cluster uses fewer blocks once
+   compressed, it is stored in place of the original data.  Unused
+   blocks are freed, and the cluster is marked as compressed.
+
+   Returns a negative value on error,
+   0 if the cluster does not compress well,
+   positive if it is compressed (whether it was already compressed
+   or whether we compressed it).
+ 
+   Assume inode is writable.
+ 
+   Called by :
+ 
+         ext2_cleanup_compressed_inode () [i_sem] 
+
+   If ever we acquire new callers, make sure that quotas are
+   initialised, and COMPRBLK is handled correctly (i.e. such
+   that ioctl() can't change the cluster size on us), and that caller
+   tests for ext2_wa==NULL.
+*/
+
+int ext2_compress_cluster(struct inode *inode, u32 cluster)
+{
+	struct buffer_head *bh[EXT2_MAX_CLUSTER_BLOCKS + 1];
+	struct page *pg[EXT2_MAX_CLUSTER_PAGES];
+	int s_nblk;  /* Equals clu_nblocks less any trailing hole blocks. */
+	unsigned u_nblk=(~(unsigned)0), c_nblk; /* Number of blocks occupied by
+				    un/compressed data. */
+	int result, n, x;
+	int ulen, maxclus=0, maxlen=0, clen=0;
+	unsigned char *dst;
+	u8 *src;
+	unsigned meth, alg;
+	int nbh = 0, npg, i;
+	unsigned char holemap_nbytes = 0;
+	unsigned last_hole_pos;
+	struct ext2_cluster_head *head;
+	int	osync_already;
+	unsigned	r_nblk;
+	struct ext2_inode_info *ei = EXT2_I(inode);
+	unsigned long saved_isize;
+
+	/* impl: Otherwise, ioctl() could change the cluster size
+	   beneath us. */
+	assert (ei->i_flags & EXT2_COMPRBLK_FL);
+
+	assert (atomic_read(&inode->i_mutex.count) <= 0); /* i.e. mutex_lock */
+
+	npg = ext2_cluster_npages(inode, cluster);
+
+	result = ext2_get_cluster_pages(inode, cluster, pg, NULL, 1);
+	if (result <= 0)
+	  goto done;
+
+	/* effic: We ought to use the page cache.  Using the page
+	   cache always costs extra CPU time, but saves I/O if the
+	   page is present.  We still need to detect holes, which
+	   unfortunately may still cause I/O.  Testing for all-zero
+	   could save us that I/O. */
+	nbh = ext2_get_cluster_blocks(inode, cluster, bh, pg, NULL, 1);
+	if (nbh <= 0)
+	  {
+	    result = nbh;
+	    goto done;
+	  }
+	s_nblk = nbh;
+
+#ifdef EXT2_COMPR_REPORT_VERBOSE
+	  {
+	    int i;
+	  
+	    printk(KERN_DEBUG "ext2_compress_cluster[begin]: buffers get for cluster=%d, inode=%ld, size=%d\n", cluster, inode->i_ino, (int)inode->i_size);
+	    for (i=0; i<s_nblk; i++)
+	      {
+		if (bh[i])
+		  {
+		    printk(KERN_DEBUG "bbuffer_head[%d]: blocknr=%lu, addr=0x%p ", i, (unsigned long) bh[i]->b_blocknr, bh[i]);
+		    if (bh[i]->b_page)
+		      printk(KERN_DEBUG "bgn:[page->index=%ld]\n", bh[i]->b_page->index);
+		    else
+		      printk(KERN_DEBUG "[No page]\n");
+		  }
+		else
+		  printk("bbuffer_head[%d] is NULL\n", i);
+	      }
+	  }
+#endif
+	/*
+	 *  Did somebody else compress the cluster while we were waiting ?
+	 *  This should never arise ...
+	 */
+	result = ext2_cluster_is_compressed_fn(inode, cluster);
+	if (result != 0) {
+		if (result > 0) {
+			ext2_warning(inode->i_sb,
+				     "ext2_compress_cluster",
+				     "compressing compressed cluster");
+		}
+		goto done;
+	}
+
+	/* I moved it here in case we need to load a module that
+	 * needs more heap that is currently allocated.
+	 * In such case "init_module" for that algorithm forces
+	 * re-allocation of ext2_wa. It should be safe here b/c the
+	 * first reference to ext2_wa comes just after and we have
+	 * locked ext2_wa before.
+	 *
+	 * I know that we may not need the compression at all
+	 * (compressing 0 or 1 block) but it's better to sacrifice
+	 * a bit than do make a total mess of this code.
+	 *
+	 * FIXME: Totally separate working areas for reading and writing.
+	 * 	Jan Rêkorajski
+	 */
+
+	meth = ei->i_compr_method;
+	assert (meth < EXT2_N_METHODS);
+	alg = ext2_method_table[meth].alg;
+#ifdef CONFIG_KMOD
+	if (!ext2_algorithm_table[alg].avail) {
+		char str[32];
+
+		sprintf(str, "ext2-compr-%s", ext2_algorithm_table[alg].name);
+		request_module(str);
+	}
+#endif
+
+	/*
+	 *  Try to get the working area.
+	 */
+	assert (ext2_wr_wa != NULL); /* Should have been tested by caller. */
+	result = -EINTR;
+	if (ext2_lock_wr_wa() == 0)
+		goto done;	/* Signal pending. */
+
+#ifdef EXT2_COMPR_REPORT_WA
+	printk(KERN_DEBUG "pid %d enters critical region\n", current->pid);
+#endif
+
+	/*
+	 * Now, we try to compress the cluster.  If the cluster does
+	 *    not compress well, we just give up.  Otherwise, we reuse
+	 *    the old blocks to store the compressed data (except that
+	 *    compressed data is contiguous in the file even if the
+	 *    uncompressed data had holes).
+	 */
+
+	/*
+	 *  Compute the block bitmap, how many bytes of data we have
+	 *    in the cluster, and the maximum interesting length after
+	 *    compression.  The bitmap will be used to reallocate blocks
+	 *    when decompressing the cluster, so that we don't create blocks
+	 *    that were previously missing.  We also pack the buffers
+	 *    together.
+	 */
+
+	head = (struct ext2_cluster_head *) ext2_wr_wa->c;
+#if EXT2_MAX_CLUSTER_BLOCKS > 32
+# error "We need to zero more bits than this."
+#endif
+	*(u32 *)(&head[1]) = 0;
+	last_hole_pos = (unsigned) (-1);
+	assert(head->holemap[0] == 0);
+	assert(head->holemap[1] == 0);
+	assert(head->holemap[2] == 0);
+	assert(head->holemap[3] == 0);
+	assert(*(u32 *)head->holemap == 0);
+	assert(count_bits(head->holemap, 4) == 0);
+
+	/* TODO: Check that i_size can't change beneath us.
+	   do_truncate() is safe because it uses i_sem around changing
+	   i_size.  For the moment, I do a runtime check. */
+
+	saved_isize = inode->i_size;
+
+#ifdef EXT2_COMPR_REPORT_VERBOSE
+	printk ("00 ext2_compress_cluster[%u]: i_size=%u, s_blocksize_bits=%u, s_nblk=%u\n", __LINE__, (unsigned) inode->i_size, inode->i_sb->s_blocksize_bits, s_nblk);
+#endif
+//	assert (ROUNDUP_RSHIFT(inode->i_size, inode->i_sb->s_blocksize_bits)
+//		>= s_nblk);
+	/* This initial guess at ulen doesn't take holes into account
+	   unless they're at end of cluster.  We ,compensate for other
+	   holes` during the loop below. */
+	ulen = MIN(s_nblk << inode->i_sb->s_blocksize_bits,
+		   inode->i_size - ext2_cluster_offset(inode, cluster));
+	r_nblk = (((ulen - 1) >> inode->i_sb->s_blocksize_bits) + 1);
+	if (r_nblk  <= 1) {
+	  goto no_compress;
+	}
+	/* Verify if more than 1 block to compress in the cluster	*/
+	nbh = 0;
+	for (x = 0; x < s_nblk; x++) {
+	  if ((bh[x] != NULL) && (bh[x]->b_blocknr != 0)) {
+	    nbh++;
+	  } else {
+	    last_hole_pos = x;
+	    head->holemap[x >> 3] |= 1 << (x & 7);
+	    ulen -= inode->i_sb->s_blocksize;
+	    /* impl: We know that it's a whole block because
+	       ext2_get_cluster_blocks trims s_nblk for trailing
+	       NULL blocks, and partial blocks only come at
+	       the end, so there can't be partial NULL blocks. */
+	  }
+	}
+	/* We don't try to compress cluster that only have one block
+	   or no block at all.  (When fragments are implemented, this code
+	   should be changed.) */
+	if (nbh <= 1)
+		goto no_compress;
+
+	u_nblk = nbh;
+	/* Copy the data in the compression area	*/
+	dst = ext2_wr_wa->u;
+	for (x = 0; x < s_nblk; x++) {
+	  if ((bh[x] != NULL) && (bh[x]->b_blocknr != 0)) {
+	    memcpy(dst, bh[x]->b_data, bh[x]->b_size);
+	    dst += bh[x]->b_size;
+	  }
+	}
+	ext2_wr_wa_ucontents.ino = inode->i_ino;
+	ext2_wr_wa_ucontents.dev = inode->i_rdev;
+	ext2_wr_wa_ucontents.cluster = cluster;
+
+	assert(count_bits(head->holemap, 4) == s_nblk - u_nblk);
+
+#if EXT2_GRAIN_SIZE != EXT2_MIN_BLOCK_SIZE
+# error "this code ought to be changed"
+#endif
+	/* ,maxlen` is the maximum length that the compressed data can
+	   be while still taking up fewer blocks on disk. */
+	holemap_nbytes = (last_hole_pos >> 3) + 1;
+	/* impl: Remember that ,last_hole_pos` starts off as being -1,
+	   so the high 3 bits of ,last_hole_pos >> 3` can be wrong.
+	   This doesn't matter if holemap_nbytes discards the high
+	   bits. */
+	assert (sizeof(holemap_nbytes) < sizeof(unsigned));
+	assert ((last_hole_pos == (unsigned) -1)
+	        == (holemap_nbytes == 0));
+	maxlen = ((((r_nblk < u_nblk) ? r_nblk : u_nblk) - 1) * inode->i_sb->s_blocksize
+		  - sizeof(struct ext2_cluster_head)
+		  - holemap_nbytes);
+	/* bug fix: e2fsck inode->i_size uncompressed cluster size TLL 02/21/07
+	 * calculate the max cluster data size */
+	maxclus = (ei->i_clu_nblocks * inode->i_sb->s_blocksize
+		  - sizeof(struct ext2_cluster_head)
+		  - (sizeof(unsigned) * 256)); /* 256 = max number of holemap_nbytes values */
+	clen = 0;
+	/* Handling of EXT2_AUTO_METH at the moment is just that we
+	   use the kernel default algorithm.  I hope that in future
+	   this can be extended to the kernel deciding when to
+	   compress and what algorithm to use, based on available disk
+	   space, CPU time, algorithms currently used by the fs,
+	   etc. */
+	if ((meth == EXT2_AUTO_METH)
+	    || !ext2_algorithm_table[alg].avail) {
+		meth = EXT2_DEFAULT_COMPR_METHOD;
+		alg = ext2_method_table[meth].alg;
+		assert (ext2_algorithm_table[alg].avail);
+	}
+	if (alg == EXT2_NONE_ALG)
+		goto no_compress;
+	clen = ext2_algorithm_table[alg].compress
+		(ext2_wr_wa->u,
+		 ext2_wr_wa->c + sizeof(struct ext2_cluster_head) + holemap_nbytes,
+		 ext2_wr_wa->heap,
+		 ulen,
+		 maxlen,
+		 ext2_method_table[meth].xarg);
+#ifdef EXT2_COMPR_REPORT_ALGORITHMS
+	printk(KERN_DEBUG "03 ext2: %lu: cluster %d+%d [%d] compressed "
+	       "into %d bytes (ulen = %d, maxlen = %d, maxclus = %d)\n",
+	       inode->i_ino,
+	       ext2_cluster_offset(inode, cluster),
+	       ext2_cluster_nblocks(inode, cluster),
+	       u_nblk, clen, ulen, maxlen, maxclus);
+#endif
+
+	if ((clen == 0) || (clen > maxlen)) {
+no_compress:
+		/* this chunk didn't compress. */
+		assert(inode->i_size == saved_isize);
+
+		/* e2fsck inode->i_size uncompressed cluster bug fix TLL 02/21/07
+		 * maxclus is only set on an attempt to compress. For
+		 * undersized files i_size is set to filesize.
+		 */
+		/* set a calculated i_size
+		 * to 4096, 8192 or ?  */
+		if (maxclus)
+			inode->i_size = ulen;
+
+#ifdef EXT2_COMPR_REPORT_WA
+		printk(KERN_DEBUG 
+			"pid %d leaves critical region, nbh=%d, u_nblk=%d, "
+			"inode->i_size=%lu, saved_isize=%lu, clen=%d, ulen=%d, maxlen=%d, maxclus=%d\n",
+			current->pid, nbh, u_nblk,
+			(long unsigned)inode->i_size, saved_isize, clen, ulen, maxlen, maxclus);
+#endif
+		result = 0;
+		ext2_unlock_wr_wa();
+		goto done;
+	}
+
+// [UH] start: verify compression moved here
+#ifdef CONFIG_EXT2_VERIFY_COMPRESSION
+	/* */
+	{
+		size_t dlen;
+		size_t comp_len;
+		char bdn[BDEVNAME_SIZE];
+
+		memset (ext2_wr_wa->u, 0, ulen);
+		// [UH] length corrected to data length
+		/* FIXME: This requires that the wr_wa have at least as much
+		   heap space as rd_wa (i.e. enough space to decompress). 
+		   Alternatively, consider using rd_wa here, but that requires 
+		   gaining the lock. */
+		dlen = ext2_algorithm_table[alg].decompress(
+			 ext2_wr_wa->c + sizeof(struct ext2_cluster_head) + holemap_nbytes,
+			 ext2_wr_wa->u,
+			 ext2_wr_wa->heap,
+			 clen,
+			 ulen,
+			 ext2_method_table[meth].xarg);
+		if (dlen != ulen) {
+			printk(KERN_ERR
+			       "compress.c: %s/%u error detected!  "
+			       "dlen=%u, ulen=%u, pid=%d, dev=%s, ino=%lu, clu=%u.  "
+			       "Please report to e2compr list.\n",
+			       ext2_algorithm_table[alg].name,
+			       ext2_method_table[meth].xarg,
+			       dlen, ulen, current->pid, bdevname(inode->i_sb->s_bdev, bdn), inode->i_ino, cluster);
+			goto no_compress;
+		}
+		dst = ext2_wr_wa->u;
+		x = 0;
+		comp_len = 0;
+		while (comp_len < ulen) {
+		  if ((bh[x] != NULL) && (bh[x]->b_blocknr != 0)) {
+		    if (memcmp(dst, bh[x]->b_data, MIN(bh[x]->b_size, ulen-comp_len)) != 0) {
+		      printk(KERN_ERR
+			     "compress.c: %s/%u error detected!  "
+			     "pid=%d, dev=%s, ino=%lu, clu=%u.  "
+			     "Please report to e2compr list.\n",
+			     ext2_algorithm_table[alg].name,
+			     ext2_method_table[meth].xarg, 
+			     current->pid, bdevname(inode->i_sb->s_bdev, bdn), inode->i_ino, cluster);
+		      goto no_compress;
+		    }
+		    dst += bh[x]->b_size;
+		    comp_len += bh[x]->b_size;
+		  }
+		  x++;
+		}
+	}
+#endif /* verify compression */
+// [UH] end
+
+#if EXT2_MAX_CLUSTER_BLOCKS > 32
+# error "We need to zero more bits than this."
+#endif
+	assert(-1 <= (int) last_hole_pos);
+	assert((int) last_hole_pos < 32);
+	assert((le32_to_cpu(*(u32 *)head->holemap)
+	        & (~0u << (1 + last_hole_pos))
+	        & (~(~0u << (8*holemap_nbytes))))
+	       == 0);
+	/* Don't change "~0u << (1 + last_hole_pos)" to "~1u << last_hole_pos" 
+	   as I almost did, as last_hole_pos can be -1 and cannot be 32. */
+	assert(count_bits(head->holemap, holemap_nbytes) == s_nblk - u_nblk);
+
+	/* Compress the blocks at the beginning of the cluster	*/
+	for (x = 0, nbh = 0; x < s_nblk; x++) {
+	  if ((bh[x] != NULL) && (bh[x]->b_blocknr != 0)) {
+	    if (nbh != x) {
+	      bh[nbh]->b_blocknr = bh[x]->b_blocknr;
+	      set_bit(BH_Mapped, &bh[nbh]->b_state);
+	      bh[x]->b_blocknr = 0;
+	      assert(buffer_mapped(bh[x]));
+	      clear_bit(BH_Mapped, &bh[x]->b_state);
+	    }
+	    nbh++;
+	  }
+	}
+	assert(nbh == u_nblk);
+	assert(count_bits(head->holemap, holemap_nbytes) == s_nblk - u_nblk);
+
+	/*
+	 * Compression was successful, so add the header and copy to blocks.
+	 */
+
+	/* Header. */
+	{
+		head->magic = cpu_to_le16(EXT2_COMPRESS_MAGIC_04X);
+		head->method = meth;
+		head->holemap_nbytes = holemap_nbytes;
+		head->ulen = cpu_to_le32(ulen);
+		head->clen = cpu_to_le32(clen);
+		barrier();
+		head->checksum = cpu_to_le32(
+			ext2_adler32 (le32_to_cpu(*(u32 *) ext2_wr_wa->c),
+				      ext2_wr_wa->c + 8,
+				      (sizeof(struct ext2_cluster_head) - 8
+				       + head->holemap_nbytes + clen)));
+	}
+
+	assert((le32_to_cpu(*(u32 *)head->holemap)
+		& (~0 << (1 + last_hole_pos))
+		& ((1 << (8*holemap_nbytes))-1)) == 0);
+	result = clen += sizeof(struct ext2_cluster_head) + holemap_nbytes;
+	c_nblk = ROUNDUP_RSHIFT(clen, inode->i_sb->s_blocksize_bits);
+
+#undef EXT2_REALLOC_BLOCKS
+#ifdef EXT2_REALLOC_BLOCKS
+	/* Deallocate existing blocks and reallocate new ones. */
+	/* TODO: Should we lock_super()?  The aim would be to prevent
+	   someone else from stealing our blocks, leaving us with
+	   ENOSPC or having the machine crash before we write the new
+	   blocks. */
+	while (nbh > 0)
+		if (bh[--nbh])
+			brelse(bh[nbh]);
+
+	/* TODO: Check return code. */
+	(void)
+	ext2_free_cluster_blocks(inode,
+				 ext2_cluster_block0(inode, cluster),
+				 ext2_cluster_nblocks(inode, cluster));
+
+	/* Allocate new blocks. */
+	assert(0 < c_nblk);
+	assert(c_nblk < u_nblk);
+	assert(u_nblk <= ext2_cluster_nblocks(inode, cluster));
+	for (nbh = 0; nbh < c_nblk; nbh++) {
+		int err;
+
+		err = ext2_get_block(inode,
+				     ext2_cluster_block0(inode, cluster) + nbh,
+				     bh[nbh],
+				     1 /* create */);
+# ifdef EXT2_COMPR_REPORT_ALLOC
+		printk(KERN_DEBUG "ext2: compress cluster allocated block = %ld:%ld\n",
+		       ext2_cluster_block0(inode, cluster) + nbh,
+		       bh[nbh]->b_blocknr);
+# endif
+	}
+#else /* !EXT2_REALLOC_BLOCKS */
+	/* Release unneeded buffer heads.  (Freeing is done later,
+           after unlocking ext2_wr_wa.) */
+	assert (nbh == u_nblk);
+	nbh = c_nblk;
+#endif /* !EXT2_REALLOC_BLOCKS */
+	trace_e2c("ext2_compress_cluster: head->clen=%d, clen=%d\n", head->clen, clen);
+	src = ext2_wr_wa->c;
+	for (n = 0; (int) clen > 0; n++) {
+		if (clen >= inode->i_sb->s_blocksize)
+			memcpy(bh[n]->b_data, src, inode->i_sb->s_blocksize);
+		else
+			memcpy(bh[n]->b_data, src, clen);
+
+		/* TODO: O_SYNC */
+
+		set_buffer_uptodate(bh[n]);
+		mark_buffer_dirty(bh[n]);
+		spin_lock(&(bh[n]->b_page->mapping->private_lock));
+		list_move_tail(&(bh[n]->b_assoc_buffers),
+		    &(inode->i_mapping->private_list));
+		spin_unlock(&(bh[n]->b_page->mapping->private_lock));
+
+		src += inode->i_sb->s_blocksize;
+		clen -= inode->i_sb->s_blocksize;
+	}
+/*Add by Yabo Ding<bobfree_cn@yahoo.com.cn>,<yding@wyse.com>
+Force write to disk.
+*/
+	if (!IS_SYNC(inode)) {
+		ll_rw_block(WRITE, nbh, bh);
+		for (i = 0; i < nbh; i++){
+			if (bh[i])
+				wait_on_buffer(bh[i]);
+		}
+	}
+
+	i = 0;	
+	assert (n == c_nblk);
+	assert((le32_to_cpu(*(u32 *)head->holemap)
+	        & (~0 << (1 + last_hole_pos))
+		& ((1 << (8*holemap_nbytes))-1)) == 0);
+
+	/* Runtime check that no-one can change i_size while i_sem is down.
+	   (See where saved_isize is set, above.) */
+	assert(inode->i_size == saved_isize);
+
+#ifndef EXT2_REALLOC_BLOCKS
+	/* Free the remaining blocks, and shuffle used blocks to start
+	   of cluster in blkaddr array. */
+	{
+		u32 free_ix, curr;
+		int err;
+
+		/* Calculate free_ix.  There should be ,c_nblk`
+		   non-hole blocks among the first ,free_ix`
+		   blocks. */
+		{
+			assert((le32_to_cpu(*(u32 *)head->holemap)
+				& (~0 << (1 + last_hole_pos))
+				& ((1 << (8*holemap_nbytes))-1)) == 0);
+			assert (n == c_nblk);
+			for (free_ix = 0;
+			     ((int) free_ix <= (int) last_hole_pos) && (n > 0);
+			     free_ix++)
+				if (!(head->holemap[free_ix >> 3]
+				      & (1 << (free_ix & 7))))
+					n--;
+			free_ix += n;
+
+			if ((free_ix < c_nblk)
+			    || (free_ix + u_nblk > s_nblk + c_nblk)
+			    || (free_ix >= ext2_cluster_nblocks(inode, cluster))
+			    || ((holemap_nbytes == 0) && (c_nblk != free_ix))) {
+			  assert (free_ix >= c_nblk);
+			  /*assert (free_ix - c_nblk <= s_nblk - u_nblk);*/
+			  assert (free_ix + u_nblk <= s_nblk + c_nblk);
+			  assert (free_ix < ext2_cluster_nblocks(inode, cluster));
+			  assert ((holemap_nbytes != 0) || (c_nblk == free_ix));
+			  assert (1 <= c_nblk);
+			  assert (c_nblk < u_nblk);
+			  assert (u_nblk <= s_nblk);
+			  assert (s_nblk <= ext2_cluster_nblocks(inode, cluster));
+			  assert (ext2_cluster_nblocks(inode, cluster) <= EXT2_MAX_CLU_NBLOCKS);
+			  ext2_error(inode->i_sb, "ext2_compress_cluster",
+				     "re assertions: c=%d, u=%d, f=%d, s=%d, n=%d, "
+				     "lhp=%d, hm=%x, hnb=%d, "
+				     "ino=%lu, clu=%u",
+				     (int) c_nblk, (int)u_nblk, (int)free_ix, (int)s_nblk,
+				     (int) ext2_cluster_nblocks(inode,cluster),
+				     (int) last_hole_pos, 
+				     (unsigned) le32_to_cpu(*(u32 *)head->holemap),
+				     (int) holemap_nbytes,
+				     inode->i_ino, cluster);
+			}
+		}
+
+		/* Free unneeded blocks, and mark cluster as
+                   compressed. */
+		err = ext2_free_cluster_blocks
+			(inode,
+			 ext2_cluster_block0(inode, cluster) + free_ix,
+			 ext2_cluster_nblocks(inode, cluster) - free_ix);
+		/* pjm 1998-06-15: This should help reduce fragmentation.
+		   Actually, we could set block to clu_block0 + clu_nbytes,
+		   and goal to the last allocated blkaddr in the compressed
+		   cluster.
+		   It would be nice if we would transfer the freed blocks
+		   to preallocation, while we're at it. */
+		write_lock(&ei->i_meta_lock);
+		ei->i_next_alloc_goal = ei->i_next_alloc_block = 0;
+		write_unlock(&ei->i_meta_lock);
+		if (err < 0) {
+			ext2_unlock_wr_wa();
+			goto done;
+		}
+		/* Note that ext2_free_cluster_blocks() marks the
+		   cluster as compressed. */
+
+		/* Shuffle used blocks to beginning of block-number array. */
+		{
+			struct ext2_bkey key;
+			unsigned i;
+
+			if (!ext2_get_key(&key,
+					  inode,
+					  ext2_cluster_block0(inode, cluster))) {
+				ei->i_flags |= EXT2_ECOMPR_FL;
+				result = -EIO;
+				free_ix = 0;
+			}
+			for (i = 0; i < free_ix; i++) {
+				curr = ext2_get_key_blkaddr(&key);
+
+				if ((c_nblk == free_ix)
+				    && (curr != bh[i]->b_blocknr))
+				  {
+				    /* "Can't happen", yet has
+				       happened a couple of times. */
+				    ext2_error(inode->i_sb,
+					       "ext2_compress_cluster",
+					       "c_nblk=free_ix=%d, "
+					       "curr=%u, b_blocknr=%lu, "
+					       "lhp=%d, hm=%x, "
+					       "ino=%lu, blk=%u",
+					       c_nblk, curr,
+					       (unsigned long) bh[i]->b_blocknr,
+					       (int) last_hole_pos, 
+					       (unsigned) le32_to_cpu(*(u32 *)head->holemap),
+					       inode->i_ino,
+					       (unsigned) ext2_cluster_block0(inode, cluster) + i);
+				  }
+				err = ext2_set_key_blkaddr(&key,
+							   (i < c_nblk
+							    ? bh[i]->b_blocknr
+							    : EXT2_COMPRESSED_BLKADDR));
+				if (err)
+					break;
+				if (!ext2_next_key(&key, 1)) {
+					ei->i_flags |= EXT2_ECOMPR_FL; /* sorry... */
+					result = -EIO;
+					break;
+				}
+			}
+			ext2_free_key(&key);
+		}
+	}
+#endif
+	/*
+	 *        Unlock the working area.
+	 */
+
+#ifdef EXT2_COMPR_REPORT_WA
+	printk(KERN_DEBUG "pid %d leaves critical region\n", current->pid);
+#endif
+
+	ext2_unlock_wr_wa();
+	head = NULL;  /* prevent any more stupid bugs */
+
+	assert (c_nblk < u_nblk);
+	ext2_mark_algorithm_use(inode, alg);
+	mark_inode_dirty_sync(inode);
+	/* COMPRBLK is already high, so no need to raise it. */
+ 	{
+ 	  int status;
+	  for (i = c_nblk; (i < EXT2_MAX_CLUSTER_BLOCKS) && (bh[i] != NULL); i++) {
+	    clear_buffer_dirty(bh[i]);
+	    bh[i]->b_blocknr = 0;
+	    clear_bit(BH_Mapped, &bh[i]->b_state);
+	    clear_bit(BH_Uptodate, &bh[i]->b_state);
+	  }
+	  for (i = 0; i < npg; i++) {
+	    if (pg[i] == NULL) {
+	      break;
+	    }
+	    ClearPageUptodate(pg[i]);
+	    if (PageDirty(pg[i]))
+	      ClearPageDirty(pg[i]);
+	    unlock_page(pg[i]);
+	    page_cache_release(pg[i]);
+	  }
+	  if (ei->i_compr_flags & EXT2_OSYNC_INODE) {
+	    osync_already = 1;
+	  } else {
+	    osync_already = 0;
+	    ei->i_compr_flags |= EXT2_OSYNC_INODE;
+	  }
+ 	  status = generic_osync_inode(inode, inode->i_mapping,
+		OSYNC_METADATA|OSYNC_DATA);
+	  if (osync_already == 0) {
+	    ei->i_compr_flags &= ~EXT2_OSYNC_INODE;
+	  }
+
+	  /* invalidate_inode_buffers replacement code: TLL 02/21/07
+	   * e2compr on post 2.6.10 kernels do not have an uptodate
+	   * mapping->assoc_mapping (other Vm(?) changes require it be
+	   * made explicit, 2.4 kernels have it implicit). Therefore, when
+	   * umount is called, a GPF ensues from a NULL ops pointer.
+	   * e2c on a USB thumbdrive mounted as the root fs does not
+	   * support repeated compress/uncompress cycles on a given file.
+	   * Inlined the flush list code to explicityly force update to
+	   * disk with a known valid bh list.
+	   */
+
+	  if (inode_has_buffers(inode)) {
+#define BH_ENTRY(list) list_entry((list), struct buffer_head, b_assoc_buffers)
+
+		struct address_space *mapping = &inode->i_data;
+		struct list_head *list = &mapping->private_list;
+		struct buffer_head *bh = BH_ENTRY(list->next);
+		struct address_space *buffer_mapping = bh->b_page->mapping;
+
+		/*
+		 * update inode buffers upon completion rather than at 
+		 * umount with fs/buffer.c/invalidate_inode_buffers.
+		 */
+
+		spin_lock(&buffer_mapping->private_lock);
+
+		while (!list_empty(list))
+			__remove_compr_assoc_queue(BH_ENTRY(list->next));
+
+		spin_unlock(&buffer_mapping->private_lock);
+	  }
+
+ 	  if (status)
+ 	    trace_e2c("ext2_compress_cluster - generic_osync_inode: status=%d\n", status);
+ 	}
+	return result;
+
+done:
+	{
+	  for (i = 0; i < npg; i++) {
+	    if (pg[i] == NULL) {
+	      break;
+	    }
+	    ClearPageUptodate(pg[i]);
+/* 	      remove_inode_page(bh[i]->b_page); */
+	    unlock_page(pg[i]);
+/* 	      clear_bit(PG_locked, &bh[i]->b_page->flags); */
+	    page_cache_release(pg[i]);
+	  }
+	  /* TLL cp to compr dir bug fix 03-25-07
+		 Truncate uncompressed files to their uncompressed
+		 length, i.e. force kernel to update inode and sb */
+
+	  ext2_truncate(inode);
+	}
+#ifdef EXT2_COMPR_REPORT_VERBOSE
+	  {
+	    int i;
+	  
+	    printk(KERN_DEBUG "ext2_compress_cluster[end]: buffers kept for cluster=%d\n", cluster);
+	    for (i=0; i<nbh; i++)
+	      {
+		if (bh[i])
+		  {
+		    printk(KERN_DEBUG "2buffer_head[%d]: blocknr=%lu, addr=0x%p ",
+		    		i, (unsigned long) bh[i]->b_blocknr, bh[i]);
+		    if (bh[i]->b_page)
+		      printk(KERN_DEBUG "2:[page->index=%ld]\n", bh[i]->b_page->index);
+		    else
+		      printk(KERN_DEBUG "[No page]\n");
+		  }
+		else
+		  printk(KERN_DEBUG "buffer_head[%d] is NULL\n", i);
+	      }
+	  }
+#endif
+
+	return result;
+}
+
+
+/* Go through all the clusters and compress them if not already
+   compressed.
+
+   This is called by ext2_put_inode() and ext2_release_file().  Later,
+   we may have ext2_ioctl() call it (when EXT2_COMPR_FL rises).  None
+   of the callers does any locking, so we do it here.
+
+   Neither of the current callers uses the return code, but we get ready
+   for if we start using it.
+
+   Returns 0 on "success" (whether or not we cleared EXT2_CLEANUP_FL
+   or EXT2_DIRTY_FL bits), -errno on error. */
+int ext2_cleanup_compressed_inode(struct inode *inode)
+{
+	u32 cluster;
+	u32 n_clusters;
+	int dirty = 0;
+	int err = 0;
+	u32 comprblk_mask;
+	atomic_t start_i_count = inode->i_count;
+	int retry = 0;
+	int have_downed;
+	struct ext2_inode_info *ei = EXT2_I(inode);
+	char bdn[BDEVNAME_SIZE];
+
+	/* impl: Actually, this assertion could fail if the kernel
+	   isn't locked.  I haven't looked, but I suppose that the
+	   kernel always is locked when this is called. */
+	assert (ei->i_compr_flags & EXT2_CLEANUP_FL);
+
+#ifdef EXT2_COMPR_REPORT_PUT
+	printk(KERN_DEBUG
+	       "ext2_cleanup_compressed_inode() called for pid %d; "
+	       "dev=%s, ino=%lu, i_state=0x%lx, i_count=%u\n",
+	       current->pid, bdevname(inode->i_sb->s_bdev, bdn), inode->i_ino,
+	       inode->i_state, atomic_read(&inode->i_count));
+#endif
+
+	/* Do these tests twice: once before down() and once after. */
+	for(have_downed = 0; ; have_downed++) {
+		if ((ei->i_flags
+		     & (EXT2_COMPR_FL | EXT2_DIRTY_FL))
+		    != (EXT2_COMPR_FL | EXT2_DIRTY_FL)) {
+			if (have_downed)
+				goto out;
+			printk(KERN_WARNING
+			       "ext2_cleanup_compressed_inode: trying to un/compress an "
+			       "already open file. Close the file and re-run chattr. "
+			       "i_flags=%#x. (dev=%s, ino=%lu, down=%d)\n",
+			       ei->i_flags, bdevname(inode->i_sb->s_bdev, bdn), 
+			       inode->i_ino, have_downed);
+			return 0;
+		}
+
+		if (mapping_mapped(inode->i_mapping))
+		  /* File is mapped by mmap */
+		  {
+		    trace_e2c("ext2_cleanup_compressed_inode: (dev. %s): ino=%ld: file mapped, does not compress cluster\n", bdevname(inode->i_sb->s_bdev, bdn), inode->i_ino);
+		    if (have_downed)
+		      goto out;
+		    else
+		      return 0;
+		  }
+
+		if (IS_RDONLY(inode)
+		    || (ei->i_flags & EXT2_ECOMPR_FL)
+		    || (ext2_wr_wa == NULL)) {
+			ei->i_compr_flags &= ~ EXT2_CLEANUP_FL;
+			if (have_downed)
+				goto out;
+			else
+				return 0;
+		}
+
+		if (have_downed)
+			break;
+
+		/* Quotas aren't otherwise kept if file is opened O_RDONLY. */
+		DQUOT_INIT(inode);
+
+		if (!(ei->i_compr_flags & EXT2_OSYNC_INODE)) {
+		  mutex_lock(&inode->i_mutex);
+		}
+	}
+
+	n_clusters = ext2_n_clusters(inode);
+
+#ifdef EXT2_COMPR_REPORT_PUT
+	printk(KERN_DEBUG "ext2: %lu: put compressed, clusters = %d, flags = %x\n",
+	       inode->i_ino, n_clusters, ei->i_flags);
+#endif
+
+	assert (atomic_read(&inode->i_mutex.count) <= 0); /* i.e. mutex_lock */
+
+	/* Try to compress the clusters.  We clear EXT2_DIRTY_FL only
+	   if we looked at every cluster and if there was no error.  */
+
+	/* impl: We raise EXT2_COMPRBLK_FL now so that ext2_ioctl()
+	   doesn't try to change the cluster size beneath us.  If need
+	   be, we restore the bit to its original setting before
+	   returning.  Note that no-one else can _change_
+	   EXT2_COMPRBLK_FL while we work because i_sem is down. */
+	/* impl: Note what's happening here with comprblk_mask.  The
+	   current state of COMPRBLK_FL (before we start) is that
+	   (comprblk == 1) || (no compressed clusters).  At the end of
+	   the procedure, comprblk == one iff (at least one compressed
+	   cluster, or an error occurred preventing us from finding
+	   out). */
+	comprblk_mask = ~EXT2_COMPRBLK_FL | ei->i_flags;
+	ei->i_flags |= EXT2_COMPRBLK_FL;
+
+	for (cluster = 0;
+	     cluster < n_clusters;
+	     cluster++) {
+		if (atomic_read(&inode->i_count) > atomic_read(&start_i_count)) {
+			/* This is a poor way of doing this (and doubly
+			   poor now that the only users of i_count are
+			   the dentries), but the idea is not to
+			   compress things tht are likely to be
+			   decompressed soon.  I guess a better way of
+			   doing this would be just to make sure tht
+			   the stuff is in the page cache. */
+			retry = 1;
+			break;
+		}
+		err = ext2_cluster_is_compressed_fn(inode, cluster);
+		if (err == 0) {
+			err = ext2_compress_cluster(inode, cluster);
+			if (err < 0)
+				dirty = 1;
+			else if (err > 0)
+				comprblk_mask = ~0ul;
+		} else if (err < 0)
+			break;
+		else {
+			err = 0;
+			assert(comprblk_mask == ~0ul); /* i.e. that EXT2_COMPRBLK_FL was high. */
+		}
+	}
+
+	if ((cluster >= n_clusters) && !dirty)
+		ei->i_flags &= ~EXT2_DIRTY_FL;
+	if (!retry) {
+		ei->i_compr_flags &= ~EXT2_CLEANUP_FL;
+		ei->i_flags &= comprblk_mask;
+	}
+
+	/* We clear EXT2_CLEANUP_FL because, otherwise, we'll get
+           called again almost immediately. */
+
+	/*
+	 *  The CLEANUP flag *MUST* be cleared, otherwise the iput routine
+	 *  calls ext2_put_inode() again (because i_dirt is set) and there
+	 *  is a loop.  The control scheme (CLEANUP + DIRTY flags) could 
+	 *  probably be improved.  On the other hand, i_dirt MUST be set
+	 *  because we may have sleeped, and we must force the iput routine
+	 *  to look again at the i_count ...
+	 */
+	/* TODO: Have a look at this cleanup scheme.  The above
+           comment sounds wrong. */
+
+	inode->i_ctime = CURRENT_TIME;
+	mark_inode_dirty_sync(inode);
+ out:
+	if (!(ei->i_compr_flags & EXT2_OSYNC_INODE)) {
+	  mutex_unlock(&inode->i_mutex);
+	}
+	return err;  /* TODO: Check that ,err` is appropriate. */
+}
+
+
+int
+ext2_recognize_compressed(struct inode *inode, unsigned cluster) 
+{
+	/* ext2_recognize_compressed(): Check tht the cluster is valid
+	   in every way, and then do the EXT2_COMPRESSED_BLKADDR
+	   thing. */
+	/* nyi, fixme.  All of the userspace stuff (EXT2_NOCOMPR_FL
+	   etc.) needs work, so I might as well leave this.  See
+	   ioctl.c for a description of what it's supposed to do. */
+	return -ENOSYS;
+}
+
+
+/* Look for compressed clusters.  If none, then clear EXT2_COMPRBLK_FL.
+
+   Called by:
+       ext2_truncate().
+       */
+void
+ext2_update_comprblk(struct inode *inode)
+{
+	unsigned block, last_block;
+	struct ext2_bkey key;
+	struct ext2_inode_info *ei = EXT2_I(inode);
+
+	assert (ei->i_flags & EXT2_COMPRBLK_FL);
+	if (inode->i_size == 0) {
+		ei->i_flags &= ~EXT2_COMPRBLK_FL;
+		return;
+	}
+	last_block = ROUNDUP_RSHIFT(inode->i_size,
+				    inode->i_sb->s_blocksize_bits) - 1;
+	block = ext2_first_cluster_nblocks(inode) - 1;
+	if (!ext2_get_key(&key, inode, block))
+		return;
+	for (;;) {
+		if (ext2_get_key_blkaddr(&key) == EXT2_COMPRESSED_BLKADDR)
+			goto out;
+		if (block >= last_block)
+			goto clear;
+		if (!ext2_next_key(&key, ei->i_clu_nblocks))
+			goto out;
+		block += ei->i_clu_nblocks;
+	}
+ clear:
+	ei->i_flags &= ~EXT2_COMPRBLK_FL;
+ out:
+	ext2_free_key(&key);
+}
diff -pruN linux-2.6.18.5.org/fs/ext2/compress_syms.c linux-2.6.18.5/fs/ext2/compress_syms.c
--- linux-2.6.18.5.org/fs/ext2/compress_syms.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/compress_syms.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,17 @@
+
+/*
+ * linux/fs/ext2/compress_syms.c
+ *
+ * Exported kernel symbols for the ext2 compression.
+ * These symbols are used by compression algorithms.
+ */
+
+#include <linux/module.h>
+
+#include <linux/mm.h>
+#include <linux/ext2_fs_c.h>
+
+EXPORT_SYMBOL(ext2_adler32);
+EXPORT_SYMBOL(ext2_register_compression_module);
+EXPORT_SYMBOL(ext2_unregister_compression_module);
+
diff -pruN linux-2.6.18.5.org/fs/ext2/debug.h linux-2.6.18.5/fs/ext2/debug.h
--- linux-2.6.18.5.org/fs/ext2/debug.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/debug.h	2007-02-03 17:07:02.000000000 -0800
@@ -0,0 +1,33 @@
+/* This file contains support for assert() and
+   contains the various debugging parameters.
+
+   Previously EXT2_COMPR_REPORT was in ext2_fs.h, but changing that file
+   causes most of the kernel to be rebuilt during subsequent compiles. */
+
+/* At some stage it would be nice to have a CONFIG_ option 
+   for the amount of time to spend catching bugs.  It would be
+   an int from 0 through 100, where 0 means `don't do any assert's
+   at all', n means `try not to spend more than n% of total system
+   (kernel) time in debugging'. */
+
+#undef  NDEBUG /* fixme: #define this for release */
+#include "assert.h"
+
+/* fs_assert: Like assert(), but truth of expression depends on
+   integrity of fs as well as on program logic. */
+#define fs_assert(_x) assert(_x)
+
+#undef  EXT2_COMPR_DEBUG
+#undef  EXT2_COMPR_REPORT
+/*
+#define EXT2_COMPR_DEBUG
+#define  EXT2_COMPR_REPORT
+*/
+#ifdef  EXT2_COMPR_REPORT
+# define EXT2_COMPR_REPORT_PUT
+# define EXT2_COMPR_REPORT_WA
+# define EXT2_COMPR_REPORT_ALLOC  /* disk allocation etc. */
+# define EXT2_COMPR_REPORT_ALGORITHMS /* Compression algorithms */
+# define EXT2_COMPR_REPORT_VERBOSE /* Various things I don't think
+				      useful at the moment. */
+#endif
diff -pruN linux-2.6.18.5.org/fs/ext2/e2compr_version.h linux-2.6.18.5/fs/ext2/e2compr_version.h
--- linux-2.6.18.5.org/fs/ext2/e2compr_version.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/e2compr_version.h	2007-03-29 10:17:15.000000000 -0700
@@ -0,0 +1 @@
+# define E2COMPR_VERSION "0.4.5-alpha2 (29 Jan 2007)"
diff -pruN linux-2.6.18.5.org/fs/ext2/ext2.h linux-2.6.18.5/fs/ext2/ext2.h
--- linux-2.6.18.5.org/fs/ext2/ext2.h	2006-12-04 13:31:58.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/ext2.h	2007-01-24 10:31:30.000000000 -0800
@@ -50,6 +50,12 @@ struct ext2_inode_info {
 	__u32	i_prealloc_block;
 	__u32	i_prealloc_count;
 	__u32	i_dir_start_lookup;
+#ifdef CONFIG_EXT2_COMPRESS
+	__u8    i_log2_clu_nblocks;
+	__u8    i_clu_nblocks;
+	__u8    i_compr_method;
+	__u8    i_compr_flags;
+#endif
 #ifdef CONFIG_EXT2_FS_XATTR
 	/*
 	 * Extended attributes can be read independently of the main file
@@ -123,6 +129,7 @@ extern void ext2_check_inodes_bitmap (st
 extern unsigned long ext2_count_free (struct buffer_head *, unsigned);
 
 /* inode.c */
+extern int ext2_get_block(struct inode *, sector_t, struct buffer_head *, int);
 extern void ext2_read_inode (struct inode *);
 extern int ext2_write_inode (struct inode *, int);
 extern void ext2_put_inode (struct inode *);
diff -pruN linux-2.6.18.5.org/fs/ext2/file.c linux-2.6.18.5/fs/ext2/file.c
--- linux-2.6.18.5.org/fs/ext2/file.c	2006-12-04 13:31:58.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/file.c	2007-03-25 22:59:05.000000000 -0700
@@ -18,23 +18,545 @@
  * 	(jj@sunsite.ms.mff.cuni.cz)
  */
 
+/*
+ *  Copyright (C) 2001 Alcatel Business Systems - R&D Illkirch FRANCE
+ *
+ *    Transparent compression code for 2.4 kernel.
+ *
+ *  Denis Richard (denis.richard@sxb.bsf.alcatel.fr)
+ *  Pierre Peiffer (pierre.peiffer@sxb.bsf.alcatel.fr)
+ *
+ *  Adapted from patch e2compr-0.4.39-patch-2.2.18 .
+ */
+
+#include <linux/fs.h>
+#include <linux/ext2_fs_c.h>
 #include <linux/time.h>
-#include "ext2.h"
 #include "xattr.h"
 #include "acl.h"
+#include "debug.h"
+#include <linux/buffer_head.h>
+#include <asm/uaccess.h>
+#ifdef CONFIG_EXT2_COMPRESS
+#include <linux/kmod.h>
+#include <linux/slab.h>
+#include <linux/swap.h>
+#include <linux/pagemap.h>
+#endif
 
 /*
  * Called when an inode is released. Note that this is different
  * from ext2_open_file: open gets called at every open, but release
  * gets called only when /all/ the files are closed.
  */
+/*
+ * pjm 1998-01-09: I would note that this is different from `when no
+ * process has the inode open'.
+ */
 static int ext2_release_file (struct inode * inode, struct file * filp)
 {
-	if (filp->f_mode & FMODE_WRITE)
+#ifdef CONFIG_EXT2_COMPRESS
+	/*
+	 * Now's as good a time as any to clean up wrt compression.
+	 * Previously (before 2.1.4x) we waited until
+	 * ext2_put_inode(), but now the dcache sometimes delays that
+	 * call until umount time.
+	 */
+	if (S_ISREG (inode->i_mode)
+	    && inode->i_nlink
+	    && (EXT2_I(inode)->i_compr_flags & EXT2_CLEANUP_FL)) {
+# ifdef EXT2_COMPR_REPORT_PUT
+		printk(KERN_DEBUG
+		    "ext2_release_file: pid=%d, i_ino=%lu, i_count=%d\n",
+		    current->pid, inode->i_ino, atomic_read(&inode->i_count));
+# endif
+		/*
+		 * todo: See how the return code of
+		 * ext2_release_file() is used, and decide whether it
+		 * might be appropriate to pass any errors to
+		 * caller.
+		 */
+		(void) ext2_cleanup_compressed_inode (inode);
+	}
+#endif
+	if (filp->f_mode & FMODE_WRITE) {
 		ext2_discard_prealloc (inode);
+	}
 	return 0;
 }
 
+#ifdef CONFIG_EXT2_COMPRESS
+struct page_cluster {
+	struct page *	page;
+	loff_t		pos;
+	unsigned	bytes;
+	unsigned long	offset;
+	unsigned char	in_range;
+	const char *	buf;
+};
+
+#define PAGE_IN_RANGE 1
+#define PAGE_KMAPPED  2
+
+/*
+ * Write to a file through the page cache.
+ *
+ * We currently put everything into the page cache prior to writing it.
+ * This is not a problem when writing full pages. With partial pages,
+ * however, we first have to read the data into the cache, then
+ * dirty the page, and finally schedule it for writing. Alternatively, we
+ * could write-through just the portion of data that would go into that
+ * page, but that would kill performance for applications that write data
+ * line by line, and it's prone to race conditions.
+ *
+ * Note that this routine doesn't try to keep track of dirty pages. Each
+ * file system has to do this all by itself, unfortunately.
+ *                                                    okir@monad.swb.de
+ */
+ssize_t
+ext2_file_write(struct file *file,const char *buf,size_t count,loff_t *ppos)
+{
+	struct address_space *mapping = file->f_dentry->d_inode->i_mapping;
+	struct inode	*inode = mapping->host;
+	unsigned long	limit = current->signal->rlim[RLIMIT_FSIZE].rlim_cur;
+	loff_t		pos;
+	struct page	*cached_page;
+	struct page_cluster pageClu[EXT2_MAX_CLUSTER_PAGES];
+	unsigned long	written;
+	long		status;
+	int		err, i;
+	unsigned	bytes;
+	int		pagesPerCluster=0; /* number of pages per cluster */
+	unsigned long	last_index;	   /* last page index */
+	u32		comprblk_mask=0;
+	const char	*curbuf = buf;
+	int		osync_already;
+	struct ext2_inode_info *ei = EXT2_I(inode);
+	struct pagevec	lru_pvec;
+
+	if (!(ei->i_flags & (EXT2_COMPR_FL|EXT2_COMPRBLK_FL))
+#ifdef DUD
+	/* bug fix: initialization of first cp to compressed dir TLL 3/24/07 */
+		|| (count < inode->i_sb->s_blocksize)
+#endif
+		)
+	/* file not compressed: fall in the default file_write */
+		return generic_file_write(file, buf, count, ppos);
+
+	if ((ssize_t) count < 0)
+		return -EINVAL;
+
+	if (!access_ok(VERIFY_READ, buf, count))
+		return -EFAULT;
+
+	pagevec_init(&lru_pvec, 0);
+
+	cached_page = NULL;
+
+//	mutex_lock(&inode->i_mutex);
+	down_read(&inode->i_alloc_sem); /* as used by ocsf2 TLL 02/21/07 */
+
+	pos = *ppos;
+	err = -EINVAL;
+	if (pos < 0)
+		goto out;
+
+	written = 0;
+
+	/* FIXME: this is for backwards compatibility with 2.4 */
+	if (!S_ISBLK(inode->i_mode) && file->f_flags & O_APPEND)
+		pos = inode->i_size;
+
+	/*
+	 * Check whether we've reached the file size limit.
+	 */
+	err = -EFBIG;
+
+	if (limit != RLIM_INFINITY) {
+		if (pos >= limit) {
+			send_sig(SIGXFSZ, current, 0);
+			goto out;
+		}
+		if (pos > 0xFFFFFFFFULL || count > limit - (u32)pos) {
+			/* send_sig(SIGXFSZ, current, 0); */
+			count = limit - (u32)pos;
+		}
+	}
+
+	/*
+	 *      LFS rule
+	 */
+	if ( pos + count > MAX_NON_LFS && !(file->f_flags&O_LARGEFILE)) {
+		if (pos >= MAX_NON_LFS) {
+			send_sig(SIGXFSZ, current, 0);
+			goto out;
+		}
+		if (count > MAX_NON_LFS - (u32)pos) {
+			/* send_sig(SIGXFSZ, current, 0); */
+			count = MAX_NON_LFS - (u32)pos;
+		}
+	}
+
+	/*
+	 *	Are we about to exceed the fs block limit ?
+	 *
+	 *	If we have written data it becomes a short write
+	 *	If we have exceeded without writing data we send
+	 *	a signal and give them an EFBIG.
+	 *
+	 *	Linus frestrict idea will clean these up nicely..
+	 */
+
+	if (!S_ISBLK(inode->i_mode)) {
+		if (pos >= inode->i_sb->s_maxbytes) {
+			if (count || pos > inode->i_sb->s_maxbytes) {
+				send_sig(SIGXFSZ, current, 0);
+				err = -EFBIG;
+				goto out;
+			}
+			/* zero-length writes at ->s_maxbytes are OK */
+		}
+
+		if (pos + count > inode->i_sb->s_maxbytes)
+			count = inode->i_sb->s_maxbytes - pos;
+	} else {
+		if (bdev_read_only(inode->i_sb->s_bdev)) {
+			err = -EPERM;
+			goto out;
+		}
+		if (pos >= inode->i_size) {
+			if (count || pos > inode->i_size) {
+				err = -ENOSPC;
+				goto out;
+			}
+		}
+
+		if (pos + count > inode->i_size)
+			count = inode->i_size - pos;
+	}
+
+	err = 0;
+	if (count == 0)
+		goto out;
+
+	status  = 0;
+
+	if (file->f_flags & O_DIRECT)
+		{
+		err = -EINVAL;
+		goto out;
+	}
+	/*
+	 *	We must still check for EXT2_ECOMPR_FL, as it may have been
+	 *	set after we got the write permission to this file.
+	 */
+	if ((ei->i_flags & (EXT2_ECOMPR_FL | EXT2_NOCOMPR_FL))
+	    == (EXT2_ECOMPR_FL | 0)) {
+		err = -EXT2_ECOMPR;
+		goto out;
+	}
+
+	remove_suid(file->f_dentry);
+	inode->i_ctime = inode->i_mtime = CURRENT_TIME;
+	mark_inode_dirty_sync(inode);
+
+	if ((pos+count) > inode->i_size)
+		last_index = (pos+count-1) >> PAGE_CACHE_SHIFT;
+	else
+		last_index = (inode->i_size-1) >> PAGE_CACHE_SHIFT;
+
+	comprblk_mask = ei->i_flags | ~EXT2_COMPRBLK_FL;
+# ifdef EXT2_COMPRESS_WHEN_CLU
+	ei->i_flags |= EXT2_COMPRBLK_FL;
+# endif
+
+	do {
+		unsigned long index, offset, clusters_page_index0, cluster_compressed=0;
+		loff_t curpos = pos;
+		size_t curcount = count;
+		char *kaddr;
+		u32  cluster=0;
+		/*
+		 * Try to find the page in the cache. If it isn't there,
+		 * allocate a free page.
+		 */
+		offset = (pos & (PAGE_CACHE_SIZE -1)); /* Within page */
+		index = pos >> PAGE_CACHE_SHIFT;
+		bytes = PAGE_CACHE_SIZE - offset;
+		if (bytes > count) {
+			bytes = count;
+		}
+
+		/* Compute number of pages per cluster on *this* cluster */
+		/* Every cluster have the same size except the first one */
+		cluster = ext2_page_to_cluster(inode, index);
+		pagesPerCluster = ext2_cluster_npages(inode, cluster);
+		/* We bring all the pages needed to store the whole cluster */
+		clusters_page_index0 = ext2_cluster_page0(inode, cluster);
+		assert((pagesPerCluster > 0) && (pagesPerCluster <= EXT2_MAX_CLUSTER_PAGES));
+		trace_e2c("ext2_file_write: cluster=%d, pagesPerCluster=%d, clusters_page_index0=%ld\n", cluster, pagesPerCluster, clusters_page_index0);
+
+		status = -ENOMEM;	/* we'll assign it later anyway */
+
+		/*
+		 * Here, for each page we will need, we compute some data
+		 * needed to call the different routine (prepare_write, ...)
+		 * The pb is that we must first call the routines on each page,
+		 * then decompress the data, and then valid the rest
+		 */
+		for (i=0; i<pagesPerCluster; i++) {
+			pageClu[i].page = NULL;
+			if (((clusters_page_index0 + i) < index) ||
+			    ((clusters_page_index0 + i) > 
+			    ((pos+count-1) >> PAGE_CACHE_SHIFT))) {
+				pageClu[i].offset	= 0;
+				pageClu[i].bytes	= 0;
+				pageClu[i].in_range	= 0;
+				pageClu[i].pos		= 0;
+				pageClu[i].buf		= NULL;
+			} else { /* we are inside the range */
+				pageClu[i].offset	= (curpos &
+				    (PAGE_CACHE_SIZE -1)); /* Within page */;
+				pageClu[i].bytes	= PAGE_CACHE_SIZE -
+				    pageClu[i].offset;;
+			pageClu[i].in_range	= 1;
+			pageClu[i].pos		= curpos;
+			pageClu[i].buf		= curbuf;
+			if (pageClu[i].bytes > curcount) {
+				pageClu[i].bytes = curcount;
+			}
+			curpos += pageClu[i].bytes;
+			curcount -= pageClu[i].bytes;
+			curbuf += pageClu[i].bytes;
+			}
+		}
+
+
+		trace_e2c("ext2_file_write: [pos=%d count=%d size=%d "
+		    "last_index=%ld]\n", (int)pos, count, (int)inode->i_size,
+		    last_index);
+
+		/*
+		 * We decompress the cluster if needed, and write
+		 * the data as normal.  The cluster will be
+		 * compressed again when the inode is cleaned up.
+		 */
+		if ((comprblk_mask == ~(u32)0)
+		    && !(ei->i_flags & EXT2_NOCOMPR_FL)) {
+			/* assert (block == pos >> inode->i_sb->s_blocksize_bits); */
+
+			cluster_compressed = ext2_cluster_is_compressed_fn(inode, cluster);
+
+			if (cluster_compressed < 0) {
+				if (! written)
+					written = cluster_compressed;
+				break;
+			}
+		}
+
+		if (cluster_compressed > 0) {
+			/* Here, decompression take place  */
+			cluster_compressed = ext2_decompress_cluster(inode, cluster);
+			if (cluster_compressed < 0) {
+				if (! written) {
+					written = cluster_compressed;
+				}
+				break;
+			}
+		}
+		/* Cluster is not compressed    */
+		for (i = 0; i < pagesPerCluster; i++) {
+			/*
+			 * Bring in the user page that we will copy from
+			 * _first_.  Otherwise there's a nasty deadlock on
+			 * copying from the same page as we're writing to,
+			 * without it being marked up-to-date.
+			 */
+			if (pageClu[i].in_range) {
+				volatile unsigned char dummy;
+				__get_user(dummy, pageClu[i].buf);
+				__get_user(dummy, pageClu[i].buf+pageClu[i].
+				    bytes-1);
+			}
+			pageClu[i].page = __grab_cache_page(mapping,
+			    clusters_page_index0+i, &cached_page, &lru_pvec);
+
+			if (!pageClu[i].page) {
+				while (i--) {
+					unlock_page(pageClu[i].page);
+					page_cache_release(pageClu[i].page);
+				}
+			status = -ENOMEM;
+			break;
+			}
+
+			/* We have exclusive IO access to the page.. */
+			if (!PageLocked(pageClu[i].page)) {
+//				PAGE_BUG(pageClu[i].page);
+				BUG();
+			}
+			if (pageClu[i].in_range) {
+				status = mapping->a_ops->prepare_write(file,
+				    pageClu[i].page, pageClu[i].offset,
+				    pageClu[i].offset+pageClu[i].bytes);
+				if (status) {
+					goto unlock;
+				}
+
+				kaddr = kmap(pageClu[i].page);
+				pageClu[i].in_range = PAGE_KMAPPED;
+
+				status = __copy_from_user(kaddr+pageClu[i].
+				    offset, pageClu[i].buf, pageClu[i].bytes);
+				flush_dcache_page(pageClu[i].page);
+				mapping->a_ops->commit_write(file,
+				    pageClu[i].page, pageClu[i].offset,
+				    pageClu[i].offset+pageClu[i].bytes);
+				if (status) {
+					status = -EFAULT;
+					goto unlock;
+				}
+
+				status = pageClu[i].bytes;
+
+				written += status;
+				count -= status;
+				pos += status;
+				buf += status;
+			}
+		}
+
+unlock:
+		/* Mark them unlocked again and drop the page.. */
+		for (i=0; (i<pagesPerCluster) && (pageClu[i].page != NULL); i++)
+		{
+			if (pageClu[i].in_range == PAGE_KMAPPED) {
+				kunmap(pageClu[i].page);
+				SetPageReferenced(pageClu[i].page);
+			}
+			unlock_page(pageClu[i].page);
+			page_cache_release(pageClu[i].page);
+		}
+
+		if (status < 0)
+			break;
+
+#ifdef EXT2_COMPRESS_WHEN_CLU
+		assert (ei->i_flags & EXT2_COMPRBLK_FL);
+		if ((ei->i_flags & EXT2_COMPR_FL)
+		    && (ext2_offset_is_clu_boundary(inode, curpos))
+		    && (ext2_wr_wa != NULL)) {
+			if (mapping_mapped(mapping))
+			/*
+			 * Pierre Peiffer: For file mapped (via mmap, I mean),
+			 * compression will occure when releasing the file.
+			 * We must, in this case, avoid the pages (possibly
+			 * mapped by a process) to be compressed under them.
+			 */
+			{
+				int error;
+				error = ext2_compress_cluster(inode, cluster);
+				/*
+				 * Actually, raising write_error may be a
+				 * mistake.  For example,
+				 * ext2_cleanup_compressed_cluster() doesn't
+				 * usually return any errors to user.  todo:
+				 * Have a look at ext2_compress_cluster, and
+				 * check whether its errors are such that they
+				 * should be returned to user.  Some of the
+				 * will be, of course, but it might be
+				 * possible for it to return without
+				 * change.
+				 */
+				if (error > 0)
+					comprblk_mask = ~(u32)0;
+			} else {
+#ifdef EXT2_COMPR_REPORT
+				char bdn[BDEVNAME_SIZE];
+				bdevname(inode->i_sb->s_bdev, bdn);
+#endif
+
+				trace_e2c("ext2_file_write: (dev. %s): "
+				    "ino=%ld, cluster=%d: file mapped, does "
+				    "not compress cluster\n",
+				    bdn, inode->i_ino, cluster);
+				ei->i_flags |= EXT2_DIRTY_FL;
+				ei->i_compr_flags |= EXT2_CLEANUP_FL;
+			}
+		}
+#endif
+
+	} while (count);
+	*ppos = pos;
+
+	if (cached_page)
+		page_cache_release(cached_page);
+	pagevec_lru_add(&lru_pvec);
+	pagevec_free(&lru_pvec);
+
+	/*
+	 * For now, when the user asks for O_SYNC, we'll actually
+	 * provide O_DSYNC.
+	 */
+	if (status >= 0) {
+		if ((file->f_flags & O_SYNC) || IS_SYNC(inode)) {
+			if (ei->i_compr_flags & EXT2_OSYNC_INODE) {
+				osync_already = 1;
+			} else {
+				osync_already = 0;
+				ei->i_compr_flags |= EXT2_OSYNC_INODE;
+			}
+/* Should 2nd arg be inode->i_mapping? */
+			status = generic_osync_inode(inode, file->f_mapping,
+			    OSYNC_METADATA|OSYNC_DATA);
+			if (osync_already == 0) {
+				ei->i_compr_flags &= ~EXT2_OSYNC_INODE;
+			}
+		}
+	}
+
+	err = written ? written : status;
+
+# ifdef EXT2_COMPRESS_WHEN_CLU
+	assert (ei->i_flags & EXT2_COMPRBLK_FL);
+	ei->i_flags &= comprblk_mask;
+	if ((ei->i_flags & EXT2_COMPR_FL)
+	    && (!ext2_offset_is_clu_boundary(inode, pos)
+	    || (ext2_wr_wa == NULL)))
+	{
+		ei->i_flags |= EXT2_DIRTY_FL;
+		ei->i_compr_flags |= EXT2_CLEANUP_FL;
+	}
+# else
+	if (ei->i_flags & EXT2_COMPR_FL) {
+		ei->i_flags |= EXT2_DIRTY_FL;
+		ei->i_compr_flags |= EXT2_CLEANUP_FL;
+	}
+# endif
+out:
+//	mutex_unlock(&inode->i_mutex);
+	up_read(&inode->i_alloc_sem); /* as used by ocsf2 TLL 02/21/07 */
+	return err;
+}
+
+/*
+ * Called when an inode is about to be open.
+ * We use this to disallow opening RW large files on 32bit systems if
+ * the caller didn't specify O_LARGEFILE.  On 64bit systems we force
+ * on this flag in sys_open.
+ * Prevent opening compressed file with O_DIRECT.
+ */
+static int ext2_file_open(struct inode * inode, struct file * filp)
+{
+	if ((filp->f_flags & O_DIRECT) && (EXT2_I(inode)->i_flags &
+	    (EXT2_COMPR_FL|EXT2_COMPRBLK_FL)))
+		return -EINVAL;
+	if (!(filp->f_flags & O_LARGEFILE) && inode->i_size > MAX_NON_LFS)
+		return -EFBIG;
+	return 0;
+}
+#endif /* CONFIG_EXT2_COMPRESS*/
+
 /*
  * We have mostly NULL's here: the current defaults are ok for
  * the ext2 filesystem.
@@ -42,12 +564,20 @@ static int ext2_release_file (struct ino
 const struct file_operations ext2_file_operations = {
 	.llseek		= generic_file_llseek,
 	.read		= generic_file_read,
+#ifdef CONFIG_EXT2_COMPRESS
+	.write		= ext2_file_write,
+#else
 	.write		= generic_file_write,
+#endif
 	.aio_read	= generic_file_aio_read,
 	.aio_write	= generic_file_aio_write,
 	.ioctl		= ext2_ioctl,
 	.mmap		= generic_file_mmap,
+#ifdef CONFIG_EXT2_COMPRESS
+	.open		= ext2_file_open,
+#else
 	.open		= generic_file_open,
+#endif
 	.release	= ext2_release_file,
 	.fsync		= ext2_sync_file,
 	.readv		= generic_file_readv,
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/ChangeLog linux-2.6.18.5/fs/ext2/gzip/ChangeLog
--- linux-2.6.18.5.org/fs/ext2/gzip/ChangeLog	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/ChangeLog	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,471 @@
+
+		ChangeLog file for zlib
+
+Changes in 1.1.3 (9 July 1998)
+- fix "an inflate input buffer bug that shows up on rare but persistent
+  occasions" (Mark)
+- fix gzread and gztell for concatenated .gz files (Didier Le Botlan)
+- fix gzseek(..., SEEK_SET) in write mode
+- fix crc check after a gzeek (Frank Faubert)
+- fix miniunzip when the last entry in a zip file is itself a zip file
+  (J Lillge)
+- add contrib/asm586 and contrib/asm686 (Brian Raiter)
+  See http://www.muppetlabs.com/~breadbox/software/assembly.html
+- add support for Delphi 3 in contrib/delphi (Bob Dellaca)
+- add support for C++Builder 3 and Delphi 3 in contrib/delphi2 (Davide Moretti)
+- do not exit prematurely in untgz if 0 at start of block (Magnus Holmgren)
+- use macro EXTERN instead of extern to support DLL for BeOS (Sander Stoks)
+- added a FAQ file
+
+- Support gzdopen on Mac with Metrowerks (Jason Linhart)
+- Do not redefine Byte on Mac (Brad Pettit & Jason Linhart)
+- define SEEK_END too if SEEK_SET is not defined (Albert Chin-A-Young)
+- avoid some warnings with Borland C (Tom Tanner)
+- fix a problem in contrib/minizip/zip.c for 16-bit MSDOS (Gilles Vollant)
+- emulate utime() for WIN32 in contrib/untgz  (Gilles Vollant)
+- allow several arguments to configure (Tim Mooney, Frodo Looijaard)
+- use libdir and includedir in Makefile.in (Tim Mooney)
+- support shared libraries on OSF1 V4 (Tim Mooney)
+- remove so_locations in "make clean"  (Tim Mooney)
+- fix maketree.c compilation error (Glenn, Mark)
+- Python interface to zlib now in Python 1.5 (Jeremy Hylton)
+- new Makefile.riscos (Rich Walker)
+- initialize static descriptors in trees.c for embedded targets (Nick Smith)
+- use "foo-gz" in example.c for RISCOS and VMS (Nick Smith)
+- add the OS/2 files in Makefile.in too (Andrew Zabolotny)
+- fix fdopen and halloc macros for Microsoft C 6.0 (Tom Lane)
+- fix maketree.c to allow clean compilation of inffixed.h (Mark)
+- fix parameter check in deflateCopy (Gunther Nikl)
+- cleanup trees.c, use compressed_len only in debug mode (Christian Spieler)
+- Many portability patches by Christian Spieler:
+  . zutil.c, zutil.h: added "const" for zmem*
+  . Make_vms.com: fixed some typos
+  . Make_vms.com: msdos/Makefile.*: removed zutil.h from some dependency lists
+  . msdos/Makefile.msc: remove "default rtl link library" info from obj files
+  . msdos/Makefile.*: use model-dependent name for the built zlib library
+  . msdos/Makefile.emx, nt/Makefile.emx, nt/Makefile.gcc:
+     new makefiles, for emx (DOS/OS2), emx&rsxnt and mingw32 (Windows 9x / NT)
+- use define instead of typedef for Bytef also for MSC small/medium (Tom Lane)
+- replace __far with _far for better portability (Christian Spieler, Tom Lane)
+- fix test for errno.h in configure (Tim Newsham)
+
+Changes in 1.1.2 (19 March 98)
+- added contrib/minzip, mini zip and unzip based on zlib (Gilles Vollant)
+  See http://www.winimage.com/zLibDll/unzip.html
+- preinitialize the inflate tables for fixed codes, to make the code
+  completely thread safe (Mark)
+- some simplifications and slight speed-up to the inflate code (Mark)
+- fix gzeof on non-compressed files (Allan Schrum)
+- add -std1 option in configure for OSF1 to fix gzprintf (Martin Mokrejs)
+- use default value of 4K for Z_BUFSIZE for 16-bit MSDOS (Tim Wegner + Glenn)
+- added os2/Makefile.def and os2/zlib.def (Andrew Zabolotny)
+- add shared lib support for UNIX_SV4.2MP (MATSUURA Takanori)
+- do not wrap extern "C" around system includes (Tom Lane)
+- mention zlib binding for TCL in README (Andreas Kupries)
+- added amiga/Makefile.pup for Amiga powerUP SAS/C PPC (Andreas Kleinert)
+- allow "make install prefix=..." even after configure (Glenn Randers-Pehrson)
+- allow "configure --prefix $HOME" (Tim Mooney)
+- remove warnings in example.c and gzio.c (Glenn Randers-Pehrson)
+- move Makefile.sas to amiga/Makefile.sas
+
+Changes in 1.1.1 (27 Feb 98)
+- fix macros _tr_tally_* in deflate.h for debug mode  (Glenn Randers-Pehrson)
+- remove block truncation heuristic which had very marginal effect for zlib
+  (smaller lit_bufsize than in gzip 1.2.4) and degraded a little the
+  compression ratio on some files. This also allows inlining _tr_tally for
+  matches in deflate_slow.
+- added msdos/Makefile.w32 for WIN32 Microsoft Visual C++ (Bob Frazier)
+
+Changes in 1.1.0 (24 Feb 98)
+- do not return STREAM_END prematurely in inflate (John Bowler)
+- revert to the zlib 1.0.8 inflate to avoid the gcc 2.8.0 bug (Jeremy Buhler)
+- compile with -DFASTEST to get compression code optimized for speed only
+- in minigzip, try mmap'ing the input file first (Miguel Albrecht)
+- increase size of I/O buffers in minigzip.c and gzio.c (not a big gain
+  on Sun but significant on HP)
+
+- add a pointer to experimental unzip library in README (Gilles Vollant)
+- initialize variable gcc in configure (Chris Herborth)
+
+Changes in 1.0.9 (17 Feb 1998)
+- added gzputs and gzgets functions
+- do not clear eof flag in gzseek (Mark Diekhans)
+- fix gzseek for files in transparent mode (Mark Diekhans)
+- do not assume that vsprintf returns the number of bytes written (Jens Krinke)
+- replace EXPORT with ZEXPORT to avoid conflict with other programs
+- added compress2 in zconf.h, zlib.def, zlib.dnt
+- new asm code from Gilles Vollant in contrib/asm386
+- simplify the inflate code (Mark):
+ . Replace ZALLOC's in huft_build() with single ZALLOC in inflate_blocks_new()
+ . ZALLOC the length list in inflate_trees_fixed() instead of using stack
+ . ZALLOC the value area for huft_build() instead of using stack
+ . Simplify Z_FINISH check in inflate()
+
+- Avoid gcc 2.8.0 comparison bug a little differently than zlib 1.0.8
+- in inftrees.c, avoid cc -O bug on HP (Farshid Elahi)
+- in zconf.h move the ZLIB_DLL stuff earlier to avoid problems with
+  the declaration of FAR (Gilles VOllant)
+- install libz.so* with mode 755 (executable) instead of 644 (Marc Lehmann)
+- read_buf buf parameter of type Bytef* instead of charf*
+- zmemcpy parameters are of type Bytef*, not charf* (Joseph Strout)
+- do not redeclare unlink in minigzip.c for WIN32 (John Bowler)
+- fix check for presence of directories in "make install" (Ian Willis)
+
+Changes in 1.0.8 (27 Jan 1998)
+- fixed offsets in contrib/asm386/gvmat32.asm (Gilles Vollant)
+- fix gzgetc and gzputc for big endian systems (Markus Oberhumer)
+- added compress2() to allow setting the compression level
+- include sys/types.h to get off_t on some systems (Marc Lehmann & QingLong)
+- use constant arrays for the static trees in trees.c instead of computing
+  them at run time (thanks to Ken Raeburn for this suggestion). To create
+  trees.h, compile with GEN_TREES_H and run "make test".
+- check return code of example in "make test" and display result
+- pass minigzip command line options to file_compress
+- simplifying code of inflateSync to avoid gcc 2.8 bug
+
+- support CC="gcc -Wall" in configure -s (QingLong)
+- avoid a flush caused by ftell in gzopen for write mode (Ken Raeburn)
+- fix test for shared library support to avoid compiler warnings
+- zlib.lib -> zlib.dll in msdos/zlib.rc (Gilles Vollant)
+- check for TARGET_OS_MAC in addition to MACOS (Brad Pettit)
+- do not use fdopen for Metrowerks on Mac (Brad Pettit))
+- add checks for gzputc and gzputc in example.c
+- avoid warnings in gzio.c and deflate.c (Andreas Kleinert)
+- use const for the CRC table (Ken Raeburn)
+- fixed "make uninstall" for shared libraries
+- use Tracev instead of Trace in infblock.c
+- in example.c use correct compressed length for test_sync
+- suppress +vnocompatwarnings in configure for HPUX (not always supported)
+
+Changes in 1.0.7 (20 Jan 1998)
+- fix gzseek which was broken in write mode
+- return error for gzseek to negative absolute position
+- fix configure for Linux (Chun-Chung Chen)
+- increase stack space for MSC (Tim Wegner)
+- get_crc_table and inflateSyncPoint are EXPORTed (Gilles Vollant)
+- define EXPORTVA for gzprintf (Gilles Vollant)
+- added man page zlib.3 (Rick Rodgers)
+- for contrib/untgz, fix makedir() and improve Makefile
+
+- check gzseek in write mode in example.c
+- allocate extra buffer for seeks only if gzseek is actually called
+- avoid signed/unsigned comparisons (Tim Wegner, Gilles Vollant)
+- add inflateSyncPoint in zconf.h
+- fix list of exported functions in nt/zlib.dnt and mdsos/zlib.def
+
+Changes in 1.0.6 (19 Jan 1998)
+- add functions gzprintf, gzputc, gzgetc, gztell, gzeof, gzseek, gzrewind and
+  gzsetparams (thanks to Roland Giersig and Kevin Ruland for some of this code)
+- Fix a deflate bug occuring only with compression level 0 (thanks to
+  Andy Buckler for finding this one).
+- In minigzip, pass transparently also the first byte for .Z files.
+- return Z_BUF_ERROR instead of Z_OK if output buffer full in uncompress()
+- check Z_FINISH in inflate (thanks to Marc Schluper)
+- Implement deflateCopy (thanks to Adam Costello)
+- make static libraries by default in configure, add --shared option.
+- move MSDOS or Windows specific files to directory msdos
+- suppress the notion of partial flush to simplify the interface
+  (but the symbol Z_PARTIAL_FLUSH is kept for compatibility with 1.0.4)
+- suppress history buffer provided by application to simplify the interface
+  (this feature was not implemented anyway in 1.0.4)
+- next_in and avail_in must be initialized before calling inflateInit or
+  inflateInit2
+- add EXPORT in all exported functions (for Windows DLL)
+- added Makefile.nt (thanks to Stephen Williams)
+- added the unsupported "contrib" directory:
+   contrib/asm386/ by Gilles Vollant <info@winimage.com>
+	386 asm code replacing longest_match().
+   contrib/iostream/ by Kevin Ruland <kevin@rodin.wustl.edu>
+        A C++ I/O streams interface to the zlib gz* functions
+   contrib/iostream2/  by Tyge Løvset <Tyge.Lovset@cmr.no>
+	Another C++ I/O streams interface
+   contrib/untgz/  by "Pedro A. Aranda Guti\irrez" <paag@tid.es>
+	A very simple tar.gz file extractor using zlib
+   contrib/visual-basic.txt by Carlos Rios <c_rios@sonda.cl>
+        How to use compress(), uncompress() and the gz* functions from VB.
+- pass params -f (filtered data), -h (huffman only), -1 to -9 (compression
+  level) in minigzip (thanks to Tom Lane)
+
+- use const for rommable constants in deflate
+- added test for gzseek and gztell in example.c
+- add undocumented function inflateSyncPoint() (hack for Paul Mackerras)
+- add undocumented function zError to convert error code to string
+  (for Tim Smithers)
+- Allow compilation of gzio with -DNO_DEFLATE to avoid the compression code.
+- Use default memcpy for Symantec MSDOS compiler.
+- Add EXPORT keyword for check_func (needed for Windows DLL)
+- add current directory to LD_LIBRARY_PATH for "make test"
+- create also a link for libz.so.1
+- added support for FUJITSU UXP/DS (thanks to Toshiaki Nomura)
+- use $(SHAREDLIB) instead of libz.so in Makefile.in (for HPUX)
+- added -soname for Linux in configure (Chun-Chung Chen,
+- assign numbers to the exported functions in zlib.def (for Windows DLL)
+- add advice in zlib.h for best usage of deflateSetDictionary
+- work around compiler bug on Atari (cast Z_NULL in call of s->checkfn)
+- allow compilation with ANSI keywords only enabled for TurboC in large model
+- avoid "versionString"[0] (Borland bug)
+- add NEED_DUMMY_RETURN for Borland
+- use variable z_verbose for tracing in debug mode (L. Peter Deutsch).
+- allow compilation with CC
+- defined STDC for OS/2 (David Charlap)	
+- limit external names to 8 chars for MVS (Thomas Lund)
+- in minigzip.c, use static buffers only for 16-bit systems
+- fix suffix check for "minigzip -d foo.gz"
+- do not return an error for the 2nd of two consecutive gzflush() (Felix Lee)
+- use _fdopen instead of fdopen for MSC >= 6.0 (Thomas Fanslau)
+- added makelcc.bat for lcc-win32 (Tom St Denis)
+- in Makefile.dj2, use copy and del instead of install and rm (Frank Donahoe)
+- Avoid expanded $Id$. Use "rcs -kb" or "cvs admin -kb" to avoid Id expansion.
+- check for unistd.h in configure (for off_t)
+- remove useless check parameter in inflate_blocks_free
+- avoid useless assignment of s->check to itself in inflate_blocks_new
+- do not flush twice in gzclose (thanks to Ken Raeburn)
+- rename FOPEN as F_OPEN to avoid clash with /usr/include/sys/file.h
+- use NO_ERRNO_H instead of enumeration of operating systems with errno.h
+- work around buggy fclose on pipes for HP/UX
+- support zlib DLL with BORLAND C++ 5.0 (thanks to Glenn Randers-Pehrson)
+- fix configure if CC is already equal to gcc
+
+Changes in 1.0.5 (3 Jan 98)
+- Fix inflate to terminate gracefully when fed corrupted or invalid data
+- Use const for rommable constants in inflate
+- Eliminate memory leaks on error conditions in inflate
+- Removed some vestigial code in inflate
+- Update web address in README
+  
+Changes in 1.0.4 (24 Jul 96)
+- In very rare conditions, deflate(s, Z_FINISH) could fail to produce an EOF
+  bit, so the decompressor could decompress all the correct data but went
+  on to attempt decompressing extra garbage data. This affected minigzip too.
+- zlibVersion and gzerror return const char* (needed for DLL)
+- port to RISCOS (no fdopen, no multiple dots, no unlink, no fileno)
+- use z_error only for DEBUG (avoid problem with DLLs)
+
+Changes in 1.0.3 (2 Jul 96)
+- use z_streamp instead of z_stream *, which is now a far pointer in MSDOS
+  small and medium models; this makes the library incompatible with previous
+  versions for these models. (No effect in large model or on other systems.)
+- return OK instead of BUF_ERROR if previous deflate call returned with
+  avail_out as zero but there is nothing to do
+- added memcmp for non STDC compilers
+- define NO_DUMMY_DECL for more Mac compilers (.h files merged incorrectly)
+- define __32BIT__ if __386__ or i386 is defined (pb. with Watcom and SCO)
+- better check for 16-bit mode MSC (avoids problem with Symantec)
+
+Changes in 1.0.2 (23 May 96)
+- added Windows DLL support
+- added a function zlibVersion (for the DLL support)
+- fixed declarations using Bytef in infutil.c (pb with MSDOS medium model)
+- Bytef is define's instead of typedef'd only for Borland C
+- avoid reading uninitialized memory in example.c
+- mention in README that the zlib format is now RFC1950
+- updated Makefile.dj2
+- added algorithm.doc
+
+Changes in 1.0.1 (20 May 96) [1.0 skipped to avoid confusion]
+- fix array overlay in deflate.c which sometimes caused bad compressed data
+- fix inflate bug with empty stored block
+- fix MSDOS medium model which was broken in 0.99
+- fix deflateParams() which could generated bad compressed data.
+- Bytef is define'd instead of typedef'ed (work around Borland bug)
+- added an INDEX file
+- new makefiles for DJGPP (Makefile.dj2), 32-bit Borland (Makefile.b32),
+  Watcom (Makefile.wat), Amiga SAS/C (Makefile.sas)
+- speed up adler32 for modern machines without auto-increment
+- added -ansi for IRIX in configure
+- static_init_done in trees.c is an int
+- define unlink as delete for VMS
+- fix configure for QNX
+- add configure branch for SCO and HPUX
+- avoid many warnings (unused variables, dead assignments, etc...)
+- no fdopen for BeOS
+- fix the Watcom fix for 32 bit mode (define FAR as empty)
+- removed redefinition of Byte for MKWERKS
+- work around an MWKERKS bug (incorrect merge of all .h files)
+
+Changes in 0.99 (27 Jan 96)
+- allow preset dictionary shared between compressor and decompressor
+- allow compression level 0 (no compression)
+- add deflateParams in zlib.h: allow dynamic change of compression level
+  and compression strategy.
+- test large buffers and deflateParams in example.c
+- add optional "configure" to build zlib as a shared library
+- suppress Makefile.qnx, use configure instead
+- fixed deflate for 64-bit systems (detected on Cray)
+- fixed inflate_blocks for 64-bit systems (detected on Alpha)
+- declare Z_DEFLATED in zlib.h (possible parameter for deflateInit2)
+- always return Z_BUF_ERROR when deflate() has nothing to do
+- deflateInit and inflateInit are now macros to allow version checking
+- prefix all global functions and types with z_ with -DZ_PREFIX
+- make falloc completely reentrant (inftrees.c)
+- fixed very unlikely race condition in ct_static_init
+- free in reverse order of allocation to help memory manager
+- use zlib-1.0/* instead of zlib/* inside the tar.gz
+- make zlib warning-free with "gcc -O3 -Wall -Wwrite-strings -Wpointer-arith
+  -Wconversion -Wstrict-prototypes -Wmissing-prototypes"
+- allow gzread on concatenated .gz files
+- deflateEnd now returns Z_DATA_ERROR if it was premature
+- deflate is finally (?) fully deterministic (no matches beyond end of input)
+- Document Z_SYNC_FLUSH
+- add uninstall in Makefile
+- Check for __cpluplus in zlib.h
+- Better test in ct_align for partial flush
+- avoid harmless warnings for Borland C++
+- initialize hash_head in deflate.c
+- avoid warning on fdopen (gzio.c) for HP cc -Aa
+- include stdlib.h for STDC compilers
+- include errno.h for Cray
+- ignore error if ranlib doesn't exist
+- call ranlib twice for NeXTSTEP
+- use exec_prefix instead of prefix for libz.a
+- renamed ct_* as _tr_* to avoid conflict with applications
+- clear z->msg in inflateInit2 before any error return
+- initialize opaque in example.c, gzio.c, deflate.c and inflate.c
+- fixed typo in zconf.h (_GNUC__ => __GNUC__)
+- check for WIN32 in zconf.h and zutil.c (avoid farmalloc in 32-bit mode)
+- fix typo in Make_vms.com (f$trnlnm -> f$getsyi)
+- in fcalloc, normalize pointer if size > 65520 bytes
+- don't use special fcalloc for 32 bit Borland C++
+- use STDC instead of __GO32__ to avoid redeclaring exit, calloc, etc...
+- use Z_BINARY instead of BINARY
+- document that gzclose after gzdopen will close the file
+- allow "a" as mode in gzopen.
+- fix error checking in gzread
+- allow skipping .gz extra-field on pipes
+- added reference to Perl interface in README
+- put the crc table in FAR data (I dislike more and more the medium model :)
+- added get_crc_table
+- added a dimension to all arrays (Borland C can't count).
+- workaround Borland C bug in declaration of inflate_codes_new & inflate_fast
+- guard against multiple inclusion of *.h (for precompiled header on Mac)
+- Watcom C pretends to be Microsoft C small model even in 32 bit mode.
+- don't use unsized arrays to avoid silly warnings by Visual C++:
+     warning C4746: 'inflate_mask' : unsized array treated as  '__far'
+     (what's wrong with far data in far model?).
+- define enum out of inflate_blocks_state to allow compilation with C++
+
+Changes in 0.95 (16 Aug 95)
+- fix MSDOS small and medium model (now easier to adapt to any compiler)
+- inlined send_bits
+- fix the final (:-) bug for deflate with flush (output was correct but
+  not completely flushed in rare occasions).
+- default window size is same for compression and decompression
+  (it's now sufficient to set MAX_WBITS in zconf.h).
+- voidp -> voidpf and voidnp -> voidp (for consistency with other
+  typedefs and because voidnp was not near in large model).
+
+Changes in 0.94 (13 Aug 95)
+- support MSDOS medium model
+- fix deflate with flush (could sometimes generate bad output)
+- fix deflateReset (zlib header was incorrectly suppressed)
+- added support for VMS
+- allow a compression level in gzopen()
+- gzflush now calls fflush
+- For deflate with flush, flush even if no more input is provided.
+- rename libgz.a as libz.a
+- avoid complex expression in infcodes.c triggering Turbo C bug
+- work around a problem with gcc on Alpha (in INSERT_STRING)
+- don't use inline functions (problem with some gcc versions)
+- allow renaming of Byte, uInt, etc... with #define.
+- avoid warning about (unused) pointer before start of array in deflate.c
+- avoid various warnings in gzio.c, example.c, infblock.c, adler32.c, zutil.c
+- avoid reserved word 'new' in trees.c
+
+Changes in 0.93 (25 June 95)
+- temporarily disable inline functions
+- make deflate deterministic
+- give enough lookahead for PARTIAL_FLUSH
+- Set binary mode for stdin/stdout in minigzip.c for OS/2
+- don't even use signed char in inflate (not portable enough)
+- fix inflate memory leak for segmented architectures
+
+Changes in 0.92 (3 May 95)
+- don't assume that char is signed (problem on SGI)
+- Clear bit buffer when starting a stored block
+- no memcpy on Pyramid
+- suppressed inftest.c
+- optimized fill_window, put longest_match inline for gcc
+- optimized inflate on stored blocks.
+- untabify all sources to simplify patches
+
+Changes in 0.91 (2 May 95)
+- Default MEM_LEVEL is 8 (not 9 for Unix) as documented in zlib.h
+- Document the memory requirements in zconf.h
+- added "make install"
+- fix sync search logic in inflateSync
+- deflate(Z_FULL_FLUSH) now works even if output buffer too short
+- after inflateSync, don't scare people with just "lo world"
+- added support for DJGPP
+
+Changes in 0.9 (1 May 95)
+- don't assume that zalloc clears the allocated memory (the TurboC bug
+  was Mark's bug after all :)
+- let again gzread copy uncompressed data unchanged (was working in 0.71)
+- deflate(Z_FULL_FLUSH), inflateReset and inflateSync are now fully implemented
+- added a test of inflateSync in example.c
+- moved MAX_WBITS to zconf.h because users might want to change that.
+- document explicitly that zalloc(64K) on MSDOS must return a normalized
+  pointer (zero offset)
+- added Makefiles for Microsoft C, Turbo C, Borland C++
+- faster crc32()
+
+Changes in 0.8 (29 April 95)
+- added fast inflate (inffast.c)
+- deflate(Z_FINISH) now returns Z_STREAM_END when done. Warning: this
+  is incompatible with previous versions of zlib which returned Z_OK.
+- work around a TurboC compiler bug (bad code for b << 0, see infutil.h)
+  (actually that was not a compiler bug, see 0.81 above)
+- gzread no longer reads one extra byte in certain cases
+- In gzio destroy(), don't reference a freed structure
+- avoid many warnings for MSDOS
+- avoid the ERROR symbol which is used by MS Windows
+
+Changes in 0.71 (14 April 95)
+- Fixed more MSDOS compilation problems :( There is still a bug with
+  TurboC large model.
+
+Changes in 0.7 (14 April 95)
+- Added full inflate support.
+- Simplified the crc32() interface. The pre- and post-conditioning
+  (one's complement) is now done inside crc32(). WARNING: this is
+  incompatible with previous versions; see zlib.h for the new usage.
+
+Changes in 0.61 (12 April 95)
+- workaround for a bug in TurboC. example and minigzip now work on MSDOS.
+
+Changes in 0.6 (11 April 95)
+- added minigzip.c
+- added gzdopen to reopen a file descriptor as gzFile
+- added transparent reading of non-gziped files in gzread.
+- fixed bug in gzread (don't read crc as data)
+- fixed bug in destroy (gzio.c) (don't return Z_STREAM_END for gzclose).
+- don't allocate big arrays in the stack (for MSDOS)
+- fix some MSDOS compilation problems
+
+Changes in 0.5:
+- do real compression in deflate.c. Z_PARTIAL_FLUSH is supported but
+  not yet Z_FULL_FLUSH.
+- support decompression but only in a single step (forced Z_FINISH)
+- added opaque object for zalloc and zfree.
+- added deflateReset and inflateReset
+- added a variable zlib_version for consistency checking.
+- renamed the 'filter' parameter of deflateInit2 as 'strategy'.
+  Added Z_FILTERED and Z_HUFFMAN_ONLY constants.
+
+Changes in 0.4:
+- avoid "zip" everywhere, use zlib instead of ziplib.
+- suppress Z_BLOCK_FLUSH, interpret Z_PARTIAL_FLUSH as block flush
+  if compression method == 8.
+- added adler32 and crc32
+- renamed deflateOptions as deflateInit2, call one or the other but not both
+- added the method parameter for deflateInit2.
+- added inflateInit2
+- simplied considerably deflateInit and inflateInit by not supporting
+  user-provided history buffer. This is supported only in deflateInit2
+  and inflateInit2.
+
+Changes in 0.3:
+- prefix all macro names with Z_
+- use Z_FINISH instead of deflateEnd to finish compression.
+- added Z_HUFFMAN_ONLY
+- added gzerror()
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/Makefile linux-2.6.18.5/fs/ext2/gzip/Makefile
--- linux-2.6.18.5.org/fs/ext2/gzip/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/Makefile	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,25 @@
+#
+# Makefile for the linux compression routines.
+#
+# Note! Dependencies are done automagically by 'make dep', which also
+# removes any old dependencies. DON'T put your own dependencies here
+# unless it's something special (ie not a .c file).
+#
+# Note 2! The CFLAGS definitions are now in the main makefile...
+
+.S.o:
+	$(CC) $(AFLAGS) -traditional -c $< -o $*.o
+
+obj-$(CONFIG_EXT2_HAVE_GZIP) += ext2-compr-gzip.o
+
+ext2-compr-gzip-y	:= e2compr_gzip.o deflate.o trees.o \
+			   inflate.o infblock.o inftrees.o infcodes.o \
+			   infutil.o inffast.o
+
+ifeq ($(CONFIG_EXT2_COMPR_X86_CODE),y)
+  ifeq ($(CONFIG_X86_F00F_BUG),y)
+  ext2-compr-gzip-y += match586.o
+  else
+  ext2-compr-gzip-y += match686.o
+  endif
+endif
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/README linux-2.6.18.5/fs/ext2/gzip/README
--- linux-2.6.18.5.org/fs/ext2/gzip/README	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/README	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,148 @@
+zlib 1.1.3 is a general purpose data compression library.  All the code
+is thread safe.  The data format used by the zlib library
+is described by RFCs (Request for Comments) 1950 to 1952 in the files 
+ftp://ds.internic.net/rfc/rfc1950.txt (zlib format), rfc1951.txt (deflate
+format) and rfc1952.txt (gzip format). These documents are also available in
+other formats from ftp://ftp.uu.net/graphics/png/documents/zlib/zdoc-index.html
+
+All functions of the compression library are documented in the file zlib.h
+(volunteer to write man pages welcome, contact jloup@gzip.org). A usage
+example of the library is given in the file example.c which also tests that
+the library is working correctly. Another example is given in the file
+minigzip.c. The compression library itself is composed of all source files
+except example.c and minigzip.c.
+
+To compile all files and run the test program, follow the instructions
+given at the top of Makefile. In short "make test; make install"
+should work for most machines. For Unix: "configure; make test; make install"
+For MSDOS, use one of the special makefiles such as Makefile.msc.
+For VMS, use Make_vms.com or descrip.mms.
+
+Questions about zlib should be sent to <zlib@quest.jpl.nasa.gov>, or to
+Gilles Vollant <info@winimage.com> for the Windows DLL version.
+The zlib home page is http://www.cdrom.com/pub/infozip/zlib/
+The official zlib ftp site is ftp://ftp.cdrom.com/pub/infozip/zlib/
+Before reporting a problem, please check those sites to verify that
+you have the latest version of zlib; otherwise get the latest version and
+check whether the problem still exists or not.
+
+Mark Nelson <markn@tiny.com> wrote an article about zlib for the Jan. 1997
+issue of  Dr. Dobb's Journal; a copy of the article is available in
+http://web2.airmail.net/markn/articles/zlibtool/zlibtool.htm
+
+The changes made in version 1.1.3 are documented in the file ChangeLog.
+The main changes since 1.1.2 are:
+
+- fix "an inflate input buffer bug that shows up on rare but persistent
+  occasions" (Mark)
+- fix gzread and gztell for concatenated .gz files (Didier Le Botlan)
+- fix gzseek(..., SEEK_SET) in write mode
+- fix crc check after a gzeek (Frank Faubert)
+- fix miniunzip when the last entry in a zip file is itself a zip file
+  (J Lillge)
+- add contrib/asm586 and contrib/asm686 (Brian Raiter)
+  See http://www.muppetlabs.com/~breadbox/software/assembly.html
+- add support for Delphi 3 in contrib/delphi (Bob Dellaca)
+- add support for C++Builder 3 and Delphi 3 in contrib/delphi2 (Davide Moretti)
+- do not exit prematurely in untgz if 0 at start of block (Magnus Holmgren)
+- use macro EXTERN instead of extern to support DLL for BeOS (Sander Stoks)
+- added a FAQ file
+
+plus many changes for portability.
+
+Unsupported third party contributions are provided in directory "contrib".
+
+A Java implementation of zlib is available in the Java Development Kit 1.1
+http://www.javasoft.com/products/JDK/1.1/docs/api/Package-java.util.zip.html
+See the zlib home page http://www.cdrom.com/pub/infozip/zlib/ for details.
+
+A Perl interface to zlib written by Paul Marquess <pmarquess@bfsec.bt.co.uk>
+is in the CPAN (Comprehensive Perl Archive Network) sites, such as:
+ftp://ftp.cis.ufl.edu/pub/perl/CPAN/modules/by-module/Compress/Compress-Zlib*
+
+A Python interface to zlib written by A.M. Kuchling <amk@magnet.com>
+is available in Python 1.5 and later versions, see
+http://www.python.org/doc/lib/module-zlib.html
+
+A zlib binding for TCL written by Andreas Kupries <a.kupries@westend.com>
+is availlable at http://www.westend.com/~kupries/doc/trf/man/man.html
+
+An experimental package to read and write files in .zip format,
+written on top of zlib by Gilles Vollant <info@winimage.com>, is
+available at http://www.winimage.com/zLibDll/unzip.html
+and also in the contrib/minizip directory of zlib.
+
+
+Notes for some targets:
+
+- To build a Windows DLL version, include in a DLL project zlib.def, zlib.rc
+  and all .c files except example.c and minigzip.c; compile with -DZLIB_DLL
+  The zlib DLL support was initially done by Alessandro Iacopetti and is
+  now maintained by Gilles Vollant <info@winimage.com>. Check the zlib DLL
+  home page at http://www.winimage.com/zLibDll
+
+  From Visual Basic, you can call the DLL functions which do not take
+  a structure as argument: compress, uncompress and all gz* functions.
+  See contrib/visual-basic.txt for more information, or get
+  http://www.tcfb.com/dowseware/cmp-z-it.zip
+
+- For 64-bit Irix, deflate.c must be compiled without any optimization.
+  With -O, one libpng test fails. The test works in 32 bit mode (with
+  the -n32 compiler flag). The compiler bug has been reported to SGI.
+
+- zlib doesn't work with gcc 2.6.3 on a DEC 3000/300LX under OSF/1 2.1   
+  it works when compiled with cc.
+
+- on Digital Unix 4.0D (formely OSF/1) on AlphaServer, the cc option -std1
+  is necessary to get gzprintf working correctly. This is done by configure.
+
+- zlib doesn't work on HP-UX 9.05 with some versions of /bin/cc. It works
+  with other compilers. Use "make test" to check your compiler.
+
+- gzdopen is not supported on RISCOS, BEOS and by some Mac compilers.
+
+- For Turbo C the small model is supported only with reduced performance to
+  avoid any far allocation; it was tested with -DMAX_WBITS=11 -DMAX_MEM_LEVEL=3
+
+- For PalmOs, see http://www.cs.uit.no/~perm/PASTA/pilot/software.html
+  Per Harald Myrvang <perm@stud.cs.uit.no>
+
+
+Acknowledgments:
+
+  The deflate format used by zlib was defined by Phil Katz. The deflate
+  and zlib specifications were written by L. Peter Deutsch. Thanks to all the
+  people who reported problems and suggested various improvements in zlib;
+  they are too numerous to cite here.
+
+Copyright notice:
+
+ (C) 1995-1998 Jean-loup Gailly and Mark Adler
+
+  This software is provided 'as-is', without any express or implied
+  warranty.  In no event will the authors be held liable for any damages
+  arising from the use of this software.
+
+  Permission is granted to anyone to use this software for any purpose,
+  including commercial applications, and to alter it and redistribute it
+  freely, subject to the following restrictions:
+
+  1. The origin of this software must not be misrepresented; you must not
+     claim that you wrote the original software. If you use this software
+     in a product, an acknowledgment in the product documentation would be
+     appreciated but is not required.
+  2. Altered source versions must be plainly marked as such, and must not be
+     misrepresented as being the original software.
+  3. This notice may not be removed or altered from any source distribution.
+
+  Jean-loup Gailly        Mark Adler
+  jloup@gzip.org          madler@alumni.caltech.edu
+
+If you use the zlib library in a product, we would appreciate *not*
+receiving lengthy legal documents to sign. The sources are provided
+for free but without warranty of any kind.  The library has been
+entirely written by Jean-loup Gailly and Mark Adler; it does not
+include third-party code.
+
+If you redistribute modified sources, we would appreciate that you include
+in the file ChangeLog history information documenting your changes.
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/README.586 linux-2.6.18.5/fs/ext2/gzip/README.586
--- linux-2.6.18.5.org/fs/ext2/gzip/README.586	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/README.586	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,43 @@
+This is a patched version of zlib modified to use
+Pentium-optimized assembly code in the deflation algorithm. The files
+changed/added by this patch are:
+
+README.586
+match.S
+
+The effectiveness of these modifications is a bit marginal, as the the
+program's bottleneck seems to be mostly L1-cache contention, for which
+there is no real way to work around without rewriting the basic
+algorithm. The speedup on average is around 5-10% (which is generally
+less than the amount of variance between subsequent executions).
+However, when used at level 9 compression, the cache contention can
+drop enough for the assembly version to achieve 10-20% speedup (and
+sometimes more, depending on the amount of overall redundancy in the
+files). Even here, though, cache contention can still be the limiting
+factor, depending on the nature of the program using the zlib library.
+This may also mean that better improvements will be seen on a Pentium
+with MMX, which suffers much less from L1-cache contention, but I have
+not yet verified this.
+
+Note that this code has been tailored for the Pentium in particular,
+and will not perform well on the Pentium Pro (due to the use of a
+partial register in the inner loop).
+
+If you are using an assembler other than GNU as, you will have to
+translate match.S to use your assembler's syntax. (Have fun.)
+
+Brian Raiter
+breadbox@muppetlabs.com
+April, 1998
+
+
+Added for zlib 1.1.3:
+
+The patches come from
+http://www.muppetlabs.com/~breadbox/software/assembly.html
+
+To compile zlib with this asm file, copy match.S to the zlib directory
+then do:
+
+CFLAGS="-O3 -DASMV" ./configure
+make OBJA=match.o
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/README.686 linux-2.6.18.5/fs/ext2/gzip/README.686
--- linux-2.6.18.5.org/fs/ext2/gzip/README.686	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/README.686	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,34 @@
+This is a patched version of zlib, modified to use
+Pentium-Pro-optimized assembly code in the deflation algorithm. The
+files changed/added by this patch are:
+
+README.686
+match.S
+
+The speedup that this patch provides varies, depending on whether the
+compiler used to build the original version of zlib falls afoul of the
+PPro's speed traps. My own tests show a speedup of around 10-20% at
+the default compression level, and 20-30% using -9, against a version
+compiled using gcc 2.7.2.3. Your mileage may vary.
+
+Note that this code has been tailored for the PPro/PII in particular,
+and will not perform particuarly well on a Pentium.
+
+If you are using an assembler other than GNU as, you will have to
+translate match.S to use your assembler's syntax. (Have fun.)
+
+Brian Raiter
+breadbox@muppetlabs.com
+April, 1998
+
+
+Added for zlib 1.1.3:
+
+The patches come from
+http://www.muppetlabs.com/~breadbox/software/assembly.html
+
+To compile zlib with this asm file, copy match.S to the zlib directory
+then do:
+
+CFLAGS="-O3 -DASMV" ./configure
+make OBJA=match.o
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/algorithm.txt linux-2.6.18.5/fs/ext2/gzip/algorithm.txt
--- linux-2.6.18.5.org/fs/ext2/gzip/algorithm.txt	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/algorithm.txt	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,213 @@
+1. Compression algorithm (deflate)
+
+The deflation algorithm used by gzip (also zip and zlib) is a variation of
+LZ77 (Lempel-Ziv 1977, see reference below). It finds duplicated strings in
+the input data.  The second occurrence of a string is replaced by a
+pointer to the previous string, in the form of a pair (distance,
+length).  Distances are limited to 32K bytes, and lengths are limited
+to 258 bytes. When a string does not occur anywhere in the previous
+32K bytes, it is emitted as a sequence of literal bytes.  (In this
+description, `string' must be taken as an arbitrary sequence of bytes,
+and is not restricted to printable characters.)
+
+Literals or match lengths are compressed with one Huffman tree, and
+match distances are compressed with another tree. The trees are stored
+in a compact form at the start of each block. The blocks can have any
+size (except that the compressed data for one block must fit in
+available memory). A block is terminated when deflate() determines that
+it would be useful to start another block with fresh trees. (This is
+somewhat similar to the behavior of LZW-based _compress_.)
+
+Duplicated strings are found using a hash table. All input strings of
+length 3 are inserted in the hash table. A hash index is computed for
+the next 3 bytes. If the hash chain for this index is not empty, all
+strings in the chain are compared with the current input string, and
+the longest match is selected.
+
+The hash chains are searched starting with the most recent strings, to
+favor small distances and thus take advantage of the Huffman encoding.
+The hash chains are singly linked. There are no deletions from the
+hash chains, the algorithm simply discards matches that are too old.
+
+To avoid a worst-case situation, very long hash chains are arbitrarily
+truncated at a certain length, determined by a runtime option (level
+parameter of deflateInit). So deflate() does not always find the longest
+possible match but generally finds a match which is long enough.
+
+deflate() also defers the selection of matches with a lazy evaluation
+mechanism. After a match of length N has been found, deflate() searches for
+a longer match at the next input byte. If a longer match is found, the
+previous match is truncated to a length of one (thus producing a single
+literal byte) and the process of lazy evaluation begins again. Otherwise,
+the original match is kept, and the next match search is attempted only N
+steps later.
+
+The lazy match evaluation is also subject to a runtime parameter. If
+the current match is long enough, deflate() reduces the search for a longer
+match, thus speeding up the whole process. If compression ratio is more
+important than speed, deflate() attempts a complete second search even if
+the first match is already long enough.
+
+The lazy match evaluation is not performed for the fastest compression
+modes (level parameter 1 to 3). For these fast modes, new strings
+are inserted in the hash table only when no match was found, or
+when the match is not too long. This degrades the compression ratio
+but saves time since there are both fewer insertions and fewer searches.
+
+
+2. Decompression algorithm (inflate)
+
+2.1 Introduction
+
+The real question is, given a Huffman tree, how to decode fast.  The most
+important realization is that shorter codes are much more common than
+longer codes, so pay attention to decoding the short codes fast, and let
+the long codes take longer to decode.
+
+inflate() sets up a first level table that covers some number of bits of
+input less than the length of longest code.  It gets that many bits from the
+stream, and looks it up in the table.  The table will tell if the next
+code is that many bits or less and how many, and if it is, it will tell
+the value, else it will point to the next level table for which inflate()
+grabs more bits and tries to decode a longer code.
+
+How many bits to make the first lookup is a tradeoff between the time it
+takes to decode and the time it takes to build the table.  If building the
+table took no time (and if you had infinite memory), then there would only
+be a first level table to cover all the way to the longest code.  However,
+building the table ends up taking a lot longer for more bits since short
+codes are replicated many times in such a table.  What inflate() does is
+simply to make the number of bits in the first table a variable, and set it
+for the maximum speed.
+
+inflate() sends new trees relatively often, so it is possibly set for a
+smaller first level table than an application that has only one tree for
+all the data.  For inflate, which has 286 possible codes for the
+literal/length tree, the size of the first table is nine bits.  Also the
+distance trees have 30 possible values, and the size of the first table is
+six bits.  Note that for each of those cases, the table ended up one bit
+longer than the ``average'' code length, i.e. the code length of an
+approximately flat code which would be a little more than eight bits for
+286 symbols and a little less than five bits for 30 symbols.  It would be
+interesting to see if optimizing the first level table for other
+applications gave values within a bit or two of the flat code size.
+
+
+2.2 More details on the inflate table lookup
+
+Ok, you want to know what this cleverly obfuscated inflate tree actually  
+looks like.  You are correct that it's not a Huffman tree.  It is simply a  
+lookup table for the first, let's say, nine bits of a Huffman symbol.  The  
+symbol could be as short as one bit or as long as 15 bits.  If a particular  
+symbol is shorter than nine bits, then that symbol's translation is duplicated
+in all those entries that start with that symbol's bits.  For example, if the  
+symbol is four bits, then it's duplicated 32 times in a nine-bit table.  If a  
+symbol is nine bits long, it appears in the table once.
+
+If the symbol is longer than nine bits, then that entry in the table points  
+to another similar table for the remaining bits.  Again, there are duplicated  
+entries as needed.  The idea is that most of the time the symbol will be short
+and there will only be one table look up.  (That's whole idea behind data  
+compression in the first place.)  For the less frequent long symbols, there  
+will be two lookups.  If you had a compression method with really long  
+symbols, you could have as many levels of lookups as is efficient.  For  
+inflate, two is enough.
+
+So a table entry either points to another table (in which case nine bits in  
+the above example are gobbled), or it contains the translation for the symbol  
+and the number of bits to gobble.  Then you start again with the next  
+ungobbled bit.
+
+You may wonder: why not just have one lookup table for how ever many bits the  
+longest symbol is?  The reason is that if you do that, you end up spending  
+more time filling in duplicate symbol entries than you do actually decoding.   
+At least for deflate's output that generates new trees every several 10's of  
+kbytes.  You can imagine that filling in a 2^15 entry table for a 15-bit code  
+would take too long if you're only decoding several thousand symbols.  At the  
+other extreme, you could make a new table for every bit in the code.  In fact,
+that's essentially a Huffman tree.  But then you spend two much time  
+traversing the tree while decoding, even for short symbols.
+
+So the number of bits for the first lookup table is a trade of the time to  
+fill out the table vs. the time spent looking at the second level and above of
+the table.
+
+Here is an example, scaled down:
+
+The code being decoded, with 10 symbols, from 1 to 6 bits long:
+
+A: 0
+B: 10
+C: 1100
+D: 11010
+E: 11011
+F: 11100
+G: 11101
+H: 11110
+I: 111110
+J: 111111
+
+Let's make the first table three bits long (eight entries):
+
+000: A,1
+001: A,1
+010: A,1
+011: A,1
+100: B,2
+101: B,2
+110: -> table X (gobble 3 bits)
+111: -> table Y (gobble 3 bits)
+
+Each entry is what the bits decode to and how many bits that is, i.e. how  
+many bits to gobble.  Or the entry points to another table, with the number of
+bits to gobble implicit in the size of the table.
+
+Table X is two bits long since the longest code starting with 110 is five bits
+long:
+
+00: C,1
+01: C,1
+10: D,2
+11: E,2
+
+Table Y is three bits long since the longest code starting with 111 is six  
+bits long:
+
+000: F,2
+001: F,2
+010: G,2
+011: G,2
+100: H,2
+101: H,2
+110: I,3
+111: J,3
+
+So what we have here are three tables with a total of 20 entries that had to  
+be constructed.  That's compared to 64 entries for a single table.  Or  
+compared to 16 entries for a Huffman tree (six two entry tables and one four  
+entry table).  Assuming that the code ideally represents the probability of  
+the symbols, it takes on the average 1.25 lookups per symbol.  That's compared
+to one lookup for the single table, or 1.66 lookups per symbol for the  
+Huffman tree.
+
+There, I think that gives you a picture of what's going on.  For inflate, the  
+meaning of a particular symbol is often more than just a letter.  It can be a  
+byte (a "literal"), or it can be either a length or a distance which  
+indicates a base value and a number of bits to fetch after the code that is  
+added to the base value.  Or it might be the special end-of-block code.  The  
+data structures created in inftrees.c try to encode all that information  
+compactly in the tables.
+
+
+Jean-loup Gailly        Mark Adler
+jloup@gzip.org          madler@alumni.caltech.edu
+
+
+References:
+
+[LZ77] Ziv J., Lempel A., ``A Universal Algorithm for Sequential Data
+Compression,'' IEEE Transactions on Information Theory, Vol. 23, No. 3,
+pp. 337-343.
+
+``DEFLATE Compressed Data Format Specification'' available in
+ftp://ds.internic.net/rfc/rfc1951.txt
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/deflate.c linux-2.6.18.5/fs/ext2/gzip/deflate.c
--- linux-2.6.18.5.org/fs/ext2/gzip/deflate.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/deflate.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,1442 @@
+/* deflate.c -- compress data using the deflation algorithm
+ * Copyright (C) 1995-1998 Jean-loup Gailly.
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/*
+ *  ALGORITHM
+ *
+ *      The "deflation" process depends on being able to identify portions
+ *      of the input text which are identical to earlier input (within a
+ *      sliding window trailing behind the input currently being processed).
+ *
+ *      The most straightforward technique turns out to be the fastest for
+ *      most input files: try all possible matches and select the longest.
+ *      The key feature of this algorithm is that insertions into the string
+ *      dictionary are very simple and thus fast, and deletions are avoided
+ *      completely. Insertions are performed at each input character, whereas
+ *      string matches are performed only when the previous match ends. So it
+ *      is preferable to spend more time in matches to allow very fast string
+ *      insertions and avoid deletions. The matching algorithm for small
+ *      strings is inspired from that of Rabin & Karp. A brute force approach
+ *      is used to find longer strings when a small match has been found.
+ *      A similar algorithm is used in comic (by Jan-Mark Wams) and freeze
+ *      (by Leonid Broukhis).
+ *         A previous version of this file used a more sophisticated algorithm
+ *      (by Fiala and Greene) which is guaranteed to run in linear amortized
+ *      time, but has a larger average cost, uses more memory and is patented.
+ *      However the F&G algorithm may be faster for some highly redundant
+ *      files if the parameter max_chain_length (described below) is too large.
+ *
+ *  ACKNOWLEDGEMENTS
+ *
+ *      The idea of lazy evaluation of matches is due to Jan-Mark Wams, and
+ *      I found it in 'freeze' written by Leonid Broukhis.
+ *      Thanks to many people for bug reports and testing.
+ *
+ *  REFERENCES
+ *
+ *      Deutsch, L.P.,"DEFLATE Compressed Data Format Specification".
+ *      Available in ftp://ds.internic.net/rfc/rfc1951.txt
+ *
+ *      A description of the Rabin and Karp algorithm is given in the book
+ *         "Algorithms" by R. Sedgewick, Addison-Wesley, p252.
+ *
+ *      Fiala,E.R., and Greene,D.H.
+ *         Data Compression with Finite Windows, Comm.ACM, 32,4 (1989) 490-595
+ *
+ */
+
+/* @(#) $Id$ */
+
+#include "deflate.h"
+
+#ifdef __KERNEL__
+# include <linux/config.h>
+# include <linux/linkage.h>
+# include <linux/sched.h>
+# include <asm/param.h>
+static unsigned long next_brk = 0;
+extern unsigned long volatile jiffies;
+#endif
+
+#define EXT2_COMPRESS
+
+#undef  ERR_RETURN
+#define ERR_RETURN(_a,_b)	return _b
+
+const char ext2_deflate_copyright[] =
+   " deflate 1.1.3 Copyright 1995-1998 Jean-loup Gailly ";
+/*
+  If you use the zlib library in a product, an acknowledgment is welcome
+  in the documentation of your product. If for some reason you cannot
+  include such an acknowledgment, I would appreciate that you keep this
+  copyright string in the executable of your product.
+ */
+
+/* ===========================================================================
+ *  Function prototypes.
+ */
+typedef enum {
+    need_more,      /* block not completed, need more input or more output */
+    block_done,     /* block flush performed */
+    finish_started, /* finish started, need only more output at next deflate */
+    finish_done     /* finish done, accept no more input or output */
+} block_state;
+
+typedef block_state (*compress_func) OF((deflate_state *s, int flush));
+/* Compression function. Returns the block state after the call. */
+
+local void fill_window    OF((deflate_state *s));
+local block_state deflate_stored OF((deflate_state *s, int flush));
+local block_state deflate_fast   OF((deflate_state *s, int flush));
+local block_state deflate_slow   OF((deflate_state *s, int flush));
+local void lm_init        OF((deflate_state *s));
+local void putShortMSB    OF((deflate_state *s, uInt b));
+local void flush_pending  OF((z_streamp strm));
+local int read_buf        OF((z_streamp strm, Byte *buf, unsigned size));
+#ifdef CONFIG_EXT2_COMPR_X86_CODE
+      void match_init OF((void)); /* asm code initialization */
+      uInt longest_match  OF((deflate_state *s, IPos cur_match));
+#else
+local uInt longest_match  OF((deflate_state *s, IPos cur_match));
+#endif
+
+#ifdef DEBUG
+local  void check_match OF((deflate_state *s, IPos start, IPos match,
+                            int length));
+#endif
+
+/* ===========================================================================
+ * Local data
+ */
+
+#define NIL 0
+/* Tail of hash chains */
+
+#ifndef TOO_FAR
+#  define TOO_FAR 4096
+#endif
+/* Matches of length 3 are discarded if their distance exceeds TOO_FAR */
+
+#define MIN_LOOKAHEAD (MAX_MATCH+MIN_MATCH+1)
+/* Minimum amount of lookahead, except at the end of the input file.
+ * See deflate.c for comments about the MIN_MATCH+1.
+ */
+
+/* Values for max_lazy_match, good_match and max_chain_length, depending on
+ * the desired pack level (0..9). The values given below have been tuned to
+ * exclude worst case performance for pathological files. Better values may be
+ * found for specific files.
+ */
+typedef struct config_s {
+   ush good_length; /* reduce lazy search above this match length */
+   ush max_lazy;    /* do not perform lazy search above this match length */
+   ush nice_length; /* quit search above this match length */
+   ush max_chain;
+   compress_func func;
+} config;
+
+local const config configuration_table[10] = {
+/*      good lazy nice chain */
+/* 0 */ {0,    0,  0,    0, deflate_stored},  /* store only */
+/* 1 */ {4,    4,  8,    4, deflate_fast}, /* maximum speed, no lazy matches */
+/* 2 */ {4,    5, 16,    8, deflate_fast},
+/* 3 */ {4,    6, 32,   32, deflate_fast},
+
+/* 4 */ {4,    4, 16,   16, deflate_slow},  /* lazy matches */
+/* 5 */ {8,   16, 32,   32, deflate_slow},
+/* 6 */ {8,   16, 128, 128, deflate_slow},
+/* 7 */ {8,   32, 128, 256, deflate_slow},
+/* 8 */ {32, 128, 258, 1024, deflate_slow},
+/* 9 */ {32, 258, 258, 4096, deflate_slow}}; /* maximum compression */
+
+/* Note: the deflate() code requires max_lazy >= MIN_MATCH and max_chain >= 4
+ * For deflate_fast() (levels <= 3) good is ignored and lazy has a different
+ * meaning.
+ */
+
+#define EQUAL 0
+/* result of memcmp for equal strings */
+
+struct static_tree_desc_s {int dummy;}; /* for buggy compilers */
+
+/* ===========================================================================
+ * Update a hash value with the given input byte
+ * IN  assertion: all calls to to UPDATE_HASH are made with consecutive
+ *    input characters, so that a running hash key can be computed from the
+ *    previous key instead of complete recalculation each time.
+ */
+#define UPDATE_HASH(s,h,c) (h = (((h)<<s->hash_shift) ^ (c)) & s->hash_mask)
+
+
+/* ===========================================================================
+ * Insert string str in the dictionary and set match_head to the previous head
+ * of the hash chain (the most recent string with same hash key). Return
+ * the previous length of the hash chain.
+ * If this file is compiled with -DFASTEST, the compression level is forced
+ * to 1, and no hash chains are maintained.
+ * IN  assertion: all calls to to INSERT_STRING are made with consecutive
+ *    input characters and the first MIN_MATCH bytes of str are valid
+ *    (except for the last MIN_MATCH-1 bytes of the input file).
+ */
+#ifdef FASTEST
+#define INSERT_STRING(s, str, match_head) \
+   (UPDATE_HASH(s, s->ins_h, s->window[(str) + (MIN_MATCH-1)]), \
+    match_head = s->head[s->ins_h], \
+    s->head[s->ins_h] = (Pos)(str))
+#else
+#define INSERT_STRING(s, str, match_head) \
+   (UPDATE_HASH(s, s->ins_h, s->window[(str) + (MIN_MATCH-1)]), \
+    s->prev[(str) & s->w_mask] = match_head = s->head[s->ins_h], \
+    s->head[s->ins_h] = (Pos)(str))
+#endif
+
+/* ===========================================================================
+ * Initialize the hash table (avoiding 64K overflow for 16 bit systems).
+ * prev[] will be initialized on the fly.
+ */
+#define CLEAR_HASH(s) \
+    s->head[s->hash_size-1] = NIL; \
+    zmemzero((Byte *)s->head, (unsigned)(s->hash_size-1)*sizeof(*s->head));
+
+/* ========================================================================= */
+#if 0
+int deflateInit_(strm, level, version, stream_size)
+    z_streamp strm;
+    int level;
+    const char *version;
+    int stream_size;
+{
+    return deflateInit2_(strm, level, Z_DEFLATED, MAX_WBITS, DEF_MEM_LEVEL,
+			 Z_DEFAULT_STRATEGY, version, stream_size);
+    /* To do: ignore strm->next_in if we use it as window */
+}
+#endif
+/* ========================================================================= */
+int ext2_deflateInit2_(strm, level, method, windowBits, memLevel, strategy,
+		  version, stream_size)
+    z_streamp strm;
+    int  level;
+    int  method;
+    int  windowBits;
+    int  memLevel;
+    int  strategy;
+    const char *version;
+    int stream_size;
+{
+    deflate_state *s;
+    int noheader = 0;
+    static const char* my_version = ZLIB_VERSION;
+
+    ush *overlay;
+    /* We overlay pending_buf and d_buf+l_buf. This works since the average
+     * output size for (length,distance) codes is <= 24 bits.
+     */
+
+    if (version == Z_NULL || version[0] != my_version[0] ||
+        stream_size != sizeof(z_stream)) {
+	return Z_VERSION_ERROR;
+    }
+    if (strm == Z_NULL) return Z_STREAM_ERROR;
+
+#if 0
+    strm->msg = Z_NULL;
+    if (strm->zalloc == Z_NULL) {
+	strm->zalloc = zcalloc;
+	strm->opaque = (voidp)0;
+    }
+    if (strm->zfree == Z_NULL) strm->zfree = zcfree;
+#else
+    if (strm->zalloc == Z_NULL || strm->zfree == Z_NULL)
+        return Z_STREAM_ERROR;
+#endif
+    if (level == Z_DEFAULT_COMPRESSION) level = 6;
+#ifdef FASTEST
+    level = 1;
+#endif
+
+    if (windowBits < 0) { /* undocumented feature: suppress zlib header */
+        noheader = 1;
+        windowBits = -windowBits;
+    }
+    if (memLevel < 1 || memLevel > MAX_MEM_LEVEL || method != Z_DEFLATED ||
+        windowBits < 8 || windowBits > 15 || level < 0 || level > 9 ||
+	strategy < 0 || strategy > Z_HUFFMAN_ONLY) {
+        return Z_STREAM_ERROR;
+    }
+    s = (deflate_state *) ZALLOC(strm, 1, sizeof(deflate_state));
+    if (s == Z_NULL) return Z_MEM_ERROR;
+    strm->state = (struct internal_state *)s;
+    s->strm = strm;
+
+    s->noheader = noheader;
+    s->w_bits = windowBits;
+    s->w_size = 1 << s->w_bits;
+    s->w_mask = s->w_size - 1;
+
+    s->hash_bits = memLevel + 7;
+    s->hash_size = 1 << s->hash_bits;
+    s->hash_mask = s->hash_size - 1;
+    s->hash_shift =  ((s->hash_bits+MIN_MATCH-1)/MIN_MATCH);
+
+#if 1
+    s->window = (Byte *)strm->next_in;
+#else
+    s->window = (Byte *) ZALLOC(strm, s->w_size, 2*sizeof(Byte));
+#endif
+    s->prev   = (Pos *)  ZALLOC(strm, s->w_size, sizeof(Pos));
+    s->head   = (Pos *)  ZALLOC(strm, s->hash_size, sizeof(Pos));
+
+    s->lit_bufsize = 1 << (memLevel + 6); /* 16K elements by default */
+
+    overlay = (ush *) ZALLOC(strm, s->lit_bufsize, sizeof(ush)+2);
+    s->pending_buf = (uch *) overlay;
+    s->pending_buf_size = (ulg)s->lit_bufsize * (sizeof(ush)+2L);
+
+    if (s->window == Z_NULL || s->prev == Z_NULL || s->head == Z_NULL ||
+        s->pending_buf == Z_NULL) {
+#if 0
+        strm->msg = (char*)ERR_MSG(Z_MEM_ERROR);
+#endif
+        ext2_deflateEnd (strm);
+        return Z_MEM_ERROR;
+    }
+    s->d_buf = overlay + s->lit_bufsize/sizeof(ush);
+    s->l_buf = s->pending_buf + (1+sizeof(ush))*s->lit_bufsize;
+
+    s->level = level;
+    s->strategy = strategy;
+    s->method = (Byte)method;
+
+    return ext2_deflateReset(strm);
+}
+
+/* ========================================================================= */
+int ext2_deflateSetDictionary (strm, dictionary, dictLength)
+    z_streamp strm;
+    const Byte *dictionary;
+    uInt  dictLength;
+{
+    deflate_state *s;
+    uInt length = dictLength;
+    uInt n;
+    IPos hash_head = 0;
+
+    if (strm == Z_NULL || strm->state == Z_NULL || dictionary == Z_NULL ||
+        strm->state->status != INIT_STATE) return Z_STREAM_ERROR;
+
+    s = strm->state;
+    strm->adler = ext2_adler32(strm->adler, dictionary, dictLength);
+
+    if (length < MIN_MATCH) return Z_OK;
+    if (length > MAX_DIST(s)) {
+	length = MAX_DIST(s);
+#ifndef USE_DICT_HEAD
+	dictionary += dictLength - length; /* use the tail of the dictionary */
+#endif
+    }
+    zmemcpy(s->window, dictionary, length);
+    s->strstart = length;
+    s->block_start = (long)length;
+
+    /* Insert all strings in the hash table (except for the last two bytes).
+     * s->lookahead stays null, so s->ins_h will be recomputed at the next
+     * call of fill_window.
+     */
+    s->ins_h = s->window[0];
+    UPDATE_HASH(s, s->ins_h, s->window[1]);
+    for (n = 0; n <= length - MIN_MATCH; n++) {
+	INSERT_STRING(s, n, hash_head);
+    }
+    if (hash_head) hash_head = 0;  /* to make compiler happy */
+    return Z_OK;
+}
+
+/* ========================================================================= */
+int ext2_deflateReset (strm)
+    z_streamp strm;
+{
+    deflate_state *s;
+    
+    if (strm == Z_NULL || strm->state == Z_NULL ||
+        strm->zalloc == Z_NULL || strm->zfree == Z_NULL) return Z_STREAM_ERROR;
+
+    strm->total_in = strm->total_out = 0;
+    strm->msg = Z_NULL; /* use zfree if we ever allocate msg dynamically */
+    strm->data_type = Z_UNKNOWN;
+
+    s = (deflate_state *)strm->state;
+#ifdef EXT2_COMPRESS
+    s->data_type = Z_UNKNOWN;
+#endif
+    s->pending = 0;
+    s->pending_out = s->pending_buf;
+
+    if (s->noheader < 0) {
+        s->noheader = 0; /* was set to -1 by deflate(..., Z_FINISH); */
+    }
+    s->status = s->noheader ? BUSY_STATE : INIT_STATE;
+    strm->adler = 1;
+    s->last_flush = Z_NO_FLUSH;
+
+    ext2_tr_init(s);
+    lm_init(s);
+
+    return Z_OK;
+}
+
+/* ========================================================================= */
+int ext2_deflateParams(strm, level, strategy)
+    z_streamp strm;
+    int level;
+    int strategy;
+{
+    deflate_state *s;
+    compress_func func;
+    int err = Z_OK;
+
+    if (strm == Z_NULL || strm->state == Z_NULL) return Z_STREAM_ERROR;
+    s = strm->state;
+
+    if (level == Z_DEFAULT_COMPRESSION) {
+	level = 6;
+    }
+    if (level < 0 || level > 9 || strategy < 0 || strategy > Z_HUFFMAN_ONLY) {
+	return Z_STREAM_ERROR;
+    }
+    func = configuration_table[s->level].func;
+
+    if (func != configuration_table[level].func && strm->total_in != 0) {
+	/* Flush the last buffer: */
+	err = ext2_deflate(strm, Z_PARTIAL_FLUSH);
+    }
+    if (s->level != level) {
+	s->level = level;
+	s->max_lazy_match   = configuration_table[level].max_lazy;
+	s->good_match       = configuration_table[level].good_length;
+	s->nice_match       = configuration_table[level].nice_length;
+	s->max_chain_length = configuration_table[level].max_chain;
+    }
+    s->strategy = strategy;
+    return err;
+}
+
+/* =========================================================================
+ * Put a short in the pending buffer. The 16-bit value is put in MSB order.
+ * IN assertion: the stream state is correct and there is enough room in
+ * pending_buf.
+ */
+local void putShortMSB (s, b)
+    deflate_state *s;
+    uInt b;
+{
+    put_byte(s, (Byte)(b >> 8));
+    put_byte(s, (Byte)(b & 0xff));
+}   
+
+/* =========================================================================
+ * Flush as much pending output as possible. All deflate() output goes
+ * through this function so some applications may wish to modify it
+ * to avoid allocating a large strm->next_out buffer and copying into it.
+ * (See also read_buf()).
+ */
+local void flush_pending(strm)
+    z_streamp strm;
+{
+    unsigned len = strm->state->pending;
+
+    if (len > strm->avail_out) len = strm->avail_out;
+    if (len == 0) return;
+
+    zmemcpy(strm->next_out, strm->state->pending_out, len);
+    strm->next_out  += len;
+    strm->state->pending_out  += len;
+    strm->total_out += len;
+    strm->avail_out  -= len;
+    strm->state->pending -= len;
+    if (strm->state->pending == 0) {
+        strm->state->pending_out = strm->state->pending_buf;
+    }
+}
+
+/* ========================================================================= */
+int ext2_deflate (strm, flush)
+    z_streamp strm;
+    int flush;
+{
+    int old_flush; /* value of flush param for previous deflate call */
+    deflate_state *s;
+
+    if (strm == Z_NULL || strm->state == Z_NULL ||
+	flush > Z_FINISH || flush < 0) {
+        return Z_STREAM_ERROR;
+    }
+    s = strm->state;
+
+    if (strm->next_out == Z_NULL ||
+        (strm->next_in == Z_NULL && strm->avail_in != 0) ||
+	(s->status == FINISH_STATE && flush != Z_FINISH)) {
+        ERR_RETURN(strm, Z_STREAM_ERROR);
+    }
+    if (strm->avail_out == 0) ERR_RETURN(strm, Z_BUF_ERROR);
+
+    s->strm = strm; /* just in case */
+    old_flush = s->last_flush;
+    s->last_flush = flush;
+
+    /* Write the zlib header */
+    if (s->status == INIT_STATE) {
+
+        uInt header = (Z_DEFLATED + ((s->w_bits-8)<<4)) << 8;
+        uInt level_flags = (s->level-1) >> 1;
+
+        if (level_flags > 3) level_flags = 3;
+        header |= (level_flags << 6);
+	if (s->strstart != 0) header |= PRESET_DICT;
+        header += 31 - (header % 31);
+
+        s->status = BUSY_STATE;
+        putShortMSB(s, header);
+
+	/* Save the adler32 of the preset dictionary: */
+	if (s->strstart != 0) {
+	    putShortMSB(s, (uInt)(strm->adler >> 16));
+	    putShortMSB(s, (uInt)(strm->adler & 0xffff));
+	}
+	strm->adler = 1L;
+    }
+
+    /* Flush as much pending output as possible */
+    if (s->pending != 0) {
+        flush_pending(strm);
+        if (strm->avail_out == 0) {
+	    /* Since avail_out is 0, deflate will be called again with
+	     * more output space, but possibly with both pending and
+	     * avail_in equal to zero. There won't be anything to do,
+	     * but this is not an error situation so make sure we
+	     * return OK instead of BUF_ERROR at next call of deflate:
+             */
+	    s->last_flush = -1;
+	    return Z_OK;
+	}
+
+    /* Make sure there is something to do and avoid duplicate consecutive
+     * flushes. For repeated and useless calls with Z_FINISH, we keep
+     * returning Z_STREAM_END instead of Z_BUFF_ERROR.
+     */
+    } else if (strm->avail_in == 0 && flush <= old_flush &&
+	       flush != Z_FINISH) {
+        ERR_RETURN(strm, Z_BUF_ERROR);
+    }
+
+    /* User must not provide more input after the first FINISH: */
+    if (s->status == FINISH_STATE && strm->avail_in != 0) {
+        ERR_RETURN(strm, Z_BUF_ERROR);
+    }
+
+    /* Start a new block or continue the current one.
+     */
+    if (strm->avail_in != 0 || s->lookahead != 0 ||
+        (flush != Z_NO_FLUSH && s->status != FINISH_STATE)) {
+        block_state bstate;
+
+	bstate = (*(configuration_table[s->level].func))(s, flush);
+
+        if (bstate == finish_started || bstate == finish_done) {
+            s->status = FINISH_STATE;
+        }
+        if (bstate == need_more || bstate == finish_started) {
+	    if (strm->avail_out == 0) {
+	        s->last_flush = -1; /* avoid BUF_ERROR next call, see above */
+	    }
+	    return Z_OK;
+	    /* If flush != Z_NO_FLUSH && avail_out == 0, the next call
+	     * of deflate should use the same flush parameter to make sure
+	     * that the flush is complete. So we don't have to output an
+	     * empty block here, this will be done at next call. This also
+	     * ensures that for a very small output buffer, we emit at most
+	     * one empty block.
+	     */
+	}
+        if (bstate == block_done) {
+            if (flush == Z_PARTIAL_FLUSH) {
+                ext2_tr_align(s);
+            } else { /* FULL_FLUSH or SYNC_FLUSH */
+                ext2_tr_stored_block(s, (char*)0, 0L, 0);
+                /* For a full flush, this empty block will be recognized
+                 * as a special marker by inflate_sync().
+                 */
+                if (flush == Z_FULL_FLUSH) {
+                    CLEAR_HASH(s);             /* forget history */
+                }
+            }
+            flush_pending(strm);
+	    if (strm->avail_out == 0) {
+	      s->last_flush = -1; /* avoid BUF_ERROR at next call, see above */
+	      return Z_OK;
+	    }
+        }
+    }
+    Assert(strm->avail_out > 0, "bug2");
+
+    if (flush != Z_FINISH) return Z_OK;
+    if (s->noheader) return Z_STREAM_END;
+
+    /* Write the zlib trailer (adler32) */
+    putShortMSB(s, (uInt)(strm->adler >> 16));
+    putShortMSB(s, (uInt)(strm->adler & 0xffff));
+    flush_pending(strm);
+    /* If avail_out is zero, the application will call deflate again
+     * to flush the rest.
+     */
+    s->noheader = -1; /* write the trailer only once! */
+    return s->pending != 0 ? Z_OK : Z_STREAM_END;
+}
+
+/* ========================================================================= */
+int ext2_deflateEnd (strm)
+    z_streamp strm;
+{
+    int status;
+
+    if (strm == Z_NULL || strm->state == Z_NULL) return Z_STREAM_ERROR;
+
+    status = strm->state->status;
+    if (status != INIT_STATE && status != BUSY_STATE &&
+	status != FINISH_STATE) {
+      return Z_STREAM_ERROR;
+    }
+
+    /* Deallocate in reverse order of allocations: */
+    TRY_FREE(strm, strm->state->pending_buf);
+    TRY_FREE(strm, strm->state->head);
+    TRY_FREE(strm, strm->state->prev);
+#if 0
+    TRY_FREE(strm, strm->state->window);
+#endif
+    ZFREE(strm, strm->state);
+    strm->state = Z_NULL;
+
+    return status == BUSY_STATE ? Z_DATA_ERROR : Z_OK;
+}
+
+/* =========================================================================
+ * Copy the source state to the destination state.
+ * To simplify the source, this is not supported for 16-bit MSDOS (which
+ * doesn't have enough memory anyway to duplicate compression states).
+ */
+#if 0
+int deflateCopy (dest, source)
+    z_streamp dest;
+    z_streamp source;
+{
+#ifdef MAXSEG_64K
+    return Z_STREAM_ERROR;
+#else
+    deflate_state *ds;
+    deflate_state *ss;
+    ush *overlay;
+
+
+    if (source == Z_NULL || dest == Z_NULL || source->state == Z_NULL) {
+        return Z_STREAM_ERROR;
+    }
+
+    ss = source->state;
+
+    *dest = *source;
+
+    ds = (deflate_state *) ZALLOC(dest, 1, sizeof(deflate_state));
+    if (ds == Z_NULL) return Z_MEM_ERROR;
+    dest->state = (struct internal_state *) ds;
+    *ds = *ss;
+    ds->strm = dest;
+
+    ds->window = (Byte *) ZALLOC(dest, ds->w_size, 2*sizeof(Byte));
+    ds->prev   = (Pos *)  ZALLOC(dest, ds->w_size, sizeof(Pos));
+    ds->head   = (Pos *)  ZALLOC(dest, ds->hash_size, sizeof(Pos));
+    overlay = (ush *) ZALLOC(dest, ds->lit_bufsize, sizeof(ush)+2);
+    ds->pending_buf = (uch *) overlay;
+
+    if (ds->window == Z_NULL || ds->prev == Z_NULL || ds->head == Z_NULL ||
+        ds->pending_buf == Z_NULL) {
+        deflateEnd (dest);
+        return Z_MEM_ERROR;
+    }
+    /* following zmemcpy do not work for 16-bit MSDOS */
+    zmemcpy(ds->window, ss->window, ds->w_size * 2 * sizeof(Byte));
+    zmemcpy(ds->prev, ss->prev, ds->w_size * sizeof(Pos));
+    zmemcpy(ds->head, ss->head, ds->hash_size * sizeof(Pos));
+    zmemcpy(ds->pending_buf, ss->pending_buf, (uInt)ds->pending_buf_size);
+
+    ds->pending_out = ds->pending_buf + (ss->pending_out - ss->pending_buf);
+    ds->d_buf = overlay + ds->lit_bufsize/sizeof(ush);
+    ds->l_buf = ds->pending_buf + (1+sizeof(ush))*ds->lit_bufsize;
+
+    ds->l_desc.dyn_tree = ds->dyn_ltree;
+    ds->d_desc.dyn_tree = ds->dyn_dtree;
+    ds->bl_desc.dyn_tree = ds->bl_tree;
+
+    return Z_OK;
+#endif
+}
+#endif
+/* ===========================================================================
+ * Read a new buffer from the current input stream, update the adler32
+ * and total number of bytes read.  All deflate() input goes through
+ * this function so some applications may wish to modify it to avoid
+ * allocating a large strm->next_in buffer and copying from it.
+ * (See also flush_pending()).
+ */
+local int read_buf(strm, buf, size)
+    z_streamp strm;
+    Byte *buf;
+    unsigned size;
+{
+    unsigned len = strm->avail_in;
+
+    if (len > size) len = size;
+    if (len == 0) return 0;
+
+    strm->avail_in  -= len;
+
+    if (!strm->state->noheader) {
+        strm->adler = ext2_adler32(strm->adler, strm->next_in, len);
+    }
+    zmemcpy(buf, strm->next_in, len);
+    strm->next_in  += len;
+    strm->total_in += len;
+
+    return (int)len;
+}
+
+/* ===========================================================================
+ * Initialize the "longest match" routines for a new zlib stream
+ */
+local void lm_init (s)
+    deflate_state *s;
+{
+    s->window_size = (ulg)2L*s->w_size;
+
+    CLEAR_HASH(s);
+
+    /* Set the default configuration parameters:
+     */
+    s->max_lazy_match   = configuration_table[s->level].max_lazy;
+    s->good_match       = configuration_table[s->level].good_length;
+    s->nice_match       = configuration_table[s->level].nice_length;
+    s->max_chain_length = configuration_table[s->level].max_chain;
+
+    s->strstart = 0;
+    s->block_start = 0L;
+    s->lookahead = 0;
+    s->match_length = s->prev_length = MIN_MATCH-1;
+    s->match_available = 0;
+    s->ins_h = 0;
+#ifdef CONFIG_EXT2_COMPR_X86_CODE
+    match_init(); /* initialize the asm code */
+#endif
+}
+
+/* ===========================================================================
+ * Set match_start to the longest match starting at the given string and
+ * return its length. Matches shorter or equal to prev_length are discarded,
+ * in which case the result is equal to prev_length and match_start is
+ * garbage.
+ * IN assertions: cur_match is the head of the hash chain for the current
+ *   string (strstart) and its distance is <= MAX_DIST, and prev_length >= 1
+ * OUT assertion: the match length is not greater than s->lookahead.
+ */
+#ifndef CONFIG_EXT2_COMPR_X86_CODE
+/* For 80x86 and 680x0, an optimized version will be provided in match.asm or
+ * match.S. The code will be functionally equivalent.
+ */
+#ifndef FASTEST
+local uInt longest_match(s, cur_match)
+    deflate_state *s;
+    IPos cur_match;                             /* current match */
+{
+    unsigned chain_length = s->max_chain_length;/* max hash chain length */
+    register Byte *scan = s->window + s->strstart; /* current string */
+    register Byte *match;                       /* matched string */
+    register int len;                           /* length of current match */
+    int best_len = s->prev_length;              /* best match length so far */
+    int nice_match = s->nice_match;             /* stop if match long enough */
+    IPos limit = s->strstart > (IPos)MAX_DIST(s) ?
+        s->strstart - (IPos)MAX_DIST(s) : NIL;
+    /* Stop when cur_match becomes <= limit. To simplify the code,
+     * we prevent matches with the string of window index 0.
+     */
+    Pos *prev = s->prev;
+    uInt wmask = s->w_mask;
+
+#ifdef UNALIGNED_OK
+    /* Compare two bytes at a time. Note: this is not always beneficial.
+     * Try with and without -DUNALIGNED_OK to check.
+     */
+    register Byte *strend = s->window + s->strstart + MAX_MATCH - 1;
+    register ush scan_start = *(ush*)scan;
+    register ush scan_end   = *(ush*)(scan+best_len-1);
+#else
+    register Byte *strend = s->window + s->strstart + MAX_MATCH;
+    register Byte scan_end1  = scan[best_len-1];
+    register Byte scan_end   = scan[best_len];
+#endif
+
+    /* The code is optimized for HASH_BITS >= 8 and MAX_MATCH-2 multiple of 16.
+     * It is easy to get rid of this optimization if necessary.
+     */
+    Assert(s->hash_bits >= 8 && MAX_MATCH == 258, "Code too clever");
+
+    /* Do not waste too much time if we already have a good match: */
+    if (s->prev_length >= s->good_match) {
+        chain_length >>= 2;
+    }
+    /* Do not look for matches beyond the end of the input. This is necessary
+     * to make deflate deterministic.
+     */
+    if ((uInt)nice_match > s->lookahead) nice_match = s->lookahead;
+
+    Assert((ulg)s->strstart <= s->window_size-MIN_LOOKAHEAD, "need lookahead");
+
+    do {
+        Assert(cur_match < s->strstart, "no future");
+        match = s->window + cur_match;
+
+        /* Skip to next match if the match length cannot increase
+         * or if the match length is less than 2:
+         */
+#if (defined(UNALIGNED_OK) && MAX_MATCH == 258)
+        /* This code assumes sizeof(unsigned short) == 2. Do not use
+         * UNALIGNED_OK if your compiler uses a different size.
+         */
+        if (*(ush*)(match+best_len-1) != scan_end ||
+            *(ush*)match != scan_start) continue;
+
+        /* It is not necessary to compare scan[2] and match[2] since they are
+         * always equal when the other bytes match, given that the hash keys
+         * are equal and that HASH_BITS >= 8. Compare 2 bytes at a time at
+         * strstart+3, +5, ... up to strstart+257. We check for insufficient
+         * lookahead only every 4th comparison; the 128th check will be made
+         * at strstart+257. If MAX_MATCH-2 is not a multiple of 8, it is
+         * necessary to put more guard bytes at the end of the window, or
+         * to check more often for insufficient lookahead.
+         */
+        Assert(scan[2] == match[2], "scan[2]?");
+        scan++, match++;
+        do {
+        } while (*(ush*)(scan+=2) == *(ush*)(match+=2) &&
+                 *(ush*)(scan+=2) == *(ush*)(match+=2) &&
+                 *(ush*)(scan+=2) == *(ush*)(match+=2) &&
+                 *(ush*)(scan+=2) == *(ush*)(match+=2) &&
+                 scan < strend);
+        /* The funny "do {}" generates better code on most compilers */
+
+        /* Here, scan <= window+strstart+257 */
+        Assert(scan <= s->window+(unsigned)(s->window_size-1), "wild scan");
+        if (*scan == *match) scan++;
+
+        len = (MAX_MATCH - 1) - (int)(strend-scan);
+        scan = strend - (MAX_MATCH-1);
+
+#else /* UNALIGNED_OK */
+
+        if (match[best_len]   != scan_end  ||
+            match[best_len-1] != scan_end1 ||
+            *match            != *scan     ||
+            *++match          != scan[1])      continue;
+
+        /* The check at best_len-1 can be removed because it will be made
+         * again later. (This heuristic is not always a win.)
+         * It is not necessary to compare scan[2] and match[2] since they
+         * are always equal when the other bytes match, given that
+         * the hash keys are equal and that HASH_BITS >= 8.
+         */
+        scan += 2, match++;
+        Assert(*scan == *match, "match[2]?");
+
+        /* We check for insufficient lookahead only every 8th comparison;
+         * the 256th check will be made at strstart+258.
+         */
+        do {
+        } while (*++scan == *++match && *++scan == *++match &&
+                 *++scan == *++match && *++scan == *++match &&
+                 *++scan == *++match && *++scan == *++match &&
+                 *++scan == *++match && *++scan == *++match &&
+                 scan < strend);
+
+        Assert(scan <= s->window+(unsigned)(s->window_size-1), "wild scan");
+
+        len = MAX_MATCH - (int)(strend - scan);
+        scan = strend - MAX_MATCH;
+
+#endif /* UNALIGNED_OK */
+
+        if (len > best_len) {
+            s->match_start = cur_match;
+            best_len = len;
+            if (len >= nice_match) break;
+#ifdef UNALIGNED_OK
+            scan_end = *(ush*)(scan+best_len-1);
+#else
+            scan_end1  = scan[best_len-1];
+            scan_end   = scan[best_len];
+#endif
+        }
+    } while ((cur_match = prev[cur_match & wmask]) > limit
+             && --chain_length != 0);
+
+    if ((uInt)best_len <= s->lookahead) return (uInt)best_len;
+    return s->lookahead;
+}
+
+#else /* FASTEST */
+/* ---------------------------------------------------------------------------
+ * Optimized version for level == 1 only
+ */
+local uInt longest_match(s, cur_match)
+    deflate_state *s;
+    IPos cur_match;                             /* current match */
+{
+    register Byte *scan = s->window + s->strstart; /* current string */
+    register Byte *match;                       /* matched string */
+    register int len;                           /* length of current match */
+    register Byte *strend = s->window + s->strstart + MAX_MATCH;
+
+    /* The code is optimized for HASH_BITS >= 8 and MAX_MATCH-2 multiple of 16.
+     * It is easy to get rid of this optimization if necessary.
+     */
+    Assert(s->hash_bits >= 8 && MAX_MATCH == 258, "Code too clever");
+
+    Assert((ulg)s->strstart <= s->window_size-MIN_LOOKAHEAD, "need lookahead");
+
+    Assert(cur_match < s->strstart, "no future");
+
+    match = s->window + cur_match;
+
+    /* Return failure if the match length is less than 2:
+     */
+    if (match[0] != scan[0] || match[1] != scan[1]) return MIN_MATCH-1;
+
+    /* The check at best_len-1 can be removed because it will be made
+     * again later. (This heuristic is not always a win.)
+     * It is not necessary to compare scan[2] and match[2] since they
+     * are always equal when the other bytes match, given that
+     * the hash keys are equal and that HASH_BITS >= 8.
+     */
+    scan += 2, match += 2;
+    Assert(*scan == *match, "match[2]?");
+
+    /* We check for insufficient lookahead only every 8th comparison;
+     * the 256th check will be made at strstart+258.
+     */
+    do {
+    } while (*++scan == *++match && *++scan == *++match &&
+	     *++scan == *++match && *++scan == *++match &&
+	     *++scan == *++match && *++scan == *++match &&
+	     *++scan == *++match && *++scan == *++match &&
+	     scan < strend);
+
+    Assert(scan <= s->window+(unsigned)(s->window_size-1), "wild scan");
+
+    len = MAX_MATCH - (int)(strend - scan);
+
+    if (len < MIN_MATCH) return MIN_MATCH - 1;
+
+    s->match_start = cur_match;
+    return len <= s->lookahead ? len : s->lookahead;
+}
+#endif /* FASTEST */
+#endif /* CONFIG_EXT2_COMPR_X86_CODE */
+
+#ifdef DEBUG
+/* ===========================================================================
+ * Check that the match at match_start is indeed a match.
+ */
+local void check_match(s, start, match, length)
+    deflate_state *s;
+    IPos start, match;
+    int length;
+{
+    /* check that the match is indeed a match */
+    if (zmemcmp(s->window + match,
+                s->window + start, length) != EQUAL) {
+        fprintf(stderr, " start %u, match %u, length %d\n",
+		start, match, length);
+        do {
+	    fprintf(stderr, "%c%c", s->window[match++], s->window[start++]);
+	} while (--length != 0);
+        z_error("invalid match");
+    }
+    if (z_verbose > 1) {
+        fprintf(stderr,"\\[%d,%d]", start-match, length);
+        do { putc(s->window[start++], stderr); } while (--length != 0);
+    }
+}
+#else
+#  define check_match(s, start, match, length)
+#endif
+
+/* ===========================================================================
+ * Fill the window when the lookahead becomes insufficient.
+ * Updates strstart and lookahead.
+ *
+ * IN assertion: lookahead < MIN_LOOKAHEAD
+ * OUT assertions: strstart <= window_size-MIN_LOOKAHEAD
+ *    At least one byte has been read, or avail_in == 0; reads are
+ *    performed for at least two bytes (required for the zip translate_eol
+ *    option -- not supported here).
+ */
+local void fill_window(s)
+    deflate_state *s;
+{
+    register unsigned n, m;
+    register Pos *p;
+    unsigned more;    /* Amount of free space at the end of the window. */
+    uInt wsize = s->w_size;
+
+    do {
+        more = (unsigned)(s->window_size -(ulg)s->lookahead -(ulg)s->strstart);
+
+        /* Deal with !@#$% 64K limit: */
+        if (more == 0 && s->strstart == 0 && s->lookahead == 0) {
+            more = wsize;
+
+        } else if (more == (unsigned)(-1)) {
+            /* Very unlikely, but possible on 16 bit machine if strstart == 0
+             * and lookahead == 1 (input done one byte at time)
+             */
+            more--;
+
+        /* If the window is almost full and there is insufficient lookahead,
+         * move the upper half to the lower one to make room in the upper half.
+         */
+        } else if (s->strstart >= wsize+MAX_DIST(s)) {
+
+            zmemcpy(s->window, s->window+wsize, (unsigned)wsize);
+            s->match_start -= wsize;
+            s->strstart    -= wsize; /* we now have strstart >= MAX_DIST */
+            s->block_start -= (long) wsize;
+
+            /* Slide the hash table (could be avoided with 32 bit values
+               at the expense of memory usage). We slide even when level == 0
+               to keep the hash table consistent if we switch back to level > 0
+               later. (Using level 0 permanently is not an optimal usage of
+               zlib, so we don't care about this pathological case.)
+             */
+	    n = s->hash_size;
+	    p = &s->head[n];
+	    do {
+		m = *--p;
+		*p = (Pos)(m >= wsize ? m-wsize : NIL);
+	    } while (--n);
+
+	    n = wsize;
+#ifndef FASTEST
+	    p = &s->prev[n];
+	    do {
+		m = *--p;
+		*p = (Pos)(m >= wsize ? m-wsize : NIL);
+		/* If n is not on any hash chain, prev[n] is garbage but
+		 * its value will never be used.
+		 */
+	    } while (--n);
+#endif
+            more += wsize;
+        }
+        if (s->strm->avail_in == 0) return;
+
+        /* If there was no sliding:
+         *    strstart <= WSIZE+MAX_DIST-1 && lookahead <= MIN_LOOKAHEAD - 1 &&
+         *    more == window_size - lookahead - strstart
+         * => more >= window_size - (MIN_LOOKAHEAD-1 + WSIZE + MAX_DIST-1)
+         * => more >= window_size - 2*WSIZE + 2
+         * In the BIG_MEM or MMAP case (not yet supported),
+         *   window_size == input_size + MIN_LOOKAHEAD  &&
+         *   strstart + s->lookahead <= input_size => more >= MIN_LOOKAHEAD.
+         * Otherwise, window_size == 2*WSIZE so more >= 2.
+         * If there was sliding, more >= WSIZE. So in all cases, more >= 2.
+         */
+        Assert(more >= 2, "more < 2");
+
+        n = read_buf(s->strm, s->window + s->strstart + s->lookahead, more);
+        s->lookahead += n;
+
+        /* Initialize the hash value now that we have some input: */
+        if (s->lookahead >= MIN_MATCH) {
+            s->ins_h = s->window[s->strstart];
+            UPDATE_HASH(s, s->ins_h, s->window[s->strstart+1]);
+#if MIN_MATCH != 3
+            Call UPDATE_HASH() MIN_MATCH-3 more times
+#endif
+        }
+        /* If the whole input has less than MIN_MATCH bytes, ins_h is garbage,
+         * but this is not important since only literal bytes will be emitted.
+         */
+
+    } while (s->lookahead < MIN_LOOKAHEAD && s->strm->avail_in != 0);
+}
+
+/* ===========================================================================
+ * Flush the current block, with given end-of-file flag.
+ * IN assertion: strstart is set to the end of the current match.
+ */
+#define FLUSH_BLOCK_ONLY(s, eof) { \
+   ext2_tr_flush_block(s, (s->block_start >= 0L ? \
+                   (char *)&s->window[(unsigned)s->block_start] : \
+                   (char *)Z_NULL), \
+		(ulg)((long)s->strstart - s->block_start), \
+		(eof)); \
+   s->block_start = s->strstart; \
+   flush_pending(s->strm); \
+   Tracev((stderr,"[FLUSH]")); \
+}
+
+/* Same but force premature exit if necessary. */
+#define FLUSH_BLOCK(s, eof) { \
+   FLUSH_BLOCK_ONLY(s, eof); \
+   if (s->strm->avail_out == 0) return (eof) ? finish_started : need_more; \
+}
+
+/* ===========================================================================
+ * Copy without compression as much as possible from the input stream, return
+ * the current block state.
+ * This function does not insert new strings in the dictionary since
+ * uncompressible data is probably not useful. This function is used
+ * only for the level=0 compression option.
+ * NOTE: this function should be optimized to avoid extra copying from
+ * window to pending_buf.
+ */
+local block_state deflate_stored(s, flush)
+    deflate_state *s;
+    int flush;
+{
+    /* Stored blocks are limited to 0xffff bytes, pending_buf is limited
+     * to pending_buf_size, and each stored block has a 5 byte header:
+     */
+    ulg max_block_size = 0xffff;
+    ulg max_start;
+
+    if (max_block_size > s->pending_buf_size - 5) {
+        max_block_size = s->pending_buf_size - 5;
+    }
+
+#ifdef EXT2_COMPRESS
+        /*
+	 *  We need that only once because the window IS the input buffer
+	 */
+
+        if (s->lookahead <= 1) {
+		fill_window(s);
+		if (s->lookahead == 0)
+			return 1;
+	}
+#endif
+	
+    /* Copy as much as possible from input to output: */
+    for (;;) {
+        /* Fill the window as much as possible: */
+        if (s->lookahead <= 1) {
+
+            Assert(s->strstart < s->w_size+MAX_DIST(s) ||
+		   s->block_start >= (long)s->w_size, "slide too late");
+#ifndef EXT2_COMPRESS
+            fill_window(s);
+#endif
+#ifdef __KERNEL__
+            /* Time slice of a fifth of a second. */
+            if (jiffies > next_brk) {
+		schedule();
+		next_brk = jiffies + HZ / 5;
+	    }
+#endif
+            if (s->lookahead == 0 && flush == Z_NO_FLUSH) return need_more;
+
+            if (s->lookahead == 0) break; /* flush the current block */
+        }
+	Assert(s->block_start >= 0L, "block gone");
+
+	s->strstart += s->lookahead;
+	s->lookahead = 0;
+
+	/* Emit a stored block if pending_buf will be full: */
+ 	max_start = s->block_start + max_block_size;
+        if (s->strstart == 0 || (ulg)s->strstart >= max_start) {
+	    /* strstart == 0 is possible when wraparound on 16-bit machine */
+	    s->lookahead = (uInt)(s->strstart - max_start);
+	    s->strstart = (uInt)max_start;
+            FLUSH_BLOCK(s, 0);
+	}
+	/* Flush if we may have to slide, otherwise block_start may become
+         * negative and the data will be gone:
+         */
+        if (s->strstart - (uInt)s->block_start >= MAX_DIST(s)) {
+            FLUSH_BLOCK(s, 0);
+	}
+    }
+    FLUSH_BLOCK(s, flush == Z_FINISH);
+    return flush == Z_FINISH ? finish_done : block_done;
+}
+
+/* ===========================================================================
+ * Compress as much as possible from the input stream, return the current
+ * block state.
+ * This function does not perform lazy evaluation of matches and inserts
+ * new strings in the dictionary only for unmatched strings or for short
+ * matches. It is used only for the fast compression options.
+ */
+local block_state deflate_fast(s, flush)
+    deflate_state *s;
+    int flush;
+{
+    IPos hash_head = NIL; /* head of the hash chain */
+    int bflush;           /* set if current block must be flushed */
+
+#ifdef EXT2_COMPRESS
+        /*
+         *  We need that only once because the window IS the input buffer
+         */
+    
+        if (s->lookahead < MIN_LOOKAHEAD) {
+	    fill_window(s);
+            if (s->lookahead == 0)
+	        return 1;
+	}
+#endif
+	
+    for (;;) {
+        /* Make sure that we always have enough lookahead, except
+         * at the end of the input file. We need MAX_MATCH bytes
+         * for the next match, plus MIN_MATCH bytes to insert the
+         * string following the next match.
+         */
+        if (s->lookahead < MIN_LOOKAHEAD) {
+#ifndef EXT2_COMPRESS
+            fill_window(s);
+#endif
+#ifdef __KERNEL__
+            /* Time slice of a fifth of a second. */
+            if (jiffies > next_brk) {
+		schedule();
+		next_brk = jiffies + HZ / 5;
+	    }
+#endif
+            if (s->lookahead < MIN_LOOKAHEAD && flush == Z_NO_FLUSH) {
+	        return need_more;
+	    }
+            if (s->lookahead == 0) break; /* flush the current block */
+        }
+
+        /* Insert the string window[strstart .. strstart+2] in the
+         * dictionary, and set hash_head to the head of the hash chain:
+         */
+        if (s->lookahead >= MIN_MATCH) {
+            INSERT_STRING(s, s->strstart, hash_head);
+        }
+
+        /* Find the longest match, discarding those <= prev_length.
+         * At this point we have always match_length < MIN_MATCH
+         */
+        if (hash_head != NIL && s->strstart - hash_head <= MAX_DIST(s)) {
+            /* To simplify the code, we prevent matches with the string
+             * of window index 0 (in particular we have to avoid a match
+             * of the string with itself at the start of the input file).
+             */
+            if (s->strategy != Z_HUFFMAN_ONLY) {
+                s->match_length = longest_match (s, hash_head);
+            }
+            /* longest_match() sets match_start */
+        }
+        if (s->match_length >= MIN_MATCH) {
+            check_match(s, s->strstart, s->match_start, s->match_length);
+
+            ext2_tr_tally_dist(s, s->strstart - s->match_start,
+                           s->match_length - MIN_MATCH, bflush);
+
+            s->lookahead -= s->match_length;
+
+            /* Insert new strings in the hash table only if the match length
+             * is not too large. This saves time but degrades compression.
+             */
+#ifndef FASTEST
+            if (s->match_length <= s->max_insert_length &&
+                s->lookahead >= MIN_MATCH) {
+                s->match_length--; /* string at strstart already in hash table */
+                do {
+                    s->strstart++;
+                    INSERT_STRING(s, s->strstart, hash_head);
+                    /* strstart never exceeds WSIZE-MAX_MATCH, so there are
+                     * always MIN_MATCH bytes ahead.
+                     */
+                } while (--s->match_length != 0);
+                s->strstart++; 
+            } else
+#endif
+	    {
+                s->strstart += s->match_length;
+                s->match_length = 0;
+                s->ins_h = s->window[s->strstart];
+                UPDATE_HASH(s, s->ins_h, s->window[s->strstart+1]);
+#if MIN_MATCH != 3
+                Call UPDATE_HASH() MIN_MATCH-3 more times
+#endif
+                /* If lookahead < MIN_MATCH, ins_h is garbage, but it does not
+                 * matter since it will be recomputed at next deflate call.
+                 */
+            }
+        } else {
+            /* No match, output a literal byte */
+            Tracevv((stderr,"%c", s->window[s->strstart]));
+            ext2_tr_tally_lit (s, s->window[s->strstart], bflush);
+            s->lookahead--;
+            s->strstart++; 
+        }
+        if (bflush) FLUSH_BLOCK(s, 0);
+    }
+    FLUSH_BLOCK(s, flush == Z_FINISH);
+    return flush == Z_FINISH ? finish_done : block_done;
+}
+
+/* ===========================================================================
+ * Same as above, but achieves better compression. We use a lazy
+ * evaluation for matches: a match is finally adopted only if there is
+ * no better match at the next window position.
+ */
+local block_state deflate_slow(s, flush)
+    deflate_state *s;
+    int flush;
+{
+    IPos hash_head = NIL;    /* head of hash chain */
+    int bflush;              /* set if current block must be flushed */
+
+#ifdef EXT2_COMPRESS
+        /*
+         *  We need that only once because the window IS the input buffer
+         */
+    
+        if (s->lookahead < MIN_LOOKAHEAD) {
+		fill_window(s);
+		if (s->lookahead == 0)
+			return 1;
+	}
+#endif
+	
+    /* Process the input block. */
+    for (;;) {
+        /* Make sure that we always have enough lookahead, except
+         * at the end of the input file. We need MAX_MATCH bytes
+         * for the next match, plus MIN_MATCH bytes to insert the
+         * string following the next match.
+         */
+        if (s->lookahead < MIN_LOOKAHEAD) {
+#ifndef EXT2_COMPRESS
+            fill_window(s);
+#endif
+#ifdef __KERNEL__
+	                /* Time slice of a fifth of a second. */
+	    if (jiffies > next_brk) {
+		schedule();
+		next_brk = jiffies + HZ / 5;
+	    }
+#endif
+            if (s->lookahead < MIN_LOOKAHEAD && flush == Z_NO_FLUSH) {
+	        return need_more;
+	    }
+            if (s->lookahead == 0) break; /* flush the current block */
+        }
+
+        /* Insert the string window[strstart .. strstart+2] in the
+         * dictionary, and set hash_head to the head of the hash chain:
+         */
+        if (s->lookahead >= MIN_MATCH) {
+            INSERT_STRING(s, s->strstart, hash_head);
+        }
+
+        /* Find the longest match, discarding those <= prev_length.
+         */
+        s->prev_length = s->match_length, s->prev_match = s->match_start;
+        s->match_length = MIN_MATCH-1;
+
+        if (hash_head != NIL && s->prev_length < s->max_lazy_match &&
+            s->strstart - hash_head <= MAX_DIST(s)) {
+            /* To simplify the code, we prevent matches with the string
+             * of window index 0 (in particular we have to avoid a match
+             * of the string with itself at the start of the input file).
+             */
+            if (s->strategy != Z_HUFFMAN_ONLY) {
+                s->match_length = longest_match (s, hash_head);
+            }
+            /* longest_match() sets match_start */
+
+            if (s->match_length <= 5 && (s->strategy == Z_FILTERED ||
+                 (s->match_length == MIN_MATCH &&
+                  s->strstart - s->match_start > TOO_FAR))) {
+
+                /* If prev_match is also MIN_MATCH, match_start is garbage
+                 * but we will ignore the current match anyway.
+                 */
+                s->match_length = MIN_MATCH-1;
+            }
+        }
+        /* If there was a match at the previous step and the current
+         * match is not better, output the previous match:
+         */
+        if (s->prev_length >= MIN_MATCH && s->match_length <= s->prev_length) {
+            uInt max_insert = s->strstart + s->lookahead - MIN_MATCH;
+            /* Do not insert strings in hash table beyond this. */
+
+            check_match(s, s->strstart-1, s->prev_match, s->prev_length);
+
+            ext2_tr_tally_dist(s, s->strstart -1 - s->prev_match,
+			   s->prev_length - MIN_MATCH, bflush);
+
+            /* Insert in hash table all strings up to the end of the match.
+             * strstart-1 and strstart are already inserted. If there is not
+             * enough lookahead, the last two strings are not inserted in
+             * the hash table.
+             */
+            s->lookahead -= s->prev_length-1;
+            s->prev_length -= 2;
+            do {
+                if (++s->strstart <= max_insert) {
+                    INSERT_STRING(s, s->strstart, hash_head);
+                }
+            } while (--s->prev_length != 0);
+            s->match_available = 0;
+            s->match_length = MIN_MATCH-1;
+            s->strstart++;
+
+            if (bflush) FLUSH_BLOCK(s, 0);
+
+        } else if (s->match_available) {
+            /* If there was no match at the previous position, output a
+             * single literal. If there was a match but the current match
+             * is longer, truncate the previous match to a single literal.
+             */
+            Tracevv((stderr,"%c", s->window[s->strstart-1]));
+	    ext2_tr_tally_lit(s, s->window[s->strstart-1], bflush);
+	    if (bflush) {
+                FLUSH_BLOCK_ONLY(s, 0);
+            }
+            s->strstart++;
+            s->lookahead--;
+            if (s->strm->avail_out == 0) return need_more;
+        } else {
+            /* There is no previous match to compare with, wait for
+             * the next step to decide.
+             */
+            s->match_available = 1;
+            s->strstart++;
+            s->lookahead--;
+        }
+    }
+    Assert (flush != Z_NO_FLUSH, "no flush?");
+    if (s->match_available) {
+        Tracevv((stderr,"%c", s->window[s->strstart-1]));
+        ext2_tr_tally_lit(s, s->window[s->strstart-1], bflush);
+        s->match_available = 0;
+    }
+    FLUSH_BLOCK(s, flush == Z_FINISH);
+    return flush == Z_FINISH ? finish_done : block_done;
+}
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/deflate.h linux-2.6.18.5/fs/ext2/gzip/deflate.h
--- linux-2.6.18.5.org/fs/ext2/gzip/deflate.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/deflate.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,317 @@
+/* deflate.h -- internal compression state
+ * Copyright (C) 1995-1998 Jean-loup Gailly
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+/* @(#) $Id$ */
+
+#ifndef _DEFLATE_H
+#define _DEFLATE_H
+
+#include "zutil.h"
+
+/* ===========================================================================
+ * Internal compression state.
+ */
+
+#define LENGTH_CODES 29
+/* number of length codes, not counting the special END_BLOCK code */
+
+#define LITERALS  256
+/* number of literal bytes 0..255 */
+
+#define L_CODES (LITERALS+1+LENGTH_CODES)
+/* number of Literal or Length codes, including the END_BLOCK code */
+
+#define D_CODES   30
+/* number of distance codes */
+
+#define BL_CODES  19
+/* number of codes used to transfer the bit lengths */
+
+#define HEAP_SIZE (2*L_CODES+1)
+/* maximum heap size */
+
+#define MAX_BITS 15
+/* All codes must not exceed MAX_BITS bits */
+
+#define INIT_STATE    42
+#define BUSY_STATE   113
+#define FINISH_STATE 666
+/* Stream status */
+
+
+/* Data structure describing a single value and its code string. */
+typedef struct ct_data_s {
+    union {
+        ush  freq;       /* frequency count */
+        ush  code;       /* bit string */
+    } fc;
+    union {
+        ush  dad;        /* father node in Huffman tree */
+        ush  len;        /* length of bit string */
+    } dl;
+} ct_data;
+
+#define Freq fc.freq
+#define Code fc.code
+#define Dad  dl.dad
+#define Len  dl.len
+
+typedef struct static_tree_desc_s  static_tree_desc;
+
+typedef struct tree_desc_s {
+    ct_data *dyn_tree;           /* the dynamic tree */
+    int     max_code;            /* largest code with non zero frequency */
+    static_tree_desc *stat_desc; /* the corresponding static tree */
+} tree_desc;
+
+typedef ush Pos;
+typedef unsigned IPos;
+
+/* A Pos is an index in the character window. We use short instead of int to
+ * save space in the various tables. IPos is used only for parameter passing.
+ */
+
+typedef struct internal_state {
+    z_streamp strm;      /* pointer back to this zlib stream */
+    int   status;        /* as the name implies */
+    Byte  *pending_buf;  /* output still pending */
+    ulg   pending_buf_size; /* size of pending_buf */
+    Byte  *pending_out;  /* next pending byte to output to the stream */
+    int   pending;       /* nb of bytes in the pending buffer */
+    int   noheader;      /* suppress zlib header and adler32 */
+    Byte  data_type;     /* UNKNOWN, BINARY or ASCII */
+    Byte  method;        /* STORED (for zip only) or DEFLATED */
+    int   last_flush;    /* value of flush param for previous deflate call */
+
+                /* used by deflate.c: */
+
+    uInt  w_size;        /* LZ77 window size (32K by default) */
+    uInt  w_bits;        /* log2(w_size)  (8..16) */
+    uInt  w_mask;        /* w_size - 1 */
+
+    Byte  *window;
+    /* Sliding window. Input bytes are read into the second half of the window,
+     * and move to the first half later to keep a dictionary of at least wSize
+     * bytes. With this organization, matches are limited to a distance of
+     * wSize-MAX_MATCH bytes, but this ensures that IO is always
+     * performed with a length multiple of the block size. Also, it limits
+     * the window size to 64K, which is quite useful on MSDOS.
+     * To do: use the user input buffer as sliding window.
+     */
+
+    ulg window_size;
+    /* Actual size of window: 2*wSize, except when the user input buffer
+     * is directly used as sliding window.
+     */
+
+    Pos  *prev;
+    /* Link to older string with same hash index. To limit the size of this
+     * array to 64K, this link is maintained only for the last 32K strings.
+     * An index in this array is thus a window index modulo 32K.
+     */
+
+    Pos  *head; /* Heads of the hash chains or NIL. */
+
+    uInt  ins_h;          /* hash index of string to be inserted */
+    uInt  hash_size;      /* number of elements in hash table */
+    uInt  hash_bits;      /* log2(hash_size) */
+    uInt  hash_mask;      /* hash_size-1 */
+
+    uInt  hash_shift;
+    /* Number of bits by which ins_h must be shifted at each input
+     * step. It must be such that after MIN_MATCH steps, the oldest
+     * byte no longer takes part in the hash key, that is:
+     *   hash_shift * MIN_MATCH >= hash_bits
+     */
+
+    long block_start;
+    /* Window position at the beginning of the current output block. Gets
+     * negative when the window is moved backwards.
+     */
+
+    uInt match_length;           /* length of best match */
+    IPos prev_match;             /* previous match */
+    int match_available;         /* set if previous match exists */
+    uInt strstart;               /* start of string to insert */
+    uInt match_start;            /* start of matching string */
+    uInt lookahead;              /* number of valid bytes ahead in window */
+
+    uInt prev_length;
+    /* Length of the best match at previous step. Matches not greater than this
+     * are discarded. This is used in the lazy match evaluation.
+     */
+
+    uInt max_chain_length;
+    /* To speed up deflation, hash chains are never searched beyond this
+     * length.  A higher limit improves compression ratio but degrades the
+     * speed.
+     */
+
+    uInt max_lazy_match;
+    /* Attempt to find a better match only when the current match is strictly
+     * smaller than this value. This mechanism is used only for compression
+     * levels >= 4.
+     */
+#   define max_insert_length  max_lazy_match
+    /* Insert new strings in the hash table only if the match length is not
+     * greater than this length. This saves time but degrades compression.
+     * max_insert_length is used only for compression levels <= 3.
+     */
+
+    int level;    /* compression level (1..9) */
+    int strategy; /* favor or force Huffman coding*/
+
+    uInt good_match;
+    /* Use a faster search when the previous match is longer than this */
+
+    int nice_match; /* Stop searching when current match exceeds this */
+
+                /* used by trees.c: */
+    /* Didn't use ct_data typedef below to supress compiler warning */
+    struct ct_data_s dyn_ltree[HEAP_SIZE];   /* literal and length tree */
+    struct ct_data_s dyn_dtree[2*D_CODES+1]; /* distance tree */
+    struct ct_data_s bl_tree[2*BL_CODES+1];  /* Huffman tree for bit lengths */
+
+    struct tree_desc_s l_desc;               /* desc. for literal tree */
+    struct tree_desc_s d_desc;               /* desc. for distance tree */
+    struct tree_desc_s bl_desc;              /* desc. for bit length tree */
+
+    ush bl_count[MAX_BITS+1];
+    /* number of codes at each bit length for an optimal tree */
+
+    int heap[2*L_CODES+1];      /* heap used to build the Huffman trees */
+    int heap_len;               /* number of elements in the heap */
+    int heap_max;               /* element of largest frequency */
+    /* The sons of heap[n] are heap[2*n] and heap[2*n+1]. heap[0] is not used.
+     * The same heap array is used to build all trees.
+     */
+
+    uch depth[2*L_CODES+1];
+    /* Depth of each subtree used as tie breaker for trees of equal frequency
+     */
+
+    uch *l_buf;          /* buffer for literals or lengths */
+
+    uInt  lit_bufsize;
+    /* Size of match buffer for literals/lengths.  There are 4 reasons for
+     * limiting lit_bufsize to 64K:
+     *   - frequencies can be kept in 16 bit counters
+     *   - if compression is not successful for the first block, all input
+     *     data is still in the window so we can still emit a stored block even
+     *     when input comes from standard input.  (This can also be done for
+     *     all blocks if lit_bufsize is not greater than 32K.)
+     *   - if compression is not successful for a file smaller than 64K, we can
+     *     even emit a stored file instead of a stored block (saving 5 bytes).
+     *     This is applicable only for zip (not gzip or zlib).
+     *   - creating new Huffman trees less frequently may not provide fast
+     *     adaptation to changes in the input data statistics. (Take for
+     *     example a binary file with poorly compressible code followed by
+     *     a highly compressible string table.) Smaller buffer sizes give
+     *     fast adaptation but have of course the overhead of transmitting
+     *     trees more frequently.
+     *   - I can't count above 4
+     */
+
+    uInt last_lit;      /* running index in l_buf */
+
+    ush *d_buf;
+    /* Buffer for distances. To simplify the code, d_buf and l_buf have
+     * the same number of elements. To use different lengths, an extra flag
+     * array would be necessary.
+     */
+
+    ulg opt_len;        /* bit length of current block with optimal trees */
+    ulg static_len;     /* bit length of current block with static trees */
+    uInt matches;       /* number of string matches in current block */
+    int last_eob_len;   /* bit length of EOB code for last block */
+
+#ifdef DEBUG
+    ulg compressed_len; /* total bit length of compressed file mod 2^32 */
+    ulg bits_sent;      /* bit length of compressed data sent mod 2^32 */
+#endif
+
+    ush bi_buf;
+    /* Output buffer. bits are inserted starting at the bottom (least
+     * significant bits).
+     */
+    int bi_valid;
+    /* Number of valid bits in bi_buf.  All bits above the last valid bit
+     * are always zero.
+     */
+
+} deflate_state;
+
+/* Output a byte on the stream.
+ * IN assertion: there is enough room in pending_buf.
+ */
+#define put_byte(s, c) {s->pending_buf[s->pending++] = (c);}
+
+
+#define MIN_LOOKAHEAD (MAX_MATCH+MIN_MATCH+1)
+/* Minimum amount of lookahead, except at the end of the input file.
+ * See deflate.c for comments about the MIN_MATCH+1.
+ */
+
+#define MAX_DIST(s)  ((s)->w_size-MIN_LOOKAHEAD)
+/* In order to simplify the code, particularly on 16 bit machines, match
+ * distances are limited to MAX_DIST instead of WSIZE.
+ */
+
+        /* in trees.c */
+void ext2_tr_init         OF((deflate_state *s));
+int  ext2_tr_tally        OF((deflate_state *s, unsigned dist, unsigned lc));
+void ext2_tr_flush_block  OF((deflate_state *s, char *buf, ulg stored_len,
+			  int eof));
+void ext2_tr_align        OF((deflate_state *s));
+void ext2_tr_stored_block OF((deflate_state *s, char *buf, ulg stored_len,
+                          int eof));
+
+#define d_code(dist) \
+   ((dist) < 256 ? _dist_code[dist] : _dist_code[256+((dist)>>7)])
+/* Mapping from a distance to a distance code. dist is the distance - 1 and
+ * must not have side effects. _dist_code[256] and _dist_code[257] are never
+ * used.
+ */
+
+#ifndef DEBUG
+/* Inline versions of _tr_tally for speed: */
+
+#if defined(GEN_TREES_H) || !defined(STDC)
+  extern uch _length_code[];
+  extern uch _dist_code[];
+#else
+  extern const uch _length_code[];
+  extern const uch _dist_code[];
+#endif
+
+# define ext2_tr_tally_lit(s, c, flush) \
+  { uch cc = (c); \
+    s->d_buf[s->last_lit] = 0; \
+    s->l_buf[s->last_lit++] = cc; \
+    s->dyn_ltree[cc].Freq++; \
+    flush = (s->last_lit == s->lit_bufsize-1); \
+   }
+# define ext2_tr_tally_dist(s, distance, length, flush) \
+  { uch len = (length); \
+    ush dist = (distance); \
+    s->d_buf[s->last_lit] = dist; \
+    s->l_buf[s->last_lit++] = len; \
+    dist--; \
+    s->dyn_ltree[_length_code[len]+LITERALS+1].Freq++; \
+    s->dyn_dtree[d_code(dist)].Freq++; \
+    flush = (s->last_lit == s->lit_bufsize-1); \
+  }
+#else
+# define ext2_tr_tally_lit(s, c, flush) flush = ext2_tr_tally(s, 0, c)
+# define ext2_tr_tally_dist(s, distance, length, flush) \
+              flush = ext2_tr_tally(s, distance, length) 
+#endif
+
+#endif
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/e2compr_gzip.c linux-2.6.18.5/fs/ext2/gzip/e2compr_gzip.c
--- linux-2.6.18.5.org/fs/ext2/gzip/e2compr_gzip.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/e2compr_gzip.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,229 @@
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/fs.h>
+#include <linux/ext2_fs_c.h>
+#include <linux/module.h>
+
+#ifdef MODULE
+MODULE_AUTHOR("Jean-loup Gailly, Mark Adler, and others");
+MODULE_DESCRIPTION("Modified Zlib (gzip) algorithm for EXT2 file compression");
+MODULE_LICENSE("GPL");
+#endif
+
+#include "zlib.h"
+
+/*
+ *	Deflate (as called by ext2_wGZIP) needs the following 
+ *	memory:
+ *	
+ *		64K	hash table	(32K entries)
+ *		64K	chain table	(32K entries)
+ *		64K	item buffer	(16K items)
+ *		 5K	internal state
+ *
+ *	The memory requirements could probably be reduced. 
+ *	With cluster sizes less than 32K, the chain table and
+ *	item buffer can be made smaller.
+ *
+ *	Inflate (as called by ext2_rGZIP) needs the following 
+ *	memory:
+ *
+ *		32K	output window
+ *			a few bytes for the code table ...
+ *			... actually it's about another ...
+ *		32K
+ *
+ */
+
+#if 0
+static char *ext2_gzip_heap_base = NULL;
+static char *ext2_gzip_heap_ptr  = NULL;
+static int   ext2_gzip_heap_used = 0;
+static int   ext2_gzip_heap_size = (64 + 64 + 64 + 8) * 1024;
+#else
+# define	ext2_gzip_heap_size_C	((64 + 64 + 64 + 8) * 1024)
+# define	ext2_gzip_heap_size_D	((32 + 32) * 1024)
+#endif
+
+#if 0
+const char *z_errmsg[10] = {
+"need dictionary",     /* Z_NEED_DICT       2  */
+"stream end",          /* Z_STREAM_END      1  */
+"",                    /* Z_OK              0  */
+"file error",          /* Z_ERRNO         (-1) */
+"stream error",        /* Z_STREAM_ERROR  (-2) */
+"data error",          /* Z_DATA_ERROR    (-3) */
+"insufficient memory", /* Z_MEM_ERROR     (-4) */
+"buffer error",        /* Z_BUF_ERROR     (-5) */
+"incompatible version",/* Z_VERSION_ERROR (-6) */
+""};
+#endif
+
+struct heap_dets {
+    unsigned char *curr_ptr; /* pointer to first unused byte on heap */
+    unsigned char *end; /* pointer past the end of the available heap space */
+};
+
+voidp  ext2_zcalloc OF((voidp const opaque, unsigned items, unsigned size))
+{
+#define heap_details ((struct heap_dets *)opaque)
+    unsigned bytes = items * size;
+    void    *ptr;
+
+    if (heap_details->curr_ptr + bytes > heap_details->end)
+	return 0;
+
+    ptr = heap_details->curr_ptr;
+    heap_details->curr_ptr += bytes;
+#if 0
+    memset(ptr, 0, bytes);
+    printf ("zcalloc (%p, %d, %d) \t-> %p\n", private, items, size, ptr);
+#endif 
+
+    return ptr;
+#undef heap_details
+}
+
+/*
+ *	Do nothing ... Could be improved ...!
+ */
+
+void ext2_zcfree  OF((voidp opaque, voidp ptr))
+{
+}
+
+
+size_t ext2_iGZIP (int action)
+{
+	switch (action) {
+		case EXT2_ALG_INIT_COMPRESS:
+			return ext2_gzip_heap_size_C;
+		case EXT2_ALG_INIT_DECOMPRESS:
+			return ext2_gzip_heap_size_D;
+		default:
+			return 0;
+	}
+}
+
+/*
+ *	Compression
+ */
+
+size_t ext2_wGZIP (__u8 *ibuf, __u8 *obuf, void *heap,
+		size_t ilen, size_t olen, int level)
+{
+    z_stream stream;
+    int      err;
+
+    if (!try_module_get(THIS_MODULE))
+	return 0;
+
+    stream.next_in   = ibuf;
+    stream.avail_in  = ilen;
+
+    stream.zalloc    = ext2_zcalloc;
+    stream.zfree     = ext2_zcfree;
+    stream.opaque    = heap;
+#define heap_details ((struct heap_dets *)heap)
+    heap_details->curr_ptr = (unsigned char *)heap + sizeof(struct heap_dets);
+    heap_details->end = (unsigned char *)heap + ext2_gzip_heap_size_C;
+#undef heap_details
+
+    if ((err = ext2_deflateInit2 (&stream, level, 8, -15, 8, Z_DEFAULT_STRATEGY)) != Z_OK) {
+	module_put(THIS_MODULE);
+	return 0;
+    }
+
+    stream.next_out  = obuf;
+    stream.avail_out = olen;
+
+    err = ext2_deflate (&stream, Z_FINISH);
+
+    if (err != Z_STREAM_END) {
+	ext2_deflateEnd (&stream);
+	module_put(THIS_MODULE);
+	return 0;
+    }
+
+    if ((err = ext2_deflateEnd (&stream)) != Z_OK) {
+	module_put(THIS_MODULE);
+	return 0;
+    }
+
+    module_put(THIS_MODULE);
+    return stream.total_out;
+}
+
+/*
+ *	Decompression
+ */
+
+size_t ext2_rGZIP (__u8 *ibuf, __u8 *obuf, void *heap,
+		size_t ilen, size_t olen, int ignored)
+{
+    z_stream stream;
+    int      err;
+
+    if (!try_module_get(THIS_MODULE))
+	return 0;
+
+    stream.next_in   = ibuf;
+    stream.avail_in  = ilen;
+
+    stream.zalloc    = ext2_zcalloc;
+    stream.zfree     = ext2_zcfree;
+    stream.opaque    = heap;
+#define heap_details ((struct heap_dets *)heap)
+    heap_details->curr_ptr = (unsigned char *)heap + sizeof(struct heap_dets);
+    heap_details->end = (unsigned char *)heap + ext2_gzip_heap_size_D;
+#undef heap_details
+
+    if ((err = ext2_inflateInit2 (&stream, -15)) != Z_OK) {
+        module_put(THIS_MODULE);
+	return 0;
+    }
+
+    stream.next_out  = obuf;
+    stream.avail_out = olen;
+
+    err = ext2_inflate (&stream, Z_FINISH);
+  
+    if (err != Z_STREAM_END && (err != Z_OK || stream.total_out < olen)) {
+	ext2_inflateEnd (&stream);
+        module_put(THIS_MODULE);
+	return err;
+    }
+
+    if ((err = ext2_inflateEnd (&stream)) != Z_OK) {
+        module_put(THIS_MODULE);
+	return 0;
+    }
+
+    module_put(THIS_MODULE);
+    return stream.total_out;
+}
+
+
+#ifdef MODULE
+
+int init_module(void)
+{
+	struct ext2_algorithm gzip_alg;
+
+	gzip_alg.name = NULL;
+	gzip_alg.avail = 1;
+	gzip_alg.init = ext2_iGZIP;
+	gzip_alg.compress = ext2_wGZIP;
+	gzip_alg.decompress = ext2_rGZIP;
+
+	return ext2_register_compression_module(EXT2_GZIP_ALG, 
+			ext2_gzip_heap_size_C, ext2_gzip_heap_size_D,
+			&gzip_alg);
+}
+
+void cleanup_module(void)
+{
+	ext2_unregister_compression_module(EXT2_GZIP_ALG);
+}
+
+#endif
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/infblock.c linux-2.6.18.5/fs/ext2/gzip/infblock.c
--- linux-2.6.18.5.org/fs/ext2/gzip/infblock.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/infblock.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,405 @@
+/* infblock.c -- interpret and process block types to last block
+ * Copyright (C) 1995-2002 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+#include <linux/string.h>
+
+#include "zutil.h"
+#include "infblock.h"
+#include "inftrees.h"
+#include "infcodes.h"
+#include "infutil.h"
+
+struct inflate_codes_state {int dummy;}; /* for buggy compilers */
+
+/* simplify the use of the inflate_huft type with some defines */
+#define exop word.what.Exop
+#define bits word.what.Bits
+
+/* Table for deflate from PKZIP's appnote.txt. */
+local const uInt border[] = { /* Order of the bit length code lengths */
+        16, 17, 18, 0, 8, 7, 9, 6, 10, 5, 11, 4, 12, 3, 13, 2, 14, 1, 15};
+
+/*
+   Notes beyond the 1.93a appnote.txt:
+
+   1. Distance pointers never point before the beginning of the output
+      stream.
+   2. Distance pointers can point back across blocks, up to 32k away.
+   3. There is an implied maximum of 7 bits for the bit length table and
+      15 bits for the actual data.
+   4. If only one code exists, then it is encoded using one bit.  (Zero
+      would be more efficient, but perhaps a little confusing.)  If two
+      codes exist, they are coded using one bit each (0 and 1).
+   5. There is no way of sending zero distance codes--a dummy must be
+      sent if there are none.  (History: a pre 2.0 version of PKZIP would
+      store blocks with no distance codes, but this was discovered to be
+      too harsh a criterion.)  Valid only for 1.93a.  2.04c does allow
+      zero distance codes, which is sent as one code of zero bits in
+      length.
+   6. There are up to 286 literal/length codes.  Code 256 represents the
+      end-of-block.  Note however that the static length tree defines
+      288 codes just to fill out the Huffman codes.  Codes 286 and 287
+      cannot be used though, since there is no length base or extra bits
+      defined for them.  Similarily, there are up to 30 distance codes.
+      However, static trees define 32 codes (all 5 bits) to fill out the
+      Huffman codes, but the last two had better not show up in the data.
+   7. Unzip can check dynamic Huffman blocks for complete code sets.
+      The exception is that a single code would not be complete (see #4).
+   8. The five bits following the block type is really the number of
+      literal codes sent minus 257.
+   9. Length codes 8,16,16 are interpreted as 13 length codes of 8 bits
+      (1+6+6).  Therefore, to output three times the length, you output
+      three codes (1+1+1), whereas to output four times the same length,
+      you only need two codes (1+3).  Hmm.
+  10. In the tree reconstruction algorithm, Code = Code + Increment
+      only if BitLength(i) is not zero.  (Pretty obvious.)
+  11. Correction: 4 Bits: # of Bit Length codes - 4     (4 - 19)
+  12. Note: length code 284 can represent 227-258, but length code 285
+      really is 258.  The last length deserves its own, short code
+      since it gets used a lot in very redundant files.  The length
+      258 is special since 258 - 3 (the min match length) is 255.
+  13. The literal/length and distance code bit lengths are read as a
+      single stream of lengths.  It is possible (and advantageous) for
+      a repeat code (16, 17, or 18) to go across the boundary between
+      the two sets of lengths.
+ */
+
+
+void ext2_inflate_blocks_reset(s, z, c)
+struct inflate_blocks_state *s;
+z_streamp z;
+uLong *c;
+{
+  if (c != Z_NULL)
+    *c = s->check;
+  if (s->mode == BTREE || s->mode == DTREE)
+    ZFREE(z, s->sub.trees.blens);
+  if (s->mode == CODES)
+    ext2_inflate_codes_free(s->sub.decode.codes, z);
+  s->mode = TYPE;
+  s->bitk = 0;
+  s->bitb = 0;
+  s->read = s->write = s->window;
+  if (s->checkfn != Z_NULL)
+    z->adler = s->check = (*s->checkfn)(0L, (const Byte *)Z_NULL, 0);
+  Tracev((stderr, "inflate:   blocks reset\n"));
+}
+
+
+struct inflate_blocks_state *ext2_inflate_blocks_new(z, c, w)
+z_streamp z;
+check_func c;
+uInt w;
+{
+  struct inflate_blocks_state *s;
+
+  if ((s = (struct inflate_blocks_state *)ZALLOC
+       (z,1,sizeof(struct inflate_blocks_state))) == Z_NULL)
+    return s;
+  if ((s->hufts =
+       (inflate_huft *)ZALLOC(z, sizeof(inflate_huft), MANY)) == Z_NULL)
+  {
+    ZFREE(z, s);
+    return Z_NULL;
+  }
+  if ((s->window = (Byte *)ZALLOC(z, 1, w)) == Z_NULL)
+  {
+    ZFREE(z, s->hufts);
+    ZFREE(z, s);
+    return Z_NULL;
+  }
+  s->end = s->window + w;
+  s->checkfn = c;
+  s->mode = TYPE;
+  Tracev((stderr, "inflate:   blocks allocated\n"));
+  ext2_inflate_blocks_reset(s, z, Z_NULL);
+  return s;
+}
+
+
+int ext2_inflate_blocks(s, z, r)
+struct inflate_blocks_state *s;
+z_streamp z;
+int r;
+{
+  uInt t;               /* temporary storage */
+  uLong b;              /* bit buffer */
+  uInt k;               /* bits in bit buffer */
+  Byte *p;             /* input data pointer */
+  uInt n;               /* bytes available there */
+  Byte *q;             /* output window write pointer */
+  uInt m;               /* bytes to end of window or read pointer */
+
+  /* copy input/output information to locals (UPDATE macro restores) */
+  LOAD
+
+  /* process input based on current state */
+  while (1) switch (s->mode)
+  {
+    case TYPE:
+      NEEDBITS(3)
+      t = (uInt)b & 7;
+      s->last = t & 1;
+      switch (t >> 1)
+      {
+        case 0:                         /* stored */
+          Tracev((stderr, "inflate:     stored block%s\n",
+                 s->last ? " (last)" : ""));
+          DUMPBITS(3)
+          t = k & 7;                    /* go to byte boundary */
+          DUMPBITS(t)
+          s->mode = LENS;               /* get length of stored block */
+          break;
+        case 1:                         /* fixed */
+          Tracev((stderr, "inflate:     fixed codes block%s\n",
+                 s->last ? " (last)" : ""));
+          {
+            uInt bl, bd;
+            inflate_huft *tl, *td;
+
+            ext2_inflate_trees_fixed(&bl, &bd, &tl, &td, z);
+            s->sub.decode.codes = ext2_inflate_codes_new(bl, bd, tl, td, z);
+            if (s->sub.decode.codes == Z_NULL)
+            {
+              r = Z_MEM_ERROR;
+              LEAVE
+            }
+          }
+          DUMPBITS(3)
+          s->mode = CODES;
+          break;
+        case 2:                         /* dynamic */
+          Tracev((stderr, "inflate:     dynamic codes block%s\n",
+                 s->last ? " (last)" : ""));
+          DUMPBITS(3)
+          s->mode = TABLE;
+          break;
+        case 3:                         /* illegal */
+          DUMPBITS(3)
+          s->mode = BAD;
+          z->msg = (char*)"invalid block type";
+          r = Z_DATA_ERROR;
+          LEAVE
+      }
+      break;
+    case LENS:
+      NEEDBITS(32)
+      if ((((~b) >> 16) & 0xffff) != (b & 0xffff))
+      {
+        s->mode = BAD;
+        z->msg = (char*)"invalid stored block lengths";
+        r = Z_DATA_ERROR;
+        LEAVE
+      }
+      s->sub.left = (uInt)b & 0xffff;
+      b = k = 0;                      /* dump bits */
+      Tracev((stderr, "inflate:       stored length %u\n", s->sub.left));
+      s->mode = s->sub.left ? STORED : (s->last ? DRY : TYPE);
+      break;
+    case STORED:
+      if (n == 0)
+        LEAVE
+      NEEDOUT
+      t = s->sub.left;
+      if (t > n) t = n;
+      if (t > m) t = m;
+      zmemcpy(q, p, t);
+      p += t;  n -= t;
+      q += t;  m -= t;
+      if ((s->sub.left -= t) != 0)
+        break;
+      Tracev((stderr, "inflate:       stored end, %lu total out\n",
+              z->total_out + (q >= s->read ? q - s->read :
+              (s->end - s->read) + (q - s->window))));
+      s->mode = s->last ? DRY : TYPE;
+      break;
+    case TABLE:
+      NEEDBITS(14)
+      s->sub.trees.table = t = (uInt)b & 0x3fff;
+#ifndef PKZIP_BUG_WORKAROUND
+      if ((t & 0x1f) > 29 || ((t >> 5) & 0x1f) > 29)
+      {
+        s->mode = BAD;
+        z->msg = (char*)"too many length or distance symbols";
+        r = Z_DATA_ERROR;
+        LEAVE
+      }
+#endif
+      t = 258 + (t & 0x1f) + ((t >> 5) & 0x1f);
+      if ((s->sub.trees.blens = (uInt*)ZALLOC(z, t, sizeof(uInt))) == Z_NULL)
+      {
+        r = Z_MEM_ERROR;
+        LEAVE
+      }
+      DUMPBITS(14)
+      s->sub.trees.index = 0;
+      Tracev((stderr, "inflate:       table sizes ok\n"));
+      s->mode = BTREE;
+    case BTREE:
+      while (s->sub.trees.index < 4 + (s->sub.trees.table >> 10))
+      {
+        NEEDBITS(3)
+        s->sub.trees.blens[border[s->sub.trees.index++]] = (uInt)b & 7;
+        DUMPBITS(3)
+      }
+      while (s->sub.trees.index < 19)
+        s->sub.trees.blens[border[s->sub.trees.index++]] = 0;
+      s->sub.trees.bb = 7;
+      t = ext2_inflate_trees_bits(s->sub.trees.blens, &s->sub.trees.bb,
+                             &s->sub.trees.tb, s->hufts, z);
+      if (t != Z_OK)
+      {
+        r = t;
+        if (r == Z_DATA_ERROR)
+	  {
+	    ZFREE(z, s->sub.trees.blens);
+	    s->mode = BAD;
+	  }
+        LEAVE
+      }
+      s->sub.trees.index = 0;
+      Tracev((stderr, "inflate:       bits tree ok\n"));
+      s->mode = DTREE;
+    case DTREE:
+      while (t = s->sub.trees.table,
+             s->sub.trees.index < 258 + (t & 0x1f) + ((t >> 5) & 0x1f))
+      {
+        inflate_huft *h;
+        uInt i, j, c;
+
+        t = s->sub.trees.bb;
+        NEEDBITS(t)
+        h = s->sub.trees.tb + ((uInt)b & ext2_inflate_mask[t]);
+        t = h->bits;
+        c = h->base;
+        if (c < 16)
+        {
+          DUMPBITS(t)
+          s->sub.trees.blens[s->sub.trees.index++] = c;
+        }
+        else /* c == 16..18 */
+        {
+          i = c == 18 ? 7 : c - 14;
+          j = c == 18 ? 11 : 3;
+          NEEDBITS(t + i)
+          DUMPBITS(t)
+          j += (uInt)b & ext2_inflate_mask[i];
+          DUMPBITS(i)
+          i = s->sub.trees.index;
+          t = s->sub.trees.table;
+          if (i + j > 258 + (t & 0x1f) + ((t >> 5) & 0x1f) ||
+              (c == 16 && i < 1))
+          {
+            ZFREE(z, s->sub.trees.blens);
+            s->mode = BAD;
+            z->msg = (char*)"invalid bit length repeat";
+            r = Z_DATA_ERROR;
+            LEAVE
+          }
+          c = c == 16 ? s->sub.trees.blens[i - 1] : 0;
+          do {
+            s->sub.trees.blens[i++] = c;
+          } while (--j);
+          s->sub.trees.index = i;
+        }
+      }
+      s->sub.trees.tb = Z_NULL;
+      {
+        uInt bl, bd;
+        inflate_huft *tl, *td;
+        struct inflate_codes_state *c;
+
+        bl = 9;         /* must be <= 9 for lookahead assumptions */
+        bd = 6;         /* must be <= 9 for lookahead assumptions */
+        t = s->sub.trees.table;
+        t = ext2_inflate_trees_dynamic(257 + (t & 0x1f), 1 + ((t >> 5) & 0x1f),
+                                  s->sub.trees.blens, &bl, &bd, &tl, &td,
+                                  s->hufts, z);
+        if (t != Z_OK)
+        {
+          if (t == (uInt)Z_DATA_ERROR)
+	    {
+	      ZFREE(z, s->sub.trees.blens);
+	      s->mode = BAD;
+	    }
+          r = t;
+          LEAVE
+        }
+	ZFREE(z, s->sub.trees.blens);
+        Tracev((stderr, "inflate:       trees ok\n"));
+        if ((c = ext2_inflate_codes_new(bl, bd, tl, td, z)) == Z_NULL)
+        {
+          r = Z_MEM_ERROR;
+          LEAVE
+        }
+        s->sub.decode.codes = c;
+      }
+      s->mode = CODES;
+    case CODES:
+      UPDATE
+      if ((r = ext2_inflate_codes(s, z, r)) != Z_STREAM_END)
+        return ext2_inflate_flush(s, z, r);
+      r = Z_OK;
+      ext2_inflate_codes_free(s->sub.decode.codes, z);
+      LOAD
+      Tracev((stderr, "inflate:       codes end, %lu total out\n",
+              z->total_out + (q >= s->read ? q - s->read :
+              (s->end - s->read) + (q - s->window))));
+      if (!s->last)
+      {
+        s->mode = TYPE;
+        break;
+      }
+      s->mode = DRY;
+    case DRY:
+      FLUSH
+      if (s->read != s->write)
+        LEAVE
+      s->mode = DONE;
+    case DONE:
+      r = Z_STREAM_END;
+      LEAVE
+    case BAD:
+      r = Z_DATA_ERROR;
+      LEAVE
+    default:
+      r = Z_STREAM_ERROR;
+      LEAVE
+  }
+}
+
+
+int ext2_inflate_blocks_free(s, z)
+struct inflate_blocks_state *s;
+z_streamp z;
+{
+  ext2_inflate_blocks_reset(s, z, Z_NULL);
+  ZFREE(z, s->window);
+  ZFREE(z, s->hufts);
+  ZFREE(z, s);
+  Tracev((stderr, "inflate:   blocks freed\n"));
+  return Z_OK;
+}
+
+
+void ext2_inflate_set_dictionary(s, d, n)
+struct inflate_blocks_state *s;
+const Byte *d;
+uInt  n;
+{
+  zmemcpy(s->window, d, n);
+  s->read = s->write = s->window + n;
+}
+
+
+/* Returns true if inflate is currently at the end of a block generated
+ * by Z_SYNC_FLUSH or Z_FULL_FLUSH. 
+ * IN assertion: s != Z_NULL
+ */
+int ext2_inflate_blocks_sync_point(s)
+struct inflate_blocks_state *s;
+{
+  return s->mode == LENS;
+}
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/infblock.h linux-2.6.18.5/fs/ext2/gzip/infblock.h
--- linux-2.6.18.5.org/fs/ext2/gzip/infblock.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/infblock.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,39 @@
+/* infblock.h -- header to use infblock.c
+ * Copyright (C) 1995-1998 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+struct inflate_blocks_state;
+
+extern struct inflate_blocks_state * ext2_inflate_blocks_new OF((
+    z_streamp z,
+    check_func c,               /* check function */
+    uInt w));                   /* window size */
+
+extern int ext2_inflate_blocks OF((
+    struct inflate_blocks_state *,
+    z_streamp ,
+    int));                      /* initial return code */
+
+extern void ext2_inflate_blocks_reset OF((
+    struct inflate_blocks_state *,
+    z_streamp ,
+    uLong *));                  /* check value on output */
+
+extern int ext2_inflate_blocks_free OF((
+    struct inflate_blocks_state *,
+    z_streamp));
+
+extern void ext2_inflate_set_dictionary OF((
+    struct inflate_blocks_state *s,
+    const Byte *d,  /* dictionary */
+    uInt  n));       /* dictionary length */
+
+extern int ext2_inflate_blocks_sync_point OF((
+    struct inflate_blocks_state *s));
+
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/infcodes.c linux-2.6.18.5/fs/ext2/gzip/infcodes.c
--- linux-2.6.18.5.org/fs/ext2/gzip/infcodes.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/infcodes.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,251 @@
+/* infcodes.c -- process literals and length/distance pairs
+ * Copyright (C) 1995-1998 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+#include "zutil.h"
+#include "inftrees.h"
+#include "infblock.h"
+#include "infcodes.h"
+#include "infutil.h"
+#include "inffast.h"
+
+/* simplify the use of the inflate_huft type with some defines */
+#define exop word.what.Exop
+#define bits word.what.Bits
+
+typedef enum {        /* waiting for "i:"=input, "o:"=output, "x:"=nothing */
+      START,    /* x: set up for LEN */
+      LEN,      /* i: get length/literal/eob next */
+      LENEXT,   /* i: getting length extra (have base) */
+      DIST,     /* i: get distance next */
+      DISTEXT,  /* i: getting distance extra */
+      COPY,     /* o: copying bytes in window, waiting for space */
+      LIT,      /* o: got literal, waiting for output space */
+      WASH,     /* o: got eob, possibly still output waiting */
+      END,      /* x: got eob and all data flushed */
+      BADCODE}  /* x: got error */
+inflate_codes_mode;
+
+/* inflate codes private state */
+struct inflate_codes_state {
+
+  /* mode */
+  inflate_codes_mode mode;      /* current inflate_codes mode */
+
+  /* mode dependent information */
+  uInt len;
+  union {
+    struct {
+      inflate_huft *tree;       /* pointer into tree */
+      uInt need;                /* bits needed */
+    } code;             /* if LEN or DIST, where in tree */
+    uInt lit;           /* if LIT, literal */
+    struct {
+      uInt get;                 /* bits to get for extra */
+      uInt dist;                /* distance back to copy from */
+    } copy;             /* if EXT or COPY, where and how much */
+  } sub;                /* submode */
+
+  /* mode independent information */
+  Byte lbits;           /* ltree bits decoded per branch */
+  Byte dbits;           /* dtree bits decoder per branch */
+  inflate_huft *ltree;          /* literal/length/eob tree */
+  inflate_huft *dtree;          /* distance tree */
+
+};
+
+
+struct inflate_codes_state *ext2_inflate_codes_new(bl, bd, tl, td, z)
+uInt bl, bd;
+inflate_huft *tl;
+inflate_huft *td; /* need separate declaration for Borland C++ */
+z_streamp z;
+{
+  struct inflate_codes_state *c;
+
+  if ((c = (struct inflate_codes_state *)
+       ZALLOC(z,1,sizeof(struct inflate_codes_state))) != Z_NULL)
+  {
+    c->mode = START;
+    c->lbits = (Byte)bl;
+    c->dbits = (Byte)bd;
+    c->ltree = tl;
+    c->dtree = td;
+    Tracev((stderr, "inflate:       codes new\n"));
+  }
+  return c;
+}
+
+
+int ext2_inflate_codes(s, z, r)
+struct inflate_blocks_state *s;
+z_streamp z;
+int r;
+{
+  uInt j;               /* temporary storage */
+  inflate_huft *t;      /* temporary pointer */
+  uInt e;               /* extra bits or operation */
+  uLong b;              /* bit buffer */
+  uInt k;               /* bits in bit buffer */
+  Byte *p;             /* input data pointer */
+  uInt n;               /* bytes available there */
+  Byte *q;             /* output window write pointer */
+  uInt m;               /* bytes to end of window or read pointer */
+  Byte *f;             /* pointer to copy strings from */
+  struct inflate_codes_state *c = s->sub.decode.codes;  /* codes state */
+
+  /* copy input/output information to locals (UPDATE macro restores) */
+  LOAD
+
+  /* process input and output based on current state */
+  while (1) switch (c->mode)
+  {             /* waiting for "i:"=input, "o:"=output, "x:"=nothing */
+    case START:         /* x: set up for LEN */
+#ifndef SLOW
+      if (m >= 258 && n >= 10)
+      {
+        UPDATE
+        r = ext2_inflate_fast(c->lbits, c->dbits, c->ltree, c->dtree, s, z);
+        LOAD
+        if (r != Z_OK)
+        {
+          c->mode = r == Z_STREAM_END ? WASH : BADCODE;
+          break;
+        }
+      }
+#endif /* !SLOW */
+      c->sub.code.need = c->lbits;
+      c->sub.code.tree = c->ltree;
+      c->mode = LEN;
+    case LEN:           /* i: get length/literal/eob next */
+      j = c->sub.code.need;
+      NEEDBITS(j)
+      t = c->sub.code.tree + ((uInt)b & ext2_inflate_mask[j]);
+      DUMPBITS(t->bits)
+      e = (uInt)(t->exop);
+      if (e == 0)               /* literal */
+      {
+        c->sub.lit = t->base;
+        Tracevv((stderr, t->base >= 0x20 && t->base < 0x7f ?
+                 "inflate:         literal '%c'\n" :
+                 "inflate:         literal 0x%02x\n", t->base));
+        c->mode = LIT;
+        break;
+      }
+      if (e & 16)               /* length */
+      {
+        c->sub.copy.get = e & 15;
+        c->len = t->base;
+        c->mode = LENEXT;
+        break;
+      }
+      if ((e & 64) == 0)        /* next table */
+      {
+        c->sub.code.need = e;
+        c->sub.code.tree = t + t->base;
+        break;
+      }
+      if (e & 32)               /* end of block */
+      {
+        Tracevv((stderr, "inflate:         end of block\n"));
+        c->mode = WASH;
+        break;
+      }
+      c->mode = BADCODE;        /* invalid code */
+      z->msg = (char*)"invalid literal/length code";
+      r = Z_DATA_ERROR;
+      LEAVE
+    case LENEXT:        /* i: getting length extra (have base) */
+      j = c->sub.copy.get;
+      NEEDBITS(j)
+      c->len += (uInt)b & ext2_inflate_mask[j];
+      DUMPBITS(j)
+      c->sub.code.need = c->dbits;
+      c->sub.code.tree = c->dtree;
+      Tracevv((stderr, "inflate:         length %u\n", c->len));
+      c->mode = DIST;
+    case DIST:          /* i: get distance next */
+      j = c->sub.code.need;
+      NEEDBITS(j)
+      t = c->sub.code.tree + ((uInt)b & ext2_inflate_mask[j]);
+      DUMPBITS(t->bits)
+      e = (uInt)(t->exop);
+      if (e & 16)               /* distance */
+      {
+        c->sub.copy.get = e & 15;
+        c->sub.copy.dist = t->base;
+        c->mode = DISTEXT;
+        break;
+      }
+      if ((e & 64) == 0)        /* next table */
+      {
+        c->sub.code.need = e;
+        c->sub.code.tree = t + t->base;
+        break;
+      }
+      c->mode = BADCODE;        /* invalid code */
+      z->msg = (char*)"invalid distance code";
+      r = Z_DATA_ERROR;
+      LEAVE
+    case DISTEXT:       /* i: getting distance extra */
+      j = c->sub.copy.get;
+      NEEDBITS(j)
+      c->sub.copy.dist += (uInt)b & ext2_inflate_mask[j];
+      DUMPBITS(j)
+      Tracevv((stderr, "inflate:         distance %u\n", c->sub.copy.dist));
+      c->mode = COPY;
+    case COPY:          /* o: copying bytes in window, waiting for space */
+      f = (uInt)(q - s->window) < c->sub.copy.dist ?
+          s->end - (c->sub.copy.dist - (q - s->window)) :
+          q - c->sub.copy.dist;
+      while (c->len)
+      {
+        NEEDOUT
+        OUTBYTE(*f++)
+        if (f == s->end)
+          f = s->window;
+        c->len--;
+      }
+      c->mode = START;
+      break;
+    case LIT:           /* o: got literal, waiting for output space */
+      NEEDOUT
+      OUTBYTE(c->sub.lit)
+      c->mode = START;
+      break;
+    case WASH:          /* o: got eob, possibly more output */
+      if (k > 7)        /* return unused byte, if any */
+      {
+        Assert(k < 16, "inflate_codes grabbed too many bytes")
+        k -= 8;
+        n++;
+        p--;            /* can always return one */
+      }
+      FLUSH
+      if (s->read != s->write)
+        LEAVE
+      c->mode = END;
+    case END:
+      r = Z_STREAM_END;
+      LEAVE
+    case BADCODE:       /* x: got error */
+      r = Z_DATA_ERROR;
+      LEAVE
+    default:
+      r = Z_STREAM_ERROR;
+      LEAVE
+  }
+#ifdef NEED_DUMMY_RETURN
+  return Z_STREAM_ERROR;  /* Some dumb compilers complain without this */
+#endif
+}
+
+
+void ext2_inflate_codes_free(c, z)
+struct inflate_codes_state *c;
+z_streamp z;
+{
+  ZFREE(z, c);
+  Tracev((stderr, "inflate:       codes free\n"));
+}
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/infcodes.h linux-2.6.18.5/fs/ext2/gzip/infcodes.h
--- linux-2.6.18.5.org/fs/ext2/gzip/infcodes.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/infcodes.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,26 @@
+/* infcodes.h -- header to use infcodes.c
+ * Copyright (C) 1995-1998 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+struct inflate_codes_state;
+
+extern struct inflate_codes_state *ext2_inflate_codes_new OF((
+    uInt, uInt,
+    inflate_huft *, inflate_huft *,
+    z_streamp ));
+
+extern int ext2_inflate_codes OF((
+    struct inflate_blocks_state *,
+    z_streamp ,
+    int));
+
+extern void ext2_inflate_codes_free OF((
+    struct inflate_codes_state *,
+    z_streamp ));
+
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/inffast.c linux-2.6.18.5/fs/ext2/gzip/inffast.c
--- linux-2.6.18.5.org/fs/ext2/gzip/inffast.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/inffast.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,170 @@
+/* inffast.c -- process literals and length/distance pairs fast
+ * Copyright (C) 1995-1998 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+#include "zutil.h"
+#include "inftrees.h"
+#include "infblock.h"
+#include "infcodes.h"
+#include "infutil.h"
+#include "inffast.h"
+
+struct inflate_codes_state {int dummy;}; /* for buggy compilers */
+
+/* simplify the use of the inflate_huft type with some defines */
+#define exop word.what.Exop
+#define bits word.what.Bits
+
+/* macros for bit input with no checking and for returning unused bytes */
+#define GRABBITS(j) {while(k<(j)){b|=((uLong)NEXTBYTE)<<k;k+=8;}}
+#define UNGRAB {c=z->avail_in-n;c=(k>>3)<c?k>>3:c;n+=c;p-=c;k-=c<<3;}
+
+/* Called with number of bytes left to write in window at least 258
+   (the maximum string length) and number of input bytes available
+   at least ten.  The ten bytes are six bytes for the longest length/
+   distance pair plus four bytes for overloading the bit buffer. */
+
+int ext2_inflate_fast(bl, bd, tl, td, s, z)
+uInt bl, bd;
+inflate_huft *tl;
+inflate_huft *td; /* need separate declaration for Borland C++ */
+struct inflate_blocks_state *s;
+z_streamp z;
+{
+  inflate_huft *t;      /* temporary pointer */
+  uInt e;               /* extra bits or operation */
+  uLong b;              /* bit buffer */
+  uInt k;               /* bits in bit buffer */
+  Byte *p;             /* input data pointer */
+  uInt n;               /* bytes available there */
+  Byte *q;             /* output window write pointer */
+  uInt m;               /* bytes to end of window or read pointer */
+  uInt ml;              /* mask for literal/length tree */
+  uInt md;              /* mask for distance tree */
+  uInt c;               /* bytes to copy */
+  uInt d;               /* distance back to copy from */
+  Byte *r;             /* copy source pointer */
+
+  /* load input, output, bit values */
+  LOAD
+
+  /* initialize masks */
+  ml = ext2_inflate_mask[bl];
+  md = ext2_inflate_mask[bd];
+
+  /* do until not enough input or output space for fast loop */
+  do {                          /* assume called with m >= 258 && n >= 10 */
+    /* get literal/length code */
+    GRABBITS(20)                /* max bits for literal/length code */
+    if ((e = (t = tl + ((uInt)b & ml))->exop) == 0)
+    {
+      DUMPBITS(t->bits)
+      Tracevv((stderr, t->base >= 0x20 && t->base < 0x7f ?
+                "inflate:         * literal '%c'\n" :
+                "inflate:         * literal 0x%02x\n", t->base));
+      *q++ = (Byte)t->base;
+      m--;
+      continue;
+    }
+    do {
+      DUMPBITS(t->bits)
+      if (e & 16)
+      {
+        /* get extra bits for length */
+        e &= 15;
+        c = t->base + ((uInt)b & ext2_inflate_mask[e]);
+        DUMPBITS(e)
+        Tracevv((stderr, "inflate:         * length %u\n", c));
+
+        /* decode distance base of block to copy */
+        GRABBITS(15);           /* max bits for distance code */
+        e = (t = td + ((uInt)b & md))->exop;
+        do {
+          DUMPBITS(t->bits)
+          if (e & 16)
+          {
+            /* get extra bits to add to distance base */
+            e &= 15;
+            GRABBITS(e)         /* get extra bits (up to 13) */
+            d = t->base + ((uInt)b & ext2_inflate_mask[e]);
+            DUMPBITS(e)
+            Tracevv((stderr, "inflate:         * distance %u\n", d));
+
+            /* do the copy */
+            m -= c;
+            if ((uInt)(q - s->window) >= d)     /* offset before dest */
+            {                                   /*  just copy */
+              r = q - d;
+              *q++ = *r++;  c--;        /* minimum count is three, */
+              *q++ = *r++;  c--;        /*  so unroll loop a little */
+            }
+            else                        /* else offset after destination */
+            {
+              e = d - (uInt)(q - s->window); /* bytes from offset to end */
+              r = s->end - e;           /* pointer to offset */
+              if (c > e)                /* if source crosses, */
+              {
+                c -= e;                 /* copy to end of window */
+                do {
+                  *q++ = *r++;
+                } while (--e);
+                r = s->window;          /* copy rest from start of window */
+              }
+            }
+            do {                        /* copy all or what's left */
+              *q++ = *r++;
+            } while (--c);
+            break;
+          }
+          else if ((e & 64) == 0)
+          {
+            t += t->base;
+            e = (t += ((uInt)b & ext2_inflate_mask[e]))->exop;
+          }
+          else
+          {
+            z->msg = (char*)"invalid distance code";
+            UNGRAB
+            UPDATE
+            return Z_DATA_ERROR;
+          }
+        } while (1);
+        break;
+      }
+      if ((e & 64) == 0)
+      {
+        t += t->base;
+        if ((e = (t += ((uInt)b & ext2_inflate_mask[e]))->exop) == 0)
+        {
+          DUMPBITS(t->bits)
+          Tracevv((stderr, t->base >= 0x20 && t->base < 0x7f ?
+                    "inflate:         * literal '%c'\n" :
+                    "inflate:         * literal 0x%02x\n", t->base));
+          *q++ = (Byte)t->base;
+          m--;
+          break;
+        }
+      }
+      else if (e & 32)
+      {
+        Tracevv((stderr, "inflate:         * end of block\n"));
+        UNGRAB
+        UPDATE
+        return Z_STREAM_END;
+      }
+      else
+      {
+        z->msg = (char*)"invalid literal/length code";
+        UNGRAB
+        UPDATE
+        return Z_DATA_ERROR;
+      }
+    } while (1);
+  } while (m >= 258 && n >= 10);
+
+  /* not enough input or output--restore pointers and return */
+  UNGRAB
+  UPDATE
+  return Z_OK;
+}
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/inffast.h linux-2.6.18.5/fs/ext2/gzip/inffast.h
--- linux-2.6.18.5.org/fs/ext2/gzip/inffast.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/inffast.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,17 @@
+/* inffast.h -- header to use inffast.c
+ * Copyright (C) 1995-1998 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+extern int ext2_inflate_fast OF((
+    uInt,
+    uInt,
+    inflate_huft *,
+    inflate_huft *,
+    struct inflate_blocks_state *,
+    z_streamp ));
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/inffixed.h linux-2.6.18.5/fs/ext2/gzip/inffixed.h
--- linux-2.6.18.5.org/fs/ext2/gzip/inffixed.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/inffixed.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,151 @@
+/* inffixed.h -- table for decoding fixed codes
+ * Generated automatically by the maketree.c program
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+local uInt fixed_bl = 9;
+local uInt fixed_bd = 5;
+local inflate_huft fixed_tl[] = {
+    {{{96,7}},256}, {{{0,8}},80}, {{{0,8}},16}, {{{84,8}},115},
+    {{{82,7}},31}, {{{0,8}},112}, {{{0,8}},48}, {{{0,9}},192},
+    {{{80,7}},10}, {{{0,8}},96}, {{{0,8}},32}, {{{0,9}},160},
+    {{{0,8}},0}, {{{0,8}},128}, {{{0,8}},64}, {{{0,9}},224},
+    {{{80,7}},6}, {{{0,8}},88}, {{{0,8}},24}, {{{0,9}},144},
+    {{{83,7}},59}, {{{0,8}},120}, {{{0,8}},56}, {{{0,9}},208},
+    {{{81,7}},17}, {{{0,8}},104}, {{{0,8}},40}, {{{0,9}},176},
+    {{{0,8}},8}, {{{0,8}},136}, {{{0,8}},72}, {{{0,9}},240},
+    {{{80,7}},4}, {{{0,8}},84}, {{{0,8}},20}, {{{85,8}},227},
+    {{{83,7}},43}, {{{0,8}},116}, {{{0,8}},52}, {{{0,9}},200},
+    {{{81,7}},13}, {{{0,8}},100}, {{{0,8}},36}, {{{0,9}},168},
+    {{{0,8}},4}, {{{0,8}},132}, {{{0,8}},68}, {{{0,9}},232},
+    {{{80,7}},8}, {{{0,8}},92}, {{{0,8}},28}, {{{0,9}},152},
+    {{{84,7}},83}, {{{0,8}},124}, {{{0,8}},60}, {{{0,9}},216},
+    {{{82,7}},23}, {{{0,8}},108}, {{{0,8}},44}, {{{0,9}},184},
+    {{{0,8}},12}, {{{0,8}},140}, {{{0,8}},76}, {{{0,9}},248},
+    {{{80,7}},3}, {{{0,8}},82}, {{{0,8}},18}, {{{85,8}},163},
+    {{{83,7}},35}, {{{0,8}},114}, {{{0,8}},50}, {{{0,9}},196},
+    {{{81,7}},11}, {{{0,8}},98}, {{{0,8}},34}, {{{0,9}},164},
+    {{{0,8}},2}, {{{0,8}},130}, {{{0,8}},66}, {{{0,9}},228},
+    {{{80,7}},7}, {{{0,8}},90}, {{{0,8}},26}, {{{0,9}},148},
+    {{{84,7}},67}, {{{0,8}},122}, {{{0,8}},58}, {{{0,9}},212},
+    {{{82,7}},19}, {{{0,8}},106}, {{{0,8}},42}, {{{0,9}},180},
+    {{{0,8}},10}, {{{0,8}},138}, {{{0,8}},74}, {{{0,9}},244},
+    {{{80,7}},5}, {{{0,8}},86}, {{{0,8}},22}, {{{192,8}},0},
+    {{{83,7}},51}, {{{0,8}},118}, {{{0,8}},54}, {{{0,9}},204},
+    {{{81,7}},15}, {{{0,8}},102}, {{{0,8}},38}, {{{0,9}},172},
+    {{{0,8}},6}, {{{0,8}},134}, {{{0,8}},70}, {{{0,9}},236},
+    {{{80,7}},9}, {{{0,8}},94}, {{{0,8}},30}, {{{0,9}},156},
+    {{{84,7}},99}, {{{0,8}},126}, {{{0,8}},62}, {{{0,9}},220},
+    {{{82,7}},27}, {{{0,8}},110}, {{{0,8}},46}, {{{0,9}},188},
+    {{{0,8}},14}, {{{0,8}},142}, {{{0,8}},78}, {{{0,9}},252},
+    {{{96,7}},256}, {{{0,8}},81}, {{{0,8}},17}, {{{85,8}},131},
+    {{{82,7}},31}, {{{0,8}},113}, {{{0,8}},49}, {{{0,9}},194},
+    {{{80,7}},10}, {{{0,8}},97}, {{{0,8}},33}, {{{0,9}},162},
+    {{{0,8}},1}, {{{0,8}},129}, {{{0,8}},65}, {{{0,9}},226},
+    {{{80,7}},6}, {{{0,8}},89}, {{{0,8}},25}, {{{0,9}},146},
+    {{{83,7}},59}, {{{0,8}},121}, {{{0,8}},57}, {{{0,9}},210},
+    {{{81,7}},17}, {{{0,8}},105}, {{{0,8}},41}, {{{0,9}},178},
+    {{{0,8}},9}, {{{0,8}},137}, {{{0,8}},73}, {{{0,9}},242},
+    {{{80,7}},4}, {{{0,8}},85}, {{{0,8}},21}, {{{80,8}},258},
+    {{{83,7}},43}, {{{0,8}},117}, {{{0,8}},53}, {{{0,9}},202},
+    {{{81,7}},13}, {{{0,8}},101}, {{{0,8}},37}, {{{0,9}},170},
+    {{{0,8}},5}, {{{0,8}},133}, {{{0,8}},69}, {{{0,9}},234},
+    {{{80,7}},8}, {{{0,8}},93}, {{{0,8}},29}, {{{0,9}},154},
+    {{{84,7}},83}, {{{0,8}},125}, {{{0,8}},61}, {{{0,9}},218},
+    {{{82,7}},23}, {{{0,8}},109}, {{{0,8}},45}, {{{0,9}},186},
+    {{{0,8}},13}, {{{0,8}},141}, {{{0,8}},77}, {{{0,9}},250},
+    {{{80,7}},3}, {{{0,8}},83}, {{{0,8}},19}, {{{85,8}},195},
+    {{{83,7}},35}, {{{0,8}},115}, {{{0,8}},51}, {{{0,9}},198},
+    {{{81,7}},11}, {{{0,8}},99}, {{{0,8}},35}, {{{0,9}},166},
+    {{{0,8}},3}, {{{0,8}},131}, {{{0,8}},67}, {{{0,9}},230},
+    {{{80,7}},7}, {{{0,8}},91}, {{{0,8}},27}, {{{0,9}},150},
+    {{{84,7}},67}, {{{0,8}},123}, {{{0,8}},59}, {{{0,9}},214},
+    {{{82,7}},19}, {{{0,8}},107}, {{{0,8}},43}, {{{0,9}},182},
+    {{{0,8}},11}, {{{0,8}},139}, {{{0,8}},75}, {{{0,9}},246},
+    {{{80,7}},5}, {{{0,8}},87}, {{{0,8}},23}, {{{192,8}},0},
+    {{{83,7}},51}, {{{0,8}},119}, {{{0,8}},55}, {{{0,9}},206},
+    {{{81,7}},15}, {{{0,8}},103}, {{{0,8}},39}, {{{0,9}},174},
+    {{{0,8}},7}, {{{0,8}},135}, {{{0,8}},71}, {{{0,9}},238},
+    {{{80,7}},9}, {{{0,8}},95}, {{{0,8}},31}, {{{0,9}},158},
+    {{{84,7}},99}, {{{0,8}},127}, {{{0,8}},63}, {{{0,9}},222},
+    {{{82,7}},27}, {{{0,8}},111}, {{{0,8}},47}, {{{0,9}},190},
+    {{{0,8}},15}, {{{0,8}},143}, {{{0,8}},79}, {{{0,9}},254},
+    {{{96,7}},256}, {{{0,8}},80}, {{{0,8}},16}, {{{84,8}},115},
+    {{{82,7}},31}, {{{0,8}},112}, {{{0,8}},48}, {{{0,9}},193},
+    {{{80,7}},10}, {{{0,8}},96}, {{{0,8}},32}, {{{0,9}},161},
+    {{{0,8}},0}, {{{0,8}},128}, {{{0,8}},64}, {{{0,9}},225},
+    {{{80,7}},6}, {{{0,8}},88}, {{{0,8}},24}, {{{0,9}},145},
+    {{{83,7}},59}, {{{0,8}},120}, {{{0,8}},56}, {{{0,9}},209},
+    {{{81,7}},17}, {{{0,8}},104}, {{{0,8}},40}, {{{0,9}},177},
+    {{{0,8}},8}, {{{0,8}},136}, {{{0,8}},72}, {{{0,9}},241},
+    {{{80,7}},4}, {{{0,8}},84}, {{{0,8}},20}, {{{85,8}},227},
+    {{{83,7}},43}, {{{0,8}},116}, {{{0,8}},52}, {{{0,9}},201},
+    {{{81,7}},13}, {{{0,8}},100}, {{{0,8}},36}, {{{0,9}},169},
+    {{{0,8}},4}, {{{0,8}},132}, {{{0,8}},68}, {{{0,9}},233},
+    {{{80,7}},8}, {{{0,8}},92}, {{{0,8}},28}, {{{0,9}},153},
+    {{{84,7}},83}, {{{0,8}},124}, {{{0,8}},60}, {{{0,9}},217},
+    {{{82,7}},23}, {{{0,8}},108}, {{{0,8}},44}, {{{0,9}},185},
+    {{{0,8}},12}, {{{0,8}},140}, {{{0,8}},76}, {{{0,9}},249},
+    {{{80,7}},3}, {{{0,8}},82}, {{{0,8}},18}, {{{85,8}},163},
+    {{{83,7}},35}, {{{0,8}},114}, {{{0,8}},50}, {{{0,9}},197},
+    {{{81,7}},11}, {{{0,8}},98}, {{{0,8}},34}, {{{0,9}},165},
+    {{{0,8}},2}, {{{0,8}},130}, {{{0,8}},66}, {{{0,9}},229},
+    {{{80,7}},7}, {{{0,8}},90}, {{{0,8}},26}, {{{0,9}},149},
+    {{{84,7}},67}, {{{0,8}},122}, {{{0,8}},58}, {{{0,9}},213},
+    {{{82,7}},19}, {{{0,8}},106}, {{{0,8}},42}, {{{0,9}},181},
+    {{{0,8}},10}, {{{0,8}},138}, {{{0,8}},74}, {{{0,9}},245},
+    {{{80,7}},5}, {{{0,8}},86}, {{{0,8}},22}, {{{192,8}},0},
+    {{{83,7}},51}, {{{0,8}},118}, {{{0,8}},54}, {{{0,9}},205},
+    {{{81,7}},15}, {{{0,8}},102}, {{{0,8}},38}, {{{0,9}},173},
+    {{{0,8}},6}, {{{0,8}},134}, {{{0,8}},70}, {{{0,9}},237},
+    {{{80,7}},9}, {{{0,8}},94}, {{{0,8}},30}, {{{0,9}},157},
+    {{{84,7}},99}, {{{0,8}},126}, {{{0,8}},62}, {{{0,9}},221},
+    {{{82,7}},27}, {{{0,8}},110}, {{{0,8}},46}, {{{0,9}},189},
+    {{{0,8}},14}, {{{0,8}},142}, {{{0,8}},78}, {{{0,9}},253},
+    {{{96,7}},256}, {{{0,8}},81}, {{{0,8}},17}, {{{85,8}},131},
+    {{{82,7}},31}, {{{0,8}},113}, {{{0,8}},49}, {{{0,9}},195},
+    {{{80,7}},10}, {{{0,8}},97}, {{{0,8}},33}, {{{0,9}},163},
+    {{{0,8}},1}, {{{0,8}},129}, {{{0,8}},65}, {{{0,9}},227},
+    {{{80,7}},6}, {{{0,8}},89}, {{{0,8}},25}, {{{0,9}},147},
+    {{{83,7}},59}, {{{0,8}},121}, {{{0,8}},57}, {{{0,9}},211},
+    {{{81,7}},17}, {{{0,8}},105}, {{{0,8}},41}, {{{0,9}},179},
+    {{{0,8}},9}, {{{0,8}},137}, {{{0,8}},73}, {{{0,9}},243},
+    {{{80,7}},4}, {{{0,8}},85}, {{{0,8}},21}, {{{80,8}},258},
+    {{{83,7}},43}, {{{0,8}},117}, {{{0,8}},53}, {{{0,9}},203},
+    {{{81,7}},13}, {{{0,8}},101}, {{{0,8}},37}, {{{0,9}},171},
+    {{{0,8}},5}, {{{0,8}},133}, {{{0,8}},69}, {{{0,9}},235},
+    {{{80,7}},8}, {{{0,8}},93}, {{{0,8}},29}, {{{0,9}},155},
+    {{{84,7}},83}, {{{0,8}},125}, {{{0,8}},61}, {{{0,9}},219},
+    {{{82,7}},23}, {{{0,8}},109}, {{{0,8}},45}, {{{0,9}},187},
+    {{{0,8}},13}, {{{0,8}},141}, {{{0,8}},77}, {{{0,9}},251},
+    {{{80,7}},3}, {{{0,8}},83}, {{{0,8}},19}, {{{85,8}},195},
+    {{{83,7}},35}, {{{0,8}},115}, {{{0,8}},51}, {{{0,9}},199},
+    {{{81,7}},11}, {{{0,8}},99}, {{{0,8}},35}, {{{0,9}},167},
+    {{{0,8}},3}, {{{0,8}},131}, {{{0,8}},67}, {{{0,9}},231},
+    {{{80,7}},7}, {{{0,8}},91}, {{{0,8}},27}, {{{0,9}},151},
+    {{{84,7}},67}, {{{0,8}},123}, {{{0,8}},59}, {{{0,9}},215},
+    {{{82,7}},19}, {{{0,8}},107}, {{{0,8}},43}, {{{0,9}},183},
+    {{{0,8}},11}, {{{0,8}},139}, {{{0,8}},75}, {{{0,9}},247},
+    {{{80,7}},5}, {{{0,8}},87}, {{{0,8}},23}, {{{192,8}},0},
+    {{{83,7}},51}, {{{0,8}},119}, {{{0,8}},55}, {{{0,9}},207},
+    {{{81,7}},15}, {{{0,8}},103}, {{{0,8}},39}, {{{0,9}},175},
+    {{{0,8}},7}, {{{0,8}},135}, {{{0,8}},71}, {{{0,9}},239},
+    {{{80,7}},9}, {{{0,8}},95}, {{{0,8}},31}, {{{0,9}},159},
+    {{{84,7}},99}, {{{0,8}},127}, {{{0,8}},63}, {{{0,9}},223},
+    {{{82,7}},27}, {{{0,8}},111}, {{{0,8}},47}, {{{0,9}},191},
+    {{{0,8}},15}, {{{0,8}},143}, {{{0,8}},79}, {{{0,9}},255}
+  };
+local inflate_huft fixed_td[] = {
+    {{{80,5}},1}, {{{87,5}},257}, {{{83,5}},17}, {{{91,5}},4097},
+    {{{81,5}},5}, {{{89,5}},1025}, {{{85,5}},65}, {{{93,5}},16385},
+    {{{80,5}},3}, {{{88,5}},513}, {{{84,5}},33}, {{{92,5}},8193},
+    {{{82,5}},9}, {{{90,5}},2049}, {{{86,5}},129}, {{{192,5}},24577},
+    {{{80,5}},2}, {{{87,5}},385}, {{{83,5}},25}, {{{91,5}},6145},
+    {{{81,5}},7}, {{{89,5}},1537}, {{{85,5}},97}, {{{93,5}},24577},
+    {{{80,5}},4}, {{{88,5}},769}, {{{84,5}},49}, {{{92,5}},12289},
+    {{{82,5}},13}, {{{90,5}},3073}, {{{86,5}},193}, {{{192,5}},24577}
+  };
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/inflate.c linux-2.6.18.5/fs/ext2/gzip/inflate.c
--- linux-2.6.18.5.org/fs/ext2/gzip/inflate.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/inflate.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,379 @@
+/* inflate.c -- zlib interface to inflate modules
+ * Copyright (C) 1995-1998 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+#include "zutil.h"
+#include "infblock.h"
+
+struct inflate_blocks_state {int dummy;}; /* for buggy compilers */
+
+typedef enum {
+      METHOD,   /* waiting for method byte */
+      FLAG,     /* waiting for flag byte */
+      DICT4,    /* four dictionary check bytes to go */
+      DICT3,    /* three dictionary check bytes to go */
+      DICT2,    /* two dictionary check bytes to go */
+      DICT1,    /* one dictionary check byte to go */
+      DICT0,    /* waiting for inflateSetDictionary */
+      BLOCKS,   /* decompressing blocks */
+      CHECK4,   /* four check bytes to go */
+      CHECK3,   /* three check bytes to go */
+      CHECK2,   /* two check bytes to go */
+      CHECK1,   /* one check byte to go */
+      DONE,     /* finished check, done */
+      BAD}      /* got an error--stay here */
+inflate_mode;
+
+/* inflate private state */
+struct internal_state {
+
+  /* mode */
+  inflate_mode  mode;   /* current inflate mode */
+
+  /* mode dependent information */
+  union {
+    uInt method;        /* if FLAGS, method byte */
+    struct {
+      uLong was;                /* computed check value */
+      uLong need;               /* stream check value */
+    } check;            /* if CHECK, check values to compare */
+    uInt marker;        /* if BAD, inflateSync's marker bytes count */
+  } sub;        /* submode */
+
+  /* mode independent information */
+  int  nowrap;          /* flag for no wrapper */
+  uInt wbits;           /* log2(window size)  (8..15, defaults to 15) */
+  struct inflate_blocks_state 
+    *blocks;            /* current inflate_blocks state */
+
+};
+
+
+int ext2_inflateReset(z)
+z_streamp z;
+{
+  if (z == Z_NULL || z->state == Z_NULL)
+    return Z_STREAM_ERROR;
+  z->total_in = z->total_out = 0;
+  z->msg = Z_NULL;
+  z->state->mode = z->state->nowrap ? BLOCKS : METHOD;
+  ext2_inflate_blocks_reset(z->state->blocks, z, Z_NULL);
+  Tracev((stderr, "inflate: reset\n"));
+  return Z_OK;
+}
+
+
+int ext2_inflateEnd(z)
+z_streamp z;
+{
+  if (z == Z_NULL || z->state == Z_NULL || z->zfree == Z_NULL)
+    return Z_STREAM_ERROR;
+  if (z->state->blocks != Z_NULL)
+    ext2_inflate_blocks_free(z->state->blocks, z);
+  ZFREE(z, z->state);
+  z->state = Z_NULL;
+  Tracev((stderr, "inflate: end\n"));
+  return Z_OK;
+}
+
+
+int ext2_inflateInit2_(z, w, version, stream_size)
+z_streamp z;
+int w;
+const char *version;
+int stream_size;
+{
+  if (version == Z_NULL || version[0] != ZLIB_VERSION[0] ||
+      stream_size != sizeof(z_stream))
+      return Z_VERSION_ERROR;
+
+  /* initialize state */
+  if (z == Z_NULL)
+    return Z_STREAM_ERROR;
+  z->msg = Z_NULL;
+#if 0
+  if (z->zalloc == Z_NULL)
+  {
+    z->zalloc = zcalloc;
+    z->opaque = (voidp)0;
+  }
+  if (z->zfree == Z_NULL) z->zfree = zcfree;
+#else
+  if (z->zalloc == Z_NULL || z->zfree == Z_NULL)
+    return Z_STREAM_ERROR;
+#endif
+  if ((z->state = (struct internal_state *)
+       ZALLOC(z,1,sizeof(struct internal_state))) == Z_NULL)
+    return Z_MEM_ERROR;
+  z->state->blocks = Z_NULL;
+
+  /* handle undocumented nowrap option (no zlib header or check) */
+  z->state->nowrap = 0;
+  if (w < 0)
+  {
+    w = - w;
+    z->state->nowrap = 1;
+  }
+
+  /* set window size */
+  if (w < 8 || w > 15)
+  {
+    ext2_inflateEnd(z);
+    return Z_STREAM_ERROR;
+  }
+  z->state->wbits = (uInt)w;
+
+  /* create inflate_blocks state */
+  if ((z->state->blocks =
+      ext2_inflate_blocks_new(z, z->state->nowrap ? Z_NULL : ext2_adler32, (uInt)1 << w))
+      == Z_NULL)
+  {
+    ext2_inflateEnd(z);
+    return Z_MEM_ERROR;
+  }
+  Tracev((stderr, "inflate: allocated\n"));
+
+  /* reset state */
+  ext2_inflateReset(z);
+  return Z_OK;
+}
+
+
+int ext2_inflateInit_(z, version, stream_size)
+z_streamp z;
+const char *version;
+int stream_size;
+{
+  return ext2_inflateInit2_(z, DEF_WBITS, version, stream_size);
+}
+
+
+#define NEEDBYTE {if(z->avail_in==0)return r;r=f;}
+#define NEXTBYTE (z->avail_in--,z->total_in++,*z->next_in++)
+
+int ext2_inflate(z, f)
+z_streamp z;
+int f;
+{
+  int r;
+  uInt b;
+
+  if (z == Z_NULL || z->state == Z_NULL || z->next_in == Z_NULL)
+    return Z_STREAM_ERROR;
+#if 0
+  f = f == Z_FINISH ? Z_BUF_ERROR : Z_OK;
+#endif
+  r = Z_BUF_ERROR;
+  while (1) switch (z->state->mode)
+  {
+    case METHOD:
+      NEEDBYTE
+      if (((z->state->sub.method = NEXTBYTE) & 0xf) != Z_DEFLATED)
+      {
+        z->state->mode = BAD;
+        z->msg = (char*)"unknown compression method";
+        z->state->sub.marker = 5;       /* can't try inflateSync */
+        break;
+      }
+      if ((z->state->sub.method >> 4) + 8 > z->state->wbits)
+      {
+        z->state->mode = BAD;
+        z->msg = (char*)"invalid window size";
+        z->state->sub.marker = 5;       /* can't try inflateSync */
+        break;
+      }
+      z->state->mode = FLAG;
+    case FLAG:
+      NEEDBYTE
+      b = NEXTBYTE;
+      if (((z->state->sub.method << 8) + b) % 31)
+      {
+        z->state->mode = BAD;
+        z->msg = (char*)"incorrect header check";
+        z->state->sub.marker = 5;       /* can't try inflateSync */
+        break;
+      }
+      Tracev((stderr, "inflate: zlib header ok\n"));
+      if (!(b & PRESET_DICT))
+      {
+        z->state->mode = BLOCKS;
+        break;
+      }
+      z->state->mode = DICT4;
+    case DICT4:
+      NEEDBYTE
+      z->state->sub.check.need = (uLong)NEXTBYTE << 24;
+      z->state->mode = DICT3;
+    case DICT3:
+      NEEDBYTE
+      z->state->sub.check.need += (uLong)NEXTBYTE << 16;
+      z->state->mode = DICT2;
+    case DICT2:
+      NEEDBYTE
+      z->state->sub.check.need += (uLong)NEXTBYTE << 8;
+      z->state->mode = DICT1;
+    case DICT1:
+      NEEDBYTE
+      z->state->sub.check.need += (uLong)NEXTBYTE;
+      z->adler = z->state->sub.check.need;
+      z->state->mode = DICT0;
+      return Z_NEED_DICT;
+    case DICT0:
+      z->state->mode = BAD;
+      z->msg = (char*)"need dictionary";
+      z->state->sub.marker = 0;       /* can try inflateSync */
+      return Z_STREAM_ERROR;
+    case BLOCKS:
+      r = ext2_inflate_blocks(z->state->blocks, z, r);
+      if (r == Z_DATA_ERROR)
+      {
+        z->state->mode = BAD;
+        z->state->sub.marker = 0;       /* can try inflateSync */
+        break;
+      }
+#if 0
+      if (r == Z_OK)
+        r = f;
+#endif
+      if (r != Z_STREAM_END)
+        return r;
+#if 0
+      r = f;
+#else
+      r = Z_OK;
+#endif
+      ext2_inflate_blocks_reset(z->state->blocks, z, &z->state->sub.check.was);
+      if (z->state->nowrap)
+      {
+        z->state->mode = DONE;
+        break;
+      }
+      z->state->mode = CHECK4;
+    case CHECK4:
+      NEEDBYTE
+      z->state->sub.check.need = (uLong)NEXTBYTE << 24;
+      z->state->mode = CHECK3;
+    case CHECK3:
+      NEEDBYTE
+      z->state->sub.check.need += (uLong)NEXTBYTE << 16;
+      z->state->mode = CHECK2;
+    case CHECK2:
+      NEEDBYTE
+      z->state->sub.check.need += (uLong)NEXTBYTE << 8;
+      z->state->mode = CHECK1;
+    case CHECK1:
+      NEEDBYTE
+      z->state->sub.check.need += (uLong)NEXTBYTE;
+
+      if (z->state->sub.check.was != z->state->sub.check.need)
+      {
+        z->state->mode = BAD;
+        z->msg = (char*)"incorrect data check";
+        z->state->sub.marker = 5;       /* can't try inflateSync */
+        break;
+      }
+      Tracev((stderr, "inflate: zlib check ok\n"));
+      z->state->mode = DONE;
+    case DONE:
+      return Z_STREAM_END;
+    case BAD:
+      return Z_DATA_ERROR;
+    default:
+      return Z_STREAM_ERROR;
+  }
+#ifdef NEED_DUMMY_RETURN
+  return Z_STREAM_ERROR;  /* Some dumb compilers complain without this */
+#endif
+}
+
+
+int ext2_inflateSetDictionary(z, dictionary, dictLength)
+z_streamp z;
+const Byte *dictionary;
+uInt  dictLength;
+{
+  uInt length = dictLength;
+
+  if (z == Z_NULL || z->state == Z_NULL || z->state->mode != DICT0)
+    return Z_STREAM_ERROR;
+
+  if (ext2_adler32(1L, dictionary, dictLength) != z->adler) return Z_DATA_ERROR;
+  z->adler = 1L;
+
+  if (length >= ((uInt)1<<z->state->wbits))
+  {
+    length = (1<<z->state->wbits)-1;
+    dictionary += dictLength - length;
+  }
+  ext2_inflate_set_dictionary(z->state->blocks, dictionary, length);
+  z->state->mode = BLOCKS;
+  return Z_OK;
+}
+
+
+int ext2_inflateSync(z)
+z_streamp z;
+{
+  uInt n;       /* number of bytes to look at */
+  Byte *p;     /* pointer to bytes */
+  uInt m;       /* number of marker bytes found in a row */
+  uLong r, w;   /* temporaries to save total_in and total_out */
+
+  /* set up */
+  if (z == Z_NULL || z->state == Z_NULL)
+    return Z_STREAM_ERROR;
+  if (z->state->mode != BAD)
+  {
+    z->state->mode = BAD;
+    z->state->sub.marker = 0;
+  }
+  if ((n = z->avail_in) == 0)
+    return Z_BUF_ERROR;
+  p = z->next_in;
+  m = z->state->sub.marker;
+
+  /* search */
+  while (n && m < 4)
+  {
+    static const Byte mark[4] = {0, 0, 0xff, 0xff};
+    if (*p == mark[m])
+      m++;
+    else if (*p)
+      m = 0;
+    else
+      m = 4 - m;
+    p++, n--;
+  }
+
+  /* restore */
+  z->total_in += p - z->next_in;
+  z->next_in = p;
+  z->avail_in = n;
+  z->state->sub.marker = m;
+
+  /* return no joy or set up to restart on a new block */
+  if (m != 4)
+    return Z_DATA_ERROR;
+  r = z->total_in;  w = z->total_out;
+  ext2_inflateReset(z);
+  z->total_in = r;  z->total_out = w;
+  z->state->mode = BLOCKS;
+  return Z_OK;
+}
+
+
+/* Returns true if inflate is currently at the end of a block generated
+ * by Z_SYNC_FLUSH or Z_FULL_FLUSH. This function is used by one PPP
+ * implementation to provide an additional safety check. PPP uses Z_SYNC_FLUSH
+ * but removes the length bytes of the resulting empty stored block. When
+ * decompressing, PPP checks that at the end of input packet, inflate is
+ * waiting for these length bytes.
+ */
+int ext2_inflateSyncPoint(z)
+z_streamp z;
+{
+  if (z == Z_NULL || z->state == Z_NULL || z->state->blocks == Z_NULL)
+    return Z_STREAM_ERROR;
+  return ext2_inflate_blocks_sync_point(z->state->blocks);
+}
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/inftrees.c linux-2.6.18.5/fs/ext2/gzip/inftrees.c
--- linux-2.6.18.5.org/fs/ext2/gzip/inftrees.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/inftrees.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,455 @@
+/* inftrees.c -- generate Huffman trees for efficient decoding
+ * Copyright (C) 1995-1998 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+#include "zutil.h"
+#include "inftrees.h"
+
+#if !defined(BUILDFIXED) && !defined(STDC)
+#  define BUILDFIXED   /* non ANSI compilers may not accept inffixed.h */
+#endif
+
+const char inflate_copyright[] =
+   " inflate 1.1.3 Copyright 1995-1998 Mark Adler ";
+/*
+  If you use the zlib library in a product, an acknowledgment is welcome
+  in the documentation of your product. If for some reason you cannot
+  include such an acknowledgment, I would appreciate that you keep this
+  copyright string in the executable of your product.
+ */
+struct internal_state  {int dummy;}; /* for buggy compilers */
+
+/* simplify the use of the inflate_huft type with some defines */
+#define exop word.what.Exop
+#define bits word.what.Bits
+
+
+local int huft_build OF((
+    uInt *,            /* code lengths in bits */
+    uInt,               /* number of codes */
+    uInt,               /* number of "simple" codes */
+    const uInt *,      /* list of base values for non-simple codes */
+    const uInt *,      /* list of extra bits for non-simple codes */
+    inflate_huft **,/* result: starting table */
+    uInt *,            /* maximum lookup bits (returns actual) */
+    inflate_huft *,     /* space for trees */
+    uInt *,             /* hufts used in space */
+    uInt * ));         /* space for values */
+
+/* Tables for deflate from PKZIP's appnote.txt. */
+local const uInt cplens[31] = { /* Copy lengths for literal codes 257..285 */
+        3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 15, 17, 19, 23, 27, 31,
+        35, 43, 51, 59, 67, 83, 99, 115, 131, 163, 195, 227, 258, 0, 0};
+        /* see note #13 above about 258 */
+local const uInt cplext[31] = { /* Extra bits for literal codes 257..285 */
+        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2,
+        3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 0, 112, 112}; /* 112==invalid */
+local const uInt cpdist[30] = { /* Copy offsets for distance codes 0..29 */
+        1, 2, 3, 4, 5, 7, 9, 13, 17, 25, 33, 49, 65, 97, 129, 193,
+        257, 385, 513, 769, 1025, 1537, 2049, 3073, 4097, 6145,
+        8193, 12289, 16385, 24577};
+local const uInt cpdext[30] = { /* Extra bits for distance codes */
+        0, 0, 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6,
+        7, 7, 8, 8, 9, 9, 10, 10, 11, 11,
+        12, 12, 13, 13};
+
+/*
+   Huffman code decoding is performed using a multi-level table lookup.
+   The fastest way to decode is to simply build a lookup table whose
+   size is determined by the longest code.  However, the time it takes
+   to build this table can also be a factor if the data being decoded
+   is not very long.  The most common codes are necessarily the
+   shortest codes, so those codes dominate the decoding time, and hence
+   the speed.  The idea is you can have a shorter table that decodes the
+   shorter, more probable codes, and then point to subsidiary tables for
+   the longer codes.  The time it costs to decode the longer codes is
+   then traded against the time it takes to make longer tables.
+
+   This results of this trade are in the variables lbits and dbits
+   below.  lbits is the number of bits the first level table for literal/
+   length codes can decode in one step, and dbits is the same thing for
+   the distance codes.  Subsequent tables are also less than or equal to
+   those sizes.  These values may be adjusted either when all of the
+   codes are shorter than that, in which case the longest code length in
+   bits is used, or when the shortest code is *longer* than the requested
+   table size, in which case the length of the shortest code in bits is
+   used.
+
+   There are two different values for the two tables, since they code a
+   different number of possibilities each.  The literal/length table
+   codes 286 possible values, or in a flat code, a little over eight
+   bits.  The distance table codes 30 possible values, or a little less
+   than five bits, flat.  The optimum values for speed end up being
+   about one bit more than those, so lbits is 8+1 and dbits is 5+1.
+   The optimum values may differ though from machine to machine, and
+   possibly even between compilers.  Your mileage may vary.
+ */
+
+
+/* If BMAX needs to be larger than 16, then h and x[] should be uLong. */
+#define BMAX 15         /* maximum bit length of any code */
+
+local int huft_build(b, n, s, d, e, t, m, hp, hn, v)
+uInt *b;               /* code lengths in bits (all assumed <= BMAX) */
+uInt n;                 /* number of codes (assumed <= 288) */
+uInt s;                 /* number of simple-valued codes (0..s-1) */
+const uInt *d;         /* list of base values for non-simple codes */
+const uInt *e;         /* list of extra bits for non-simple codes */
+inflate_huft **t;  /* result: starting table */
+uInt *m;               /* maximum lookup bits, returns actual */
+inflate_huft *hp;       /* space for trees */
+uInt *hn;               /* hufts used in space */
+uInt *v;               /* working area: values in order of bit length */
+/* Given a list of code lengths and a maximum table size, make a set of
+   tables to decode that set of codes.  Return Z_OK on success, Z_BUF_ERROR
+   if the given code set is incomplete (the tables are still built in this
+   case), Z_DATA_ERROR if the input is invalid (an over-subscribed set of
+   lengths), or Z_MEM_ERROR if not enough memory. */
+{
+
+  uInt a;                       /* counter for codes of length k */
+  uInt c[BMAX+1];               /* bit length count table */
+  uInt f;                       /* i repeats in table every f entries */
+  int g;                        /* maximum code length */
+  int h;                        /* table level */
+  register uInt i;              /* counter, current code */
+  register uInt j;              /* counter */
+  register int k;               /* number of bits in current code */
+  int l;                        /* bits per table (returned in m) */
+  uInt mask;                    /* (1 << w) - 1, to avoid cc -O bug on HP */
+  register uInt *p;            /* pointer into c[], b[], or v[] */
+  inflate_huft *q;              /* points to current table */
+  struct inflate_huft_s r;      /* table entry for structure assignment */
+  inflate_huft *u[BMAX];        /* table stack */
+  register int w;               /* bits before this table == (l * h) */
+  uInt x[BMAX+1];               /* bit offsets, then code stack */
+  uInt *xp;                    /* pointer into x */
+  int y;                        /* number of dummy codes added */
+  uInt z;                       /* number of entries in current table */
+
+
+  /* Generate counts for each bit length */
+  p = c;
+#define C0 *p++ = 0;
+#define C2 C0 C0 C0 C0
+#define C4 C2 C2 C2 C2
+  C4                            /* clear c[]--assume BMAX+1 is 16 */
+  p = b;  i = n;
+  do {
+    c[*p++]++;                  /* assume all entries <= BMAX */
+  } while (--i);
+  if (c[0] == n)                /* null input--all zero length codes */
+  {
+    *t = (inflate_huft *)Z_NULL;
+    *m = 0;
+    return Z_OK;
+  }
+
+
+  /* Find minimum and maximum length, bound *m by those */
+  l = *m;
+  for (j = 1; j <= BMAX; j++)
+    if (c[j])
+      break;
+  k = j;                        /* minimum code length */
+  if ((uInt)l < j)
+    l = j;
+  for (i = BMAX; i; i--)
+    if (c[i])
+      break;
+  g = i;                        /* maximum code length */
+  if ((uInt)l > i)
+    l = i;
+  *m = l;
+
+
+  /* Adjust last length count to fill out codes, if needed */
+  for (y = 1 << j; j < i; j++, y <<= 1)
+    if ((y -= c[j]) < 0)
+      return Z_DATA_ERROR;
+  if ((y -= c[i]) < 0)
+    return Z_DATA_ERROR;
+  c[i] += y;
+
+
+  /* Generate starting offsets into the value table for each length */
+  x[1] = j = 0;
+  p = c + 1;  xp = x + 2;
+  while (--i) {                 /* note that i == g from above */
+    *xp++ = (j += *p++);
+  }
+
+
+  /* Make a table of values in order of bit lengths */
+  p = b;  i = 0;
+  do {
+    if ((j = *p++) != 0)
+      v[x[j]++] = i;
+  } while (++i < n);
+  n = x[g];                     /* set n to length of v */
+
+
+  /* Generate the Huffman codes and for each, make the table entries */
+  x[0] = i = 0;                 /* first Huffman code is zero */
+  p = v;                        /* grab values in bit order */
+  h = -1;                       /* no tables yet--level -1 */
+  w = -l;                       /* bits decoded == (l * h) */
+  u[0] = (inflate_huft *)Z_NULL;        /* just to keep compilers happy */
+  q = (inflate_huft *)Z_NULL;   /* ditto */
+  z = 0;                        /* ditto */
+
+  /* go through the bit lengths (k already is bits in shortest code) */
+  for (; k <= g; k++)
+  {
+    a = c[k];
+    while (a--)
+    {
+      /* here i is the Huffman code of length k bits for value *p */
+      /* make tables up to required level */
+      while (k > w + l)
+      {
+        h++;
+        w += l;                 /* previous table always l bits */
+
+        /* compute minimum size table less than or equal to l bits */
+        z = g - w;
+        z = z > (uInt)l ? l : z;        /* table size upper limit */
+        if ((f = 1 << (j = k - w)) > a + 1)     /* try a k-w bit table */
+        {                       /* too few codes for k-w bit table */
+          f -= a + 1;           /* deduct codes from patterns left */
+          xp = c + k;
+          if (j < z)
+            while (++j < z)     /* try smaller tables up to z bits */
+            {
+              if ((f <<= 1) <= *++xp)
+                break;          /* enough codes to use up j bits */
+              f -= *xp;         /* else deduct codes from patterns */
+            }
+        }
+        z = 1 << j;             /* table entries for j-bit table */
+
+        /* allocate new table */
+        if (*hn + z > MANY)     /* (note: doesn't matter for fixed) */
+          return Z_MEM_ERROR;   /* not enough memory */
+        u[h] = q = hp + *hn;
+        *hn += z;
+
+        /* connect to last table, if there is one */
+        if (h)
+        {
+          x[h] = i;             /* save pattern for backing up */
+          r.bits = (Byte)l;     /* bits to dump before this table */
+          r.exop = (Byte)j;     /* bits in this table */
+          j = i >> (w - l);
+          r.base = (uInt)(q - u[h-1] - j);   /* offset to this table */
+          u[h-1][j] = r;        /* connect to last table */
+        }
+        else
+          *t = q;               /* first table is returned result */
+      }
+
+      /* set up table entry in r */
+      r.bits = (Byte)(k - w);
+      if (p >= v + n)
+        r.exop = 128 + 64;      /* out of values--invalid code */
+      else if (*p < s)
+      {
+        r.exop = (Byte)(*p < 256 ? 0 : 32 + 64);     /* 256 is end-of-block */
+        r.base = *p++;          /* simple code is just the value */
+      }
+      else
+      {
+        r.exop = (Byte)(e[*p - s] + 16 + 64);/* non-simple--look up in lists */
+        r.base = d[*p++ - s];
+      }
+
+      /* fill code-like entries with r */
+      f = 1 << (k - w);
+      for (j = i >> w; j < z; j += f)
+        q[j] = r;
+
+      /* backwards increment the k-bit code i */
+      for (j = 1 << (k - 1); i & j; j >>= 1)
+        i ^= j;
+      i ^= j;
+
+      /* backup over finished tables */
+      mask = (1 << w) - 1;      /* needed on HP, cc -O bug */
+      while ((i & mask) != x[h])
+      {
+        h--;                    /* don't need to update q */
+        w -= l;
+        mask = (1 << w) - 1;
+      }
+    }
+  }
+
+
+  /* Return Z_BUF_ERROR if we were given an incomplete table */
+  return y != 0 && g != 1 ? Z_BUF_ERROR : Z_OK;
+}
+
+
+int ext2_inflate_trees_bits(c, bb, tb, hp, z)
+uInt *c;               /* 19 code lengths */
+uInt *bb;              /* bits tree desired/actual depth */
+inflate_huft **tb; /* bits tree result */
+inflate_huft *hp;       /* space for trees */
+z_streamp z;            /* for messages */
+{
+  int r;
+  uInt hn = 0;          /* hufts used in space */
+  uInt *v;             /* work area for huft_build */
+
+  if ((v = (uInt*)ZALLOC(z, 19, sizeof(uInt))) == Z_NULL)
+    return Z_MEM_ERROR;
+  r = huft_build(c, 19, 19, (uInt*)Z_NULL, (uInt*)Z_NULL,
+                 tb, bb, hp, &hn, v);
+  if (r == Z_DATA_ERROR)
+    z->msg = (char*)"oversubscribed dynamic bit lengths tree";
+  else if (r == Z_BUF_ERROR || *bb == 0)
+  {
+    z->msg = (char*)"incomplete dynamic bit lengths tree";
+    r = Z_DATA_ERROR;
+  }
+  ZFREE(z, v);
+  return r;
+}
+
+
+int ext2_inflate_trees_dynamic(nl, nd, c, bl, bd, tl, td, hp, z)
+uInt nl;                /* number of literal/length codes */
+uInt nd;                /* number of distance codes */
+uInt *c;               /* that many (total) code lengths */
+uInt *bl;              /* literal desired/actual bit depth */
+uInt *bd;              /* distance desired/actual bit depth */
+inflate_huft **tl; /* literal/length tree result */
+inflate_huft **td; /* distance tree result */
+inflate_huft *hp;       /* space for trees */
+z_streamp z;            /* for messages */
+{
+  int r;
+  uInt hn = 0;          /* hufts used in space */
+  uInt *v;             /* work area for huft_build */
+
+  /* allocate work area */
+  if ((v = (uInt*)ZALLOC(z, 288, sizeof(uInt))) == Z_NULL)
+    return Z_MEM_ERROR;
+
+  /* build literal/length tree */
+  r = huft_build(c, nl, 257, cplens, cplext, tl, bl, hp, &hn, v);
+  if (r != Z_OK || *bl == 0)
+  {
+    if (r == Z_DATA_ERROR)
+      z->msg = (char*)"oversubscribed literal/length tree";
+    else if (r != Z_MEM_ERROR)
+    {
+      z->msg = (char*)"incomplete literal/length tree";
+      r = Z_DATA_ERROR;
+    }
+    ZFREE(z, v);
+    return r;
+  }
+
+  /* build distance tree */
+  r = huft_build(c + nl, nd, 0, cpdist, cpdext, td, bd, hp, &hn, v);
+  if (r != Z_OK || (*bd == 0 && nl > 257))
+  {
+    if (r == Z_DATA_ERROR)
+      z->msg = (char*)"oversubscribed distance tree";
+    else if (r == Z_BUF_ERROR) {
+#ifdef PKZIP_BUG_WORKAROUND
+      r = Z_OK;
+    }
+#else
+      z->msg = (char*)"incomplete distance tree";
+      r = Z_DATA_ERROR;
+    }
+    else if (r != Z_MEM_ERROR)
+    {
+      z->msg = (char*)"empty distance tree with lengths";
+      r = Z_DATA_ERROR;
+    }
+    ZFREE(z, v);
+    return r;
+#endif
+  }
+
+  /* done */
+  ZFREE(z, v);
+  return Z_OK;
+}
+
+
+/* build fixed tables only once--keep them here */
+#ifdef BUILDFIXED
+local int fixed_built = 0;
+#define FIXEDH 544      /* number of hufts used by fixed tables */
+local inflate_huft fixed_mem[FIXEDH];
+local uInt fixed_bl;
+local uInt fixed_bd;
+local inflate_huft *fixed_tl;
+local inflate_huft *fixed_td;
+#else
+#include "inffixed.h"
+#endif
+
+
+int ext2_inflate_trees_fixed(bl, bd, tl, td, z)
+uInt *bl;               /* literal desired/actual bit depth */
+uInt *bd;               /* distance desired/actual bit depth */
+inflate_huft **tl;  /* literal/length tree result */
+inflate_huft **td;  /* distance tree result */
+z_streamp z;             /* for memory allocation */
+{
+#ifdef BUILDFIXED
+  /* build fixed tables if not already */
+  if (!fixed_built)
+  {
+    int k;              /* temporary variable */
+    uInt f = 0;         /* number of hufts used in fixed_mem */
+    uInt *c;           /* length list for huft_build */
+    uInt *v;           /* work area for huft_build */
+
+    /* allocate memory */
+    if ((c = (uInt*)ZALLOC(z, 288, sizeof(uInt))) == Z_NULL)
+      return Z_MEM_ERROR;
+    if ((v = (uInt*)ZALLOC(z, 288, sizeof(uInt))) == Z_NULL)
+    {
+      ZFREE(z, c);
+      return Z_MEM_ERROR;
+    }
+
+    /* literal table */
+    for (k = 0; k < 144; k++)
+      c[k] = 8;
+    for (; k < 256; k++)
+      c[k] = 9;
+    for (; k < 280; k++)
+      c[k] = 7;
+    for (; k < 288; k++)
+      c[k] = 8;
+    fixed_bl = 9;
+    huft_build(c, 288, 257, cplens, cplext, &fixed_tl, &fixed_bl,
+               fixed_mem, &f, v);
+
+    /* distance table */
+    for (k = 0; k < 30; k++)
+      c[k] = 5;
+    fixed_bd = 5;
+    huft_build(c, 30, 0, cpdist, cpdext, &fixed_td, &fixed_bd,
+               fixed_mem, &f, v);
+
+    /* done */
+    ZFREE(z, v);
+    ZFREE(z, c);
+    fixed_built = 1;
+  }
+#endif
+  *bl = fixed_bl;
+  *bd = fixed_bd;
+  *tl = fixed_tl;
+  *td = fixed_td;
+  return Z_OK;
+}
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/inftrees.h linux-2.6.18.5/fs/ext2/gzip/inftrees.h
--- linux-2.6.18.5.org/fs/ext2/gzip/inftrees.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/inftrees.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,58 @@
+/* inftrees.h -- header to use inftrees.c
+ * Copyright (C) 1995-1998 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+/* Huffman code lookup table entry--this entry is four bytes for machines
+   that have 16-bit pointers (e.g. PC's in the small or medium model). */
+
+typedef struct inflate_huft_s inflate_huft;
+
+struct inflate_huft_s {
+  union {
+    struct {
+      Byte Exop;        /* number of extra bits or operation */
+      Byte Bits;        /* number of bits in this code or subcode */
+    } what;
+    uInt pad;           /* pad structure to a power of 2 (4 bytes for */
+  } word;               /*  16-bit, 8 bytes for 32-bit int's) */
+  uInt base;            /* literal, length base, distance base,
+                           or table offset */
+};
+
+/* Maximum size of dynamic tree.  The maximum found in a long but non-
+   exhaustive search was 1004 huft structures (850 for length/literals
+   and 154 for distances, the latter actually the result of an
+   exhaustive search).  The actual maximum is not known, but the
+   value below is more than safe. */
+#define MANY 1440
+
+extern int ext2_inflate_trees_bits OF((
+    uInt *,                    /* 19 code lengths */
+    uInt *,                    /* bits tree desired/actual depth */
+    inflate_huft **,       /* bits tree result */
+    inflate_huft *,             /* space for trees */
+    z_streamp));                /* for messages */
+
+extern int ext2_inflate_trees_dynamic OF((
+    uInt,                       /* number of literal/length codes */
+    uInt,                       /* number of distance codes */
+    uInt *,                    /* that many (total) code lengths */
+    uInt *,                    /* literal desired/actual bit depth */
+    uInt *,                    /* distance desired/actual bit depth */
+    inflate_huft **,       /* literal/length tree result */
+    inflate_huft **,       /* distance tree result */
+    inflate_huft *,             /* space for trees */
+    z_streamp));                /* for messages */
+
+extern int ext2_inflate_trees_fixed OF((
+    uInt *,                    /* literal desired/actual bit depth */
+    uInt *,                    /* distance desired/actual bit depth */
+    inflate_huft **,       /* literal/length tree result */
+    inflate_huft **,       /* distance tree result */
+    z_streamp));                /* for memory allocation */
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/infutil.c linux-2.6.18.5/fs/ext2/gzip/infutil.c
--- linux-2.6.18.5.org/fs/ext2/gzip/infutil.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/infutil.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,89 @@
+/* inflate_util.c -- data and routines common to blocks and codes
+ * Copyright (C) 1995-1998 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+#include <linux/string.h>
+
+#include "zutil.h"
+#include "infblock.h"
+#include "inftrees.h"
+#include "infcodes.h"
+#include "infutil.h"
+
+struct inflate_codes_state {int dummy;}; /* for buggy compilers */
+
+/* And'ing with mask[n] masks the lower n bits */
+uInt ext2_inflate_mask[17] = {
+    0x0000,
+    0x0001, 0x0003, 0x0007, 0x000f, 0x001f, 0x003f, 0x007f, 0x00ff,
+    0x01ff, 0x03ff, 0x07ff, 0x0fff, 0x1fff, 0x3fff, 0x7fff, 0xffff
+};
+
+
+/* copy as much as possible from the sliding window to the output area */
+int ext2_inflate_flush(s, z, r)
+struct inflate_blocks_state *s;
+z_streamp z;
+int r;
+{
+  uInt n;
+  Byte *p;
+  Byte *q;
+
+  /* local copies of source and destination pointers */
+  p = z->next_out;
+  q = s->read;
+
+  /* compute number of bytes to copy as far as end of window */
+  n = (uInt)((q <= s->write ? s->write : s->end) - q);
+  if (n > z->avail_out) n = z->avail_out;
+  if (n && r == Z_BUF_ERROR) r = Z_OK;
+
+  /* update counters */
+  z->avail_out -= n;
+  z->total_out += n;
+
+  /* update check information */
+  if (s->checkfn != Z_NULL)
+    z->adler = s->check = (*s->checkfn)(s->check, q, n);
+
+  /* copy as far as end of window */
+  zmemcpy(p, q, n);
+  p += n;
+  q += n;
+
+  /* see if more to copy at beginning of window */
+  if (q == s->end)
+  {
+    /* wrap pointers */
+    q = s->window;
+    if (s->write == s->end)
+      s->write = s->window;
+
+    /* compute bytes to copy */
+    n = (uInt)(s->write - q);
+    if (n > z->avail_out) n = z->avail_out;
+    if (n && r == Z_BUF_ERROR) r = Z_OK;
+
+    /* update counters */
+    z->avail_out -= n;
+    z->total_out += n;
+
+    /* update check information */
+    if (s->checkfn != Z_NULL)
+      z->adler = s->check = (*s->checkfn)(s->check, q, n);
+
+    /* copy */
+    zmemcpy(p, q, n);
+    p += n;
+    q += n;
+  }
+
+  /* update pointers */
+  z->next_out = p;
+  s->read = q;
+
+  /* done */
+  return r;
+}
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/infutil.h linux-2.6.18.5/fs/ext2/gzip/infutil.h
--- linux-2.6.18.5.org/fs/ext2/gzip/infutil.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/infutil.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,98 @@
+/* infutil.h -- types and macros common to blocks and codes
+ * Copyright (C) 1995-1998 Mark Adler
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+#ifndef _INFUTIL_H
+#define _INFUTIL_H
+
+typedef enum {
+      TYPE,     /* get type bits (3, including end bit) */
+      LENS,     /* get lengths for stored */
+      STORED,   /* processing stored block */
+      TABLE,    /* get table lengths */
+      BTREE,    /* get bit lengths tree for a dynamic block */
+      DTREE,    /* get length, distance trees for a dynamic block */
+      CODES,    /* processing fixed or dynamic block */
+      DRY,      /* output remaining window bytes */
+      DONE,     /* finished last block, done */
+      BAD}      /* got a data error--stuck here */
+inflate_block_mode;
+
+/* inflate blocks semi-private state */
+struct inflate_blocks_state {
+
+  /* mode */
+  inflate_block_mode  mode;     /* current inflate_block mode */
+
+  /* mode dependent information */
+  union {
+    uInt left;          /* if STORED, bytes left to copy */
+    struct {
+      uInt table;               /* table lengths (14 bits) */
+      uInt index;               /* index into blens (or border) */
+      uInt *blens;             /* bit lengths of codes */
+      uInt bb;                  /* bit length tree depth */
+      inflate_huft *tb;         /* bit length decoding tree */
+    } trees;            /* if DTREE, decoding info for trees */
+    struct {
+      struct inflate_codes_state 
+         *codes;
+    } decode;           /* if CODES, current state */
+  } sub;                /* submode */
+  uInt last;            /* true if this block is the last block */
+
+  /* mode independent information */
+  uInt bitk;            /* bits in bit buffer */
+  uLong bitb;           /* bit buffer */
+  inflate_huft *hufts;  /* single malloc for tree space */
+  Byte *window;        /* sliding window */
+  Byte *end;           /* one byte after sliding window */
+  Byte *read;          /* window read pointer */
+  Byte *write;         /* window write pointer */
+  check_func checkfn;   /* check function */
+  uLong check;          /* check on output */
+
+};
+
+
+/* defines for inflate input/output */
+/*   update pointers and return */
+#define UPDBITS {s->bitb=b;s->bitk=k;}
+#define UPDIN {z->avail_in=n;z->total_in+=p-z->next_in;z->next_in=p;}
+#define UPDOUT {s->write=q;}
+#define UPDATE {UPDBITS UPDIN UPDOUT}
+#define LEAVE {UPDATE return ext2_inflate_flush(s,z,r);}
+/*   get bytes and bits */
+#define LOADIN {p=z->next_in;n=z->avail_in;b=s->bitb;k=s->bitk;}
+#define NEEDBYTE {if(n)r=Z_OK;else LEAVE}
+#define NEXTBYTE (n--,*p++)
+#define NEEDBITS(j) {while(k<(j)){NEEDBYTE;b|=((uLong)NEXTBYTE)<<k;k+=8;}}
+#define DUMPBITS(j) {b>>=(j);k-=(j);}
+/*   output bytes */
+#define WAVAIL (uInt)(q<s->read?s->read-q-1:s->end-q)
+#define LOADOUT {q=s->write;m=(uInt)WAVAIL;}
+#define WRAP {if(q==s->end&&s->read!=s->window){q=s->window;m=(uInt)WAVAIL;}}
+#define FLUSH {UPDOUT r=ext2_inflate_flush(s,z,r); LOADOUT}
+#define NEEDOUT {if(m==0){WRAP if(m==0){FLUSH WRAP if(m==0) LEAVE}}r=Z_OK;}
+#define OUTBYTE(a) {*q++=(Byte)(a);m--;}
+/*   load local pointers */
+#define LOAD {LOADIN LOADOUT}
+
+/* masks for lower bits (size given to avoid silly warnings with Visual C++) */
+extern uInt ext2_inflate_mask[17];
+
+/* copy as much as possible from the sliding window to the output area */
+extern int ext2_inflate_flush OF((
+    struct inflate_blocks_state *,
+    z_streamp ,
+    int));
+
+struct internal_state      {int dummy;}; /* for buggy compilers */
+
+#endif
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/match586.S linux-2.6.18.5/fs/ext2/gzip/match586.S
--- linux-2.6.18.5.org/fs/ext2/gzip/match586.S	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/match586.S	2007-02-23 13:19:49.000000000 -0800
@@ -0,0 +1,383 @@
+/* match.s -- Pentium-optimized version of longest_match()
+ * Written for zlib 1.1.2
+ * Copyright (C) 1998 Brian Raiter <breadbox@muppetlabs.com>
+ *
+ * This is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License.
+ */
+/*
+#ifndef NO_UNDERLINE
+#define	match_init	_match_init
+#define	longest_match	_longest_match
+#endif
+*/
+
+#define	MAX_MATCH	(258)
+#define	MIN_MATCH	(3)
+#define	MIN_LOOKAHEAD	(MAX_MATCH + MIN_MATCH + 1)
+#define	MAX_MATCH_8	((MAX_MATCH + 7) & ~7)
+
+/* stack frame offsets */
+
+#define	wmask			0	/* local copy of s->wmask	*/
+#define	window			4	/* local copy of s->window	*/
+#define	windowbestlen		8	/* s->window + bestlen		*/
+#define	chainlenscanend		12	/* high word: current chain len	*/
+					/* low word: last bytes sought	*/
+#define	scanstart		16	/* first two bytes of string	*/
+#define	scanalign		20	/* dword-misalignment of string	*/
+#define	nicematch		24	/* a good enough match size	*/
+#define	bestlen			28	/* size of best match so far	*/
+#define	scan			32	/* ptr to string wanting match	*/
+
+#define	LocalVarsSize		(36)
+
+#ifdef CONFIG_REGPARM
+
+/* Use registers rather than stack for param passing */
+#define	deflatestate		36	/* the function arguments	*/
+#define	curmatch		40
+/*	saved ebx		44 */
+/*	saved edi		48 */
+/*	saved esi		52 */
+/*	saved ebp		56 */
+/*	return address		60 */
+
+#else /* !CONFIG_REGPARM */
+
+/* Use stack for param passing */
+/*	saved ebx		36 */
+/*	saved edi		40 */
+/*	saved esi		44 */
+/*	saved ebp		48 */
+/*	return address		52 */
+#define	deflatestate		56	/* the function arguments	*/
+#define	curmatch		60
+
+#endif
+
+/* Offsets for fields in the deflate_state structure. These numbers
+ * are calculated from the definition of deflate_state, with the
+ * assumption that the compiler will dword-align the fields. (Thus,
+ * changing the definition of deflate_state could easily cause this
+ * program to crash horribly, without so much as a warning at
+ * compile time. Sigh.)
+ */
+#define	dsWSize			36
+#define	dsWMask			44
+#define	dsWindow		48
+#define	dsPrev			56
+#define	dsMatchLen		88
+#define	dsPrevMatch		92
+#define	dsStrStart		100
+#define	dsMatchStart		104
+#define	dsLookahead		108
+#define	dsPrevLen		112
+#define	dsMaxChainLen		116
+#define	dsGoodMatch		132
+#define	dsNiceMatch		136
+
+
+.file "match586.S"
+
+.globl	match_init, longest_match
+
+.text
+
+/* uInt longest_match(deflate_state *deflatestate, IPos curmatch) */
+
+longest_match:
+
+/* Save registers that the compiler may be using, and adjust %esp to	*/
+/* make room for our stack frame.					*/
+
+		pushl	%ebp
+		pushl	%edi
+		pushl	%esi
+		pushl	%ebx
+
+#ifdef CONFIG_REGPARM
+		pushl	%edx	// curmatch
+		pushl	%esi	// deflatestate
+#endif
+
+		subl	$LocalVarsSize, %esp
+
+/* Retrieve the function arguments. %ecx will hold cur_match		*/
+/* throughout the entire function. %edx will hold the pointer to the	*/
+/* deflate_state structure during the function's setup (before		*/
+/* entering the main loop).						*/
+
+		movl	deflatestate(%esp), %edx
+		movl	curmatch(%esp), %ecx
+
+/* if ((uInt)nice_match > s->lookahead) nice_match = s->lookahead;	*/
+
+		movl	dsNiceMatch(%edx), %eax
+		movl	dsLookahead(%edx), %ebx
+		cmpl	%eax, %ebx
+		jl	LookaheadLess
+		movl	%eax, %ebx
+LookaheadLess:	movl	%ebx, nicematch(%esp)
+
+/* register Bytef *scan = s->window + s->strstart;			*/
+
+		movl	dsWindow(%edx), %esi
+		movl	%esi, window(%esp)
+		movl	dsStrStart(%edx), %ebp
+		lea	(%esi,%ebp), %edi
+		movl	%edi, scan(%esp)
+
+/* Determine how many bytes the scan ptr is off from being		*/
+/* dword-aligned.							*/
+
+		movl	%edi, %eax
+		negl	%eax
+		andl	$3, %eax
+		movl	%eax, scanalign(%esp)
+
+/* IPos limit = s->strstart > (IPos)MAX_DIST(s) ?			*/
+/*     s->strstart - (IPos)MAX_DIST(s) : NIL;				*/
+
+		movl	dsWSize(%edx), %eax
+		subl	$MIN_LOOKAHEAD, %eax
+		subl	%eax, %ebp
+		jg	LimitPositive
+		xorl	%ebp, %ebp
+LimitPositive:
+
+/* unsigned chain_length = s->max_chain_length;				*/
+/* if (s->prev_length >= s->good_match) {				*/
+/*     chain_length >>= 2;						*/
+/* }									*/
+
+		movl	dsPrevLen(%edx), %eax
+		movl	dsGoodMatch(%edx), %ebx
+		cmpl	%ebx, %eax
+		movl	dsMaxChainLen(%edx), %ebx
+		jl	LastMatchGood
+		shrl	$2, %ebx
+LastMatchGood:
+
+/* chainlen is decremented once beforehand so that the function can	*/
+/* use the sign flag instead of the zero flag for the exit test.	*/
+/* It is then shifted into the high word, to make room for the scanend	*/
+/* scanend value, which it will always accompany.			*/
+
+		decl	%ebx
+		shll	$16, %ebx
+
+/* int best_len = s->prev_length;					*/
+
+		movl	dsPrevLen(%edx), %eax
+		movl	%eax, bestlen(%esp)
+
+/* Store the sum of s->window + best_len in %esi locally, and in %esi.	*/
+
+		addl	%eax, %esi
+		movl	%esi, windowbestlen(%esp)
+
+/* register ush scan_start = *(ushf*)scan;				*/
+/* register ush scan_end   = *(ushf*)(scan+best_len-1);			*/
+
+		movw	(%edi), %bx
+		movw	%bx, scanstart(%esp)
+		movw	-1(%edi,%eax), %bx
+		movl	%ebx, chainlenscanend(%esp)
+
+/* Posf *prev = s->prev;						*/
+/* uInt wmask = s->w_mask;						*/
+
+		movl	dsPrev(%edx), %edi
+		movl	dsWMask(%edx), %edx
+		mov	%edx, wmask(%esp)
+
+/* Jump into the main loop.						*/
+
+		jmp	LoopEntry
+
+.balign 16
+
+/* do {
+ *     match = s->window + cur_match;
+ *     if (*(ushf*)(match+best_len-1) != scan_end ||
+ *         *(ushf*)match != scan_start) continue;
+ *     [...]
+ * } while ((cur_match = prev[cur_match & wmask]) > limit
+ *          && --chain_length != 0);
+ *
+ * Here is the inner loop of the function. The function will spend the
+ * majority of its time in this loop, and majority of that time will
+ * be spent in the first ten instructions.
+ *
+ * Within this loop:
+ * %ebx = chainlenscanend - i.e., ((chainlen << 16) | scanend)
+ * %ecx = curmatch
+ * %edx = curmatch & wmask
+ * %esi = windowbestlen - i.e., (window + bestlen)
+ * %edi = prev
+ * %ebp = limit
+ *
+ * Two optimization notes on the choice of instructions:
+ *
+ * The first instruction uses a 16-bit address, which costs an extra,
+ * unpairable cycle. This is cheaper than doing a 32-bit access and
+ * zeroing the high word, due to the 3-cycle misalignment penalty which
+ * would occur half the time. This also turns out to be cheaper than
+ * doing two separate 8-bit accesses, as the memory is so rarely in the
+ * L1 cache.
+ *
+ * The window buffer, however, apparently spends a lot of time in the
+ * cache, and so it is faster to retrieve the word at the end of the
+ * match string with two 8-bit loads. The instructions that test the
+ * word at the beginning of the match string, however, are executed
+ * much less frequently, and there it was cheaper to use 16-bit
+ * instructions, which avoided the necessity of saving off and
+ * subsequently reloading one of the other registers.
+ */
+LookupLoop:
+							/* 1 U & V  */
+		movw	(%edi,%edx,2), %cx		/* 2 U pipe */
+		movl	wmask(%esp), %edx		/* 2 V pipe */
+		cmpl	%ebp, %ecx			/* 3 U pipe */
+		jbe	LeaveNow			/* 3 V pipe */
+		subl	$0x00010000, %ebx		/* 4 U pipe */
+		js	LeaveNow			/* 4 V pipe */
+LoopEntry:	movb	-1(%esi,%ecx), %al		/* 5 U pipe */
+		andl	%ecx, %edx			/* 5 V pipe */
+		cmpb	%bl, %al			/* 6 U pipe */
+		jnz	LookupLoop			/* 6 V pipe */
+		movb	(%esi,%ecx), %ah
+		cmpb	%bh, %ah
+		jnz	LookupLoop
+		movl	window(%esp), %eax
+		movw	(%eax,%ecx), %ax
+		cmpw	scanstart(%esp), %ax
+		jnz	LookupLoop
+
+/* Store the current value of chainlen.					*/
+
+		movl	%ebx, chainlenscanend(%esp)
+
+/* Point %edi to the string under scrutiny, and %esi to the string we	*/
+/* are hoping to match it up with. In actuality, %esi and %edi are	*/
+/* both pointed (MAX_MATCH_8 - scanalign) bytes ahead, and %edx is	*/
+/* initialized to -(MAX_MATCH_8 - scanalign).				*/
+
+		movl	window(%esp), %esi
+		movl	scan(%esp), %edi
+		addl	%ecx, %esi
+		movl	scanalign(%esp), %eax
+		movl	$(-MAX_MATCH_8), %edx
+		lea	MAX_MATCH_8(%edi,%eax), %edi
+		lea	MAX_MATCH_8(%esi,%eax), %esi
+
+/* Test the strings for equality, 8 bytes at a time. At the end,
+ * adjust %edx so that it is offset to the exact byte that mismatched.
+ *
+ * We already know at this point that the first three bytes of the
+ * strings match each other, and they can be safely passed over before
+ * starting the compare loop. So what this code does is skip over 0-3
+ * bytes, as much as necessary in order to dword-align the %edi
+ * pointer. (%esi will still be misaligned three times out of four.)
+ *
+ * It should be confessed that this loop usually does not represent
+ * much of the total running time. Replacing it with a more
+ * straightforward "rep cmpsb" would not drastically degrade
+ * performance.
+ */
+LoopCmps:
+		movl	(%esi,%edx), %eax
+		movl	(%edi,%edx), %ebx
+		xorl	%ebx, %eax
+		jnz	LeaveLoopCmps
+		movl	4(%esi,%edx), %eax
+		movl	4(%edi,%edx), %ebx
+		xorl	%ebx, %eax
+		jnz	LeaveLoopCmps4
+		addl	$8, %edx
+		jnz	LoopCmps
+		jmp	LenMaximum
+LeaveLoopCmps4:	addl	$4, %edx
+LeaveLoopCmps:	testl	$0x0000FFFF, %eax
+		jnz	LenLower
+		addl	$2, %edx
+		shrl	$16, %eax
+LenLower:	subb	$1, %al
+		adcl	$0, %edx
+
+/* Calculate the length of the match. If it is longer than MAX_MATCH,	*/
+/* then automatically accept it as the best possible match and leave.	*/
+
+		lea	(%edi,%edx), %eax
+		movl	scan(%esp), %edi
+		subl	%edi, %eax
+		cmpl	$MAX_MATCH, %eax
+		jge	LenMaximum
+
+/* If the length of the match is not longer than the best match we	*/
+/* have so far, then forget it and return to the lookup loop.		*/
+
+		movl	deflatestate(%esp), %edx
+		movl	bestlen(%esp), %ebx
+		cmpl	%ebx, %eax
+		jg	LongerMatch
+		movl	chainlenscanend(%esp), %ebx
+		movl	windowbestlen(%esp), %esi
+		movl	dsPrev(%edx), %edi
+		movl	wmask(%esp), %edx
+		andl	%ecx, %edx
+		jmp	LookupLoop
+
+/*         s->match_start = cur_match;					*/
+/*         best_len = len;						*/
+/*         if (len >= nice_match) break;				*/
+/*         scan_end = *(ushf*)(scan+best_len-1);			*/
+
+LongerMatch:	movl	nicematch(%esp), %ebx
+		movl	%eax, bestlen(%esp)
+		movl	%ecx, dsMatchStart(%edx)
+		cmpl	%ebx, %eax
+		jge	LeaveNow
+		movl	window(%esp), %esi
+		addl	%eax, %esi
+		movl	%esi, windowbestlen(%esp)
+		movl	chainlenscanend(%esp), %ebx
+		movw	-1(%edi,%eax), %bx
+		movl	dsPrev(%edx), %edi
+		movl	%ebx, chainlenscanend(%esp)
+		movl	wmask(%esp), %edx
+		andl	%ecx, %edx
+		jmp	LookupLoop
+
+/* Accept the current string, with the maximum possible length.		*/
+
+LenMaximum:	movl	deflatestate(%esp), %edx
+		movl	$MAX_MATCH, bestlen(%esp)
+		movl	%ecx, dsMatchStart(%edx)
+
+/* if ((uInt)best_len <= s->lookahead) return (uInt)best_len;		*/
+/* return s->lookahead;							*/
+
+LeaveNow:
+		movl	deflatestate(%esp), %edx
+		movl	bestlen(%esp), %ebx
+		movl	dsLookahead(%edx), %eax
+		cmpl	%eax, %ebx
+		jg	LookaheadRet
+		movl	%ebx, %eax
+LookaheadRet:
+
+/* Restore the stack and return from whence we came.			*/
+
+#ifndef CONFIG_REGPARM
+ 		addl	$LocalVarsSize, %esp
+#else
+		addl	$LocalVarsSize+8, %esp
+#endif
+
+		popl	%ebx
+		popl	%esi
+		popl	%edi
+		popl	%ebp
+match_init:	ret
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/match686.S linux-2.6.18.5/fs/ext2/gzip/match686.S
--- linux-2.6.18.5.org/fs/ext2/gzip/match686.S	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/match686.S	2007-02-23 13:24:45.000000000 -0800
@@ -0,0 +1,355 @@
+/* match.s -- Pentium-Pro-optimized version of longest_match()
+ * Written for zlib 1.1.2
+ * Copyright (C) 1998 Brian Raiter <breadbox@muppetlabs.com>
+ *
+ * This is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License.
+ */
+/*
+#ifndef NO_UNDERLINE
+#define	match_init	_match_init
+#define	longest_match	_longest_match
+#endif
+*/
+#define	MAX_MATCH	(258)
+#define	MIN_MATCH	(3)
+#define	MIN_LOOKAHEAD	(MAX_MATCH + MIN_MATCH + 1)
+#define	MAX_MATCH_8	((MAX_MATCH + 7) & ~7)
+
+/* stack frame offsets */
+
+#define	chainlenwmask		0	/* high word: current chain len	*/
+					/* low word: s->wmask		*/
+#define	window			4	/* local copy of s->window	*/
+#define	windowbestlen		8	/* s->window + bestlen		*/
+#define	scanstart		16	/* first two bytes of string	*/
+#define	scanend			12	/* last two bytes of string	*/
+#define	scanalign		20	/* dword-misalignment of string	*/
+#define	nicematch		24	/* a good enough match size	*/
+#define	bestlen			28	/* size of best match so far	*/
+#define	scan			32	/* ptr to string wanting match	*/
+
+#define	LocalVarsSize		(36)
+
+#ifdef CONFIG_REGPARM
+
+/* Use registers rather than stack for param passing */
+#define	deflatestate		36	/* the function arguments	*/
+#define	curmatch		40
+/*	saved ebx		44 */
+/*	saved edi		48 */
+/*	saved esi		52 */
+/*	saved ebp		56 */
+/*	return address		60 */
+
+#else /* !CONFIG_REGPARM */
+
+/* Use stack for param passing */
+/*	saved ebx		36 */
+/*	saved edi		40 */
+/*	saved esi		44 */
+/*	saved ebp		48 */
+/*	return address		52 */
+#define	deflatestate		56	/* the function arguments	*/
+#define	curmatch		60
+
+#endif
+
+/* Offsets for fields in the deflate_state structure. These numbers
+ * are calculated from the definition of deflate_state, with the
+ * assumption that the compiler will dword-align the fields. (Thus,
+ * changing the definition of deflate_state could easily cause this
+ * program to crash horribly, without so much as a warning at
+ * compile time. Sigh.)
+ */
+#define	dsWSize			36
+#define	dsWMask			44
+#define	dsWindow		48
+#define	dsPrev			56
+#define	dsMatchLen		88
+#define	dsPrevMatch		92
+#define	dsStrStart		100
+#define	dsMatchStart		104
+#define	dsLookahead		108
+#define	dsPrevLen		112
+#define	dsMaxChainLen		116
+#define	dsGoodMatch		132
+#define	dsNiceMatch		136
+
+
+.file "match686.S"
+
+.globl	match_init, longest_match
+
+.text
+
+/* uInt longest_match(deflate_state *deflatestate, IPos curmatch) */
+
+longest_match:
+
+/* Save registers that the compiler may be using, and adjust %esp to	*/
+/* make room for our stack frame.					*/
+
+		pushl	%ebp
+		pushl	%edi
+		pushl	%esi
+		pushl	%ebx
+
+#ifdef CONFIG_REGPARM
+		pushl	%edx	// curmatch
+		pushl	%esi	// deflatestate
+#endif
+
+		subl	$LocalVarsSize, %esp
+
+/* Retrieve the function arguments. %ecx will hold cur_match		*/
+/* throughout the entire function. %edx will hold the pointer to the	*/
+/* deflate_state structure during the function's setup (before		*/
+/* entering the main loop).						*/
+
+		movl	deflatestate(%esp), %edx
+		movl	curmatch(%esp), %ecx
+
+/* uInt wmask = s->w_mask;						*/
+/* unsigned chain_length = s->max_chain_length;				*/
+/* if (s->prev_length >= s->good_match) {				*/
+/*     chain_length >>= 2;						*/
+/* }									*/
+
+		movl	dsPrevLen(%edx), %eax
+		movl	dsGoodMatch(%edx), %ebx
+		cmpl	%ebx, %eax
+		movl	dsWMask(%edx), %eax
+		movl	dsMaxChainLen(%edx), %ebx
+		jl	LastMatchGood
+		shrl	$2, %ebx
+LastMatchGood:
+
+/* chainlen is decremented once beforehand so that the function can	*/
+/* use the sign flag instead of the zero flag for the exit test.	*/
+/* It is then shifted into the high word, to make room for the wmask	*/
+/* value, which it will always accompany.				*/
+
+		decl	%ebx
+		shll	$16, %ebx
+		orl	%eax, %ebx
+		movl	%ebx, chainlenwmask(%esp)
+
+/* if ((uInt)nice_match > s->lookahead) nice_match = s->lookahead;	*/
+
+		movl	dsNiceMatch(%edx), %eax
+		movl	dsLookahead(%edx), %ebx
+		cmpl	%eax, %ebx
+		jl	LookaheadLess
+		movl	%eax, %ebx
+LookaheadLess:	movl	%ebx, nicematch(%esp)
+
+/* register Bytef *scan = s->window + s->strstart;			*/
+
+		movl	dsWindow(%edx), %esi
+		movl	%esi, window(%esp)
+		movl	dsStrStart(%edx), %ebp
+		lea	(%esi,%ebp), %edi
+		movl	%edi, scan(%esp)
+
+/* Determine how many bytes the scan ptr is off from being		*/
+/* dword-aligned.							*/
+
+		movl	%edi, %eax
+		negl	%eax
+		andl	$3, %eax
+		movl	%eax, scanalign(%esp)
+
+/* IPos limit = s->strstart > (IPos)MAX_DIST(s) ?			*/
+/*     s->strstart - (IPos)MAX_DIST(s) : NIL;				*/
+
+		movl	dsWSize(%edx), %eax
+		subl	$MIN_LOOKAHEAD, %eax
+		subl	%eax, %ebp
+		jg	LimitPositive
+		xorl	%ebp, %ebp
+LimitPositive:
+
+/* int best_len = s->prev_length;					*/
+
+		movl	dsPrevLen(%edx), %eax
+		movl	%eax, bestlen(%esp)
+
+/* Store the sum of s->window + best_len in %esi locally, and in %esi.	*/
+
+		addl	%eax, %esi
+		movl	%esi, windowbestlen(%esp)
+
+/* register ush scan_start = *(ushf*)scan;				*/
+/* register ush scan_end   = *(ushf*)(scan+best_len-1);			*/
+/* Posf *prev = s->prev;						*/
+
+		movzwl	(%edi), %ebx
+		movl	%ebx, scanstart(%esp)
+		movzwl	-1(%edi,%eax), %ebx
+		movl	%ebx, scanend(%esp)
+		movl	dsPrev(%edx), %edi
+
+/* Jump into the main loop.						*/
+
+		movl	chainlenwmask(%esp), %edx
+		jmp	LoopEntry
+
+.balign 16
+
+/* do {
+ *     match = s->window + cur_match;
+ *     if (*(ushf*)(match+best_len-1) != scan_end ||
+ *         *(ushf*)match != scan_start) continue;
+ *     [...]
+ * } while ((cur_match = prev[cur_match & wmask]) > limit
+ *          && --chain_length != 0);
+ *
+ * Here is the inner loop of the function. The function will spend the
+ * majority of its time in this loop, and majority of that time will
+ * be spent in the first ten instructions.
+ *
+ * Within this loop:
+ * %ebx = scanend
+ * %ecx = curmatch
+ * %edx = chainlenwmask - i.e., ((chainlen << 16) | wmask)
+ * %esi = windowbestlen - i.e., (window + bestlen)
+ * %edi = prev
+ * %ebp = limit
+ */
+LookupLoop:
+		andl	%edx, %ecx
+		movzwl	(%edi,%ecx,2), %ecx
+		cmpl	%ebp, %ecx
+		jbe	LeaveNow
+		subl	$0x00010000, %edx
+		js	LeaveNow
+LoopEntry:	movzwl	-1(%esi,%ecx), %eax
+		cmpl	%ebx, %eax
+		jnz	LookupLoop
+		movl	window(%esp), %eax
+		movzwl	(%eax,%ecx), %eax
+		cmpl	scanstart(%esp), %eax
+		jnz	LookupLoop
+
+/* Store the current value of chainlen.					*/
+
+		movl	%edx, chainlenwmask(%esp)
+
+/* Point %edi to the string under scrutiny, and %esi to the string we	*/
+/* are hoping to match it up with. In actuality, %esi and %edi are	*/
+/* both pointed (MAX_MATCH_8 - scanalign) bytes ahead, and %edx is	*/
+/* initialized to -(MAX_MATCH_8 - scanalign).				*/
+
+		movl	window(%esp), %esi
+		movl	scan(%esp), %edi
+		addl	%ecx, %esi
+		movl	scanalign(%esp), %eax
+		movl	$(-MAX_MATCH_8), %edx
+		lea	MAX_MATCH_8(%edi,%eax), %edi
+		lea	MAX_MATCH_8(%esi,%eax), %esi
+
+/* Test the strings for equality, 8 bytes at a time. At the end,
+ * adjust %edx so that it is offset to the exact byte that mismatched.
+ *
+ * We already know at this point that the first three bytes of the
+ * strings match each other, and they can be safely passed over before
+ * starting the compare loop. So what this code does is skip over 0-3
+ * bytes, as much as necessary in order to dword-align the %edi
+ * pointer. (%esi will still be misaligned three times out of four.)
+ *
+ * It should be confessed that this loop usually does not represent
+ * much of the total running time. Replacing it with a more
+ * straightforward "rep cmpsb" would not drastically degrade
+ * performance.
+ */
+LoopCmps:
+		movl	(%esi,%edx), %eax
+		xorl	(%edi,%edx), %eax
+		jnz	LeaveLoopCmps
+		movl	4(%esi,%edx), %eax
+		xorl	4(%edi,%edx), %eax
+		jnz	LeaveLoopCmps4
+		addl	$8, %edx
+		jnz	LoopCmps
+		jmp	LenMaximum
+LeaveLoopCmps4:	addl	$4, %edx
+LeaveLoopCmps:	testl	$0x0000FFFF, %eax
+		jnz	LenLower
+		addl	$2, %edx
+		shrl	$16, %eax
+LenLower:	subb	$1, %al
+		adcl	$0, %edx
+
+/* Calculate the length of the match. If it is longer than MAX_MATCH,	*/
+/* then automatically accept it as the best possible match and leave.	*/
+
+		lea	(%edi,%edx), %eax
+		movl	scan(%esp), %edi
+		subl	%edi, %eax
+		cmpl	$MAX_MATCH, %eax
+		jge	LenMaximum
+
+/* If the length of the match is not longer than the best match we	*/
+/* have so far, then forget it and return to the lookup loop.		*/
+
+		movl	deflatestate(%esp), %edx
+		movl	bestlen(%esp), %ebx
+		cmpl	%ebx, %eax
+		jg	LongerMatch
+		movl	windowbestlen(%esp), %esi
+		movl	dsPrev(%edx), %edi
+		movl	scanend(%esp), %ebx
+		movl	chainlenwmask(%esp), %edx
+		jmp	LookupLoop
+
+/*         s->match_start = cur_match;					*/
+/*         best_len = len;						*/
+/*         if (len >= nice_match) break;				*/
+/*         scan_end = *(ushf*)(scan+best_len-1);			*/
+
+LongerMatch:	movl	nicematch(%esp), %ebx
+		movl	%eax, bestlen(%esp)
+		movl	%ecx, dsMatchStart(%edx)
+		cmpl	%ebx, %eax
+		jge	LeaveNow
+		movl	window(%esp), %esi
+		addl	%eax, %esi
+		movl	%esi, windowbestlen(%esp)
+		movzwl	-1(%edi,%eax), %ebx
+		movl	dsPrev(%edx), %edi
+		movl	%ebx, scanend(%esp)
+		movl	chainlenwmask(%esp), %edx
+		jmp	LookupLoop
+
+/* Accept the current string, with the maximum possible length.		*/
+
+LenMaximum:	movl	deflatestate(%esp), %edx
+		movl	$MAX_MATCH, bestlen(%esp)
+		movl	%ecx, dsMatchStart(%edx)
+
+/* if ((uInt)best_len <= s->lookahead) return (uInt)best_len;		*/
+/* return s->lookahead;							*/
+
+LeaveNow:
+		movl	deflatestate(%esp), %edx
+		movl	bestlen(%esp), %ebx
+		movl	dsLookahead(%edx), %eax
+		cmpl	%eax, %ebx
+		jg	LookaheadRet
+		movl	%ebx, %eax
+LookaheadRet:
+
+/* Restore the stack and return from whence we came.			*/
+
+#ifndef CONFIG_REGPARM
+ 		addl	$LocalVarsSize, %esp
+#else
+		addl	$LocalVarsSize, %esp
+#endif
+
+		popl	%ebx
+		popl	%esi
+		popl	%edi
+		popl	%ebp
+match_init:	ret
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/trees.c linux-2.6.18.5/fs/ext2/gzip/trees.c
--- linux-2.6.18.5.org/fs/ext2/gzip/trees.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/trees.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,1214 @@
+/* trees.c -- output deflated data using Huffman coding
+ * Copyright (C) 1995-1998 Jean-loup Gailly
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/*
+ *  ALGORITHM
+ *
+ *      The "deflation" process uses several Huffman trees. The more
+ *      common source values are represented by shorter bit sequences.
+ *
+ *      Each code tree is stored in a compressed form which is itself
+ * a Huffman encoding of the lengths of all the code strings (in
+ * ascending order by source values).  The actual code strings are
+ * reconstructed from the lengths in the inflate process, as described
+ * in the deflate specification.
+ *
+ *  REFERENCES
+ *
+ *      Deutsch, L.P.,"'Deflate' Compressed Data Format Specification".
+ *      Available in ftp.uu.net:/pub/archiving/zip/doc/deflate-1.1.doc
+ *
+ *      Storer, James A.
+ *          Data Compression:  Methods and Theory, pp. 49-50.
+ *          Computer Science Press, 1988.  ISBN 0-7167-8156-5.
+ *
+ *      Sedgewick, R.
+ *          Algorithms, p290.
+ *          Addison-Wesley, 1983. ISBN 0-201-06672-6.
+ */
+
+/* @(#) $Id$ */
+
+/* #define GEN_TREES_H */
+
+#include "deflate.h"
+
+#ifdef DEBUG
+#  include <ctype.h>
+#endif
+
+/* ===========================================================================
+ * Constants
+ */
+
+#define MAX_BL_BITS 7
+/* Bit length codes must not exceed MAX_BL_BITS bits */
+
+#define END_BLOCK 256
+/* end of block literal code */
+
+#define REP_3_6      16
+/* repeat previous bit length 3-6 times (2 bits of repeat count) */
+
+#define REPZ_3_10    17
+/* repeat a zero length 3-10 times  (3 bits of repeat count) */
+
+#define REPZ_11_138  18
+/* repeat a zero length 11-138 times  (7 bits of repeat count) */
+
+local const int extra_lbits[LENGTH_CODES] /* extra bits for each length code */
+   = {0,0,0,0,0,0,0,0,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,0};
+
+local const int extra_dbits[D_CODES] /* extra bits for each distance code */
+   = {0,0,0,0,1,1,2,2,3,3,4,4,5,5,6,6,7,7,8,8,9,9,10,10,11,11,12,12,13,13};
+
+local const int extra_blbits[BL_CODES]/* extra bits for each bit length code */
+   = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,3,7};
+
+local const uch bl_order[BL_CODES]
+   = {16,17,18,0,8,7,9,6,10,5,11,4,12,3,13,2,14,1,15};
+/* The lengths of the bit length codes are sent in order of decreasing
+ * probability, to avoid transmitting the lengths for unused bit length codes.
+ */
+
+#define Buf_size (8 * 2*sizeof(char))
+/* Number of bits used within bi_buf. (bi_buf might be implemented on
+ * more than 16 bits on some systems.)
+ */
+
+/* ===========================================================================
+ * Local data. These are initialized only once.
+ */
+
+#define DIST_CODE_LEN  512 /* see definition of array dist_code below */
+
+#if defined(GEN_TREES_H) || !defined(STDC)
+/* non ANSI compilers may not accept trees.h */
+
+local ct_data static_ltree[L_CODES+2];
+/* The static literal tree. Since the bit lengths are imposed, there is no
+ * need for the L_CODES extra codes used during heap construction. However
+ * The codes 286 and 287 are needed to build a canonical tree (see _tr_init
+ * below).
+ */
+
+local ct_data static_dtree[D_CODES];
+/* The static distance tree. (Actually a trivial tree since all codes use
+ * 5 bits.)
+ */
+
+uch _dist_code[DIST_CODE_LEN];
+/* Distance codes. The first 256 values correspond to the distances
+ * 3 .. 258, the last 256 values correspond to the top 8 bits of
+ * the 15 bit distances.
+ */
+
+uch _length_code[MAX_MATCH-MIN_MATCH+1];
+/* length code for each normalized match length (0 == MIN_MATCH) */
+
+local int base_length[LENGTH_CODES];
+/* First normalized length for each code (0 = MIN_MATCH) */
+
+local int base_dist[D_CODES];
+/* First normalized distance for each code (0 = distance of 1) */
+
+#else
+#  include "trees.h"
+#endif /* GEN_TREES_H */
+
+struct static_tree_desc_s {
+    const ct_data *static_tree;  /* static tree or NULL */
+    const int *extra_bits;      /* extra bits for each code or NULL */
+    int     extra_base;          /* base index for extra_bits */
+    int     elems;               /* max number of elements in the tree */
+    int     max_length;          /* max bit length for the codes */
+};
+
+local static_tree_desc  static_l_desc =
+{static_ltree, extra_lbits, LITERALS+1, L_CODES, MAX_BITS};
+
+local static_tree_desc  static_d_desc =
+{static_dtree, extra_dbits, 0,          D_CODES, MAX_BITS};
+
+local static_tree_desc  static_bl_desc =
+{(const ct_data *)0, extra_blbits, 0,   BL_CODES, MAX_BL_BITS};
+
+/* ===========================================================================
+ * Local (static) routines in this file.
+ */
+
+local void tr_static_init OF((void));
+local void init_block     OF((deflate_state *s));
+local void pqdownheap     OF((deflate_state *s, ct_data *tree, int k));
+local void gen_bitlen     OF((deflate_state *s, tree_desc *desc));
+local void gen_codes      OF((ct_data *tree, int max_code, ush *bl_count));
+local void build_tree     OF((deflate_state *s, tree_desc *desc));
+local void scan_tree      OF((deflate_state *s, ct_data *tree, int max_code));
+local void send_tree      OF((deflate_state *s, ct_data *tree, int max_code));
+local int  build_bl_tree  OF((deflate_state *s));
+local void send_all_trees OF((deflate_state *s, int lcodes, int dcodes,
+                              int blcodes));
+local void compress_block OF((deflate_state *s, ct_data *ltree,
+                              ct_data *dtree));
+local void set_data_type  OF((deflate_state *s));
+local unsigned bi_reverse OF((unsigned value, int length));
+local void bi_windup      OF((deflate_state *s));
+local void bi_flush       OF((deflate_state *s));
+local void copy_block     OF((deflate_state *s, char *buf, unsigned len,
+                              int header));
+
+#ifdef GEN_TREES_H
+local void gen_trees_header OF((void));
+#endif
+
+#ifndef DEBUG
+#  define send_code(s, c, tree) send_bits(s, tree[c].Code, tree[c].Len)
+   /* Send a code of the given tree. c and tree must not have side effects */
+
+#else /* DEBUG */
+#  define send_code(s, c, tree) \
+     { if (z_verbose>2) fprintf(stderr,"\ncd %3d ",(c)); \
+       send_bits(s, tree[c].Code, tree[c].Len); }
+#endif
+
+/* ===========================================================================
+ * Output a short LSB first on the stream.
+ * IN assertion: there is enough room in pendingBuf.
+ */
+#define put_short(s, w) { \
+    put_byte(s, (uch)((w) & 0xff)); \
+    put_byte(s, (uch)((ush)(w) >> 8)); \
+}
+
+/* ===========================================================================
+ * Send a value on a given number of bits.
+ * IN assertion: length <= 16 and value fits in length bits.
+ */
+#ifdef DEBUG
+local void send_bits      OF((deflate_state *s, int value, int length));
+
+local void send_bits(s, value, length)
+    deflate_state *s;
+    int value;  /* value to send */
+    int length; /* number of bits */
+{
+    Tracevv((stderr," l %2d v %4x ", length, value));
+    Assert(length > 0 && length <= 15, "invalid length");
+    s->bits_sent += (ulg)length;
+
+    /* If not enough room in bi_buf, use (valid) bits from bi_buf and
+     * (16 - bi_valid) bits from value, leaving (width - (16-bi_valid))
+     * unused bits in value.
+     */
+    if (s->bi_valid > (int)Buf_size - length) {
+        s->bi_buf |= (value << s->bi_valid);
+        put_short(s, s->bi_buf);
+        s->bi_buf = (ush)value >> (Buf_size - s->bi_valid);
+        s->bi_valid += length - Buf_size;
+    } else {
+        s->bi_buf |= value << s->bi_valid;
+        s->bi_valid += length;
+    }
+}
+#else /* !DEBUG */
+
+#define send_bits(s, value, length) \
+{ int len = length;\
+  if (s->bi_valid > (int)Buf_size - len) {\
+    int val = value;\
+    s->bi_buf |= (val << s->bi_valid);\
+    put_short(s, s->bi_buf);\
+    s->bi_buf = (ush)val >> (Buf_size - s->bi_valid);\
+    s->bi_valid += len - Buf_size;\
+  } else {\
+    s->bi_buf |= (value) << s->bi_valid;\
+    s->bi_valid += len;\
+  }\
+}
+#endif /* DEBUG */
+
+
+#define MAX(a,b) (a >= b ? a : b)
+/* the arguments must not have side effects */
+
+/* ===========================================================================
+ * Initialize the various 'constant' tables.
+ */
+local void tr_static_init()
+{
+#if defined(GEN_TREES_H) || !defined(STDC)
+    static int static_init_done = 0;
+    int n;        /* iterates over tree elements */
+    int bits;     /* bit counter */
+    int length;   /* length value */
+    int code;     /* code value */
+    int dist;     /* distance index */
+    ush bl_count[MAX_BITS+1];
+    /* number of codes at each bit length for an optimal tree */
+
+    if (static_init_done) return;
+
+    /* For some embedded targets, global variables are not initialized: */
+    static_l_desc.static_tree = static_ltree;
+    static_l_desc.extra_bits = extra_lbits;
+    static_d_desc.static_tree = static_dtree;
+    static_d_desc.extra_bits = extra_dbits;
+    static_bl_desc.extra_bits = extra_blbits;
+
+    /* Initialize the mapping length (0..255) -> length code (0..28) */
+    length = 0;
+    for (code = 0; code < LENGTH_CODES-1; code++) {
+        base_length[code] = length;
+        for (n = 0; n < (1<<extra_lbits[code]); n++) {
+            _length_code[length++] = (uch)code;
+        }
+    }
+    Assert (length == 256, "tr_static_init: length != 256");
+    /* Note that the length 255 (match length 258) can be represented
+     * in two different ways: code 284 + 5 bits or code 285, so we
+     * overwrite length_code[255] to use the best encoding:
+     */
+    _length_code[length-1] = (uch)code;
+
+    /* Initialize the mapping dist (0..32K) -> dist code (0..29) */
+    dist = 0;
+    for (code = 0 ; code < 16; code++) {
+        base_dist[code] = dist;
+        for (n = 0; n < (1<<extra_dbits[code]); n++) {
+            _dist_code[dist++] = (uch)code;
+        }
+    }
+    Assert (dist == 256, "tr_static_init: dist != 256");
+    dist >>= 7; /* from now on, all distances are divided by 128 */
+    for ( ; code < D_CODES; code++) {
+        base_dist[code] = dist << 7;
+        for (n = 0; n < (1<<(extra_dbits[code]-7)); n++) {
+            _dist_code[256 + dist++] = (uch)code;
+        }
+    }
+    Assert (dist == 256, "tr_static_init: 256+dist != 512");
+
+    /* Construct the codes of the static literal tree */
+    for (bits = 0; bits <= MAX_BITS; bits++) bl_count[bits] = 0;
+    n = 0;
+    while (n <= 143) static_ltree[n++].Len = 8, bl_count[8]++;
+    while (n <= 255) static_ltree[n++].Len = 9, bl_count[9]++;
+    while (n <= 279) static_ltree[n++].Len = 7, bl_count[7]++;
+    while (n <= 287) static_ltree[n++].Len = 8, bl_count[8]++;
+    /* Codes 286 and 287 do not exist, but we must include them in the
+     * tree construction to get a canonical Huffman tree (longest code
+     * all ones)
+     */
+    gen_codes((ct_data *)static_ltree, L_CODES+1, bl_count);
+
+    /* The static distance tree is trivial: */
+    for (n = 0; n < D_CODES; n++) {
+        static_dtree[n].Len = 5;
+        static_dtree[n].Code = bi_reverse((unsigned)n, 5);
+    }
+    static_init_done = 1;
+
+#  ifdef GEN_TREES_H
+    gen_trees_header();
+#  endif
+#endif /* defined(GEN_TREES_H) || !defined(STDC) */
+}
+
+/* ===========================================================================
+ * Genererate the file trees.h describing the static trees.
+ */
+#ifdef GEN_TREES_H
+#  ifndef DEBUG
+#    include <stdio.h>
+#  endif
+
+#  define SEPARATOR(i, last, width) \
+      ((i) == (last)? "\n};\n\n" :    \
+       ((i) % (width) == (width)-1 ? ",\n" : ", "))
+
+void gen_trees_header()
+{
+    FILE *header = fopen("trees.h", "w");
+    int i;
+
+    Assert (header != NULL, "Can't open trees.h");
+    fprintf(header,
+	    "/* header created automatically with -DGEN_TREES_H */\n\n");
+
+    fprintf(header, "local const ct_data static_ltree[L_CODES+2] = {\n");
+    for (i = 0; i < L_CODES+2; i++) {
+	fprintf(header, "{{%3u},{%3u}}%s", static_ltree[i].Code,
+		static_ltree[i].Len, SEPARATOR(i, L_CODES+1, 5));
+    }
+
+    fprintf(header, "local const ct_data static_dtree[D_CODES] = {\n");
+    for (i = 0; i < D_CODES; i++) {
+	fprintf(header, "{{%2u},{%2u}}%s", static_dtree[i].Code,
+		static_dtree[i].Len, SEPARATOR(i, D_CODES-1, 5));
+    }
+
+    fprintf(header, "const uch _dist_code[DIST_CODE_LEN] = {\n");
+    for (i = 0; i < DIST_CODE_LEN; i++) {
+	fprintf(header, "%2u%s", _dist_code[i],
+		SEPARATOR(i, DIST_CODE_LEN-1, 20));
+    }
+
+    fprintf(header, "const uch _length_code[MAX_MATCH-MIN_MATCH+1]= {\n");
+    for (i = 0; i < MAX_MATCH-MIN_MATCH+1; i++) {
+	fprintf(header, "%2u%s", _length_code[i],
+		SEPARATOR(i, MAX_MATCH-MIN_MATCH, 20));
+    }
+
+    fprintf(header, "local const int base_length[LENGTH_CODES] = {\n");
+    for (i = 0; i < LENGTH_CODES; i++) {
+	fprintf(header, "%1u%s", base_length[i],
+		SEPARATOR(i, LENGTH_CODES-1, 20));
+    }
+
+    fprintf(header, "local const int base_dist[D_CODES] = {\n");
+    for (i = 0; i < D_CODES; i++) {
+	fprintf(header, "%5u%s", base_dist[i],
+		SEPARATOR(i, D_CODES-1, 10));
+    }
+
+    fclose(header);
+}
+#endif /* GEN_TREES_H */
+
+/* ===========================================================================
+ * Initialize the tree data structures for a new zlib stream.
+ */
+void ext2_tr_init(s)
+    deflate_state *s;
+{
+    tr_static_init();
+
+    s->l_desc.dyn_tree = s->dyn_ltree;
+    s->l_desc.stat_desc = &static_l_desc;
+
+    s->d_desc.dyn_tree = s->dyn_dtree;
+    s->d_desc.stat_desc = &static_d_desc;
+
+    s->bl_desc.dyn_tree = s->bl_tree;
+    s->bl_desc.stat_desc = &static_bl_desc;
+
+    s->bi_buf = 0;
+    s->bi_valid = 0;
+    s->last_eob_len = 8; /* enough lookahead for inflate */
+#ifdef DEBUG
+    s->compressed_len = 0L;
+    s->bits_sent = 0L;
+#endif
+
+    /* Initialize the first block of the first file: */
+    init_block(s);
+}
+
+/* ===========================================================================
+ * Initialize a new block.
+ */
+local void init_block(s)
+    deflate_state *s;
+{
+    int n; /* iterates over tree elements */
+
+    /* Initialize the trees. */
+    for (n = 0; n < L_CODES;  n++) s->dyn_ltree[n].Freq = 0;
+    for (n = 0; n < D_CODES;  n++) s->dyn_dtree[n].Freq = 0;
+    for (n = 0; n < BL_CODES; n++) s->bl_tree[n].Freq = 0;
+
+    s->dyn_ltree[END_BLOCK].Freq = 1;
+    s->opt_len = s->static_len = 0L;
+    s->last_lit = s->matches = 0;
+}
+
+#define SMALLEST 1
+/* Index within the heap array of least frequent node in the Huffman tree */
+
+
+/* ===========================================================================
+ * Remove the smallest element from the heap and recreate the heap with
+ * one less element. Updates heap and heap_len.
+ */
+#define pqremove(s, tree, top) \
+{\
+    top = s->heap[SMALLEST]; \
+    s->heap[SMALLEST] = s->heap[s->heap_len--]; \
+    pqdownheap(s, tree, SMALLEST); \
+}
+
+/* ===========================================================================
+ * Compares to subtrees, using the tree depth as tie breaker when
+ * the subtrees have equal frequency. This minimizes the worst case length.
+ */
+#define smaller(tree, n, m, depth) \
+   (tree[n].Freq < tree[m].Freq || \
+   (tree[n].Freq == tree[m].Freq && depth[n] <= depth[m]))
+
+/* ===========================================================================
+ * Restore the heap property by moving down the tree starting at node k,
+ * exchanging a node with the smallest of its two sons if necessary, stopping
+ * when the heap property is re-established (each father smaller than its
+ * two sons).
+ */
+local void pqdownheap(s, tree, k)
+    deflate_state *s;
+    ct_data *tree;  /* the tree to restore */
+    int k;               /* node to move down */
+{
+    int v = s->heap[k];
+    int j = k << 1;  /* left son of k */
+    while (j <= s->heap_len) {
+        /* Set j to the smallest of the two sons: */
+        if (j < s->heap_len &&
+            smaller(tree, s->heap[j+1], s->heap[j], s->depth)) {
+            j++;
+        }
+        /* Exit if v is smaller than both sons */
+        if (smaller(tree, v, s->heap[j], s->depth)) break;
+
+        /* Exchange v with the smallest son */
+        s->heap[k] = s->heap[j];  k = j;
+
+        /* And continue down the tree, setting j to the left son of k */
+        j <<= 1;
+    }
+    s->heap[k] = v;
+}
+
+/* ===========================================================================
+ * Compute the optimal bit lengths for a tree and update the total bit length
+ * for the current block.
+ * IN assertion: the fields freq and dad are set, heap[heap_max] and
+ *    above are the tree nodes sorted by increasing frequency.
+ * OUT assertions: the field len is set to the optimal bit length, the
+ *     array bl_count contains the frequencies for each bit length.
+ *     The length opt_len is updated; static_len is also updated if stree is
+ *     not null.
+ */
+local void gen_bitlen(s, desc)
+    deflate_state *s;
+    tree_desc *desc;    /* the tree descriptor */
+{
+    ct_data *tree        = desc->dyn_tree;
+    int max_code         = desc->max_code;
+    const ct_data *stree = desc->stat_desc->static_tree;
+    const int *extra    = desc->stat_desc->extra_bits;
+    int base             = desc->stat_desc->extra_base;
+    int max_length       = desc->stat_desc->max_length;
+    int h;              /* heap index */
+    int n, m;           /* iterate over the tree elements */
+    int bits;           /* bit length */
+    int xbits;          /* extra bits */
+    ush f;              /* frequency */
+    int overflow = 0;   /* number of elements with bit length too large */
+
+    for (bits = 0; bits <= MAX_BITS; bits++) s->bl_count[bits] = 0;
+
+    /* In a first pass, compute the optimal bit lengths (which may
+     * overflow in the case of the bit length tree).
+     */
+    tree[s->heap[s->heap_max]].Len = 0; /* root of the heap */
+
+    for (h = s->heap_max+1; h < HEAP_SIZE; h++) {
+        n = s->heap[h];
+        bits = tree[tree[n].Dad].Len + 1;
+        if (bits > max_length) bits = max_length, overflow++;
+        tree[n].Len = (ush)bits;
+        /* We overwrite tree[n].Dad which is no longer needed */
+
+        if (n > max_code) continue; /* not a leaf node */
+
+        s->bl_count[bits]++;
+        xbits = 0;
+        if (n >= base) xbits = extra[n-base];
+        f = tree[n].Freq;
+        s->opt_len += (ulg)f * (bits + xbits);
+        if (stree) s->static_len += (ulg)f * (stree[n].Len + xbits);
+    }
+    if (overflow == 0) return;
+
+    Trace((stderr,"\nbit length overflow\n"));
+    /* This happens for example on obj2 and pic of the Calgary corpus */
+
+    /* Find the first bit length which could increase: */
+    do {
+        bits = max_length-1;
+        while (s->bl_count[bits] == 0) bits--;
+        s->bl_count[bits]--;      /* move one leaf down the tree */
+        s->bl_count[bits+1] += 2; /* move one overflow item as its brother */
+        s->bl_count[max_length]--;
+        /* The brother of the overflow item also moves one step up,
+         * but this does not affect bl_count[max_length]
+         */
+        overflow -= 2;
+    } while (overflow > 0);
+
+    /* Now recompute all bit lengths, scanning in increasing frequency.
+     * h is still equal to HEAP_SIZE. (It is simpler to reconstruct all
+     * lengths instead of fixing only the wrong ones. This idea is taken
+     * from 'ar' written by Haruhiko Okumura.)
+     */
+    for (bits = max_length; bits != 0; bits--) {
+        n = s->bl_count[bits];
+        while (n != 0) {
+            m = s->heap[--h];
+            if (m > max_code) continue;
+            if (tree[m].Len != (unsigned) bits) {
+                Trace((stderr,"code %d bits %d->%d\n", m, tree[m].Len, bits));
+                s->opt_len += ((long)bits - (long)tree[m].Len)
+                              *(long)tree[m].Freq;
+                tree[m].Len = (ush)bits;
+            }
+            n--;
+        }
+    }
+}
+
+/* ===========================================================================
+ * Generate the codes for a given tree and bit counts (which need not be
+ * optimal).
+ * IN assertion: the array bl_count contains the bit length statistics for
+ * the given tree and the field len is set for all tree elements.
+ * OUT assertion: the field code is set for all tree elements of non
+ *     zero code length.
+ */
+local void gen_codes (tree, max_code, bl_count)
+    ct_data *tree;             /* the tree to decorate */
+    int max_code;              /* largest code with non zero frequency */
+    ush *bl_count;            /* number of codes at each bit length */
+{
+    ush next_code[MAX_BITS+1]; /* next code value for each bit length */
+    ush code = 0;              /* running code value */
+    int bits;                  /* bit index */
+    int n;                     /* code index */
+
+    /* The distribution counts are first used to generate the code values
+     * without bit reversal.
+     */
+    for (bits = 1; bits <= MAX_BITS; bits++) {
+        next_code[bits] = code = (code + bl_count[bits-1]) << 1;
+    }
+    /* Check that the bit counts in bl_count are consistent. The last code
+     * must be all ones.
+     */
+    Assert (code + bl_count[MAX_BITS]-1 == (1<<MAX_BITS)-1,
+            "inconsistent bit counts");
+    Tracev((stderr,"\ngen_codes: max_code %d ", max_code));
+
+    for (n = 0;  n <= max_code; n++) {
+        int len = tree[n].Len;
+        if (len == 0) continue;
+        /* Now reverse the bits */
+        tree[n].Code = bi_reverse(next_code[len]++, len);
+
+        Tracecv(tree != static_ltree, (stderr,"\nn %3d %c l %2d c %4x (%x) ",
+             n, (isgraph(n) ? n : ' '), len, tree[n].Code, next_code[len]-1));
+    }
+}
+
+/* ===========================================================================
+ * Construct one Huffman tree and assigns the code bit strings and lengths.
+ * Update the total bit length for the current block.
+ * IN assertion: the field freq is set for all tree elements.
+ * OUT assertions: the fields len and code are set to the optimal bit length
+ *     and corresponding code. The length opt_len is updated; static_len is
+ *     also updated if stree is not null. The field max_code is set.
+ */
+local void build_tree(s, desc)
+    deflate_state *s;
+    tree_desc *desc; /* the tree descriptor */
+{
+    ct_data *tree         = desc->dyn_tree;
+    const ct_data *stree  = desc->stat_desc->static_tree;
+    int elems             = desc->stat_desc->elems;
+    int n, m;          /* iterate over heap elements */
+    int max_code = -1; /* largest code with non zero frequency */
+    int node;          /* new node being created */
+
+    /* Construct the initial heap, with least frequent element in
+     * heap[SMALLEST]. The sons of heap[n] are heap[2*n] and heap[2*n+1].
+     * heap[0] is not used.
+     */
+    s->heap_len = 0, s->heap_max = HEAP_SIZE;
+
+    for (n = 0; n < elems; n++) {
+        if (tree[n].Freq != 0) {
+            s->heap[++(s->heap_len)] = max_code = n;
+            s->depth[n] = 0;
+        } else {
+            tree[n].Len = 0;
+        }
+    }
+
+    /* The pkzip format requires that at least one distance code exists,
+     * and that at least one bit should be sent even if there is only one
+     * possible code. So to avoid special checks later on we force at least
+     * two codes of non zero frequency.
+     */
+    while (s->heap_len < 2) {
+        node = s->heap[++(s->heap_len)] = (max_code < 2 ? ++max_code : 0);
+        tree[node].Freq = 1;
+        s->depth[node] = 0;
+        s->opt_len--; if (stree) s->static_len -= stree[node].Len;
+        /* node is 0 or 1 so it does not have extra bits */
+    }
+    desc->max_code = max_code;
+
+    /* The elements heap[heap_len/2+1 .. heap_len] are leaves of the tree,
+     * establish sub-heaps of increasing lengths:
+     */
+    for (n = s->heap_len/2; n >= 1; n--) pqdownheap(s, tree, n);
+
+    /* Construct the Huffman tree by repeatedly combining the least two
+     * frequent nodes.
+     */
+    node = elems;              /* next internal node of the tree */
+    do {
+        pqremove(s, tree, n);  /* n = node of least frequency */
+        m = s->heap[SMALLEST]; /* m = node of next least frequency */
+
+        s->heap[--(s->heap_max)] = n; /* keep the nodes sorted by frequency */
+        s->heap[--(s->heap_max)] = m;
+
+        /* Create a new node father of n and m */
+        tree[node].Freq = tree[n].Freq + tree[m].Freq;
+        s->depth[node] = (uch) (MAX(s->depth[n], s->depth[m]) + 1);
+        tree[n].Dad = tree[m].Dad = (ush)node;
+#ifdef DUMP_BL_TREE
+        if (tree == s->bl_tree) {
+            fprintf(stderr,"\nnode %d(%d), sons %d(%d) %d(%d)",
+                    node, tree[node].Freq, n, tree[n].Freq, m, tree[m].Freq);
+        }
+#endif
+        /* and insert the new node in the heap */
+        s->heap[SMALLEST] = node++;
+        pqdownheap(s, tree, SMALLEST);
+
+    } while (s->heap_len >= 2);
+
+    s->heap[--(s->heap_max)] = s->heap[SMALLEST];
+
+    /* At this point, the fields freq and dad are set. We can now
+     * generate the bit lengths.
+     */
+    gen_bitlen(s, (tree_desc *)desc);
+
+    /* The field len is now set, we can generate the bit codes */
+    gen_codes ((ct_data *)tree, max_code, s->bl_count);
+}
+
+/* ===========================================================================
+ * Scan a literal or distance tree to determine the frequencies of the codes
+ * in the bit length tree.
+ */
+local void scan_tree (s, tree, max_code)
+    deflate_state *s;
+    ct_data *tree;   /* the tree to be scanned */
+    int max_code;    /* and its largest code of non zero frequency */
+{
+    int n;                     /* iterates over all tree elements */
+    int prevlen = -1;          /* last emitted length */
+    int curlen;                /* length of current code */
+    int nextlen = tree[0].Len; /* length of next code */
+    int count = 0;             /* repeat count of the current code */
+    int max_count = 7;         /* max repeat count */
+    int min_count = 4;         /* min repeat count */
+
+    if (nextlen == 0) max_count = 138, min_count = 3;
+    tree[max_code+1].Len = (ush)0xffff; /* guard */
+
+    for (n = 0; n <= max_code; n++) {
+        curlen = nextlen; nextlen = tree[n+1].Len;
+        if (++count < max_count && curlen == nextlen) {
+            continue;
+        } else if (count < min_count) {
+            s->bl_tree[curlen].Freq += count;
+        } else if (curlen != 0) {
+            if (curlen != prevlen) s->bl_tree[curlen].Freq++;
+            s->bl_tree[REP_3_6].Freq++;
+        } else if (count <= 10) {
+            s->bl_tree[REPZ_3_10].Freq++;
+        } else {
+            s->bl_tree[REPZ_11_138].Freq++;
+        }
+        count = 0; prevlen = curlen;
+        if (nextlen == 0) {
+            max_count = 138, min_count = 3;
+        } else if (curlen == nextlen) {
+            max_count = 6, min_count = 3;
+        } else {
+            max_count = 7, min_count = 4;
+        }
+    }
+}
+
+/* ===========================================================================
+ * Send a literal or distance tree in compressed form, using the codes in
+ * bl_tree.
+ */
+local void send_tree (s, tree, max_code)
+    deflate_state *s;
+    ct_data *tree; /* the tree to be scanned */
+    int max_code;       /* and its largest code of non zero frequency */
+{
+    int n;                     /* iterates over all tree elements */
+    int prevlen = -1;          /* last emitted length */
+    int curlen;                /* length of current code */
+    int nextlen = tree[0].Len; /* length of next code */
+    int count = 0;             /* repeat count of the current code */
+    int max_count = 7;         /* max repeat count */
+    int min_count = 4;         /* min repeat count */
+
+    /* tree[max_code+1].Len = -1; */  /* guard already set */
+    if (nextlen == 0) max_count = 138, min_count = 3;
+
+    for (n = 0; n <= max_code; n++) {
+        curlen = nextlen; nextlen = tree[n+1].Len;
+        if (++count < max_count && curlen == nextlen) {
+            continue;
+        } else if (count < min_count) {
+            do { send_code(s, curlen, s->bl_tree); } while (--count != 0);
+
+        } else if (curlen != 0) {
+            if (curlen != prevlen) {
+                send_code(s, curlen, s->bl_tree); count--;
+            }
+            Assert(count >= 3 && count <= 6, " 3_6?");
+            send_code(s, REP_3_6, s->bl_tree); send_bits(s, count-3, 2);
+
+        } else if (count <= 10) {
+            send_code(s, REPZ_3_10, s->bl_tree); send_bits(s, count-3, 3);
+
+        } else {
+            send_code(s, REPZ_11_138, s->bl_tree); send_bits(s, count-11, 7);
+        }
+        count = 0; prevlen = curlen;
+        if (nextlen == 0) {
+            max_count = 138, min_count = 3;
+        } else if (curlen == nextlen) {
+            max_count = 6, min_count = 3;
+        } else {
+            max_count = 7, min_count = 4;
+        }
+    }
+}
+
+/* ===========================================================================
+ * Construct the Huffman tree for the bit lengths and return the index in
+ * bl_order of the last bit length code to send.
+ */
+local int build_bl_tree(s)
+    deflate_state *s;
+{
+    int max_blindex;  /* index of last bit length code of non zero freq */
+
+    /* Determine the bit length frequencies for literal and distance trees */
+    scan_tree(s, (ct_data *)s->dyn_ltree, s->l_desc.max_code);
+    scan_tree(s, (ct_data *)s->dyn_dtree, s->d_desc.max_code);
+
+    /* Build the bit length tree: */
+    build_tree(s, (tree_desc *)(&(s->bl_desc)));
+    /* opt_len now includes the length of the tree representations, except
+     * the lengths of the bit lengths codes and the 5+5+4 bits for the counts.
+     */
+
+    /* Determine the number of bit length codes to send. The pkzip format
+     * requires that at least 4 bit length codes be sent. (appnote.txt says
+     * 3 but the actual value used is 4.)
+     */
+    for (max_blindex = BL_CODES-1; max_blindex >= 3; max_blindex--) {
+        if (s->bl_tree[bl_order[max_blindex]].Len != 0) break;
+    }
+    /* Update opt_len to include the bit length tree and counts */
+    s->opt_len += 3*(max_blindex+1) + 5+5+4;
+    Tracev((stderr, "\ndyn trees: dyn %ld, stat %ld",
+            s->opt_len, s->static_len));
+
+    return max_blindex;
+}
+
+/* ===========================================================================
+ * Send the header for a block using dynamic Huffman trees: the counts, the
+ * lengths of the bit length codes, the literal tree and the distance tree.
+ * IN assertion: lcodes >= 257, dcodes >= 1, blcodes >= 4.
+ */
+local void send_all_trees(s, lcodes, dcodes, blcodes)
+    deflate_state *s;
+    int lcodes, dcodes, blcodes; /* number of codes for each tree */
+{
+    int rank;                    /* index in bl_order */
+
+    Assert (lcodes >= 257 && dcodes >= 1 && blcodes >= 4, "not enough codes");
+    Assert (lcodes <= L_CODES && dcodes <= D_CODES && blcodes <= BL_CODES,
+            "too many codes");
+    Tracev((stderr, "\nbl counts: "));
+    send_bits(s, lcodes-257, 5); /* not +255 as stated in appnote.txt */
+    send_bits(s, dcodes-1,   5);
+    send_bits(s, blcodes-4,  4); /* not -3 as stated in appnote.txt */
+    for (rank = 0; rank < blcodes; rank++) {
+        Tracev((stderr, "\nbl code %2d ", bl_order[rank]));
+        send_bits(s, s->bl_tree[bl_order[rank]].Len, 3);
+    }
+    Tracev((stderr, "\nbl tree: sent %ld", s->bits_sent));
+
+    send_tree(s, (ct_data *)s->dyn_ltree, lcodes-1); /* literal tree */
+    Tracev((stderr, "\nlit tree: sent %ld", s->bits_sent));
+
+    send_tree(s, (ct_data *)s->dyn_dtree, dcodes-1); /* distance tree */
+    Tracev((stderr, "\ndist tree: sent %ld", s->bits_sent));
+}
+
+/* ===========================================================================
+ * Send a stored block
+ */
+void ext2_tr_stored_block(s, buf, stored_len, eof)
+    deflate_state *s;
+    char *buf;       /* input block */
+    ulg stored_len;   /* length of input block */
+    int eof;          /* true if this is the last block for a file */
+{
+    send_bits(s, (STORED_BLOCK<<1)+eof, 3);  /* send block type */
+#ifdef DEBUG
+    s->compressed_len = (s->compressed_len + 3 + 7) & (ulg)~7L;
+    s->compressed_len += (stored_len + 4) << 3;
+#endif
+    copy_block(s, buf, (unsigned)stored_len, 1); /* with header */
+}
+
+/* ===========================================================================
+ * Send one empty static block to give enough lookahead for inflate.
+ * This takes 10 bits, of which 7 may remain in the bit buffer.
+ * The current inflate code requires 9 bits of lookahead. If the
+ * last two codes for the previous block (real code plus EOB) were coded
+ * on 5 bits or less, inflate may have only 5+3 bits of lookahead to decode
+ * the last real code. In this case we send two empty static blocks instead
+ * of one. (There are no problems if the previous block is stored or fixed.)
+ * To simplify the code, we assume the worst case of last real code encoded
+ * on one bit only.
+ */
+void ext2_tr_align(s)
+    deflate_state *s;
+{
+    send_bits(s, STATIC_TREES<<1, 3);
+    send_code(s, END_BLOCK, static_ltree);
+#ifdef DEBUG
+    s->compressed_len += 10L; /* 3 for block type, 7 for EOB */
+#endif
+    bi_flush(s);
+    /* Of the 10 bits for the empty block, we have already sent
+     * (10 - bi_valid) bits. The lookahead for the last real code (before
+     * the EOB of the previous block) was thus at least one plus the length
+     * of the EOB plus what we have just sent of the empty static block.
+     */
+    if (1 + s->last_eob_len + 10 - s->bi_valid < 9) {
+        send_bits(s, STATIC_TREES<<1, 3);
+        send_code(s, END_BLOCK, static_ltree);
+#ifdef DEBUG
+        s->compressed_len += 10L;
+#endif
+        bi_flush(s);
+    }
+    s->last_eob_len = 7;
+}
+
+/* ===========================================================================
+ * Determine the best encoding for the current block: dynamic trees, static
+ * trees or store, and output the encoded block to the zip file.
+ */
+void ext2_tr_flush_block(s, buf, stored_len, eof)
+    deflate_state *s;
+    char *buf;       /* input block, or NULL if too old */
+    ulg stored_len;   /* length of input block */
+    int eof;          /* true if this is the last block for a file */
+{
+    ulg opt_lenb, static_lenb; /* opt_len and static_len in bytes */
+    int max_blindex = 0;  /* index of last bit length code of non zero freq */
+
+    /* Build the Huffman trees unless a stored block is forced */
+    if (s->level > 0) {
+
+	 /* Check if the file is ascii or binary */
+	if (s->data_type == Z_UNKNOWN) set_data_type(s);
+
+	/* Construct the literal and distance trees */
+	build_tree(s, (tree_desc *)(&(s->l_desc)));
+	Tracev((stderr, "\nlit data: dyn %ld, stat %ld", s->opt_len,
+		s->static_len));
+
+	build_tree(s, (tree_desc *)(&(s->d_desc)));
+	Tracev((stderr, "\ndist data: dyn %ld, stat %ld", s->opt_len,
+		s->static_len));
+	/* At this point, opt_len and static_len are the total bit lengths of
+	 * the compressed block data, excluding the tree representations.
+	 */
+
+	/* Build the bit length tree for the above two trees, and get the index
+	 * in bl_order of the last bit length code to send.
+	 */
+	max_blindex = build_bl_tree(s);
+
+	/* Determine the best encoding. Compute first the block length in bytes*/
+	opt_lenb = (s->opt_len+3+7)>>3;
+	static_lenb = (s->static_len+3+7)>>3;
+
+	Tracev((stderr, "\nopt %lu(%lu) stat %lu(%lu) stored %lu lit %u ",
+		opt_lenb, s->opt_len, static_lenb, s->static_len, stored_len,
+		s->last_lit));
+
+	if (static_lenb <= opt_lenb) opt_lenb = static_lenb;
+
+    } else {
+        Assert(buf != (char*)0, "lost buf");
+	opt_lenb = static_lenb = stored_len + 5; /* force a stored block */
+    }
+
+#ifdef FORCE_STORED
+    if (buf != (char*)0) { /* force stored block */
+#else
+    if (stored_len+4 <= opt_lenb && buf != (char*)0) {
+                       /* 4: two words for the lengths */
+#endif
+        /* The test buf != NULL is only necessary if LIT_BUFSIZE > WSIZE.
+         * Otherwise we can't have processed more than WSIZE input bytes since
+         * the last block flush, because compression would have been
+         * successful. If LIT_BUFSIZE <= WSIZE, it is never too late to
+         * transform a block into a stored block.
+         */
+        ext2_tr_stored_block(s, buf, stored_len, eof);
+
+#ifdef FORCE_STATIC
+    } else if (static_lenb >= 0) { /* force static trees */
+#else
+    } else if (static_lenb == opt_lenb) {
+#endif
+        send_bits(s, (STATIC_TREES<<1)+eof, 3);
+        compress_block(s, (ct_data *)static_ltree, (ct_data *)static_dtree);
+#ifdef DEBUG
+        s->compressed_len += 3 + s->static_len;
+#endif
+    } else {
+        send_bits(s, (DYN_TREES<<1)+eof, 3);
+        send_all_trees(s, s->l_desc.max_code+1, s->d_desc.max_code+1,
+                       max_blindex+1);
+        compress_block(s, (ct_data *)s->dyn_ltree, (ct_data *)s->dyn_dtree);
+#ifdef DEBUG
+        s->compressed_len += 3 + s->opt_len;
+#endif
+    }
+    Assert (s->compressed_len == s->bits_sent, "bad compressed size");
+    /* The above check is made mod 2^32, for files larger than 512 MB
+     * and uLong implemented on 32 bits.
+     */
+    init_block(s);
+
+    if (eof) {
+        bi_windup(s);
+#ifdef DEBUG
+        s->compressed_len += 7;  /* align on byte boundary */
+#endif
+    }
+    Tracev((stderr,"\ncomprlen %lu(%lu) ", s->compressed_len>>3,
+           s->compressed_len-7*eof));
+}
+
+/* ===========================================================================
+ * Save the match info and tally the frequency counts. Return true if
+ * the current block must be flushed.
+ */
+int ext2_tr_tally (s, dist, lc)
+    deflate_state *s;
+    unsigned dist;  /* distance of matched string */
+    unsigned lc;    /* match length-MIN_MATCH or unmatched char (if dist==0) */
+{
+    s->d_buf[s->last_lit] = (ush)dist;
+    s->l_buf[s->last_lit++] = (uch)lc;
+    if (dist == 0) {
+        /* lc is the unmatched char */
+        s->dyn_ltree[lc].Freq++;
+    } else {
+        s->matches++;
+        /* Here, lc is the match length - MIN_MATCH */
+        dist--;             /* dist = match distance - 1 */
+        Assert((ush)dist < (ush)MAX_DIST(s) &&
+               (ush)lc <= (ush)(MAX_MATCH-MIN_MATCH) &&
+               (ush)d_code(dist) < (ush)D_CODES,  "_tr_tally: bad match");
+
+        s->dyn_ltree[_length_code[lc]+LITERALS+1].Freq++;
+        s->dyn_dtree[d_code(dist)].Freq++;
+    }
+
+#ifdef TRUNCATE_BLOCK
+    /* Try to guess if it is profitable to stop the current block here */
+    if ((s->last_lit & 0x1fff) == 0 && s->level > 2) {
+        /* Compute an upper bound for the compressed length */
+        ulg out_length = (ulg)s->last_lit*8L;
+        ulg in_length = (ulg)((long)s->strstart - s->block_start);
+        int dcode;
+        for (dcode = 0; dcode < D_CODES; dcode++) {
+            out_length += (ulg)s->dyn_dtree[dcode].Freq *
+                (5L+extra_dbits[dcode]);
+        }
+        out_length >>= 3;
+        Tracev((stderr,"\nlast_lit %u, in %ld, out ~%ld(%ld%%) ",
+               s->last_lit, in_length, out_length,
+               100L - out_length*100L/in_length));
+        if (s->matches < s->last_lit/2 && out_length < in_length/2) return 1;
+    }
+#endif
+    return (s->last_lit == s->lit_bufsize-1);
+    /* We avoid equality with lit_bufsize because of wraparound at 64K
+     * on 16 bit machines and because stored blocks are restricted to
+     * 64K-1 bytes.
+     */
+}
+
+/* ===========================================================================
+ * Send the block data compressed using the given Huffman trees
+ */
+local void compress_block(s, ltree, dtree)
+    deflate_state *s;
+    ct_data *ltree; /* literal tree */
+    ct_data *dtree; /* distance tree */
+{
+    unsigned dist;      /* distance of matched string */
+    int lc;             /* match length or unmatched char (if dist == 0) */
+    unsigned lx = 0;    /* running index in l_buf */
+    unsigned code;      /* the code to send */
+    int extra;          /* number of extra bits to send */
+
+    if (s->last_lit != 0) do {
+        dist = s->d_buf[lx];
+        lc = s->l_buf[lx++];
+        if (dist == 0) {
+            send_code(s, lc, ltree); /* send a literal byte */
+            Tracecv(isgraph(lc), (stderr," '%c' ", lc));
+        } else {
+            /* Here, lc is the match length - MIN_MATCH */
+            code = _length_code[lc];
+            send_code(s, code+LITERALS+1, ltree); /* send the length code */
+            extra = extra_lbits[code];
+            if (extra != 0) {
+                lc -= base_length[code];
+                send_bits(s, lc, extra);       /* send the extra length bits */
+            }
+            dist--; /* dist is now the match distance - 1 */
+            code = d_code(dist);
+            Assert (code < D_CODES, "bad d_code");
+
+            send_code(s, code, dtree);       /* send the distance code */
+            extra = extra_dbits[code];
+            if (extra != 0) {
+                dist -= base_dist[code];
+                send_bits(s, dist, extra);   /* send the extra distance bits */
+            }
+        } /* literal or match pair ? */
+
+        /* Check that the overlay between pending_buf and d_buf+l_buf is ok: */
+        Assert(s->pending < s->lit_bufsize + 2*lx, "pendingBuf overflow");
+
+    } while (lx < s->last_lit);
+
+    send_code(s, END_BLOCK, ltree);
+    s->last_eob_len = ltree[END_BLOCK].Len;
+}
+
+/* ===========================================================================
+ * Set the data type to ASCII or BINARY, using a crude approximation:
+ * binary if more than 20% of the bytes are <= 6 or >= 128, ascii otherwise.
+ * IN assertion: the fields freq of dyn_ltree are set and the total of all
+ * frequencies does not exceed 64K (to fit in an int on 16 bit machines).
+ */
+local void set_data_type(s)
+    deflate_state *s;
+{
+    int n = 0;
+    unsigned ascii_freq = 0;
+    unsigned bin_freq = 0;
+    while (n < 7)        bin_freq += s->dyn_ltree[n++].Freq;
+    while (n < 128)    ascii_freq += s->dyn_ltree[n++].Freq;
+    while (n < LITERALS) bin_freq += s->dyn_ltree[n++].Freq;
+    s->data_type = (Byte)(bin_freq > (ascii_freq >> 2) ? Z_BINARY : Z_ASCII);
+}
+
+/* ===========================================================================
+ * Reverse the first len bits of a code, using straightforward code (a faster
+ * method would use a table)
+ * IN assertion: 1 <= len <= 15
+ */
+local unsigned bi_reverse(code, len)
+    unsigned code; /* the value to invert */
+    int len;       /* its bit length */
+{
+    register unsigned res = 0;
+    do {
+        res |= code & 1;
+        code >>= 1, res <<= 1;
+    } while (--len > 0);
+    return res >> 1;
+}
+
+/* ===========================================================================
+ * Flush the bit buffer, keeping at most 7 bits in it.
+ */
+local void bi_flush(s)
+    deflate_state *s;
+{
+    if (s->bi_valid == 16) {
+        put_short(s, s->bi_buf);
+        s->bi_buf = 0;
+        s->bi_valid = 0;
+    } else if (s->bi_valid >= 8) {
+        put_byte(s, (Byte)s->bi_buf);
+        s->bi_buf >>= 8;
+        s->bi_valid -= 8;
+    }
+}
+
+/* ===========================================================================
+ * Flush the bit buffer and align the output on a byte boundary
+ */
+local void bi_windup(s)
+    deflate_state *s;
+{
+    if (s->bi_valid > 8) {
+        put_short(s, s->bi_buf);
+    } else if (s->bi_valid > 0) {
+        put_byte(s, (Byte)s->bi_buf);
+    }
+    s->bi_buf = 0;
+    s->bi_valid = 0;
+#ifdef DEBUG
+    s->bits_sent = (s->bits_sent+7) & ~7;
+#endif
+}
+
+/* ===========================================================================
+ * Copy a stored block, storing first the length and its
+ * one's complement if requested.
+ */
+local void copy_block(s, buf, len, header)
+    deflate_state *s;
+    char    *buf;    /* the input data */
+    unsigned len;     /* its length */
+    int      header;  /* true if block header must be written */
+{
+    bi_windup(s);        /* align on byte boundary */
+    s->last_eob_len = 8; /* enough lookahead for inflate */
+
+    if (header) {
+        put_short(s, (ush)len);   
+        put_short(s, (ush)~len);
+#ifdef DEBUG
+        s->bits_sent += 2*16;
+#endif
+    }
+#ifdef DEBUG
+    s->bits_sent += (ulg)len<<3;
+#endif
+    while (len--) {
+        put_byte(s, *buf++);
+    }
+}
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/trees.h linux-2.6.18.5/fs/ext2/gzip/trees.h
--- linux-2.6.18.5.org/fs/ext2/gzip/trees.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/trees.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,128 @@
+/* header created automatically with -DGEN_TREES_H */
+
+local const ct_data static_ltree[L_CODES+2] = {
+{{ 12},{  8}}, {{140},{  8}}, {{ 76},{  8}}, {{204},{  8}}, {{ 44},{  8}},
+{{172},{  8}}, {{108},{  8}}, {{236},{  8}}, {{ 28},{  8}}, {{156},{  8}},
+{{ 92},{  8}}, {{220},{  8}}, {{ 60},{  8}}, {{188},{  8}}, {{124},{  8}},
+{{252},{  8}}, {{  2},{  8}}, {{130},{  8}}, {{ 66},{  8}}, {{194},{  8}},
+{{ 34},{  8}}, {{162},{  8}}, {{ 98},{  8}}, {{226},{  8}}, {{ 18},{  8}},
+{{146},{  8}}, {{ 82},{  8}}, {{210},{  8}}, {{ 50},{  8}}, {{178},{  8}},
+{{114},{  8}}, {{242},{  8}}, {{ 10},{  8}}, {{138},{  8}}, {{ 74},{  8}},
+{{202},{  8}}, {{ 42},{  8}}, {{170},{  8}}, {{106},{  8}}, {{234},{  8}},
+{{ 26},{  8}}, {{154},{  8}}, {{ 90},{  8}}, {{218},{  8}}, {{ 58},{  8}},
+{{186},{  8}}, {{122},{  8}}, {{250},{  8}}, {{  6},{  8}}, {{134},{  8}},
+{{ 70},{  8}}, {{198},{  8}}, {{ 38},{  8}}, {{166},{  8}}, {{102},{  8}},
+{{230},{  8}}, {{ 22},{  8}}, {{150},{  8}}, {{ 86},{  8}}, {{214},{  8}},
+{{ 54},{  8}}, {{182},{  8}}, {{118},{  8}}, {{246},{  8}}, {{ 14},{  8}},
+{{142},{  8}}, {{ 78},{  8}}, {{206},{  8}}, {{ 46},{  8}}, {{174},{  8}},
+{{110},{  8}}, {{238},{  8}}, {{ 30},{  8}}, {{158},{  8}}, {{ 94},{  8}},
+{{222},{  8}}, {{ 62},{  8}}, {{190},{  8}}, {{126},{  8}}, {{254},{  8}},
+{{  1},{  8}}, {{129},{  8}}, {{ 65},{  8}}, {{193},{  8}}, {{ 33},{  8}},
+{{161},{  8}}, {{ 97},{  8}}, {{225},{  8}}, {{ 17},{  8}}, {{145},{  8}},
+{{ 81},{  8}}, {{209},{  8}}, {{ 49},{  8}}, {{177},{  8}}, {{113},{  8}},
+{{241},{  8}}, {{  9},{  8}}, {{137},{  8}}, {{ 73},{  8}}, {{201},{  8}},
+{{ 41},{  8}}, {{169},{  8}}, {{105},{  8}}, {{233},{  8}}, {{ 25},{  8}},
+{{153},{  8}}, {{ 89},{  8}}, {{217},{  8}}, {{ 57},{  8}}, {{185},{  8}},
+{{121},{  8}}, {{249},{  8}}, {{  5},{  8}}, {{133},{  8}}, {{ 69},{  8}},
+{{197},{  8}}, {{ 37},{  8}}, {{165},{  8}}, {{101},{  8}}, {{229},{  8}},
+{{ 21},{  8}}, {{149},{  8}}, {{ 85},{  8}}, {{213},{  8}}, {{ 53},{  8}},
+{{181},{  8}}, {{117},{  8}}, {{245},{  8}}, {{ 13},{  8}}, {{141},{  8}},
+{{ 77},{  8}}, {{205},{  8}}, {{ 45},{  8}}, {{173},{  8}}, {{109},{  8}},
+{{237},{  8}}, {{ 29},{  8}}, {{157},{  8}}, {{ 93},{  8}}, {{221},{  8}},
+{{ 61},{  8}}, {{189},{  8}}, {{125},{  8}}, {{253},{  8}}, {{ 19},{  9}},
+{{275},{  9}}, {{147},{  9}}, {{403},{  9}}, {{ 83},{  9}}, {{339},{  9}},
+{{211},{  9}}, {{467},{  9}}, {{ 51},{  9}}, {{307},{  9}}, {{179},{  9}},
+{{435},{  9}}, {{115},{  9}}, {{371},{  9}}, {{243},{  9}}, {{499},{  9}},
+{{ 11},{  9}}, {{267},{  9}}, {{139},{  9}}, {{395},{  9}}, {{ 75},{  9}},
+{{331},{  9}}, {{203},{  9}}, {{459},{  9}}, {{ 43},{  9}}, {{299},{  9}},
+{{171},{  9}}, {{427},{  9}}, {{107},{  9}}, {{363},{  9}}, {{235},{  9}},
+{{491},{  9}}, {{ 27},{  9}}, {{283},{  9}}, {{155},{  9}}, {{411},{  9}},
+{{ 91},{  9}}, {{347},{  9}}, {{219},{  9}}, {{475},{  9}}, {{ 59},{  9}},
+{{315},{  9}}, {{187},{  9}}, {{443},{  9}}, {{123},{  9}}, {{379},{  9}},
+{{251},{  9}}, {{507},{  9}}, {{  7},{  9}}, {{263},{  9}}, {{135},{  9}},
+{{391},{  9}}, {{ 71},{  9}}, {{327},{  9}}, {{199},{  9}}, {{455},{  9}},
+{{ 39},{  9}}, {{295},{  9}}, {{167},{  9}}, {{423},{  9}}, {{103},{  9}},
+{{359},{  9}}, {{231},{  9}}, {{487},{  9}}, {{ 23},{  9}}, {{279},{  9}},
+{{151},{  9}}, {{407},{  9}}, {{ 87},{  9}}, {{343},{  9}}, {{215},{  9}},
+{{471},{  9}}, {{ 55},{  9}}, {{311},{  9}}, {{183},{  9}}, {{439},{  9}},
+{{119},{  9}}, {{375},{  9}}, {{247},{  9}}, {{503},{  9}}, {{ 15},{  9}},
+{{271},{  9}}, {{143},{  9}}, {{399},{  9}}, {{ 79},{  9}}, {{335},{  9}},
+{{207},{  9}}, {{463},{  9}}, {{ 47},{  9}}, {{303},{  9}}, {{175},{  9}},
+{{431},{  9}}, {{111},{  9}}, {{367},{  9}}, {{239},{  9}}, {{495},{  9}},
+{{ 31},{  9}}, {{287},{  9}}, {{159},{  9}}, {{415},{  9}}, {{ 95},{  9}},
+{{351},{  9}}, {{223},{  9}}, {{479},{  9}}, {{ 63},{  9}}, {{319},{  9}},
+{{191},{  9}}, {{447},{  9}}, {{127},{  9}}, {{383},{  9}}, {{255},{  9}},
+{{511},{  9}}, {{  0},{  7}}, {{ 64},{  7}}, {{ 32},{  7}}, {{ 96},{  7}},
+{{ 16},{  7}}, {{ 80},{  7}}, {{ 48},{  7}}, {{112},{  7}}, {{  8},{  7}},
+{{ 72},{  7}}, {{ 40},{  7}}, {{104},{  7}}, {{ 24},{  7}}, {{ 88},{  7}},
+{{ 56},{  7}}, {{120},{  7}}, {{  4},{  7}}, {{ 68},{  7}}, {{ 36},{  7}},
+{{100},{  7}}, {{ 20},{  7}}, {{ 84},{  7}}, {{ 52},{  7}}, {{116},{  7}},
+{{  3},{  8}}, {{131},{  8}}, {{ 67},{  8}}, {{195},{  8}}, {{ 35},{  8}},
+{{163},{  8}}, {{ 99},{  8}}, {{227},{  8}}
+};
+
+local const ct_data static_dtree[D_CODES] = {
+{{ 0},{ 5}}, {{16},{ 5}}, {{ 8},{ 5}}, {{24},{ 5}}, {{ 4},{ 5}},
+{{20},{ 5}}, {{12},{ 5}}, {{28},{ 5}}, {{ 2},{ 5}}, {{18},{ 5}},
+{{10},{ 5}}, {{26},{ 5}}, {{ 6},{ 5}}, {{22},{ 5}}, {{14},{ 5}},
+{{30},{ 5}}, {{ 1},{ 5}}, {{17},{ 5}}, {{ 9},{ 5}}, {{25},{ 5}},
+{{ 5},{ 5}}, {{21},{ 5}}, {{13},{ 5}}, {{29},{ 5}}, {{ 3},{ 5}},
+{{19},{ 5}}, {{11},{ 5}}, {{27},{ 5}}, {{ 7},{ 5}}, {{23},{ 5}}
+};
+
+const uch _dist_code[DIST_CODE_LEN] = {
+ 0,  1,  2,  3,  4,  4,  5,  5,  6,  6,  6,  6,  7,  7,  7,  7,  8,  8,  8,  8,
+ 8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10,
+10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,
+11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,
+12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13,
+13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,
+13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,
+14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,
+14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,
+14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15,
+15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,
+15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,
+15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,  0,  0, 16, 17,
+18, 18, 19, 19, 20, 20, 20, 20, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22,
+23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
+24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,
+26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,
+26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27,
+27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,
+27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,
+28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,
+28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,
+28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,
+29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,
+29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,
+29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29
+};
+
+const uch _length_code[MAX_MATCH-MIN_MATCH+1]= {
+ 0,  1,  2,  3,  4,  5,  6,  7,  8,  8,  9,  9, 10, 10, 11, 11, 12, 12, 12, 12,
+13, 13, 13, 13, 14, 14, 14, 14, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16,
+17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19,
+19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,
+21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22,
+22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23,
+23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
+24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,
+25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,
+25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26,
+26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26,
+26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,
+27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28
+};
+
+local const int base_length[LENGTH_CODES] = {
+0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 14, 16, 20, 24, 28, 32, 40, 48, 56,
+64, 80, 96, 112, 128, 160, 192, 224, 0
+};
+
+local const int base_dist[D_CODES] = {
+    0,     1,     2,     3,     4,     6,     8,    12,    16,    24,
+   32,    48,    64,    96,   128,   192,   256,   384,   512,   768,
+ 1024,  1536,  2048,  3072,  4096,  6144,  8192, 12288, 16384, 24576
+};
+
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/zconf.h linux-2.6.18.5/fs/ext2/gzip/zconf.h
--- linux-2.6.18.5.org/fs/ext2/gzip/zconf.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/zconf.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,121 @@
+/* zconf.h -- configuration of the zlib compression library
+ * Copyright (C) 1995-1998 Jean-loup Gailly.
+ * For conditions of distribution and use, see copyright notice in zlib.h 
+ */
+
+/* @(#) $Id$ */
+
+#ifndef _ZCONF_H
+#define _ZCONF_H
+
+#if (defined(_WIN32) || defined(__WIN32__)) && !defined(WIN32)
+#  define WIN32
+#endif
+#if defined(__GNUC__) || defined(WIN32) || defined(__386__) || defined(i386)
+#  ifndef __32BIT__
+#    define __32BIT__
+#  endif
+#endif
+#if defined(__MSDOS__) && !defined(MSDOS)
+#  define MSDOS
+#endif
+
+/*
+ * Compile with -DMAXSEG_64K if the alloc function cannot allocate more
+ * than 64k bytes at a time (needed on systems with 16-bit int).
+ */
+#if defined(MSDOS) && !defined(__32BIT__)
+#  define MAXSEG_64K
+#endif
+#ifdef MSDOS
+#  define UNALIGNED_OK
+#endif
+
+#if (defined(MSDOS) || defined(_WINDOWS) || defined(WIN32))  && !defined(STDC)
+#  define STDC
+#endif
+#if defined(__STDC__) || defined(__cplusplus) || defined(__OS2__)
+#  ifndef STDC
+#    define STDC
+#  endif
+#endif
+
+#ifndef STDC
+#  ifndef const /* cannot use !defined(STDC) && !defined(const) on Mac */
+#    define const
+#  endif
+#endif
+
+/* Some Mac compilers merge all .h files incorrectly: */
+#if defined(__MWERKS__) || defined(applec) ||defined(THINK_C) ||defined(__SC__)
+#  define NO_DUMMY_DECL
+#endif
+
+/* Old Borland C incorrectly complains about missing returns: */
+#if defined(__BORLANDC__) && (__BORLANDC__ < 0x500)
+#  define NEED_DUMMY_RETURN
+#endif
+
+
+/* Maximum value for memLevel in deflateInit2 */
+#ifndef MAX_MEM_LEVEL
+#  ifdef MAXSEG_64K
+#    define MAX_MEM_LEVEL 8
+#  else
+#    define MAX_MEM_LEVEL 9
+#  endif
+#endif
+
+/* Maximum value for windowBits in deflateInit2 and inflateInit2.
+ * WARNING: reducing MAX_WBITS makes minigzip unable to extract .gz files
+ * created by gzip. (Files created by minigzip can still be extracted by
+ * gzip.)
+ */
+#ifndef MAX_WBITS
+#  define MAX_WBITS   15 /* 32K LZ77 window */
+#endif
+
+/* The memory requirements for deflate are (in bytes):
+            (1 << (windowBits+2)) +  (1 << (memLevel+9))
+ that is: 128K for windowBits=15  +  128K for memLevel = 8  (default values)
+ plus a few kilobytes for small objects. For example, if you want to reduce
+ the default memory requirements from 256K to 128K, compile with
+     make CFLAGS="-O -DMAX_WBITS=14 -DMAX_MEM_LEVEL=7"
+ Of course this will generally degrade compression (there's no free lunch).
+
+   The memory requirements for inflate are (in bytes) 1 << windowBits
+ that is, 32K for windowBits=15 (default value) plus a few kilobytes
+ for small objects.
+*/
+
+                        /* Type declarations */
+
+#ifndef OF /* function prototypes */
+#  ifdef STDC
+#    define OF(args)  args
+#  else
+#    define OF(args)  ()
+#  endif
+#endif
+
+#if defined (__BEOS__)
+#  if defined (ZLIB_DLL)
+#    define ZEXTERN extern __declspec(dllexport)
+#  else
+#    define ZEXTERN extern __declspec(dllimport)
+#  endif
+#endif
+
+#if !defined(MACOS) && !defined(TARGET_OS_MAC)
+typedef unsigned char  Byte;  /* 8 bits */
+#endif
+typedef unsigned int   uInt;  /* 16 bits or more */
+typedef unsigned long  uLong; /* 32 bits or more */
+
+#ifdef STDC
+   typedef void     *voidp;
+#else
+   typedef Byte     *voidp;
+#endif
+
+#endif /* _ZCONF_H */
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/zlib.h linux-2.6.18.5/fs/ext2/gzip/zlib.h
--- linux-2.6.18.5.org/fs/ext2/gzip/zlib.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/zlib.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,876 @@
+/* zlib.h -- interface of the 'zlib' general purpose compression library
+  version 1.1.3, July 9th, 1998
+
+  Copyright (C) 1995-1998 Jean-loup Gailly and Mark Adler
+
+  This software is provided 'as-is', without any express or implied
+  warranty.  In no event will the authors be held liable for any damages
+  arising from the use of this software.
+
+  Permission is granted to anyone to use this software for any purpose,
+  including commercial applications, and to alter it and redistribute it
+  freely, subject to the following restrictions:
+
+  1. The origin of this software must not be misrepresented; you must not
+     claim that you wrote the original software. If you use this software
+     in a product, an acknowledgment in the product documentation would be
+     appreciated but is not required.
+  2. Altered source versions must be plainly marked as such, and must not be
+     misrepresented as being the original software.
+  3. This notice may not be removed or altered from any source distribution.
+
+  Jean-loup Gailly        Mark Adler
+  jloup@gzip.org          madler@alumni.caltech.edu
+
+
+  The data format used by the zlib library is described by RFCs (Request for
+  Comments) 1950 to 1952 in the files ftp://ds.internic.net/rfc/rfc1950.txt
+  (zlib format), rfc1951.txt (deflate format) and rfc1952.txt (gzip format).
+*/
+
+#ifndef _ZLIB_H
+#define _ZLIB_H
+
+#include "zconf.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#define ZLIB_VERSION "1.1.3"
+
+/* 
+     The 'zlib' compression library provides in-memory compression and
+  decompression functions, including integrity checks of the uncompressed
+  data.  This version of the library supports only one compression method
+  (deflation) but other algorithms will be added later and will have the same
+  stream interface.
+
+     Compression can be done in a single step if the buffers are large
+  enough (for example if an input file is mmap'ed), or can be done by
+  repeated calls of the compression function.  In the latter case, the
+  application must provide more input and/or consume the output
+  (providing more output space) before each call.
+
+     The library also supports reading and writing files in gzip (.gz) format
+  with an interface similar to that of stdio.
+
+     The library does not install any signal handler. The decoder checks
+  the consistency of the compressed data, so the library should never
+  crash even in case of corrupted input.
+*/
+
+typedef voidp  (*alloc_func) OF((voidp opaque, uInt items, uInt size));
+typedef void   (*free_func)  OF((voidp opaque, voidp address));
+
+struct internal_state;
+
+typedef struct z_stream_s {
+    Byte     *next_in;  /* next input byte */
+    uInt     avail_in;  /* number of bytes available at next_in */
+    uLong    total_in;  /* total nb of input bytes read so far */
+
+    Byte     *next_out; /* next output byte should be put there */
+    uInt     avail_out; /* remaining free space at next_out */
+    uLong    total_out; /* total nb of bytes output so far */
+
+    char     *msg;      /* last error message, NULL if no error */
+    struct internal_state *state; /* not visible by applications */
+
+    alloc_func zalloc;  /* used to allocate the internal state */
+    free_func  zfree;   /* used to free the internal state */
+    voidp      opaque;  /* private data object passed to zalloc and zfree */
+
+    int     data_type;  /* best guess about the data type: ascii or binary */
+    uLong   adler;      /* adler32 value of the uncompressed data */
+    uLong   reserved;   /* reserved for future use */
+} z_stream;
+
+typedef z_stream *z_streamp;
+
+/*
+   The application must update next_in and avail_in when avail_in has
+   dropped to zero. It must update next_out and avail_out when avail_out
+   has dropped to zero. The application must initialize zalloc, zfree and
+   opaque before calling the init function. All other fields are set by the
+   compression library and must not be updated by the application.
+
+   The opaque value provided by the application will be passed as the first
+   parameter for calls of zalloc and zfree. This can be useful for custom
+   memory management. The compression library attaches no meaning to the
+   opaque value.
+
+   zalloc must return Z_NULL if there is not enough memory for the object.
+   If zlib is used in a multi-threaded application, zalloc and zfree must be
+   thread safe.
+
+   On 16-bit systems, the functions zalloc and zfree must be able to allocate
+   exactly 65536 bytes, but will not be required to allocate more than this
+   if the symbol MAXSEG_64K is defined (see zconf.h). WARNING: On MSDOS,
+   pointers returned by zalloc for objects of exactly 65536 bytes *must*
+   have their offset normalized to zero. The default allocation function
+   provided by this library ensures this (see zutil.c). To reduce memory
+   requirements and avoid any allocation of 64K objects, at the expense of
+   compression ratio, compile the library with -DMAX_WBITS=14 (see zconf.h).
+
+   The fields total_in and total_out can be used for statistics or
+   progress reports. After compression, total_in holds the total size of
+   the uncompressed data and may be saved for use in the decompressor
+   (particularly if the decompressor wants to decompress everything in
+   a single step).
+*/
+
+                        /* constants */
+
+#define Z_NO_FLUSH      0
+#define Z_PARTIAL_FLUSH 1 /* will be removed, use Z_SYNC_FLUSH instead */
+#define Z_SYNC_FLUSH    2
+#define Z_FULL_FLUSH    3
+#define Z_FINISH        4
+/* Allowed flush values; see deflate() below for details */
+
+#define Z_OK            0
+#define Z_STREAM_END    1
+#define Z_NEED_DICT     2
+#define Z_ERRNO        (-1)
+#define Z_STREAM_ERROR (-2)
+#define Z_DATA_ERROR   (-3)
+#define Z_MEM_ERROR    (-4)
+#define Z_BUF_ERROR    (-5)
+#define Z_VERSION_ERROR (-6)
+/* Return codes for the compression/decompression functions. Negative
+ * values are errors, positive values are used for special but normal events.
+ */
+
+#define Z_NO_COMPRESSION         0
+#define Z_BEST_SPEED             1
+#define Z_BEST_COMPRESSION       9
+#define Z_DEFAULT_COMPRESSION  (-1)
+/* compression levels */
+
+#define Z_FILTERED            1
+#define Z_HUFFMAN_ONLY        2
+#define Z_DEFAULT_STRATEGY    0
+/* compression strategy; see deflateInit2() below for details */
+
+#define Z_BINARY   0
+#define Z_ASCII    1
+#define Z_UNKNOWN  2
+/* Possible values of the data_type field */
+
+#define Z_DEFLATED   8
+/* The deflate compression method (the only one supported in this version) */
+
+#define Z_NULL  0  /* for initializing zalloc, zfree, opaque */
+
+#define zlib_version zlibVersion()
+/* for compatibility with versions < 1.0.2 */
+
+                        /* basic functions */
+
+extern const char * zlibVersion OF((void));
+/* The application can compare zlibVersion and ZLIB_VERSION for consistency.
+   If the first character differs, the library code actually used is
+   not compatible with the zlib.h header file used by the application.
+   This check is automatically made by deflateInit and inflateInit.
+ */
+
+/* 
+extern int deflateInit OF((z_streamp strm, int level));
+
+     Initializes the internal stream state for compression. The fields
+   zalloc, zfree and opaque must be initialized before by the caller.
+   If zalloc and zfree are set to Z_NULL, deflateInit updates them to
+   use default allocation functions.
+
+     The compression level must be Z_DEFAULT_COMPRESSION, or between 0 and 9:
+   1 gives best speed, 9 gives best compression, 0 gives no compression at
+   all (the input data is simply copied a block at a time).
+   Z_DEFAULT_COMPRESSION requests a default compromise between speed and
+   compression (currently equivalent to level 6).
+
+     deflateInit returns Z_OK if success, Z_MEM_ERROR if there was not
+   enough memory, Z_STREAM_ERROR if level is not a valid compression level,
+   Z_VERSION_ERROR if the zlib library version (zlib_version) is incompatible
+   with the version assumed by the caller (ZLIB_VERSION).
+   msg is set to null if there is no error message.  deflateInit does not
+   perform any compression: this will be done by deflate().
+*/
+
+
+extern int ext2_deflate OF((z_streamp strm, int flush));
+/*
+    deflate compresses as much data as possible, and stops when the input
+  buffer becomes empty or the output buffer becomes full. It may introduce some
+  output latency (reading input without producing any output) except when
+  forced to flush.
+
+    The detailed semantics are as follows. deflate performs one or both of the
+  following actions:
+
+  - Compress more input starting at next_in and update next_in and avail_in
+    accordingly. If not all input can be processed (because there is not
+    enough room in the output buffer), next_in and avail_in are updated and
+    processing will resume at this point for the next call of deflate().
+
+  - Provide more output starting at next_out and update next_out and avail_out
+    accordingly. This action is forced if the parameter flush is non zero.
+    Forcing flush frequently degrades the compression ratio, so this parameter
+    should be set only when necessary (in interactive applications).
+    Some output may be provided even if flush is not set.
+
+  Before the call of deflate(), the application should ensure that at least
+  one of the actions is possible, by providing more input and/or consuming
+  more output, and updating avail_in or avail_out accordingly; avail_out
+  should never be zero before the call. The application can consume the
+  compressed output when it wants, for example when the output buffer is full
+  (avail_out == 0), or after each call of deflate(). If deflate returns Z_OK
+  and with zero avail_out, it must be called again after making room in the
+  output buffer because there might be more output pending.
+
+    If the parameter flush is set to Z_SYNC_FLUSH, all pending output is
+  flushed to the output buffer and the output is aligned on a byte boundary, so
+  that the decompressor can get all input data available so far. (In particular
+  avail_in is zero after the call if enough output space has been provided
+  before the call.)  Flushing may degrade compression for some compression
+  algorithms and so it should be used only when necessary.
+
+    If flush is set to Z_FULL_FLUSH, all output is flushed as with
+  Z_SYNC_FLUSH, and the compression state is reset so that decompression can
+  restart from this point if previous compressed data has been damaged or if
+  random access is desired. Using Z_FULL_FLUSH too often can seriously degrade
+  the compression.
+
+    If deflate returns with avail_out == 0, this function must be called again
+  with the same value of the flush parameter and more output space (updated
+  avail_out), until the flush is complete (deflate returns with non-zero
+  avail_out).
+
+    If the parameter flush is set to Z_FINISH, pending input is processed,
+  pending output is flushed and deflate returns with Z_STREAM_END if there
+  was enough output space; if deflate returns with Z_OK, this function must be
+  called again with Z_FINISH and more output space (updated avail_out) but no
+  more input data, until it returns with Z_STREAM_END or an error. After
+  deflate has returned Z_STREAM_END, the only possible operations on the
+  stream are deflateReset or deflateEnd.
+  
+    Z_FINISH can be used immediately after deflateInit if all the compression
+  is to be done in a single step. In this case, avail_out must be at least
+  0.1% larger than avail_in plus 12 bytes.  If deflate does not return
+  Z_STREAM_END, then it must be called again as described above.
+
+    deflate() sets strm->adler to the adler32 checksum of all input read
+  so far (that is, total_in bytes).
+
+    deflate() may update data_type if it can make a good guess about
+  the input data type (Z_ASCII or Z_BINARY). In doubt, the data is considered
+  binary. This field is only for information purposes and does not affect
+  the compression algorithm in any manner.
+
+    deflate() returns Z_OK if some progress has been made (more input
+  processed or more output produced), Z_STREAM_END if all input has been
+  consumed and all output has been produced (only when flush is set to
+  Z_FINISH), Z_STREAM_ERROR if the stream state was inconsistent (for example
+  if next_in or next_out was NULL), Z_BUF_ERROR if no progress is possible
+  (for example avail_in or avail_out was zero).
+*/
+
+
+extern int ext2_deflateEnd OF((z_streamp strm));
+/*
+     All dynamically allocated data structures for this stream are freed.
+   This function discards any unprocessed input and does not flush any
+   pending output.
+
+     deflateEnd returns Z_OK if success, Z_STREAM_ERROR if the
+   stream state was inconsistent, Z_DATA_ERROR if the stream was freed
+   prematurely (some input or output was discarded). In the error case,
+   msg may be set but then points to a static string (which must not be
+   deallocated).
+*/
+
+
+/* 
+extern int inflateInit OF((z_streamp strm));
+
+     Initializes the internal stream state for decompression. The fields
+   next_in, avail_in, zalloc, zfree and opaque must be initialized before by
+   the caller. If next_in is not Z_NULL and avail_in is large enough (the exact
+   value depends on the compression method), inflateInit determines the
+   compression method from the zlib header and allocates all data structures
+   accordingly; otherwise the allocation will be deferred to the first call of
+   inflate.  If zalloc and zfree are set to Z_NULL, inflateInit updates them to
+   use default allocation functions.
+
+     inflateInit returns Z_OK if success, Z_MEM_ERROR if there was not enough
+   memory, Z_VERSION_ERROR if the zlib library version is incompatible with the
+   version assumed by the caller.  msg is set to null if there is no error
+   message. inflateInit does not perform any decompression apart from reading
+   the zlib header if present: this will be done by inflate().  (So next_in and
+   avail_in may be modified, but next_out and avail_out are unchanged.)
+*/
+
+
+extern int ext2_inflate OF((z_streamp strm, int flush));
+/*
+    inflate decompresses as much data as possible, and stops when the input
+  buffer becomes empty or the output buffer becomes full. It may some
+  introduce some output latency (reading input without producing any output)
+  except when forced to flush.
+
+  The detailed semantics are as follows. inflate performs one or both of the
+  following actions:
+
+  - Decompress more input starting at next_in and update next_in and avail_in
+    accordingly. If not all input can be processed (because there is not
+    enough room in the output buffer), next_in is updated and processing
+    will resume at this point for the next call of inflate().
+
+  - Provide more output starting at next_out and update next_out and avail_out
+    accordingly.  inflate() provides as much output as possible, until there
+    is no more input data or no more space in the output buffer (see below
+    about the flush parameter).
+
+  Before the call of inflate(), the application should ensure that at least
+  one of the actions is possible, by providing more input and/or consuming
+  more output, and updating the next_* and avail_* values accordingly.
+  The application can consume the uncompressed output when it wants, for
+  example when the output buffer is full (avail_out == 0), or after each
+  call of inflate(). If inflate returns Z_OK and with zero avail_out, it
+  must be called again after making room in the output buffer because there
+  might be more output pending.
+
+    If the parameter flush is set to Z_SYNC_FLUSH, inflate flushes as much
+  output as possible to the output buffer. The flushing behavior of inflate is
+  not specified for values of the flush parameter other than Z_SYNC_FLUSH
+  and Z_FINISH, but the current implementation actually flushes as much output
+  as possible anyway.
+
+    inflate() should normally be called until it returns Z_STREAM_END or an
+  error. However if all decompression is to be performed in a single step
+  (a single call of inflate), the parameter flush should be set to
+  Z_FINISH. In this case all pending input is processed and all pending
+  output is flushed; avail_out must be large enough to hold all the
+  uncompressed data. (The size of the uncompressed data may have been saved
+  by the compressor for this purpose.) The next operation on this stream must
+  be inflateEnd to deallocate the decompression state. The use of Z_FINISH
+  is never required, but can be used to inform inflate that a faster routine
+  may be used for the single inflate() call.
+
+     If a preset dictionary is needed at this point (see inflateSetDictionary
+  below), inflate sets strm-adler to the adler32 checksum of the
+  dictionary chosen by the compressor and returns Z_NEED_DICT; otherwise 
+  it sets strm->adler to the adler32 checksum of all output produced
+  so far (that is, total_out bytes) and returns Z_OK, Z_STREAM_END or
+  an error code as described below. At the end of the stream, inflate()
+  checks that its computed adler32 checksum is equal to that saved by the
+  compressor and returns Z_STREAM_END only if the checksum is correct.
+
+    inflate() returns Z_OK if some progress has been made (more input processed
+  or more output produced), Z_STREAM_END if the end of the compressed data has
+  been reached and all uncompressed output has been produced, Z_NEED_DICT if a
+  preset dictionary is needed at this point, Z_DATA_ERROR if the input data was
+  corrupted (input stream not conforming to the zlib format or incorrect
+  adler32 checksum), Z_STREAM_ERROR if the stream structure was inconsistent
+  (for example if next_in or next_out was NULL), Z_MEM_ERROR if there was not
+  enough memory, Z_BUF_ERROR if no progress is possible or if there was not
+  enough room in the output buffer when Z_FINISH is used. In the Z_DATA_ERROR
+  case, the application may then call inflateSync to look for a good
+  compression block.
+*/
+
+
+extern int ext2_inflateEnd OF((z_streamp strm));
+/*
+     All dynamically allocated data structures for this stream are freed.
+   This function discards any unprocessed input and does not flush any
+   pending output.
+
+     inflateEnd returns Z_OK if success, Z_STREAM_ERROR if the stream state
+   was inconsistent. In the error case, msg may be set but then points to a
+   static string (which must not be deallocated).
+*/
+
+                        /* Advanced functions */
+
+/*
+    The following functions are needed only in some special applications.
+*/
+
+/*   
+extern int deflateInit2 OF((z_streamp strm,
+                                     int  level,
+                                     int  method,
+                                     int  windowBits,
+                                     int  memLevel,
+                                     int  strategy));
+
+     This is another version of deflateInit with more compression options. The
+   fields next_in, zalloc, zfree and opaque must be initialized before by
+   the caller.
+
+     The method parameter is the compression method. It must be Z_DEFLATED in
+   this version of the library.
+
+     The windowBits parameter is the base two logarithm of the window size
+   (the size of the history buffer).  It should be in the range 8..15 for this
+   version of the library. Larger values of this parameter result in better
+   compression at the expense of memory usage. The default value is 15 if
+   deflateInit is used instead.
+
+     The memLevel parameter specifies how much memory should be allocated
+   for the internal compression state. memLevel=1 uses minimum memory but
+   is slow and reduces compression ratio; memLevel=9 uses maximum memory
+   for optimal speed. The default value is 8. See zconf.h for total memory
+   usage as a function of windowBits and memLevel.
+
+     The strategy parameter is used to tune the compression algorithm. Use the
+   value Z_DEFAULT_STRATEGY for normal data, Z_FILTERED for data produced by a
+   filter (or predictor), or Z_HUFFMAN_ONLY to force Huffman encoding only (no
+   string match).  Filtered data consists mostly of small values with a
+   somewhat random distribution. In this case, the compression algorithm is
+   tuned to compress them better. The effect of Z_FILTERED is to force more
+   Huffman coding and less string matching; it is somewhat intermediate
+   between Z_DEFAULT and Z_HUFFMAN_ONLY. The strategy parameter only affects
+   the compression ratio but not the correctness of the compressed output even
+   if it is not set appropriately.
+
+      deflateInit2 returns Z_OK if success, Z_MEM_ERROR if there was not enough
+   memory, Z_STREAM_ERROR if a parameter is invalid (such as an invalid
+   method). msg is set to null if there is no error message.  deflateInit2 does
+   not perform any compression: this will be done by deflate().
+*/
+                            
+extern int ext2_deflateSetDictionary OF((z_streamp strm,
+                                             const Byte *dictionary,
+                                             uInt  dictLength));
+/*
+     Initializes the compression dictionary from the given byte sequence
+   without producing any compressed output. This function must be called
+   immediately after deflateInit, deflateInit2 or deflateReset, before any
+   call of deflate. The compressor and decompressor must use exactly the same
+   dictionary (see inflateSetDictionary).
+
+     The dictionary should consist of strings (byte sequences) that are likely
+   to be encountered later in the data to be compressed, with the most commonly
+   used strings preferably put towards the end of the dictionary. Using a
+   dictionary is most useful when the data to be compressed is short and can be
+   predicted with good accuracy; the data can then be compressed better than
+   with the default empty dictionary.
+
+     Depending on the size of the compression data structures selected by
+   deflateInit or deflateInit2, a part of the dictionary may in effect be
+   discarded, for example if the dictionary is larger than the window size in
+   deflate or deflate2. Thus the strings most likely to be useful should be
+   put at the end of the dictionary, not at the front.
+
+     Upon return of this function, strm->adler is set to the Adler32 value
+   of the dictionary; the decompressor may later use this value to determine
+   which dictionary has been used by the compressor. (The Adler32 value
+   applies to the whole dictionary even if only a subset of the dictionary is
+   actually used by the compressor.)
+
+     deflateSetDictionary returns Z_OK if success, or Z_STREAM_ERROR if a
+   parameter is invalid (such as NULL dictionary) or the stream state is
+   inconsistent (for example if deflate has already been called for this stream
+   or if the compression method is bsort). deflateSetDictionary does not
+   perform any compression: this will be done by deflate().
+*/
+
+extern int ext2_deflateCopy OF((z_streamp dest,
+                                    z_streamp source));
+/*
+     Sets the destination stream as a complete copy of the source stream.
+
+     This function can be useful when several compression strategies will be
+   tried, for example when there are several ways of pre-processing the input
+   data with a filter. The streams that will be discarded should then be freed
+   by calling deflateEnd.  Note that deflateCopy duplicates the internal
+   compression state which can be quite large, so this strategy is slow and
+   can consume lots of memory.
+
+     deflateCopy returns Z_OK if success, Z_MEM_ERROR if there was not
+   enough memory, Z_STREAM_ERROR if the source stream state was inconsistent
+   (such as zalloc being NULL). msg is left unchanged in both source and
+   destination.
+*/
+
+extern int ext2_deflateReset OF((z_streamp strm));
+/*
+     This function is equivalent to deflateEnd followed by deflateInit,
+   but does not free and reallocate all the internal compression state.
+   The stream will keep the same compression level and any other attributes
+   that may have been set by deflateInit2.
+
+      deflateReset returns Z_OK if success, or Z_STREAM_ERROR if the source
+   stream state was inconsistent (such as zalloc or state being NULL).
+*/
+
+extern int ext2_deflateParams OF((z_streamp strm,
+				      int level,
+				      int strategy));
+/*
+     Dynamically update the compression level and compression strategy.  The
+   interpretation of level and strategy is as in deflateInit2.  This can be
+   used to switch between compression and straight copy of the input data, or
+   to switch to a different kind of input data requiring a different
+   strategy. If the compression level is changed, the input available so far
+   is compressed with the old level (and may be flushed); the new level will
+   take effect only at the next call of deflate().
+
+     Before the call of deflateParams, the stream state must be set as for
+   a call of deflate(), since the currently available input may have to
+   be compressed and flushed. In particular, strm->avail_out must be non-zero.
+
+     deflateParams returns Z_OK if success, Z_STREAM_ERROR if the source
+   stream state was inconsistent or if a parameter was invalid, Z_BUF_ERROR
+   if strm->avail_out was zero.
+*/
+
+/*   
+extern int inflateInit2 OF((z_streamp strm,
+                                     int  windowBits));
+
+     This is another version of inflateInit with an extra parameter. The
+   fields next_in, avail_in, zalloc, zfree and opaque must be initialized
+   before by the caller.
+
+     The windowBits parameter is the base two logarithm of the maximum window
+   size (the size of the history buffer).  It should be in the range 8..15 for
+   this version of the library. The default value is 15 if inflateInit is used
+   instead. If a compressed stream with a larger window size is given as
+   input, inflate() will return with the error code Z_DATA_ERROR instead of
+   trying to allocate a larger window.
+
+      inflateInit2 returns Z_OK if success, Z_MEM_ERROR if there was not enough
+   memory, Z_STREAM_ERROR if a parameter is invalid (such as a negative
+   memLevel). msg is set to null if there is no error message.  inflateInit2
+   does not perform any decompression apart from reading the zlib header if
+   present: this will be done by inflate(). (So next_in and avail_in may be
+   modified, but next_out and avail_out are unchanged.)
+*/
+
+extern int ext2_inflateSetDictionary OF((z_streamp strm,
+                                             const Byte *dictionary,
+                                             uInt  dictLength));
+/*
+     Initializes the decompression dictionary from the given uncompressed byte
+   sequence. This function must be called immediately after a call of inflate
+   if this call returned Z_NEED_DICT. The dictionary chosen by the compressor
+   can be determined from the Adler32 value returned by this call of
+   inflate. The compressor and decompressor must use exactly the same
+   dictionary (see deflateSetDictionary).
+
+     inflateSetDictionary returns Z_OK if success, Z_STREAM_ERROR if a
+   parameter is invalid (such as NULL dictionary) or the stream state is
+   inconsistent, Z_DATA_ERROR if the given dictionary doesn't match the
+   expected one (incorrect Adler32 value). inflateSetDictionary does not
+   perform any decompression: this will be done by subsequent calls of
+   inflate().
+*/
+
+extern int ext2_inflateSync OF((z_streamp strm));
+/* 
+    Skips invalid compressed data until a full flush point (see above the
+  description of deflate with Z_FULL_FLUSH) can be found, or until all
+  available input is skipped. No output is provided.
+
+    inflateSync returns Z_OK if a full flush point has been found, Z_BUF_ERROR
+  if no more input was provided, Z_DATA_ERROR if no flush point has been found,
+  or Z_STREAM_ERROR if the stream structure was inconsistent. In the success
+  case, the application may save the current current value of total_in which
+  indicates where valid compressed data was found. In the error case, the
+  application may repeatedly call inflateSync, providing more input each time,
+  until success or end of the input data.
+*/
+
+extern int ext2_inflateReset OF((z_streamp strm));
+/*
+     This function is equivalent to inflateEnd followed by inflateInit,
+   but does not free and reallocate all the internal decompression state.
+   The stream will keep attributes that may have been set by inflateInit2.
+
+      inflateReset returns Z_OK if success, or Z_STREAM_ERROR if the source
+   stream state was inconsistent (such as zalloc or state being NULL).
+*/
+
+
+                        /* utility functions */
+
+/*
+     The following utility functions are implemented on top of the
+   basic stream-oriented functions. To simplify the interface, some
+   default options are assumed (compression level and memory usage,
+   standard memory allocation functions). The source code of these
+   utility functions can easily be modified if you need special options.
+*/
+
+extern int compress OF((Byte *dest,   uLong *destLen,
+                                 const Byte *source, uLong sourceLen));
+/*
+     Compresses the source buffer into the destination buffer.  sourceLen is
+   the byte length of the source buffer. Upon entry, destLen is the total
+   size of the destination buffer, which must be at least 0.1% larger than
+   sourceLen plus 12 bytes. Upon exit, destLen is the actual size of the
+   compressed buffer.
+     This function can be used to compress a whole file at once if the
+   input file is mmap'ed.
+     compress returns Z_OK if success, Z_MEM_ERROR if there was not
+   enough memory, Z_BUF_ERROR if there was not enough room in the output
+   buffer.
+*/
+
+extern int compress2 OF((Byte *dest,   uLong *destLen,
+                                  const Byte *source, uLong sourceLen,
+                                  int level));
+/*
+     Compresses the source buffer into the destination buffer. The level
+   parameter has the same meaning as in deflateInit.  sourceLen is the byte
+   length of the source buffer. Upon entry, destLen is the total size of the
+   destination buffer, which must be at least 0.1% larger than sourceLen plus
+   12 bytes. Upon exit, destLen is the actual size of the compressed buffer.
+
+     compress2 returns Z_OK if success, Z_MEM_ERROR if there was not enough
+   memory, Z_BUF_ERROR if there was not enough room in the output buffer,
+   Z_STREAM_ERROR if the level parameter is invalid.
+*/
+
+extern int uncompress OF((Byte *dest,   uLong *destLen,
+                                   const Byte *source, uLong sourceLen));
+/*
+     Decompresses the source buffer into the destination buffer.  sourceLen is
+   the byte length of the source buffer. Upon entry, destLen is the total
+   size of the destination buffer, which must be large enough to hold the
+   entire uncompressed data. (The size of the uncompressed data must have
+   been saved previously by the compressor and transmitted to the decompressor
+   by some mechanism outside the scope of this compression library.)
+   Upon exit, destLen is the actual size of the compressed buffer.
+     This function can be used to decompress a whole file at once if the
+   input file is mmap'ed.
+
+     uncompress returns Z_OK if success, Z_MEM_ERROR if there was not
+   enough memory, Z_BUF_ERROR if there was not enough room in the output
+   buffer, or Z_DATA_ERROR if the input data was corrupted.
+*/
+
+
+typedef voidp gzFile;
+
+extern gzFile gzopen  OF((const char *path, const char *mode));
+/*
+     Opens a gzip (.gz) file for reading or writing. The mode parameter
+   is as in fopen ("rb" or "wb") but can also include a compression level
+   ("wb9") or a strategy: 'f' for filtered data as in "wb6f", 'h' for
+   Huffman only compression as in "wb1h". (See the description
+   of deflateInit2 for more information about the strategy parameter.)
+
+     gzopen can be used to read a file which is not in gzip format; in this
+   case gzread will directly read from the file without decompression.
+
+     gzopen returns NULL if the file could not be opened or if there was
+   insufficient memory to allocate the (de)compression state; errno
+   can be checked to distinguish the two cases (if errno is zero, the
+   zlib error is Z_MEM_ERROR).  */
+
+extern gzFile gzdopen  OF((int fd, const char *mode));
+/*
+     gzdopen() associates a gzFile with the file descriptor fd.  File
+   descriptors are obtained from calls like open, dup, creat, pipe or
+   fileno (in the file has been previously opened with fopen).
+   The mode parameter is as in gzopen.
+     The next call of gzclose on the returned gzFile will also close the
+   file descriptor fd, just like fclose(fdopen(fd), mode) closes the file
+   descriptor fd. If you want to keep fd open, use gzdopen(dup(fd), mode).
+     gzdopen returns NULL if there was insufficient memory to allocate
+   the (de)compression state.
+*/
+
+extern int gzsetparams OF((gzFile file, int level, int strategy));
+/*
+     Dynamically update the compression level or strategy. See the description
+   of deflateInit2 for the meaning of these parameters.
+     gzsetparams returns Z_OK if success, or Z_STREAM_ERROR if the file was not
+   opened for writing.
+*/
+
+extern int    gzread  OF((gzFile file, voidp buf, unsigned len));
+/*
+     Reads the given number of uncompressed bytes from the compressed file.
+   If the input file was not in gzip format, gzread copies the given number
+   of bytes into the buffer.
+     gzread returns the number of uncompressed bytes actually read (0 for
+   end of file, -1 for error). */
+
+extern int    gzwrite OF((gzFile file, 
+				   const voidp buf, unsigned len));
+/*
+     Writes the given number of uncompressed bytes into the compressed file.
+   gzwrite returns the number of uncompressed bytes actually written
+   (0 in case of error).
+*
+
+extern int ZEXPORTVA   gzprintf OF((gzFile file, const char *format, ...));
+*
+     Converts, formats, and writes the args to the compressed file under
+   control of the format string, as in fprintf. gzprintf returns the number of
+   uncompressed bytes actually written (0 in case of error).
+*/
+
+extern int gzputs OF((gzFile file, const char *s));
+/*
+      Writes the given null-terminated string to the compressed file, excluding
+   the terminating null character.
+      gzputs returns the number of characters written, or -1 in case of error.
+*/
+
+extern char * gzgets OF((gzFile file, char *buf, int len));
+/*
+      Reads bytes from the compressed file until len-1 characters are read, or
+   a newline character is read and transferred to buf, or an end-of-file
+   condition is encountered.  The string is then terminated with a null
+   character.
+      gzgets returns buf, or Z_NULL in case of error.
+*/
+
+extern int    gzputc OF((gzFile file, int c));
+/*
+      Writes c, converted to an unsigned char, into the compressed file.
+   gzputc returns the value that was written, or -1 in case of error.
+*/
+
+extern int    gzgetc OF((gzFile file));
+/*
+      Reads one byte from the compressed file. gzgetc returns this byte
+   or -1 in case of end of file or error.
+*/
+
+extern int    gzflush OF((gzFile file, int flush));
+/*
+     Flushes all pending output into the compressed file. The parameter
+   flush is as in the deflate() function. The return value is the zlib
+   error number (see function gzerror below). gzflush returns Z_OK if
+   the flush parameter is Z_FINISH and all output could be flushed.
+     gzflush should be called only when strictly necessary because it can
+   degrade compression.
+*
+
+extern z_off_t    gzseek OF((gzFile file,
+				      z_off_t offset, int whence));
+* 
+      Sets the starting position for the next gzread or gzwrite on the
+   given compressed file. The offset represents a number of bytes in the
+   uncompressed data stream. The whence parameter is defined as in lseek(2);
+   the value SEEK_END is not supported.
+     If the file is opened for reading, this function is emulated but can be
+   extremely slow. If the file is opened for writing, only forward seeks are
+   supported; gzseek then compresses a sequence of zeroes up to the new
+   starting position.
+
+      gzseek returns the resulting offset location as measured in bytes from
+   the beginning of the uncompressed stream, or -1 in case of error, in
+   particular if the file is opened for writing and the new starting position
+   would be before the current position.
+*/
+
+extern int    gzrewind OF((gzFile file));
+/*
+     Rewinds the given file. This function is supported only for reading.
+
+   gzrewind(file) is equivalent to (int)gzseek(file, 0L, SEEK_SET)
+*
+
+extern z_off_t    gztell OF((gzFile file));
+*
+     Returns the starting position for the next gzread or gzwrite on the
+   given compressed file. This position represents a number of bytes in the
+   uncompressed data stream.
+
+   gztell(file) is equivalent to gzseek(file, 0L, SEEK_CUR)
+*/
+
+extern int gzeof OF((gzFile file));
+/*
+     Returns 1 when EOF has previously been detected reading the given
+   input stream, otherwise zero.
+*/
+
+extern int    gzclose OF((gzFile file));
+/*
+     Flushes all pending output if necessary, closes the compressed file
+   and deallocates all the (de)compression state. The return value is the zlib
+   error number (see function gzerror below).
+*/
+
+extern const char * gzerror OF((gzFile file, int *errnum));
+/*
+     Returns the error message for the last error which occurred on the
+   given compressed file. errnum is set to zlib error number. If an
+   error occurred in the file system and not in the compression library,
+   errnum is set to Z_ERRNO and the application may consult errno
+   to get the exact error code.
+*/
+
+                        /* checksum functions */
+
+/*
+     These functions are not related to compression but are exported
+   anyway because they might be useful in applications using the
+   compression library.
+*/
+
+extern uLong crc32   OF((uLong crc, const Byte *buf, uInt len));
+/*
+     Update a running crc with the bytes buf[0..len-1] and return the updated
+   crc. If buf is NULL, this function returns the required initial value
+   for the crc. Pre- and post-conditioning (one's complement) is performed
+   within this function so it shouldn't be done by the application.
+   Usage example:
+
+     uLong crc = crc32(0L, Z_NULL, 0);
+
+     while (read_buffer(buffer, length) != EOF) {
+       crc = crc32(crc, buffer, length);
+     }
+     if (crc != original_crc) error();
+*/
+
+
+                        /* various hacks, don't look :) */
+
+/* deflateInit and inflateInit are macros to allow checking the zlib version
+ * and the compiler's view of z_stream:
+ */
+extern int ext2_deflateInit_ OF((z_streamp strm, int level,
+                                     const char *version, int stream_size));
+extern int ext2_inflateInit_ OF((z_streamp strm,
+                                     const char *version, int stream_size));
+extern int ext2_deflateInit2_ OF((z_streamp strm, int  level, int  method,
+                                      int windowBits, int memLevel,
+                                      int strategy, const char *version,
+                                      int stream_size));
+extern int ext2_inflateInit2_ OF((z_streamp strm, int  windowBits,
+                                      const char *version, int stream_size));
+#define ext2_deflateInit(strm, level) \
+        ext2_deflateInit_((strm), (level),       ZLIB_VERSION, sizeof(z_stream))
+#define ext2_inflateInit(strm) \
+        ext2_inflateInit_((strm),                ZLIB_VERSION, sizeof(z_stream))
+#define ext2_deflateInit2(strm, level, method, windowBits, memLevel, strategy) \
+        ext2_deflateInit2_((strm),(level),(method),(windowBits),(memLevel),\
+                      (strategy),           ZLIB_VERSION, sizeof(z_stream))
+#define ext2_inflateInit2(strm, windowBits) \
+        ext2_inflateInit2_((strm), (windowBits), ZLIB_VERSION, sizeof(z_stream))
+
+
+#if !defined(_Z_UTIL_H) && !defined(NO_DUMMY_DECL)
+    struct internal_state {int dummy;}; /* hack for buggy compilers */
+#endif
+
+extern const char   * zError           OF((int err));
+extern int            ext2_inflateSyncPoint OF((z_streamp z));
+extern const uLong  * get_crc_table    OF((void));
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _ZLIB_H */
diff -pruN linux-2.6.18.5.org/fs/ext2/gzip/zutil.h linux-2.6.18.5/fs/ext2/gzip/zutil.h
--- linux-2.6.18.5.org/fs/ext2/gzip/zutil.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/gzip/zutil.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,241 @@
+/* zutil.h -- internal interface and configuration of the compression library
+ * Copyright (C) 1995-1998 Jean-loup Gailly.
+ * For conditions of distribution and use, see copyright notice in zlib.h
+ */
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the compression library and is
+   subject to change. Applications should only use zlib.h.
+ */
+
+/* @(#) $Id$ */
+
+#ifndef _Z_UTIL_H
+#define _Z_UTIL_H
+
+#include "zlib.h"
+
+#ifndef __KERNEL__
+# ifdef STDC
+#  include <stddef.h>
+#  include <string.h>
+#  include <stdlib.h>
+# endif
+
+# ifdef NO_ERRNO_H
+    extern int errno;
+# else
+#   include <errno.h>
+# endif
+#endif
+
+#ifndef local
+#  define local static
+#endif
+/* compile with -Dlocal if your debugger can't find static symbols */
+
+typedef unsigned char  uch;
+typedef unsigned short ush;
+typedef unsigned long  ulg;
+
+extern const char *z_errmsg[10]; /* indexed by 2-zlib_error */
+/* (size given to avoid silly warnings with Visual C++) */
+
+#define ERR_MSG(err) z_errmsg[Z_NEED_DICT-(err)]
+
+#define ERR_RETURN(strm,err) \
+  return (strm->msg = (char*)ERR_MSG(err), (err))
+/* To be used only when the state is known to be valid */
+
+        /* common constants */
+
+#ifndef DEF_WBITS
+#  define DEF_WBITS MAX_WBITS
+#endif
+/* default windowBits for decompression. MAX_WBITS is for compression only */
+
+#if MAX_MEM_LEVEL >= 8
+#  define DEF_MEM_LEVEL 8
+#else
+#  define DEF_MEM_LEVEL  MAX_MEM_LEVEL
+#endif
+/* default memLevel */
+
+#define STORED_BLOCK 0
+#define STATIC_TREES 1
+#define DYN_TREES    2
+/* The three kinds of block type */
+
+#define MIN_MATCH  3
+#define MAX_MATCH  258
+/* The minimum and maximum match lengths */
+
+#define PRESET_DICT 0x20 /* preset dictionary flag in zlib header */
+
+        /* target dependencies */
+
+#ifdef MSDOS
+#  define OS_CODE  0x00
+#  if defined(__TURBOC__) || defined(__BORLANDC__)
+#    if(__STDC__ == 1) && (defined(__LARGE__) || defined(__COMPACT__))
+       /* Allow compilation with ANSI keywords only enabled */
+       void _Cdecl farfree( void *block );
+       void *_Cdecl farmalloc( unsigned long nbytes );
+#    else
+#     include <alloc.h>
+#    endif
+#  else /* MSC or DJGPP */
+#    include <malloc.h>
+#  endif
+#endif
+
+#ifdef OS2
+#  define OS_CODE  0x06
+#endif
+
+#ifdef WIN32 /* Window 95 & Windows NT */
+#  define OS_CODE  0x0b
+#endif
+
+#if defined(VAXC) || defined(VMS)
+#  define OS_CODE  0x02
+#  define F_OPEN(name, mode) \
+     fopen((name), (mode), "mbc=60", "ctx=stm", "rfm=fix", "mrs=512")
+#endif
+
+#ifdef AMIGA
+#  define OS_CODE  0x01
+#endif
+
+#if defined(ATARI) || defined(atarist)
+#  define OS_CODE  0x05
+#endif
+
+#if defined(MACOS) || defined(TARGET_OS_MAC)
+#  define OS_CODE  0x07
+#  if defined(__MWERKS__) && __dest_os != __be_os && __dest_os != __win32_os
+#    include <unix.h> /* for fdopen */
+#  else
+#    ifndef fdopen
+#      define fdopen(fd,mode) NULL /* No fdopen() */
+#    endif
+#  endif
+#endif
+
+#ifdef __50SERIES /* Prime/PRIMOS */
+#  define OS_CODE  0x0F
+#endif
+
+#ifdef TOPS20
+#  define OS_CODE  0x0a
+#endif
+
+#if defined(_BEOS_) || defined(RISCOS)
+#  define fdopen(fd,mode) NULL /* No fdopen() */
+#endif
+
+#if (defined(_MSC_VER) && (_MSC_VER > 600))
+#  define fdopen(fd,type)  _fdopen(fd,type)
+#endif
+
+
+        /* Common defaults */
+
+#ifndef OS_CODE
+#  define OS_CODE  0x03  /* assume Unix */
+#endif
+
+#ifndef F_OPEN
+#  define F_OPEN(name, mode) fopen((name), (mode))
+#endif
+
+         /* functions */
+
+#ifdef HAVE_STRERROR
+   extern char *strerror OF((int));
+#  define zstrerror(errnum) strerror(errnum)
+#else
+#  define zstrerror(errnum) ""
+#endif
+
+#if defined(pyr)
+#  define NO_MEMCPY
+#endif
+#if defined(SMALL_MEDIUM) && !defined(_MSC_VER) && !defined(__SC__)
+ /* Use our own functions for small and medium model with MSC <= 5.0.
+  * You may have to use the same strategy for Borland C (untested).
+  * The __SC__ check is for Symantec.
+  */
+#  define NO_MEMCPY
+#endif
+#if defined(STDC) && !defined(HAVE_MEMCPY) && !defined(NO_MEMCPY)
+#  define HAVE_MEMCPY
+#endif
+#ifdef HAVE_MEMCPY
+#  ifdef SMALL_MEDIUM /* MSDOS small or medium model */
+#    define zmemcpy _fmemcpy
+#    define zmemcmp _fmemcmp
+#    define zmemzero(dest, len) _fmemset(dest, 0, len)
+#  else
+#    define zmemcpy memcpy
+#    define zmemcmp memcmp
+#    define zmemzero(dest, len) memset(dest, 0, len)
+#  endif
+#else
+   extern void zmemcpy  OF((Byte* dest, const Byte* source, uInt len));
+   extern int  zmemcmp  OF((const Byte* s1, const Byte* s2, uInt len));
+   extern void zmemzero OF((Byte* dest, uInt len));
+#endif
+
+/* Diagnostic functions */
+#ifdef DEBUG
+#  include <stdio.h>
+   extern int z_verbose;
+   extern void z_error    OF((char *m));
+#  define Assert(cond,msg) {if(!(cond)) z_error(msg);}
+#  define Trace(x) {if (z_verbose>=0) fprintf x ;}
+#  define Tracev(x) {if (z_verbose>0) fprintf x ;}
+#  define Tracevv(x) {if (z_verbose>1) fprintf x ;}
+#  define Tracec(c,x) {if (z_verbose>0 && (c)) fprintf x ;}
+#  define Tracecv(c,x) {if (z_verbose>1 && (c)) fprintf x ;}
+#else
+#  define Assert(cond,msg)
+#  define Trace(x)
+#  define Tracev(x)
+#  define Tracevv(x)
+#  define Tracec(c,x)
+#  define Tracecv(c,x)
+#endif
+
+
+typedef uLong (*check_func) OF((uLong check, const Byte *buf,
+				       uInt len));
+voidp  ext2_zcalloc OF((voidp opaque, unsigned items, unsigned size));
+void   ext2_zcfree  OF((voidp opaque, voidp ptr));
+
+extern uLong ext2_adler32 OF((uLong adler, const Byte *buf, uInt len));
+
+/*
+     Update a running Adler-32 checksum with the bytes buf[0..len-1] and
+   return the updated checksum. If buf is NULL, this function returns
+   the required initial value for the checksum.
+   An Adler-32 checksum is almost as reliable as a CRC32 but can be computed
+   much faster. Usage example:
+
+     uLong adler = adler32(0L, Z_NULL, 0);
+
+     while (read_buffer(buffer, length) != EOF) {
+       adler = adler32(adler, buffer, length);
+     }
+     if (adler != original_adler) error();
+*/
+
+#define zcalloc ext2_zcalloc
+#define zcfree ext2_zcfree
+
+#define ZALLOC(strm, items, size) \
+           (*((strm)->zalloc))((strm)->opaque, (items), (size))
+#define ZFREE(strm, addr)  (*((strm)->zfree))((strm)->opaque, (voidp)(addr))
+#define TRY_FREE(s, p) {if (p) ZFREE(s, p);}
+
+#endif /* _Z_UTIL_H */
diff -pruN linux-2.6.18.5.org/fs/ext2/ialloc.c linux-2.6.18.5/fs/ext2/ialloc.c
--- linux-2.6.18.5.org/fs/ext2/ialloc.c	2006-12-04 13:31:58.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/ialloc.c	2007-01-24 10:07:06.000000000 -0800
@@ -12,6 +12,19 @@
  *        David S. Miller (davem@caip.rutgers.edu), 1995
  */
 
+/*
+ *  Copyright (C) 1995  Antoine Dumesnil de Maricourt (dumesnil@etca.fr)
+ *    (transparent compression code)
+ */
+
+/*
+ *  Copyright (C) 2001 Alcatel Business Systems - R&D Illkirch
+ *    (transparent compression code)
+ *  Pierre Peiffer (pierre.peiffer@sxb.bsf.alcatel.fr),
+ *  Denis Richard (denis.richard@sxb.bsf.alcatel.fr)
+ *  Adapted from patch e2compr-0.4.39-patch-2.2.18 .
+ */
+
 #include <linux/quotaops.h>
 #include <linux/sched.h>
 #include <linux/backing-dev.h>
@@ -20,6 +33,7 @@
 #include "ext2.h"
 #include "xattr.h"
 #include "acl.h"
+#include "debug.h"
 
 /*
  * ialloc.c contains the inodes allocation and deallocation routines
@@ -579,6 +593,17 @@ got:
 	inode->i_mtime = inode->i_atime = inode->i_ctime = CURRENT_TIME_SEC;
 	memset(ei->i_data, 0, sizeof(ei->i_data));
 	ei->i_flags = EXT2_I(dir)->i_flags & ~EXT2_BTREE_FL;
+#ifdef CONFIG_EXT2_COMPRESS
+	/*
+	 *      The EXT2_COMPR flag is inherited from the parent
+	 *      directory as well as the cluster size and the compression
+	 *      algorithm.
+	 */
+	ei->i_log2_clu_nblocks = EXT2_I(dir)->i_log2_clu_nblocks;
+	ei->i_clu_nblocks = EXT2_I(dir)->i_clu_nblocks;
+	ei->i_compr_method = EXT2_I(dir)->i_compr_method;
+	ei->i_compr_flags = 0;
+#endif
 	if (S_ISLNK(mode))
 		ei->i_flags &= ~(EXT2_IMMUTABLE_FL|EXT2_APPEND_FL);
 	/* dirsync is only applied to directories */
diff -pruN linux-2.6.18.5.org/fs/ext2/inode.c linux-2.6.18.5/fs/ext2/inode.c
--- linux-2.6.18.5.org/fs/ext2/inode.c	2006-12-04 13:31:58.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/inode.c	2007-03-18 01:33:40.000000000 -0700
@@ -22,6 +22,23 @@
  *  Assorted race fixes, rewrite of ext2_get_block() by Al Viro, 2000
  */
 
+/*
+ *  Copyright (C) 1995  Antoine Dumesnil de Maricourt (dumesnil@etca.fr)
+ *    (transparent compression code)
+ */
+
+/*
+ *  Copyright (C) 2001 Alcatel Business Systems - R&D Illkirch FRANCE
+ *
+ *    Transparent compression code for 2.4 kernel.
+ *
+ *  Denis Richard (denis.richard@sxb.bsf.alcatel.fr)
+ *  Pierre Peiffer (pierre.peiffer@sxb.bsf.alcatel.fr)
+ *
+ *  Adapted from patch e2compr-0.4.39-patch-2.2.18 .
+ */
+
+#include <linux/kmod.h>
 #include <linux/smp_lock.h>
 #include <linux/time.h>
 #include <linux/highuid.h>
@@ -31,12 +48,18 @@
 #include <linux/writeback.h>
 #include <linux/buffer_head.h>
 #include <linux/mpage.h>
-#include "ext2.h"
+#include <linux/ext2_fs_c.h>
 #include "acl.h"
 #include "xip.h"
+#include "debug.h"
 
+#ifdef CONFIG_EXT2_COMPRESS
+MODULE_AUTHOR("Remy Card and others; compression extension by Antoine Dumesnil de Maricourt, Peter Moulder, and others");
+MODULE_DESCRIPTION("Second Extended Filesystem with transparent compression");
+#else
 MODULE_AUTHOR("Remy Card and others");
 MODULE_DESCRIPTION("Second Extended Filesystem");
+#endif
 MODULE_LICENSE("GPL");
 
 static int ext2_update_inode(struct inode * inode, int do_sync);
@@ -62,8 +85,22 @@ static inline int ext2_inode_is_fast_sym
  */
 void ext2_put_inode(struct inode *inode)
 {
-	if (!is_bad_inode(inode))
-		ext2_discard_prealloc(inode);
+#ifdef CONFIG_EXT2_COMPRESS
+	if (S_ISREG (inode->i_mode)
+	    && inode->i_nlink
+	    && (EXT2_I(inode)->i_compr_flags & EXT2_CLEANUP_FL)) {
+# ifdef EXT2_COMPR_REPORT_PUT
+		printk(KERN_DEBUG
+		    "put_inode: pid=%d, i_ino=%ld, "
+		    "compr_flags=0x%x, i_count=%d\n",
+		    current->pid, inode->i_ino,
+		    EXT2_I(inode)->i_compr_flags,
+		    atomic_read(&inode->i_count));
+# endif
+		(void) ext2_cleanup_compressed_inode (inode);
+	}
+#endif
+	ext2_discard_prealloc(inode);
 }
 
 /*
@@ -136,7 +173,7 @@ static int ext2_alloc_block (struct inod
 				 &ei->i_prealloc_count,
 				 &ei->i_prealloc_block, err);
 		else
-			result = ext2_new_block(inode, goal, NULL, NULL, err);
+			result = ext2_new_block (inode, goal, 0, 0, err);
 	}
 #else
 	result = ext2_new_block (inode, goal, 0, 0, err);
@@ -274,7 +311,7 @@ static Indirect *ext2_get_branch(struct 
 	*err = 0;
 	/* i_data is not going away, no lock needed */
 	add_chain (chain, NULL, EXT2_I(inode)->i_data + *offsets);
-	if (!p->key)
+	if (HOLE_BLKADDR(p->key))
 		goto no_block;
 	while (--depth) {
 		bh = sb_bread(sb, le32_to_cpu(p->key));
@@ -285,7 +322,7 @@ static Indirect *ext2_get_branch(struct 
 			goto changed;
 		add_chain(++p, bh, (__le32*)bh->b_data + *++offsets);
 		read_unlock(&EXT2_I(inode)->i_meta_lock);
-		if (!p->key)
+		if (HOLE_BLKADDR(p->key))
 			goto no_block;
 	}
 	return NULL;
@@ -331,7 +368,7 @@ static unsigned long ext2_find_near(stru
 
 	/* Try to find previous block */
 	for (p = ind->p - 1; p >= start; p--)
-		if (*p)
+		if (!HOLE_BLKADDR(*p))
 			return le32_to_cpu(*p);
 
 	/* No such thing, so let's try location of indirect block */
@@ -444,13 +481,13 @@ static int ext2_alloc_branch(struct inod
 			err = -EIO;
 			break;
 		}
-		lock_buffer(bh);
+		if (!buffer_uptodate(bh))
+			wait_on_buffer(bh);
 		memset(bh->b_data, 0, blocksize);
 		branch[n].bh = bh;
 		branch[n].p = (__le32 *) bh->b_data + offsets[n];
 		*branch[n].p = branch[n].key;
 		set_buffer_uptodate(bh);
-		unlock_buffer(bh);
 		mark_buffer_dirty_inode(bh, inode);
 		/* We used to sync bh here if IS_SYNC(inode).
 		 * But we now rely upon generic_osync_inode()
@@ -500,7 +537,7 @@ static inline int ext2_splice_branch(str
 	/* Verify that place we are splicing to is still there and vacant */
 
 	write_lock(&ei->i_meta_lock);
-	if (!verify_chain(chain, where-1) || *where->p)
+	if (!verify_chain(chain, where-1) || !HOLE_BLKADDR(*where->p))
 		goto changed;
 
 	/* That's it */
@@ -560,7 +597,6 @@ int ext2_get_block(struct inode *inode, 
 
 reread:
 	partial = ext2_get_branch(inode, depth, offsets, chain, &err);
-
 	/* Simplest case - block found, no allocation needed */
 	if (!partial) {
 got_it:
@@ -619,27 +655,433 @@ out:
 
 changed:
 	while (partial > chain) {
-		brelse(partial->bh);
+		bforget(partial->bh);
+//		brelse(partial->bh);
 		partial--;
 	}
 	goto reread;
 }
 
+#ifdef CONFIG_EXT2_COMPRESS
+/*
+ *    Readpage method that will take care of decompression.
+ */
+/* effic: I (pjm) think tht at present, reading a 32KB cluster 4KB at
+   a time does `decompress 4KB' for the first 4KB, then `decompress
+   8KB' for the second, and so on.  See if we can provide the page
+   cache with all the pages in a cluster.  The problem is, we don't
+   want to erase anything tht hasn't been written to disk, so we can't
+   just call update_vm_cache().  The plan at present is to remember
+   what the contents of ext2_rd_wa.u come from, and don't bother
+   decompressing anything if the working area already contains the
+   right data.  However, this is only a win where adjacent calls to
+   ext2_decompress_blocks() request the same cluster.  We could force
+   that by copying some code from generic_file_read() (but check for
+   deadlocks before doing anything like that), but instead I'm taking
+   the more passive approach of hoping for the best. */
+static int ext2_readpage(struct file *file, struct page *page)
+{
+	struct inode *inode = page->mapping->host;
+	struct page *pg[EXT2_MAX_CLUSTER_PAGES], *epg[EXT2_MAX_CLUSTER_PAGES];
+	u32 cluster0, max_cluster;
+	int i, blockOfCluster, blocksToDo, npg;
+	const int inc = PAGE_SIZE >> inode->i_sb->s_blocksize_bits;
+	struct ext2_inode_info *ei = EXT2_I(page->mapping->host);
+
+	/* For directories, fall out through default routine */
+	if (S_ISDIR(inode->i_mode))
+		return block_read_full_page(page,ext2_get_block);
+
+	/* The semaphore prevents us trying to compress and decompress
+	   the cluster at the same time, or compress a cluster in the
+	   middle of reading it (thinking it to be uncompressed).
+
+	   You may not like the fact that we hold the semaphore across
+	   readpage (given that it isn't held without e2compr compiled
+	   in), but it does guarantee that we won't compress the
+	   cluster during readpage.  (OTOH, it's unlikely, if not
+	   impossible, for someone to ,compress a cluster and rewrite
+	   the blocks` before the readpage completes.) */
+	/* This procedure used to have `#ifndef EXT2_LOCK_BUFFERS'
+	   around all the semaphore stuff, and unlocked each buffer
+	   before brelsing them ifdef EXT2_LOCK_BUFFERS.  I (pjm,
+	   1998-01-20) have removed that because (a) EXT2_LOCK_BUFFERS
+	   isn't #defined anywhere, and doesn't appear outside of this
+	   function, and (b) I haven't looked at what effect locking
+	   the buffers has.  You may like to reintroduce the idea of
+	   buffer locking to this function if you're more familiar
+	   with buffer locking than I, and believe that the full i_sem
+	   isn't necessary to protect from races (people seeing raw
+	   compressed data) between readpage and ext2_file_write(),
+	   ext2_compress_cluster() and ext2_truncate(). */
+	unlock_page(page);
+	mutex_lock(&inode->i_mutex);
+	assert (atomic_read(&inode->i_mutex.count) <= 0); /* i.e. mutex_lock */
+
+	if (!(ei->i_flags & EXT2_COMPRBLK_FL)
+	    || ext2_compression_disabled(inode))
+		goto readpage_uncompressed;
+
+	/* */
+	{
+		register u32 blockOfFile
+		    = (page->index << PAGE_CACHE_SHIFT) >> inode->i_sb->s_blocksize_bits;
+
+		blocksToDo = PAGE_SIZE >> inode->i_sb->s_blocksize_bits;
+		cluster0 = ext2_block_to_cluster(inode, blockOfFile);
+		max_cluster = ext2_block_to_cluster
+		    (inode, blockOfFile + blocksToDo - 1);
+		blockOfCluster
+		    = blockOfFile - ext2_cluster_block0(inode, cluster0);
+	}
+
+
+	/* Check if any part of the requested area contains part of a
+	   compressed cluster.  If not, we can use default ext2_readpage().
+
+	   (Note that we don't have to worry about a cluster becoming
+	   compressed in the meantime, because we have the semaphore.)
+
+	   A page can cover up to 9 clusters.  (The maximum can only
+	   occur with 32KB pages, 4KB clusters, and a non-page-aligned
+	   offset.  Thanks go to Kurt Fitzner for reporting that
+	   page offsets needn't be aligned; see generic_file_mmap().)  */
+
+	/* */
+	{
+	int isCmp[(PAGE_SIZE >> 12) + 1];
+	u8 *dst;
+	unsigned clu_ix;
+
+	assert (max_cluster - cluster0 < sizeof(isCmp)/sizeof(*isCmp));
+	for (clu_ix = 0; cluster0 + clu_ix <= max_cluster; clu_ix++) {
+		isCmp[clu_ix] = ext2_cluster_is_compressed_fn (inode, cluster0 + clu_ix);
+		if (isCmp[clu_ix] < 0)
+			goto io_error;
+	}
+	for (clu_ix = 0; cluster0 + clu_ix <= max_cluster; clu_ix++)
+		if (isCmp[clu_ix] > 0)
+			goto readpage_compressed;
+	/* fall through */
+	readpage_uncompressed:
+	/* */
+	{
+		int rc=0;
+
+		lock_page(page);
+
+		/* Did somebody else fill it already? */
+		if (PageUptodate(page))
+			unlock_page(page);
+		else
+			rc = block_read_full_page(page,ext2_get_block);
+		mutex_unlock(&inode->i_mutex);
+		return rc;
+	}
+
+	readpage_compressed:
+	if (ext2_rd_wa == NULL)
+		goto io_error;
+
+	/* Copied from block_read_full_page */
+/*	if (!PageLocked(page)) */
+/*	PAGE_BUG(page); */
+	lock_page(page);
+	if (PageUptodate(page)) {
+		unlock_page(page);
+		mutex_unlock(&inode->i_mutex);
+		return(0);
+	}
+	get_page(page);
+
+	ClearPageUptodate(page);
+	ClearPageError(page);
+
+	dst = (u8 *) page_address(page);
+	for (clu_ix = 0; cluster0 + clu_ix <= max_cluster; clu_ix++) {
+		struct buffer_head *bh[EXT2_MAX_CLUSTER_BLOCKS];
+		int nbh, blocksThisClu;
+
+/*		clear_bit(PG_locked, &page->flags); */
+		npg = ext2_cluster_npages(inode, cluster0 + clu_ix);
+		nbh =  ext2_get_cluster_pages(inode, cluster0 + clu_ix, pg, page, 0);
+		if (nbh <= 0) {
+			for (i = 0; i < EXT2_MAX_CLUSTER_PAGES; i++)
+			epg[i] = NULL;
+			goto out;
+		}
+		nbh =  ext2_get_cluster_extra_pages(inode, cluster0 + clu_ix, pg, epg);
+		if (nbh <= 0) {
+			for (i = 0; i < EXT2_MAX_CLUSTER_PAGES; i++)
+			epg[i] = NULL;
+			goto out;
+		}
+		nbh = ext2_get_cluster_blocks(inode, cluster0 + clu_ix, bh, pg, epg, 0);
+		if (nbh <= 0)
+			goto out;
+		/* How many blocks (including holes) we need from this cluster. */
+		{
+			blocksThisClu = (ext2_cluster_nblocks(inode, cluster0 +
+			    clu_ix) - blockOfCluster);
+			if (blocksThisClu > blocksToDo)
+				blocksThisClu = blocksToDo;
+		}
+
+		if (isCmp[clu_ix]) {
+			u8 const *src;
+			int n, nbytes_wanted;
+			struct ext2_cluster_head *head;
+			unsigned meth;
+# ifdef CONFIG_KMOD
+			unsigned alg;
+# endif
+
+			head = (struct ext2_cluster_head *) bh[0]->b_data;
+
+			/* jmr 1998-10-28 Hope this is the last time I'm moving this code.
+			 * Module loading must be done _before_ we lock wa, just think what
+			 * can happen if we reallocate wa when somebody else uses it...
+			 */
+			meth = head->method; /* only a byte, so no swabbing needed. */
+			if (meth >= EXT2_N_METHODS) {
+				ext2_warning(inode->i_sb,
+				    "illegal method id",
+				    "inode = %lu, id = %u",
+				    inode->i_ino, meth);
+				goto out;
+			}
+# ifdef CONFIG_KMOD
+			alg = ext2_method_table[meth].alg;
+			if (!ext2_algorithm_table[alg].avail) {
+				char str[32];
+
+				sprintf(str, "ext2-compr-%s", ext2_algorithm_table[alg].name);
+				request_module(str);
+			}
+# endif /* CONFIG_KMOD */
+
+			ext2_lock_rd_wa_uninterruptible();
+
+			/* Calculate nbytes_wanted. */
+			{
+				unsigned nblk_wanted, i;
+
+				/* We want to decompress the whole cluster */
+				nblk_wanted = ext2_cluster_nblocks(inode, cluster0 + clu_ix);
+				for (i = nblk_wanted; i != 0;)
+					if (((--i >> 3) < head->holemap_nbytes)
+					    && (head->holemap[i >> 3] & (1 << (i & 7))))
+						--nblk_wanted;
+				nbytes_wanted = (nblk_wanted
+				    << inode->i_sb->s_blocksize_bits);
+			}
+
+			/* Decompress. */
+			n = ext2_decompress_blocks(inode, bh, nbh, nbytes_wanted);
+			if (n < 0) {
+				assert (nbh >= 0);
+				ext2_unlock_rd_wa();
+				goto out;
+			}
+			if ((nbytes_wanted >= le32_to_cpu(head->ulen))
+			    && (n >= nbytes_wanted)) {
+				assert (ext2_rd_wa_ucontents.ino == ~0ul);
+				ext2_rd_wa_ucontents.ino = inode->i_ino;
+				ext2_rd_wa_ucontents.dev = inode->i_rdev;
+				ext2_rd_wa_ucontents.cluster = cluster0 + clu_ix;
+			}
+
+# ifdef EXT2_COMPR_REPORT_VERBOSE
+			if (ei->i_flags & EXT2_COMPR_FL)
+				printk(KERN_DEBUG "ext2: mmap %04x:%lu: blocksToDo=%d, blockOfCluster=%d, blocksThisClu=%d, clu_nblocks=%d\n",
+				    inode->i_rdev,
+				    inode->i_ino,
+				    blocksToDo,
+				    blockOfCluster,
+				    blocksThisClu,
+				    ext2_cluster_nblocks(inode, cluster0 + clu_ix));
+# endif
+
+			/* */
+			{
+			unsigned i;
+			int ipg;
+
+			i = ext2_cluster_nblocks(inode, cluster0 + clu_ix) - 1;
+			blockOfCluster = 0;
+			assert(n > 0);
+			src = ext2_rd_wa->u + nbytes_wanted - inode->i_sb->s_blocksize;
+			trace_e2c("ext2_readpage: copy data inc=%d blocksThisClu=%d, n=%d\n", inc, blocksThisClu, n);
+			for (ipg = npg - 1; ipg >= 0; ipg--) {
+				if (pg[ipg] == NULL) {
+				i -= inc;
+				src -= PAGE_SIZE;
+				continue;
+			}
+			if (((inode->i_size-1) >> PAGE_SHIFT) ==  pg[ipg]->index) {
+				n = ((inode->i_size-1) & (PAGE_SIZE -1)) >> inode->i_sb->s_blocksize_bits;
+				i -= ((blocksThisClu-1) - n);
+				src -= ((blocksThisClu-1) - n) << inode->i_sb->s_blocksize_bits;
+			} else {
+				n = blocksThisClu - 1;
+			}
+			if (PageUptodate(pg[ipg])) {
+				for (;n >= 0;n--, i--) {
+					if (((i >> 3) >= head->holemap_nbytes)
+					    || !(head->holemap[i >> 3] & (1 << (i & 7)))) {
+						src -= inode->i_sb->s_blocksize;
+					}
+				}
+			} else {
+				dst = (u8 *) page_address(pg[ipg]) + (n << inode->i_sb->s_blocksize_bits);
+				for (;
+				    n >= 0;
+				    n--, i--, dst -= inode->i_sb->s_blocksize) {
+					clear_buffer_dirty(bh[i]);
+					if (((i >> 3) >= head->holemap_nbytes)
+					    || !(head->holemap[i >> 3] & (1 << (i & 7)))) {
+						memcpy(dst, src, inode->i_sb->s_blocksize);
+						src -= inode->i_sb->s_blocksize;
+					} else {
+						memset (dst, 0, inode->i_sb->s_blocksize);
+					}
+/*					clear_bit(BH_Uptodate, &bh[i]->b_state); */
+				}
+				SetPageUptodate(pg[ipg]);
+			}
+			}
+			}
+			ext2_unlock_rd_wa();
+		} else {
+			/* Uncompressed cluster.  Just copy the data.  */
+			int n;
+
+# ifdef EXT2_COMPR_REPORT_VERBOSE
+			if (ei->i_flags & EXT2_COMPR_FL)
+				printk(KERN_DEBUG
+				    "ext2: mmap %lu: blocksToDo = %d, "
+				    "blockOfCluster = %d, clu_nblocks = %d\n",
+				    inode->i_ino, blocksToDo, blockOfCluster,
+				    ext2_cluster_nblocks(inode, cluster0 +
+				    clu_ix));
+# endif
+
+			for (n = 0;
+			    n < blocksThisClu;
+			    n++, dst += inode->i_sb->s_blocksize) {
+				if ((blockOfCluster + n < nbh)
+				    && (bh[blockOfCluster + n] != NULL))
+					memcpy(dst,
+					    bh[blockOfCluster + n]->b_data,
+					    inode->i_sb->s_blocksize);
+				else
+					memset(dst, 0, inode->i_sb->s_blocksize);
+			}
+			blockOfCluster = 0;
+		}
+
+		blocksToDo -= blocksThisClu;
+
+		for (i = 0; i < npg; i++) {
+			if (pg[i] == NULL)
+				break;
+			if (pg[i] == page)
+				continue;
+			unlock_page(pg[i]);
+			page_cache_release(pg[i]);
+			if (epg[i] != NULL) {
+				try_to_free_buffers(epg[i]);
+				unlock_page(epg[i]);
+				assert(page_count(epg[i]) == 1);
+				page_cache_release(epg[i]);
+			}
+		}
+	}
+	}
+
+	SetPageUptodate(page);
+/* Yabo Ding tweak 28 Feb 2005 */
+	unlock_page(page);
+	atomic_dec(&page->_count);
+	mutex_unlock(&inode->i_mutex);
+	return 0;
+
+	out:
+	for (i = 0; i < npg; i++) {
+		if (pg[i] == NULL)
+			break;
+		if (pg[i] == page)
+			continue;
+		unlock_page(pg[i]);
+		page_cache_release(pg[i]);
+		if (epg[i] != NULL) {
+			try_to_free_buffers(epg[i]);
+			unlock_page(epg[i]);
+			assert(page_count(epg[i]) == 1);
+			page_cache_release(epg[i]);
+		}
+	}
+
+	io_error:
+	set_bit(PG_error, &page->flags);
+/* Yabo Ding tweak 28 Feb 2005 */
+	unlock_page(page);
+	atomic_dec(&page->_count);
+	mutex_unlock(&inode->i_mutex);
+	return -EIO; /* it is tested in do_generic_file_read(), ...     */
+}
+#endif /* CONFIG_EXT2_COMPRESS */
+
 static int ext2_writepage(struct page *page, struct writeback_control *wbc)
 {
 	return block_write_full_page(page, ext2_get_block, wbc);
 }
 
+#ifndef CONFIG_EXT2_COMPRESS
 static int ext2_readpage(struct file *file, struct page *page)
 {
 	return mpage_readpage(page, ext2_get_block);
 }
+#endif
 
 static int
 ext2_readpages(struct file *file, struct address_space *mapping,
 		struct list_head *pages, unsigned nr_pages)
 {
+#ifdef CONFIG_EXT2_COMPRESS
+/*
+ * For now, just read each page into cache and don't worry about emitting BIOs.
+ * (whitpa 02 Aug 2004).
+ */
+
+	unsigned page_idx;
+	struct pagevec lru_pvec;
+
+	pagevec_init(&lru_pvec, 0);
+	for (page_idx = 0; page_idx < nr_pages; page_idx++) {
+		struct page *page = list_entry(pages->prev, struct page, lru);
+
+		prefetchw(&page->flags);
+		if (!list_empty(&page->lru))
+			list_del(&page->lru);
+		if (!add_to_page_cache(page, mapping, page->index,
+				GFP_KERNEL)) {
+			if (!PageUptodate(page))
+				(void) ext2_readpage(file, page);
+			else
+				unlock_page(page);
+			if (!pagevec_add(&lru_pvec, page))
+				__pagevec_lru_add(&lru_pvec);
+		} else {
+			page_cache_release(page);
+		}
+	}
+	pagevec_lru_add(&lru_pvec);
+	BUG_ON(!list_empty(pages));
+	return 0;
+#else
 	return mpage_readpages(mapping, pages, nr_pages, ext2_get_block);
+#endif
 }
 
 static int
@@ -662,11 +1104,58 @@ static int ext2_nobh_writepage(struct pa
 	return nobh_writepage(page, ext2_get_block, wbc);
 }
 
+#ifdef CONFIG_EXT2_COMPRESS
+static sector_t ext2_do_bmap(struct address_space *mapping, sector_t block)
+#else
 static sector_t ext2_bmap(struct address_space *mapping, sector_t block)
+#endif
 {
 	return generic_block_bmap(mapping,block,ext2_get_block);
 }
 
+#ifdef CONFIG_EXT2_COMPRESS
+/* Return 0 instead of EXT2_COMPRESSED_BLKADDR if EXT2_NOCOMPR_FL
+ * high.  This is necessary for us to be able to use
+ * generic_readpage() when EXT2_NOCOMPR_FL is high.
+ */
+static sector_t ext2_bmap(struct address_space *mapping, sector_t block)
+{
+	sector_t result;
+	struct inode *inode = mapping->host;
+
+	if ((EXT2_I(inode)->i_flags & (EXT2_COMPRBLK_FL | EXT2_NOCOMPR_FL))
+	    == (EXT2_COMPRBLK_FL | 0)) {
+		int err;
+
+		err = ext2_cluster_is_compressed_fn
+		    (inode, ext2_block_to_cluster(inode, block));
+		if (err > 0)
+			ext2_warning (inode->i_sb, "ext2_bmap",
+			    "compressed cluster, inode %lu",
+			    inode->i_ino);
+		if (err != 0)
+			return 0;
+	}
+
+	result = ext2_do_bmap(mapping, block);
+	if (result != EXT2_COMPRESSED_BLKADDR)
+		return result;
+
+	if (!(EXT2_SB(inode->i_sb)->s_es->s_feature_incompat
+	    & cpu_to_le32(EXT2_FEATURE_INCOMPAT_COMPRESSION)))
+		ext2_error(inode->i_sb, "ext2_bmap",
+		    "compressed_blkaddr (ino %lu, blk %lu) "
+		    "on non-compressed fs",
+		    inode->i_ino, (unsigned long) block);
+	if (!S_ISREG(inode->i_mode))
+		ext2_error(inode->i_sb, "ext2_bmap",
+		    "compressed_blkaddr for non-regular file "
+		    "(ino %lu, blk %lu)",
+		    inode->i_ino, (unsigned long) block);
+	return 0;
+}
+#endif /* CONFIG_EXT2_COMPRESS */
+
 static ssize_t
 ext2_direct_IO(int rw, struct kiocb *iocb, const struct iovec *iov,
 			loff_t offset, unsigned long nr_segs)
@@ -681,7 +1170,7 @@ ext2_direct_IO(int rw, struct kiocb *ioc
 static int
 ext2_writepages(struct address_space *mapping, struct writeback_control *wbc)
 {
-	return mpage_writepages(mapping, wbc, ext2_get_block);
+ 	return mpage_writepages(mapping, wbc, ext2_get_block);
 }
 
 const struct address_space_operations ext2_aops = {
@@ -828,6 +1317,12 @@ static inline void ext2_free_data(struct
 
 	for ( ; p < q ; p++) {
 		nr = le32_to_cpu(*p);
+#ifdef CONFIG_EXT2_COMPRESS
+		if (nr == EXT2_COMPRESSED_BLKADDR) {
+			*p = 0;
+			continue;
+		}
+#endif
 		if (nr) {
 			*p = 0;
 			/* accumulate blocks to free if they're contiguous */
@@ -872,6 +1367,12 @@ static void ext2_free_branches(struct in
 			nr = le32_to_cpu(*p);
 			if (!nr)
 				continue;
+#ifdef CONFIG_EXT2_COMPRESS
+			if (nr == EXT2_COMPRESSED_BLKADDR) {
+			  *p = 0;
+			  continue;
+			}
+#endif
 			*p = 0;
 			bh = sb_bread(inode->i_sb, nr);
 			/*
@@ -896,6 +1397,88 @@ static void ext2_free_branches(struct in
 		ext2_free_data(inode, p, q);
 }
 
+/* pjm 1998-01-14: As far as I can tell, "I don't do any locking" is
+   no longer correct, as i_sem is downed for all write() and
+   truncate() stuff except where it doesn't matter (e.g. new inode). */
+
+#ifdef CONFIG_EXT2_COMPRESS
+/* If the EXT2_ECOMPR_FL bit is high, then things can go rather badly.
+   This can only happen if access permission was obtained before the
+   flag was raised.  Also, it shouldn't be too much of a problem
+   unless the end point of truncation is a compressed cluster with a
+   compression error. */
+
+  /* From what I (Antoine) understand, the complexity of the truncate
+     code is due to the fact that we don't want to free blocks that
+     are still referenced.  It does not ensure that concurrent read
+     operation will terminate properly, i.e., the semantic of reading
+     while somebody truncates is undefined (you can either get the old
+     data if you got the blocks before, or get plenty of zeros
+     otherwise). */
+
+/* todo: Provide error trapping in readiness for when i_op->truncate
+   allows a return code. */
+static void fix_compression (struct inode * inode)
+{
+	struct ext2_inode_info *ei = EXT2_I(inode);
+
+	assert (ei->i_flags & EXT2_COMPRBLK_FL);
+	assert ((atomic_read(&inode->i_mutex.count) < 1)
+		|| ((inode->i_nlink == 0)
+		    && (atomic_read(&inode->i_count) == 0)));
+	/* pjm 1998-01-14: I think the below comment can safely be removed, as
+	   it's impossible for someone to be compressing during truncate(), because
+	   i_sem is down. */
+	/*   Dans le cas ou les clusters peuvent etre compresses, cela pose
+	     un probleme : il faudrait stopper aussi si le cluster est
+	     comprime et ne contient pas plus de donnees que i_size ne
+	     permet. Sinon, on peut passer son temps a decompresser un
+	     cluster que quelqu'un d'autre compresse en meme
+	     temps... (TODO).  Cela ne peut arriver que si on reverifie apres
+	     coup si le cluster est non compresse (ce qu'on fait a l'heure
+	     actuelle) => faire autrement.
+
+	     pjm fixme tr
+
+	     If the clusters can be compressed, we'd have a problem: we'd
+	     also need to stop if the cluster is compressed and doesn't
+	     contain more data than i_size permits.  Otherwise we can spend
+	     time decompressing a cluster that someone else is compressing
+	     at the same time.  (TODO.)  This can only happen if we reverify
+	     "apres coup" ("after the event"? "after each time"?) "si" ("if"
+	     or "that") the cluster is not compressed (as we are currently
+	     doing) => do differently. */
+
+	/* todo: Handle errors from ext2_cluster_is_compressed().
+	   (Except ext2_truncate() currently silently ignores errors
+	   anyway.) */
+
+	if (!ext2_offset_is_clu_boundary(inode, inode->i_size)
+	    && ext2_compression_enabled(inode)
+	    && (ext2_cluster_is_compressed_fn
+		  (inode, ext2_offset_to_cluster (inode, inode->i_size))
+		> 0)) {
+		ext2_decompress_cluster(inode, ext2_offset_to_cluster(inode, inode->i_size));
+		/* todo: Check the return code of
+		   ext2_decompress_cluster().  (Then again, I don't
+		   know how to report an error anyway.
+		   ext2_truncate() silently ignores errors.) */
+	  
+		/* Organise for the cluster to be recompressed later. */
+		assert (ei->i_flags & EXT2_COMPR_FL);
+		ei->i_flags |= EXT2_DIRTY_FL;
+		ei->i_compr_flags |= EXT2_CLEANUP_FL;
+		mark_inode_dirty(inode);
+	} else
+		/* If there are no more compressed clusters, then
+		   remove the EXT2_COMPRBLK_FL.  Not essential from a
+		   safety point of view, but friendlier.  We only do
+		   this in the `else' because the cleanup function
+		   will handle it in the `if' case. */
+		ext2_update_comprblk(inode);
+}
+#endif
+
 void ext2_truncate (struct inode * inode)
 {
 	__le32 *i_data = EXT2_I(inode)->i_data;
@@ -917,6 +1500,25 @@ void ext2_truncate (struct inode * inode
 		return;
 
 	ext2_discard_prealloc(inode);
+#ifdef CONFIG_EXT2_COMPRESS
+	/* If the new size is in the middle of a compressed cluster,
+	   then we decompress it, and set things up to be recompressed
+	   later.
+
+	   todo: It isn't very nice to get ENOSPC on truncate.  We
+	   can't completely remove the possibility (unless the
+	   compression algorithms obey the rule `shorter input never
+	   gives longer output') but we could greatly reduce the
+	   possibility, e.g. by moving the fix_compression() function
+	   to compress.c, and have it decompress and immediately
+	   recompress the cluster, without allocating blocks for the
+	   full decompressed data. */
+	if (EXT2_I(inode)->i_flags & EXT2_COMPRBLK_FL) {
+	  	trace_e2c("ext2_truncate: ino=%ld sz=%d\n", inode->i_ino, (int)inode->i_size);
+		fix_compression (inode);
+		truncate_inode_pages(inode->i_mapping, inode->i_size);
+	}
+#endif
 
 	blocksize = inode->i_sb->s_blocksize;
 	iblock = (inode->i_size + blocksize-1)
@@ -1096,7 +1698,63 @@ void ext2_read_inode (struct inode * ino
 	}
 	inode->i_blksize = PAGE_SIZE;	/* This is the optimal IO size (for stat), not the fs block size */
 	inode->i_blocks = le32_to_cpu(raw_inode->i_blocks);
-	ei->i_flags = le32_to_cpu(raw_inode->i_flags);
+#ifdef CONFIG_EXT2_COMPRESS
+	ei->i_flags = 0x807fffff & le32_to_cpu(raw_inode->i_flags);
+	ei->i_compr_flags = 0;
+	if (S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode)) {
+		if (S_ISDIR(inode->i_mode))
+			ei->i_flags &= ~(EXT2_COMPRBLK_FL | EXT2_DIRTY_FL);
+		/* The above shouldn't be necessary unless someone's
+		 * been playing with EXT2_IOC_SETFLAGS on a non-e2compr
+		 * kernel, or the inode has been scribbled on.
+		 */
+		if (ei->i_flags & (EXT2_COMPR_FL | EXT2_COMPRBLK_FL)) {
+			ei->i_compr_method
+			    = (le32_to_cpu(raw_inode->i_flags) >> 26) & 0x1f;
+			ei->i_log2_clu_nblocks
+			    = (le32_to_cpu(raw_inode->i_flags) >> 23) & 0x7;
+			if ((ei->i_log2_clu_nblocks < 2)
+			    || (ei->i_log2_clu_nblocks > 5)) {
+				if ((ei->i_log2_clu_nblocks == 0)
+				    && !(ei->i_flags & EXT2_COMPRBLK_FL)) {
+					/* The EXT2_COMPR_FL flag was
+					 * raised under a kernel
+					 * without e2compr support.
+					 */
+					if (S_ISREG(inode->i_mode))
+						ei->i_flags |= EXT2_DIRTY_FL;
+					/* Todo: once we're sure the kernel can
+					 * handle [log2_]clu_nblocks==0, get rid
+					 * of the next statement.
+					 */
+					ei->i_log2_clu_nblocks
+					    = EXT2_DEFAULT_LOG2_CLU_NBLOCKS;
+				} else {
+					ei->i_flags |= EXT2_ECOMPR_FL;
+					ext2_error(inode->i_sb,
+					    "ext2_read_inode",
+					    "inode %lu is corrupted: "
+					    "log2_clu_nblocks=%u",
+					    inode->i_ino,
+					    ei->i_log2_clu_nblocks);
+				}
+			}
+		} else {
+			ei->i_compr_method = EXT2_DEFAULT_COMPR_METHOD;
+			ei->i_log2_clu_nblocks
+			    = EXT2_DEFAULT_LOG2_CLU_NBLOCKS;
+		}
+	if (ei->i_log2_clu_nblocks >
+	    (EXT2_LOG2_MAX_CLUSTER_BYTES - inode->i_sb->s_blocksize_bits))
+		ei->i_log2_clu_nblocks = (EXT2_LOG2_MAX_CLUSTER_BYTES
+		    - inode->i_sb->s_blocksize_bits);
+	ei->i_clu_nblocks = 1 << ei->i_log2_clu_nblocks;
+	if (ei->i_flags & EXT2_DIRTY_FL)
+		ei->i_compr_flags = EXT2_CLEANUP_FL;
+	}
+#else /* !CONFIG_EXT2_COMPRESS */
+        ei->i_flags = le32_to_cpu(raw_inode->i_flags);
+#endif
 	ei->i_faddr = le32_to_cpu(raw_inode->i_faddr);
 	ei->i_frag_no = raw_inode->i_frag;
 	ei->i_frag_size = raw_inode->i_fsize;
@@ -1218,7 +1876,30 @@ static int ext2_update_inode(struct inod
 
 	raw_inode->i_blocks = cpu_to_le32(inode->i_blocks);
 	raw_inode->i_dtime = cpu_to_le32(ei->i_dtime);
+#ifdef CONFIG_EXT2_COMPRESS
+	if ((S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode))
+	    && (ei->i_flags & (EXT2_COMPR_FL | EXT2_COMPRBLK_FL))) {
+		if ((ei->i_log2_clu_nblocks < 2)
+		    || (ei->i_log2_clu_nblocks > 5)) {
+			ei->i_flags |= EXT2_ECOMPR_FL;
+			ext2_error (inode->i_sb, "ext2_write_inode",
+			    "inode %lu is corrupted: log2_clu_nblocks=%u",
+			    inode->i_ino, ei->i_log2_clu_nblocks);
+		}
+		assert (ei->i_clu_nblocks == (1 << ei->i_log2_clu_nblocks));
+		assert (ei->i_compr_method < 0x20);
+		raw_inode->i_flags = cpu_to_le32
+		    ((ei->i_flags & 0x807fffff)
+		    | (ei->i_compr_method << 26)
+		    | (ei->i_log2_clu_nblocks << 23));
+	} else
+		raw_inode->i_flags = cpu_to_le32
+		    (ei->i_flags
+		    & 0x807fffff /* no compr meth/size */
+		    & ~(EXT2_COMPR_FL | EXT2_COMPRBLK_FL | EXT2_IMMUTABLE_FL | EXT2_ECOMPR_FL | EXT2_NOCOMPR_FL));
+#else
 	raw_inode->i_flags = cpu_to_le32(ei->i_flags);
+#endif
 	raw_inode->i_faddr = cpu_to_le32(ei->i_faddr);
 	raw_inode->i_frag = ei->i_frag_no;
 	raw_inode->i_fsize = ei->i_frag_size;
@@ -1302,6 +1983,7 @@ int ext2_setattr(struct dentry *dentry, 
 			return error;
 	}
 	error = inode_setattr(inode, iattr);
+/* Paul Whittaker tweak 19 Feb 2005 */
 	if (!error && (iattr->ia_valid & ATTR_MODE))
 		error = ext2_acl_chmod(inode);
 	return error;
diff -pruN linux-2.6.18.5.org/fs/ext2/ioctl.c linux-2.6.18.5/fs/ext2/ioctl.c
--- linux-2.6.18.5.org/fs/ext2/ioctl.c	2006-12-04 13:31:58.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/ioctl.c	2007-03-03 15:27:19.000000000 -0800
@@ -7,19 +7,43 @@
  * Universite Pierre et Marie Curie (Paris VI)
  */
 
-#include "ext2.h"
+/*
+ *  Copyright (C) 2001 Alcatel Business Systems - R&D Illkirch
+ *  Added by Pierre Peiffer (pierre.peiffer@sxb.bsf.alcatel.fr) (2001-08-09)
+ *  (Copied from pjm patch e2compr-0.4.39-patch-2.2.18)
+ *      e2compress stuff.
+ *
+ * << Copyright (C) 1995  Antoine Dumesnil de Maricourt (dumesnil@etca.fr) >>
+ * <<     (transparent compression code)                                 >>
+ */
+
+#include <linux/fs.h>
+#include <linux/ext2_fs_c.h>
 #include <linux/capability.h>
 #include <linux/time.h>
 #include <linux/sched.h>
 #include <asm/current.h>
 #include <asm/uaccess.h>
+#include <linux/stat.h>
 
+#ifdef CONFIG_EXT2_COMPRESS
+#include <linux/kmod.h>
+#include "assert.h"
+#endif
+
+#ifndef MIN
+# define MIN(a,b) ((a) < (b) ? (a) : (b))
+#endif
 
 int ext2_ioctl (struct inode * inode, struct file * filp, unsigned int cmd,
 		unsigned long arg)
 {
 	struct ext2_inode_info *ei = EXT2_I(inode);
 	unsigned int flags;
+#ifdef CONFIG_EXT2_COMPRESS
+	unsigned long datum;
+	int err;
+#endif
 
 	ext2_debug ("cmd = %u, arg = %lu\n", cmd, arg);
 
@@ -56,7 +80,97 @@ int ext2_ioctl (struct inode * inode, st
 		}
 
 		flags = flags & EXT2_FL_USER_MODIFIABLE;
+#ifdef CONFIG_EXT2_COMPRESS
+		if (S_ISREG (inode->i_mode) || S_ISDIR (inode->i_mode)) {
+
+			/* pjm 1998-01-14: In previous versions of
+			     e2compr, the kernel forbade raising
+			     EXT2_ECOMPR_FL from userspace.  I can't
+			     think of any purpose for forbidding this,
+			     and I find it useful to raise
+			     EXT2_ECOMPR_FL for testing purposes, so
+			     I've removed the forbidding code. */
+
+			if (S_ISREG (inode->i_mode)
+			    && (EXT2_NOCOMPR_FL
+				& (flags ^ ei->i_flags))) {
+
+				/* NOCOMPR_FL can only be changed if
+				   nobody else has the file opened.  */
+				/* pjm 1998-02-16: inode->i_count is
+				   useless to us because only dentries
+				   use inodes now.  Unfortunately,
+				   there isn't an easy way of finding
+				   the equivalent.  We'd have to go
+				   through all dentries using the
+				   inode, and sum their d_count
+				   values.  Rather than do that, I'd
+				   rather get rid of the exclusion
+				   constraint.  todo. */
+				if (/*atomic_read(&inode->i_count) > 1*/ 0)
+					return -ETXTBSY;
+				else {
+					/* pjm 970429: Discarding
+					     cached pages is not very
+					     clean, but should work. */
+					/* pjm 980114: Not quite.  We
+					     should also sync any
+					     mappings to buffers first.
+					     This isn't very important,
+					     as none of the current
+					     e2compr programs can
+					     trigger this, but todo. */
+					invalidate_remote_inode (inode);
+				}
+			}
+
+			if (EXT2_COMPR_FL
+			    & (flags ^ ei->i_flags)) {
+				if (flags & EXT2_COMPR_FL) {
+					if (ei->i_flags & EXT2_COMPRBLK_FL) {
+						/* There shouldn't actually be any
+						   compressed blocks, AFAIK.  However,
+						   this is still possible because sometimes
+						   COMPRBLK gets raised just to stop 
+						   us changing cluster size at the wrong
+						   time.
+
+						   todo: Call a function that just
+						   checks that there are not compressed
+						   clusters, and print a warning if any are
+						   found. */
+					} else {
+						int bits = MIN(EXT2_DEFAULT_LOG2_CLU_NBLOCKS,
+							       (EXT2_LOG2_MAX_CLUSTER_BYTES
+								- inode->i_sb->s_blocksize_bits));
+
+						ei->i_log2_clu_nblocks = bits;
+						ei->i_clu_nblocks = 1 << bits;
+					}
+					ei->i_compr_method = EXT2_DEFAULT_COMPR_METHOD;
+					if (S_ISREG (inode->i_mode)) {
+						flags |= EXT2_DIRTY_FL;
+						ei->i_compr_flags |= EXT2_CLEANUP_FL;
+					}
+				} else if (S_ISREG (inode->i_mode)) {
+					if (ei->i_flags & EXT2_COMPRBLK_FL) {
+						int err;
+
+						err = ext2_decompress_inode(inode);
+						if (err)
+							return err;
+					}
+					ei->i_flags &= ~EXT2_DIRTY_FL;
+					ei->i_compr_flags &= ~EXT2_CLEANUP_FL;
+				}
+			}
+		}
+#endif
 		flags |= oldflags & ~EXT2_FL_USER_MODIFIABLE;
+
+		/* bug fix: scrub 'B' flag from uncompressed files TLL 02/28/07 */
+		if (!(flags & EXT2_COMPR_FL)) flags &= ~EXT2_COMPRBLK_FL;
+
 		ei->i_flags = flags;
 
 		ext2_set_inode_flags(inode);
@@ -76,6 +190,173 @@ int ext2_ioctl (struct inode * inode, st
 		inode->i_ctime = CURRENT_TIME_SEC;
 		mark_inode_dirty(inode);
 		return 0;
+#ifdef CONFIG_EXT2_COMPRESS
+	case EXT2_IOC_GETCOMPRMETHOD:	/* Result means nothing if COMPR_FL is not set */
+		return put_user (ei->i_compr_method, (long *) arg);
+	case EXT2_IOC_SETCOMPRMETHOD:
+		if ((current->fsuid != inode->i_uid) && !capable(CAP_FOWNER))
+			return -EPERM;
+		if (IS_RDONLY (inode))
+			return -EROFS;
+		if (get_user (datum, (long*) arg))
+			return -EFAULT;
+		if (!S_ISREG (inode->i_mode) && !S_ISDIR (inode->i_mode)) 
+			return -ENOSYS;
+		/* todo: Allow the below, but set initial value of
+		   i_compr_meth at read_inode() time (using default if
+		   !comprblk) instead of +c time.  Same for cluster
+		   size. */
+		if ((unsigned) datum >= EXT2_N_METHODS)
+			return -EINVAL;
+		if (ei->i_compr_method != datum) {
+			if ((ei->i_compr_method == EXT2_NEVER_METH)
+			    && (ei->i_flags & EXT2_COMPR_FL))
+				return -EPERM;
+			/* If the previous method was `defer' then
+			   take a look at all uncompressed clusters
+			   and try to compress them.  (pjm 1997-04-16) */
+			if ((ei->i_compr_method == EXT2_DEFER_METH)
+			    && S_ISREG (inode->i_mode)) {
+				ei->i_flags |= EXT2_DIRTY_FL;
+				ei->i_compr_flags |= EXT2_CLEANUP_FL;
+			}
+			if ((datum == EXT2_NEVER_METH)
+			    && S_ISREG (inode->i_mode)) {
+				if ((ei->i_flags & EXT2_COMPRBLK_FL)
+				    && ((err = ext2_decompress_inode(inode))
+					< 0))
+					return err;
+				ei->i_flags &= ~EXT2_DIRTY_FL;
+				ei->i_compr_flags &= ~EXT2_CLEANUP_FL;
+			}
+			ei->i_compr_method = datum;
+			inode->i_ctime = CURRENT_TIME;
+			mark_inode_dirty(inode);
+		}
+#ifdef CONFIG_KMOD
+		if (!ext2_algorithm_table[ext2_method_table[datum].alg].avail) {
+			char str[32];
+
+			sprintf(str, "ext2-compr-%s", ext2_algorithm_table[ext2_method_table[datum].alg].name);
+			request_module(str);
+		}
+#endif
+		datum = ((datum < EXT2_N_METHODS)
+			 && (ext2_algorithm_table[ext2_method_table[datum].alg].avail));
+		return put_user(datum, (long *)arg);
+
+	case EXT2_IOC_GETCLUSTERBIT:
+		if (get_user (datum, (long*) arg))
+			return -EFAULT;
+		if (!S_ISREG (inode->i_mode))
+			return -ENOSYS;
+		/* We don't do `down(&inode->i_sem)' here because
+		   there's no way for userspace to do the
+		   corresponding up().  Userspace must rely on
+		   EXT2_NOCOMPR_FL if it needs to lock. */
+		err = ext2_cluster_is_compressed (inode, datum);
+		if (err < 0)
+			return err;
+		return put_user ((err ? 1 : 0),
+				 (long *) arg);
+
+	case EXT2_IOC_RECOGNIZE_COMPRESSED:
+		if (get_user (datum, (long*) arg))
+			return -EFAULT;
+		if (!S_ISREG (inode->i_mode))
+			return -ENOSYS;
+		if (IS_RDONLY (inode))
+			return -EROFS;
+		return ext2_recognize_compressed (inode, datum);
+
+	case EXT2_IOC_GETCLUSTERSIZE:
+		/* Result means nothing if COMPR_FL is not set (until
+                   SETCLUSTERSIZE w/o COMPR_FL is implemented;
+                   todo). */
+		if (!S_ISREG (inode->i_mode)
+		    && !S_ISDIR (inode->i_mode)) 
+			return -ENOSYS;
+		return put_user (ei->i_clu_nblocks, (long *) arg);
+
+	case EXT2_IOC_GETFIRSTCLUSTERSIZE:
+		/* Result means nothing if COMPR_FL is not set (until
+                   SETCLUSTERSIZE w/o COMPR_FL is implemented;
+                   todo). */
+		if (!S_ISREG (inode->i_mode)
+		    && !S_ISDIR (inode->i_mode)) 
+			return -ENOSYS;
+		return put_user (ext2_first_cluster_nblocks(inode), (long *) arg);
+
+	case EXT2_IOC_SETCLUSTERSIZE:
+		if ((current->fsuid != inode->i_uid) && !capable(CAP_FOWNER))
+			return -EPERM;
+		if (IS_RDONLY (inode))
+			return -EROFS;
+		if (get_user (datum, (long *) arg))
+			return -EFAULT;
+		if (!S_ISREG (inode->i_mode)
+		    && !S_ISDIR (inode->i_mode)) 
+			return -ENOSYS;
+
+		/* These are the only possible cluster sizes.  The
+		   cluster size must be a power of two so that
+		   clusters don't straddle address (aka indirect)
+		   blocks.  At the moment, the upper limit is constrained
+		   by how much memory is allocated for de/compression.
+		   Also, the gzip algorithms have some optimisations
+		   that assume tht the input is no more than 32KB,
+		   and in compress.c we would need to zero more bits
+		   of head->holemap.  (In previous releases, the file
+		   format was limited to 32 blocks and under 64KB.) */
+// #if EXT2_MAX_CLUSTER_BLOCKS > 32 || EXT2_MAX_CLUSTER_NBYTES > 32768
+// # error "This code not updated for cluster size yet."
+// #endif
+		switch (datum) {
+		case (1 << 2): datum = 2; break;
+		case (1 << 3): datum = 3; break;
+		case (1 << 4): datum = 4; break;
+		case (1 << 5): datum = 5; break;
+		default: return -EINVAL;
+		}
+
+		assert (ei->i_clu_nblocks == (1 << ei->i_log2_clu_nblocks));
+		if (datum == ei->i_log2_clu_nblocks)
+			return 0;
+
+		if (ei->i_flags & EXT2_ECOMPR_FL)
+			return -EPERM;
+		if (!(ei->i_flags & EXT2_COMPR_FL))
+			return -ENOSYS;
+
+		/* We currently lack a mechanism to change the cluster
+		   size if there are already some compressed clusters.
+		   The compression must be done in userspace
+		   (e.g. with the e2compress program) instead.  */
+		if (ei->i_flags & EXT2_COMPRBLK_FL)
+			return -ENOSYS;
+
+		if (datum + inode->i_sb->s_blocksize_bits
+		    > EXT2_LOG2_MAX_CLUSTER_BYTES)
+			return -EINVAL;
+
+		ei->i_log2_clu_nblocks = datum;
+		ei->i_clu_nblocks = 1 << datum;
+		inode->i_ctime = CURRENT_TIME;
+		mark_inode_dirty(inode);
+		return 0;
+
+	case EXT2_IOC_GETCOMPRRATIO:
+		if (!S_ISREG (inode->i_mode)) 
+			return -ENOSYS;
+		if (ei->i_flags & EXT2_ECOMPR_FL)
+			return -EPERM;
+		if ((long) (datum = ext2_count_blocks (inode)) < 0)
+			return datum;
+		if ((err = put_user ((long) datum, (long*) arg)))
+			return err;
+		return put_user ((long) inode->i_blocks, (long*) arg + 1);
+#endif
+
 	default:
 		return -ENOTTY;
 	}
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/Makefile linux-2.6.18.5/fs/ext2/lzo/Makefile
--- linux-2.6.18.5.org/fs/ext2/lzo/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/Makefile	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,13 @@
+#
+# Makefile for the linux compression routines.
+#
+# Note! Dependencies are done automagically by 'make dep', which also
+# removes any old dependencies. DON'T put your own dependencies here
+# unless it's something special (ie not a .c file).
+#
+# Note 2! The CFLAGS definitions are now in the main makefile...
+
+EXTRA_CFLAGS += -I$(TOPDIR)/fs/ext2
+
+obj-$(CONFIG_EXT2_HAVE_LZO) := ext2-compr-lzo.o
+ext2-compr-lzo-y := e2compr_lzo.o lzo1x_1o.o lzo1x_d1.o
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/config1x.h linux-2.6.18.5/fs/ext2/lzo/config1x.h
--- linux-2.6.18.5.org/fs/ext2/lzo/config1x.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/config1x.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,106 @@
+/* config1x.h -- configuration for the LZO1X algorithm
+
+   This file is part of the LZO real-time data compression library.
+
+   Copyright (C) 1997 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1996 Markus Franz Xaver Johannes Oberhumer
+
+   The LZO library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of
+   the License, or (at your option) any later version.
+
+   The LZO library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with the LZO library; see the file COPYING.
+   If not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+
+   Markus F.X.J. Oberhumer
+   markus.oberhumer@jk.uni-linz.ac.at
+ */
+
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the library and is subject
+   to change.
+ */
+
+
+#ifndef __LZO_CONFIG1X_H
+#define __LZO_CONFIG1X_H
+
+#if !defined(LZO1X) && !defined(LZO1Y) && !defined(LZO1Z)
+#  define LZO1X
+#endif
+
+#if !defined(__LZO_IN_MINILZO)
+#include "lzo1x.h"
+#endif
+#include "lzo_conf.h"
+#include "lzo_util.h"
+
+
+/***********************************************************************
+//
+************************************************************************/
+
+#define LZO_EOF_CODE
+#undef LZO_DETERMINISTIC
+
+#define M1_MAX_OFFSET	0x0400
+#ifndef M2_MAX_OFFSET
+#define M2_MAX_OFFSET	0x0800
+#endif
+#define M3_MAX_OFFSET	0x4000
+#define M4_MAX_OFFSET	0xbfff
+
+#define MX_MAX_OFFSET	(M1_MAX_OFFSET + M2_MAX_OFFSET)
+
+#define M1_MIN_LEN		2
+#define M1_MAX_LEN		2
+#define M2_MIN_LEN		3
+#ifndef M2_MAX_LEN
+#define M2_MAX_LEN		8
+#endif
+#define M3_MIN_LEN		3
+#define M3_MAX_LEN		33
+#define M4_MIN_LEN		3
+#define M4_MAX_LEN		9
+
+#define M1_MARKER		0
+#define M2_MARKER		64
+#define M3_MARKER		32
+#define M4_MARKER		16
+
+
+/***********************************************************************
+//
+************************************************************************/
+
+#ifndef MIN_LOOKAHEAD
+#define MIN_LOOKAHEAD		(M2_MAX_LEN + 1)
+#endif
+
+#if defined(LZO_NEED_DICT_H)
+
+#ifndef LZO_HASH
+#define LZO_HASH			LZO_HASH_LZO_INCREMENTAL_B
+#endif
+#define DL_MIN_LEN			M2_MIN_LEN
+#include "lzo_dict.h"
+
+#endif
+
+
+
+#endif /* already included */
+
+/*
+vi:ts=4
+*/
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/e2compr_lzo.c linux-2.6.18.5/fs/ext2/lzo/e2compr_lzo.c
--- linux-2.6.18.5.org/fs/ext2/lzo/e2compr_lzo.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/e2compr_lzo.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,150 @@
+#include <linux/fs.h>
+#include <linux/ext2_fs_c.h>
+#include "e2compr_lzo.h"
+#include "lzo1x.h"
+#ifdef __KERNEL__
+# include <linux/kernel.h>
+# include <linux/module.h>
+#endif
+
+#ifdef MODULE
+MODULE_AUTHOR("Markus Franz Xaver Johannes Oberhumer");
+MODULE_DESCRIPTION("Lev-Zimpel-Oberhumer algorithm for EXT2 file compression");
+MODULE_LICENSE("GPL");
+#endif
+
+#if 0
+static void *heap_start = NULL;
+static u8 *orig_start = NULL;
+#endif
+
+#if EXT2_MAX_CLU_NBYTES > (32*1024)
+# error "Check that lzo overrun allowance is still correct for this cluster size."
+#endif
+#define MAGIC_BYTE 42
+
+size_t 
+ext2_wLZO (u8 *in_buf, 
+	   u8 *out_buf,
+	   void *heap,
+	   size_t num_in_buf,
+	   size_t num_out_buf,
+	   int param)
+{
+	size_t clen;
+
+	if (!try_module_get(THIS_MODULE))
+		return 0;
+	if (heap == NULL)
+		goto err;
+	if (((u8 *)heap < out_buf)
+	    || ((u8 *)heap > out_buf + EXT2_MAX_CLUSTER_BYTES)) {
+#ifdef __KERNEL__
+		printk(KERN_WARNING
+		       "lzo: heap doesn't seem to be contiguous with out_buf.  orig_start: %p, out_buf: %p, diff: %d.  Returning zero.\n",
+			(u8 *)heap, out_buf, (u8 *)heap - out_buf);
+#else
+		fprintf(stderr,
+			"lzo: heap/out_buf misplacement.  "
+			"orig_start: %p, out_buf: %p, diff: %d\n",
+			heap, out_buf, heap - out_buf);
+#endif
+		goto err;
+	}
+	((unsigned char *)heap)[LZO_OVERRUN_LEN] = MAGIC_BYTE;
+	if (lzo1x_1_15_compress(in_buf, num_in_buf,
+				out_buf, &clen,
+				heap + LZO_OVERRUN_LEN + 1))
+		clen = 0;
+	if (((unsigned char *)heap)[LZO_OVERRUN_LEN] != MAGIC_BYTE) {
+		printk(KERN_ERR
+		       "lzo: overran allowed area.  Please report to e2compr list.\n");
+		goto err; /* play it safe */
+	}
+	if (clen > num_out_buf)
+		goto err;
+	module_put(THIS_MODULE);
+	return clen;
+
+err:
+	module_put(THIS_MODULE);
+	return 0;
+}
+
+size_t 
+ext2_rLZO (unsigned char *in_buf, 
+           unsigned char *out_buf,
+	   void *heap,
+           size_t num_in_buf,
+           size_t num_out_buf, /* N.B.: ignored. */
+           int param)
+{
+	int err;
+
+	if (!try_module_get(THIS_MODULE))
+		return 0;
+#if 0
+	if (heap_start == NULL) {
+		module_put(THIS_MODULE);
+		return 0;
+	}
+
+	err = lzo1x_decompress(in_buf, num_in_buf, out_buf, &num_out_buf,
+		heap_start);
+#else
+	err = lzo1x_decompress(in_buf, num_in_buf, out_buf, &num_out_buf, heap);
+#endif
+	if (err != LZO_E_OK) {
+		printk(KERN_WARNING
+		       "lzo1x_decompress() returned %d.\n", err);
+		module_put(THIS_MODULE);
+		return 0;
+	}
+	module_put(THIS_MODULE);
+	return num_out_buf;
+}
+
+size_t 
+ext2_iLZO (int action)
+{
+	/* The compressor can overrun its output buffer by 531 bytes
+	   (LZO_OVERRUN_LEN).  We use the 532nd byte to hold a magic
+	   number (which we check to detect overruns of the overrun
+	   buffer).  128KB is the working memory for _15.
+		orig_start_C = (u8 *) mem_start;
+		heap_start_C = orig_start_C + LZO_OVERRUN_LEN + 1;
+	*/
+	switch (action) {
+		case EXT2_ALG_INIT_COMPRESS:
+			return (128 * 1024) + LZO_OVERRUN_LEN + 1;
+		case EXT2_ALG_INIT_DECOMPRESS:
+			return 0;
+		default:
+			return 0;
+	}
+}
+
+#ifdef MODULE
+
+int init_module(void)
+{
+	struct ext2_algorithm lzo_alg;
+
+	lzo_alg.name = NULL;
+	lzo_alg.avail = 1;
+	lzo_alg.init = ext2_iLZO;
+	lzo_alg.compress = ext2_wLZO;
+	lzo_alg.decompress = ext2_rLZO;
+
+	return ext2_register_compression_module(EXT2_LZO_ALG, 
+			(128 * 1024) + LZO_OVERRUN_LEN + 1,
+			0,
+			&lzo_alg);
+}
+
+void cleanup_module(void)
+{
+	ext2_unregister_compression_module(EXT2_LZO_ALG);
+}
+
+#endif
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/e2compr_lzo.h linux-2.6.18.5/fs/ext2/lzo/e2compr_lzo.h
--- linux-2.6.18.5.org/fs/ext2/lzo/e2compr_lzo.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/e2compr_lzo.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,28 @@
+#ifdef __KERNEL__
+# include <linux/types.h>
+#else
+# include <stdio.h> /* size_t */
+#endif
+
+#define LZO_OVERRUN_LEN 531
+#define LZO_REQMEM ((128 * 1024) + LZO_OVERRUN_LEN + 1)
+
+extern size_t 
+ext2_wLZO ( unsigned char *in_buf, 
+            unsigned char *out_buf,
+	    void *heap,
+            size_t num_in_buf,
+            size_t num_out_buf,
+            int param );
+
+extern size_t 
+ext2_rLZO ( unsigned char *in_buf, 
+            unsigned char *out_buf,
+	    void *heap,
+            size_t num_in_buf,
+            size_t num_out_buf,
+            int param );
+
+
+extern size_t 
+ext2_iLZO ( int action );
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/lzo1_d.ch linux-2.6.18.5/fs/ext2/lzo/lzo1_d.ch
--- linux-2.6.18.5.org/fs/ext2/lzo/lzo1_d.ch	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/lzo1_d.ch	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,139 @@
+/* lzo1_d.ch -- common decompression stuff
+
+   This file is part of the LZO real-time data compression library.
+
+   Copyright (C) 1997 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1996 Markus Franz Xaver Johannes Oberhumer
+
+   The LZO library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of
+   the License, or (at your option) any later version.
+
+   The LZO library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with the LZO library; see the file COPYING.
+   If not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+
+   Markus F.X.J. Oberhumer
+   markus.oberhumer@jk.uni-linz.ac.at
+ */
+
+
+
+#if defined(LZO_TEST_DECOMPRESS_OVERRUN)
+#  if !defined(LZO_TEST_DECOMPRESS_OVERRUN_INPUT)
+#    define LZO_TEST_DECOMPRESS_OVERRUN_INPUT		2
+#  endif
+#  if !defined(LZO_TEST_DECOMPRESS_OVERRUN_OUTPUT)
+#    define LZO_TEST_DECOMPRESS_OVERRUN_OUTPUT		2
+#  endif
+#  if !defined(LZO_TEST_DECOMPRESS_OVERRUN_LOOKBEHIND)
+#    define LZO_TEST_DECOMPRESS_OVERRUN_LOOKBEHIND
+#  endif
+#endif
+
+
+/***********************************************************************
+// Overrun detection is internally handled by these macros:
+//
+//   TEST_IP    test input overrun at loop begin
+//   NEED_IP    test input overrun at every input byte
+//
+//   TEST_OP    test output overrun at loop begin
+//   NEED_OP    test output overrun at every output byte
+//
+//   TEST_LOOKBEHIND    test match postion
+//
+// The fastest decompressor results when testing for no overruns
+// and using LZO_EOF_CODE.
+************************************************************************/
+
+#undef TEST_IP
+#undef TEST_OP
+#undef TEST_LOOKBEHIND
+#undef NEED_IP
+#undef NEED_OP
+#undef HAVE_TEST_IP
+#undef HAVE_TEST_OP
+#undef HAVE_NEED_IP
+#undef HAVE_NEED_OP
+#undef HAVE_ANY_IP
+#undef HAVE_ANY_OP
+
+
+#if defined(LZO_TEST_DECOMPRESS_OVERRUN_INPUT)
+#  if (LZO_TEST_DECOMPRESS_OVERRUN_INPUT >= 1)
+#    define TEST_IP				(ip < ip_end)
+#  endif
+#  if (LZO_TEST_DECOMPRESS_OVERRUN_INPUT >= 2)
+#    define NEED_IP(x) \
+			if ((lzo_uint)(ip_end - ip) < (lzo_uint)(x))  goto input_overrun
+#  endif
+#endif
+
+#if defined(LZO_TEST_DECOMPRESS_OVERRUN_OUTPUT)
+#  if (LZO_TEST_DECOMPRESS_OVERRUN_OUTPUT >= 1)
+#    define TEST_OP				(op <= op_end)
+#  endif
+#  if (LZO_TEST_DECOMPRESS_OVERRUN_OUTPUT >= 2)
+#    undef TEST_OP				/* don't need both of the tests here */
+#    define NEED_OP(x) \
+			if ((lzo_uint)(op_end - op) < (lzo_uint)(x))  goto output_overrun
+#  endif
+#endif
+
+#if defined(LZO_TEST_DECOMPRESS_OVERRUN_LOOKBEHIND)
+#  define TEST_LOOKBEHIND(m_pos,out)	if (m_pos < out) goto lookbehind_overrun
+#else
+#  define TEST_LOOKBEHIND(m_pos,op)		((void) 0)
+#endif
+
+
+#if !defined(LZO_EOF_CODE) && !defined(TEST_IP)
+   /* if we have no EOF code, we have to test for the end of the input */
+#  define TEST_IP				(ip < ip_end)
+#endif
+
+
+#if defined(TEST_IP)
+#  define HAVE_TEST_IP
+#else
+#  define TEST_IP				1
+#endif
+#if defined(TEST_OP)
+#  define HAVE_TEST_OP
+#else
+#  define TEST_OP				1
+#endif
+
+#if defined(NEED_IP)
+#  define HAVE_NEED_IP
+#else
+#  define NEED_IP(x)			((void) 0)
+#endif
+#if defined(NEED_OP)
+#  define HAVE_NEED_OP
+#else
+#  define NEED_OP(x)			((void) 0)
+#endif
+
+
+#if defined(HAVE_TEST_IP) || defined(HAVE_NEED_IP)
+#  define HAVE_ANY_IP
+#endif
+#if defined(HAVE_TEST_OP) || defined(HAVE_NEED_OP)
+#  define HAVE_ANY_OP
+#endif
+
+
+
+/*
+vi:ts=4
+*/
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/lzo1x.h linux-2.6.18.5/fs/ext2/lzo/lzo1x.h
--- linux-2.6.18.5.org/fs/ext2/lzo/lzo1x.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/lzo1x.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,168 @@
+/* lzo1x.h -- public interface of the LZO1X compression algorithm
+
+   This file is part of the LZO real-time data compression library.
+
+   Copyright (C) 1998 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1997 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1996 Markus Franz Xaver Johannes Oberhumer
+
+   The LZO library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of
+   the License, or (at your option) any later version.
+
+   The LZO library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with the LZO library; see the file COPYING.
+   If not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+
+   Markus F.X.J. Oberhumer
+   <markus.oberhumer@jk.uni-linz.ac.at>
+   http://wildsau.idv.uni-linz.ac.at/mfx/lzo.html
+ */
+
+
+#ifndef __LZO1X_H
+#define __LZO1X_H
+
+#ifndef __LZOCONF_H
+#include "lzoconf.h"
+#endif
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+
+/***********************************************************************
+//
+************************************************************************/
+
+/* Memory required for the wrkmem parameter.
+ * When the required size is 0, you can also pass a NULL pointer.
+ */
+
+#define LZO1X_MEM_COMPRESS      LZO1X_1_MEM_COMPESS
+#define LZO1X_MEM_DECOMPRESS    (0)
+#define LZO1X_MEM_OPTIMIZE      (0)
+
+
+/* decompression */
+LZO_EXTERN(int)
+lzo1x_decompress        ( const lzo_byte *src, lzo_uint  src_len,
+                                lzo_byte *dst, lzo_uint *dst_len,
+                                lzo_voidp wrkmem /* NOT USED */ );
+
+/* safe decompression with overrun testing */
+LZO_EXTERN(int)
+lzo1x_decompress_safe   ( const lzo_byte *src, lzo_uint  src_len,
+                                lzo_byte *dst, lzo_uint *dst_len,
+                                lzo_voidp wrkmem /* NOT USED */ );
+
+
+/***********************************************************************
+//
+************************************************************************/
+
+#define LZO1X_1_MEM_COMPRESS    ((lzo_uint32) (16384L * lzo_sizeof_dict_t))
+
+LZO_EXTERN(int)
+lzo1x_1_compress        ( const lzo_byte *src, lzo_uint  src_len,
+                                lzo_byte *dst, lzo_uint *dst_len,
+                                lzo_voidp wrkmem );
+
+
+/***********************************************************************
+// special compressor versions
+************************************************************************/
+
+/* this version needs only 8 kB work memory */
+#define LZO1X_1_11_MEM_COMPRESS ((lzo_uint32) (2048L * lzo_sizeof_dict_t))
+
+LZO_EXTERN(int)
+lzo1x_1_11_compress     ( const lzo_byte *src, lzo_uint  src_len,
+                                lzo_byte *dst, lzo_uint *dst_len,
+                                lzo_voidp wrkmem );
+
+
+/* this version needs 16 kB work memory */
+#define LZO1X_1_12_MEM_COMPRESS ((lzo_uint32) (4096L * lzo_sizeof_dict_t))
+
+LZO_EXTERN(int)
+lzo1x_1_12_compress     ( const lzo_byte *src, lzo_uint  src_len,
+                                lzo_byte *dst, lzo_uint *dst_len,
+                                lzo_voidp wrkmem );
+
+
+/* use this version if you need a little more compression speed */
+#define LZO1X_1_15_MEM_COMPRESS ((lzo_uint32) (32768L * lzo_sizeof_dict_t))
+
+LZO_EXTERN(int)
+lzo1x_1_15_compress     ( const lzo_byte *src, lzo_uint  src_len,
+                                lzo_byte *dst, lzo_uint *dst_len,
+                                lzo_voidp wrkmem );
+
+
+/***********************************************************************
+// better compression ratio at the cost of more memory and time
+************************************************************************/
+
+#define LZO1X_999_MEM_COMPRESS  ((lzo_uint32) (14 * 16384L * sizeof(short)))
+
+#if !defined(LZO_999_UNSUPPORTED)
+LZO_EXTERN(int)
+lzo1x_999_compress      ( const lzo_byte *src, lzo_uint  src_len,
+                                lzo_byte *dst, lzo_uint *dst_len,
+                                lzo_voidp wrkmem );
+#endif
+
+
+/***********************************************************************
+//
+************************************************************************/
+
+#if !defined(LZO_999_UNSUPPORTED)
+LZO_EXTERN(int)
+lzo1x_999_compress_dict     ( const lzo_byte *in , lzo_uint  in_len,
+                                    lzo_byte *out, lzo_uint *out_len,
+                                    lzo_voidp wrkmem,
+                              const lzo_byte *dict, lzo_uint dict_len );
+
+LZO_EXTERN(int)
+lzo1x_999_compress_level    ( const lzo_byte *in , lzo_uint  in_len,
+                                    lzo_byte *out, lzo_uint *out_len,
+                                    lzo_voidp wrkmem,
+                              const lzo_byte *dict, lzo_uint dict_len,
+                                    lzo_progress_callback_t cb,
+                                    int compression_level );
+#endif
+
+LZO_EXTERN(int)
+lzo1x_decompress_dict_safe ( const lzo_byte *in,  lzo_uint  in_len,
+                                   lzo_byte *out, lzo_uint *out_len,
+                                   lzo_voidp wrkmem /* NOT USED */,
+                             const lzo_byte *dict, lzo_uint dict_len );
+
+
+/***********************************************************************
+// optimize a compressed data block
+************************************************************************/
+
+LZO_EXTERN(int)
+lzo1x_optimize          (       lzo_byte *in , lzo_uint  in_len,
+                                lzo_byte *out, lzo_uint *out_len,
+                                lzo_voidp wrkmem );
+
+
+
+#ifdef __cplusplus
+} /* extern "C" */
+#endif
+
+#endif /* already included */
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/lzo1x_1o.c linux-2.6.18.5/fs/ext2/lzo/lzo1x_1o.c
--- linux-2.6.18.5.org/fs/ext2/lzo/lzo1x_1o.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/lzo1x_1o.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,38 @@
+/* lzo1x_1o.c -- LZO1X-1(15) compression
+
+   This file is part of the LZO real-time data compression library.
+
+   Copyright (C) 1998 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1997 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1996 Markus Franz Xaver Johannes Oberhumer
+
+   The LZO library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of
+   the License, or (at your option) any later version.
+
+   The LZO library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with the LZO library; see the file COPYING.
+   If not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+
+   Markus F.X.J. Oberhumer
+   markus.oberhumer@jk.uni-linz.ac.at
+ */
+
+
+#define LZO_NEED_DICT_H
+#define D_BITS			15
+#define D_INDEX1(d,p)		d = DM((0x21*DX3(p,5,5,6)) >> 5)
+#define D_INDEX2(d,p)		d = (d & (D_MASK & 0x7ff)) ^ (D_HIGH | 0x1f)
+
+#include "config1x.h"
+
+#define DO_COMPRESS		lzo1x_1_15_compress
+
+#include "lzo1x_c.ch"
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/lzo1x_c.ch linux-2.6.18.5/fs/ext2/lzo/lzo1x_c.ch
--- linux-2.6.18.5.org/fs/ext2/lzo/lzo1x_c.ch	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/lzo1x_c.ch	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,352 @@
+/* lzo1x_1.c -- implementation of the LZO1X-1 compression algorithm
+
+   This file is part of the LZO real-time data compression library.
+
+   Copyright (C) 1998 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1997 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1996 Markus Franz Xaver Johannes Oberhumer
+
+   The LZO library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of
+   the License, or (at your option) any later version.
+
+   The LZO library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with the LZO library; see the file COPYING.
+   If not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+
+   Markus F.X.J. Oberhumer
+   markus.oberhumer@jk.uni-linz.ac.at
+ */
+
+
+
+/***********************************************************************
+// compress a block of data.
+************************************************************************/
+
+static
+lzo_uint do_compress     ( const lzo_byte *in , lzo_uint  in_len,
+                                 lzo_byte *out, lzo_uint *out_len,
+                                 lzo_voidp wrkmem )
+{
+#if 0 && defined(__GNUC__) && defined(__i386__)
+	register const lzo_byte *ip __asm__("%esi");
+#else
+	register const lzo_byte *ip;
+#endif
+	lzo_byte *op;
+	const lzo_byte * const in_end = in + in_len;
+	const lzo_byte * const ip_end = in + in_len - M2_MAX_LEN - 5;
+	const lzo_byte *ii;
+	lzo_dict_p const dict = (lzo_dict_p) wrkmem;
+
+	op = out;
+	ip = in;
+	ii = ip;
+
+	ip += 4;
+	for (;;)
+	{
+#if 0 && defined(__GNUC__) && defined(__i386__)
+		register const lzo_byte *m_pos __asm__("%edi");
+#else
+		register const lzo_byte *m_pos;
+#endif
+		lzo_moff_t m_off;
+		lzo_uint m_len;
+		lzo_uint dindex;
+
+		DINDEX1(dindex,ip);
+		GINDEX(m_pos,m_off,dict,dindex,in);
+		if (LZO_CHECK_MPOS_NON_DET(m_pos,m_off,in,ip,M4_MAX_OFFSET))
+			goto literal;
+#if 1
+		if (m_off <= M2_MAX_OFFSET || m_pos[3] == ip[3])
+			goto try_match;
+		DINDEX2(dindex,ip);
+#endif
+		GINDEX(m_pos,m_off,dict,dindex,in);
+		if (LZO_CHECK_MPOS_NON_DET(m_pos,m_off,in,ip,M4_MAX_OFFSET))
+			goto literal;
+		if (m_off <= M2_MAX_OFFSET || m_pos[3] == ip[3])
+			goto try_match;
+		goto literal;
+
+
+try_match:
+#if 1 && defined(LZO_UNALIGNED_OK_2)
+		if (* (const lzo_ushortp) m_pos != * (const lzo_ushortp) ip)
+#else
+		if (m_pos[0] != ip[0] || m_pos[1] != ip[1])
+#endif
+		{
+		}
+		else
+		{
+			if (m_pos[2] == ip[2])
+			{
+#if 0
+				if (m_off <= M2_MAX_OFFSET)
+					goto match;
+				if (lit <= 3)
+					goto match;
+				if (lit == 3)			/* better compression, but slower */
+				{
+					assert(op - 2 > out); op[-2] |= LZO_BYTE(3);
+					*op++ = *ii++; *op++ = *ii++; *op++ = *ii++;
+					goto code_match;
+				}
+				if (m_pos[3] == ip[3])
+#endif
+					goto match;
+			}
+			else
+			{
+				/* still need a better way for finding M1 matches */
+#if 0
+				/* a M1 match */
+#if 0
+				if (m_off <= M1_MAX_OFFSET && lit > 0 && lit <= 3)
+#else
+				if (m_off <= M1_MAX_OFFSET && lit == 3)
+#endif
+				{
+					register lzo_uint t;
+
+					t = lit;
+					assert(op - 2 > out); op[-2] |= LZO_BYTE(t);
+					do *op++ = *ii++; while (--t > 0);
+					assert(ii == ip);
+					m_off -= 1;
+					*op++ = LZO_BYTE(M1_MARKER | ((m_off & 3) << 2));
+					*op++ = LZO_BYTE(m_off >> 2);
+					ip += 2;
+					goto match_done;
+				}
+#endif
+			}
+		}
+
+
+	/* a literal */
+literal:
+		UPDATE_I(dict,0,dindex,ip,in);
+		++ip;
+		if (ip >= ip_end)
+			break;
+		continue;
+
+
+	/* a match */
+match:
+		UPDATE_I(dict,0,dindex,ip,in);
+		/* store current literal run */
+		if (ip - ii > 0)
+		{
+			register lzo_uint t = ip - ii;
+
+			if (t <= 3)
+			{
+				assert(op - 2 > out);
+				op[-2] |= LZO_BYTE(t);
+			}
+			else if (t <= 18)
+				*op++ = LZO_BYTE(t - 3);
+			else
+			{
+				register lzo_uint tt = t - 18;
+
+				*op++ = 0;
+				while (tt > 255)
+				{
+					tt -= 255;
+					*op++ = 0;
+				}
+				assert(tt > 0);
+				*op++ = LZO_BYTE(tt);
+			}
+			do *op++ = *ii++; while (--t > 0);
+		}
+
+		/* code the match */
+		assert(ii == ip);
+		ip += 3;
+		if (m_pos[3] != *ip++ || m_pos[4] != *ip++ || m_pos[5] != *ip++ ||
+		    m_pos[6] != *ip++ || m_pos[7] != *ip++ || m_pos[8] != *ip++
+#ifdef LZO1Y
+		    || m_pos[ 9] != *ip++ || m_pos[10] != *ip++ || m_pos[11] != *ip++
+		    || m_pos[12] != *ip++ || m_pos[13] != *ip++ || m_pos[14] != *ip++
+#endif
+           )
+		{
+			--ip;
+			m_len = ip - ii;
+			assert(m_len >= 3); assert(m_len <= M2_MAX_LEN);
+
+			if (m_off <= M2_MAX_OFFSET)
+			{
+				m_off -= 1;
+#if defined(LZO1X)
+				*op++ = LZO_BYTE(((m_len - 1) << 5) | ((m_off & 7) << 2));
+				*op++ = LZO_BYTE(m_off >> 3);
+#elif defined(LZO1Y)
+				*op++ = LZO_BYTE(((m_len + 1) << 4) | ((m_off & 3) << 2));
+				*op++ = LZO_BYTE(m_off >> 2);
+#endif
+			}
+			else if (m_off <= M3_MAX_OFFSET)
+			{
+				m_off -= 1;
+				*op++ = LZO_BYTE(M3_MARKER | (m_len - 2));
+				goto m3_m4_offset;
+			}
+			else
+#if defined(LZO1X)
+			{
+				m_off -= 0x4000;
+				assert(m_off > 0); assert(m_off <= 0x7fff);
+				*op++ = LZO_BYTE(M4_MARKER |
+				                 ((m_off & 0x4000) >> 11) | (m_len - 2));
+				goto m3_m4_offset;
+			}
+#elif defined(LZO1Y)
+				goto m4_match;
+#endif
+		}
+		else
+		{
+			{
+				const lzo_byte *end = in_end;
+				const lzo_byte *m = m_pos + M2_MAX_LEN + 1;
+				while (ip < end && *m == *ip)
+					m++, ip++;
+				m_len = (ip - ii);
+			}
+			assert(m_len > M2_MAX_LEN);
+
+			if (m_off <= M3_MAX_OFFSET)
+			{
+				m_off -= 1;
+				if (m_len <= 33)
+					*op++ = LZO_BYTE(M3_MARKER | (m_len - 2));
+				else
+				{
+					m_len -= 33;
+					*op++ = M3_MARKER | 0;
+					goto m3_m4_len;
+				}
+			}
+			else
+			{
+#if defined(LZO1Y)
+m4_match:
+#endif
+				m_off -= 0x4000;
+				assert(m_off > 0); assert(m_off <= 0x7fff);
+				if (m_len <= M4_MAX_LEN)
+					*op++ = LZO_BYTE(M4_MARKER |
+					                 ((m_off & 0x4000) >> 11) | (m_len - 2));
+				else
+				{
+					m_len -= M4_MAX_LEN;
+					*op++ = LZO_BYTE(M4_MARKER | ((m_off & 0x4000) >> 11));
+m3_m4_len:
+					while (m_len > 255)
+					{
+						m_len -= 255;
+						*op++ = 0;
+					}
+					assert(m_len > 0);
+					*op++ = LZO_BYTE(m_len);
+				}
+			}
+
+m3_m4_offset:
+			*op++ = LZO_BYTE((m_off & 63) << 2);
+			*op++ = LZO_BYTE(m_off >> 6);
+		}
+
+#if 0
+match_done:
+#endif
+		ii = ip;
+		if (ip >= ip_end)
+			break;
+	}
+
+	*out_len = op - out;
+	return (lzo_uint) (in_end - ii);
+}
+
+
+/***********************************************************************
+// public entry point
+************************************************************************/
+
+LZO_PUBLIC(int)
+DO_COMPRESS      ( const lzo_byte *in , lzo_uint  in_len,
+                         lzo_byte *out, lzo_uint *out_len,
+                         lzo_voidp wrkmem )
+{
+	lzo_byte *op = out;
+	lzo_uint t;
+
+#if defined(__LZO_QUERY_COMPRESS)
+	if (__LZO_IS_COMPRESS_QUERY(in,in_len,out,out_len,wrkmem))
+		return __LZO_QUERY_COMPRESS(in,in_len,out,out_len,wrkmem,D_SIZE,lzo_sizeof(lzo_dict_t));
+#endif
+
+	if (in_len <= M2_MAX_LEN + 5)
+		t = in_len;
+	else
+	{
+		t = do_compress(in,in_len,op,out_len,wrkmem);
+		op += *out_len;
+	}
+
+	if (t > 0)
+	{
+		const lzo_byte *ii = in + in_len - t;
+
+		if (op == out && t <= 238)
+			*op++ = LZO_BYTE(17 + t);
+		else if (t <= 3)
+			op[-2] |= LZO_BYTE(t);
+		else if (t <= 18)
+			*op++ = LZO_BYTE(t - 3);
+		else
+		{
+			lzo_uint tt = t - 18;
+
+			*op++ = 0;
+			while (tt > 255)
+			{
+				tt -= 255;
+				*op++ = 0;
+			}
+			assert(tt > 0);
+			*op++ = LZO_BYTE(tt);
+		}
+		do *op++ = *ii++; while (--t > 0);
+	}
+
+	*op++ = M4_MARKER | 1;
+	*op++ = 0;
+	*op++ = 0;
+
+	*out_len = op - out;
+	return LZO_E_OK;
+}
+
+
+/*
+vi:ts=4
+*/
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/lzo1x_d.ch linux-2.6.18.5/fs/ext2/lzo/lzo1x_d.ch
--- linux-2.6.18.5.org/fs/ext2/lzo/lzo1x_d.ch	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/lzo1x_d.ch	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,443 @@
+/* lzo1x_d.ch -- implementation of the LZO1X decompression algorithm
+
+   This file is part of the LZO real-time data compression library.
+
+   Copyright (C) 1997 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1996 Markus Franz Xaver Johannes Oberhumer
+
+   The LZO library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of
+   the License, or (at your option) any later version.
+
+   The LZO library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with the LZO library; see the file COPYING.
+   If not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+
+   Markus F.X.J. Oberhumer
+   markus.oberhumer@jk.uni-linz.ac.at
+ */
+
+
+#include "lzo1_d.ch"
+
+
+/***********************************************************************
+// decompress a block of data.
+************************************************************************/
+
+#if defined(DO_DECOMPRESS)
+LZO_PUBLIC(int)
+DO_DECOMPRESS  ( const lzo_byte *in , lzo_uint  in_len,
+                       lzo_byte *out, lzo_uint *out_len,
+                       lzo_voidp wrkmem )
+#endif
+{
+	register lzo_byte *op;
+	register const lzo_byte *ip;
+	register lzo_uint t;
+#if defined(COPY_DICT)
+	lzo_uint m_off;
+	const lzo_byte *dict_end;
+#else
+	register const lzo_byte *m_pos;
+#endif
+
+	const lzo_byte * const ip_end = in + in_len;
+#if defined(HAVE_ANY_OP)
+	lzo_byte * const op_end = out + *out_len;
+#endif
+#if defined(LZO1Z)
+	lzo_uint last_m_off = 0;
+#endif
+
+	LZO_UNUSED(wrkmem);
+
+#if defined(__LZO_QUERY_DECOMPRESS)
+	if (__LZO_IS_DECOMPRESS_QUERY(in,in_len,out,out_len,wrkmem))
+		return __LZO_QUERY_DECOMPRESS(in,in_len,out,out_len,wrkmem,0,0);
+#endif
+
+#if defined(COPY_DICT)
+	if (dict)
+	{
+		if (dict_len > M4_MAX_OFFSET)
+		{
+			dict += dict_len - M4_MAX_OFFSET;
+			dict_len = M4_MAX_OFFSET;
+		}
+		dict_end = dict + dict_len;
+	}
+	else
+	{
+		dict_len = 0;
+		dict_end = NULL;
+	}
+#endif /* COPY_DICT */
+
+	*out_len = 0;
+
+	op = out;
+	ip = in;
+
+	if (*ip > 17)
+	{
+		t = *ip++ - 17;
+		if (t < 4)
+			goto match_next;
+		assert(t > 0); NEED_OP(t); NEED_IP(t+1);
+		do *op++ = *ip++; while (--t > 0);
+		goto first_literal_run;
+	}
+
+	while (TEST_IP && TEST_OP)
+	{
+		t = *ip++;
+		if (t >= 16)
+			goto match;
+		/* a literal run */
+		if (t == 0)
+		{
+			NEED_IP(1);
+			while (*ip == 0)
+			{
+				t += 255;
+				ip++;
+				NEED_IP(1);
+			}
+			t += 15 + *ip++;
+		}
+		/* copy literals */
+		assert(t > 0); NEED_OP(t+3); NEED_IP(t+4);
+#if defined(LZO_UNALIGNED_OK_4) || defined(LZO_ALIGNED_OK_4)
+#if !defined(LZO_UNALIGNED_OK_4)
+		if (PTR_ALIGNED2_4(op,ip))
+		{
+#endif
+		* (lzo_uint32p) op = * (const lzo_uint32p) ip;
+		op += 4; ip += 4;
+		if (--t > 0)
+		{
+			if (t >= 4)
+			{
+				do {
+					* (lzo_uint32p) op = * (const lzo_uint32p) ip;
+					op += 4; ip += 4; t -= 4;
+				} while (t >= 4);
+				if (t > 0) do *op++ = *ip++; while (--t > 0);
+			}
+			else
+				do *op++ = *ip++; while (--t > 0);
+		}
+#if !defined(LZO_UNALIGNED_OK_4)
+		}
+		else
+#endif
+#endif
+#if !defined(LZO_UNALIGNED_OK_4)
+		{
+			*op++ = *ip++; *op++ = *ip++; *op++ = *ip++;
+			do *op++ = *ip++; while (--t > 0);
+		}
+#endif
+
+
+first_literal_run:
+
+
+		t = *ip++;
+		if (t >= 16)
+			goto match;
+#if defined(COPY_DICT)
+#if defined(LZO1Z)
+		m_off = (1 + M2_MAX_OFFSET) + (t << 6) + (*ip++ >> 2);
+		last_m_off = m_off;
+#else
+		m_off = (1 + M2_MAX_OFFSET) + (t >> 2) + (*ip++ << 2);
+#endif
+		NEED_OP(3);
+		t = 3; COPY_DICT(t,m_off)
+#else /* !COPY_DICT */
+#if defined(LZO1Z)
+		t = (1 + M2_MAX_OFFSET) + (t << 6) + (*ip++ >> 2);
+		m_pos = op - t;
+		last_m_off = t;
+#else
+		m_pos = op - (1 + M2_MAX_OFFSET);
+		m_pos -= t >> 2;
+		m_pos -= *ip++ << 2;
+#endif
+		TEST_LOOKBEHIND(m_pos,out); NEED_OP(3);
+		*op++ = *m_pos++; *op++ = *m_pos++; *op++ = *m_pos;
+#endif /* COPY_DICT */
+		goto match_done;
+
+
+		/* handle matches */
+		while (TEST_IP && TEST_OP)
+		{
+match:
+			if (t >= 64)				/* a M2 match */
+			{
+#if defined(COPY_DICT)
+#if defined(LZO1X)
+				m_off = 1 + ((t >> 2) & 7) + (*ip++ << 3);
+				t = (t >> 5) - 1;
+#elif defined(LZO1Y)
+				m_off = 1 + ((t >> 2) & 3) + (*ip++ << 2);
+				t = (t >> 4) - 3;
+#elif defined(LZO1Z)
+				m_off = t & 0x1f;
+				if (m_off >= 0x1c)
+					m_off = last_m_off;
+				else
+				{
+					m_off = 1 + (m_off << 6) + (*ip++ >> 2);
+					last_m_off = m_off;
+				}
+				t = (t >> 5) - 1;
+#endif
+#else /* !COPY_DICT */
+#if defined(LZO1X)
+				m_pos = op - 1;
+				m_pos -= (t >> 2) & 7;
+				m_pos -= *ip++ << 3;
+				t = (t >> 5) - 1;
+#elif defined(LZO1Y)
+				m_pos = op - 1;
+				m_pos -= (t >> 2) & 3;
+				m_pos -= *ip++ << 2;
+				t = (t >> 4) - 3;
+#elif defined(LZO1Z)
+				{
+					lzo_uint off = t & 0x1f;
+					m_pos = op;
+					if (off >= 0x1c)
+					{
+						assert(last_m_off > 0);
+						m_pos -= last_m_off;
+					}
+					else
+					{
+						off = 1 + (off << 6) + (*ip++ >> 2);
+						m_pos -= off;
+						last_m_off = off;
+					}
+				}
+				t = (t >> 5) - 1;
+#endif
+				TEST_LOOKBEHIND(m_pos,out); assert(t > 0); NEED_OP(t+3-1);
+				goto copy_match;
+#endif /* COPY_DICT */
+			}
+			else if (t >= 32)			/* a M3 match */
+			{
+				t &= 31;
+				if (t == 0)
+				{
+					NEED_IP(1);
+					while (*ip == 0)
+					{
+						t += 255;
+						ip++;
+						NEED_IP(1);
+					}
+					t += 31 + *ip++;
+				}
+#if defined(COPY_DICT)
+#if defined(LZO1Z)
+				m_off = 1 + (ip[0] << 6) + (ip[1] >> 2);
+				last_m_off = m_off;
+#else
+				m_off = 1 + (ip[0] >> 2) + (ip[1] << 6);
+#endif
+#else /* !COPY_DICT */
+#if defined(LZO1Z)
+				{
+					lzo_uint off = 1 + (ip[0] << 6) + (ip[1] >> 2);
+					m_pos = op - off;
+					last_m_off = off;
+				}
+#elif defined(LZO_UNALIGNED_OK_2) && (LZO_BYTE_ORDER == LZO_LITTLE_ENDIAN)
+				m_pos = op - 1;
+				m_pos -= (* (const lzo_ushortp) ip) >> 2;
+#else
+				m_pos = op - 1;
+				m_pos -= (ip[0] >> 2) + (ip[1] << 6);
+#endif
+#endif /* COPY_DICT */
+				ip += 2;
+			}
+			else if (t >= 16)			/* a M4 match */
+			{
+#if defined(COPY_DICT)
+				m_off = (t & 8) << 11;
+#else /* !COPY_DICT */
+				m_pos = op;
+				m_pos -= (t & 8) << 11;
+#endif /* COPY_DICT */
+				t &= 7;
+				if (t == 0)
+				{
+					NEED_IP(1);
+					while (*ip == 0)
+					{
+						t += 255;
+						ip++;
+						NEED_IP(1);
+					}
+					t += 7 + *ip++;
+				}
+#if defined(COPY_DICT)
+#if defined(LZO1Z)
+				m_off += (ip[0] << 6) + (ip[1] >> 2);
+#else
+				m_off += (ip[0] >> 2) + (ip[1] << 6);
+#endif
+				ip += 2;
+				if (m_off == 0)
+					goto eof_found;
+				m_off += 0x4000;
+#if defined(LZO1Z)
+				last_m_off = m_off;
+#endif
+#else /* !COPY_DICT */
+#if defined(LZO1Z)
+				m_pos -= (ip[0] << 6) + (ip[1] >> 2);
+#elif defined(LZO_UNALIGNED_OK_2) && (LZO_BYTE_ORDER == LZO_LITTLE_ENDIAN)
+				m_pos -= (* (const lzo_ushortp) ip) >> 2;
+#else
+				m_pos -= (ip[0] >> 2) + (ip[1] << 6);
+#endif
+				ip += 2;
+				if (m_pos == op)
+					goto eof_found;
+				m_pos -= 0x4000;
+#if defined(LZO1Z)
+				last_m_off = op - m_pos;
+#endif
+#endif /* COPY_DICT */
+			}
+			else							/* a M1 match */
+			{
+#if defined(COPY_DICT)
+#if defined(LZO1Z)
+				m_off = 1 + (t << 6) + (*ip++ >> 2);
+				last_m_off = m_off;
+#else
+				m_off = 1 + (t >> 2) + (*ip++ << 2);
+#endif
+				NEED_OP(2);
+				t = 2; COPY_DICT(t,m_off)
+#else /* !COPY_DICT */
+#if defined(LZO1Z)
+				t = 1 + (t << 6) + (*ip++ >> 2);
+				m_pos = op - t;
+				last_m_off = t;
+#else
+				m_pos = op - 1;
+				m_pos -= t >> 2;
+				m_pos -= *ip++ << 2;
+#endif
+				TEST_LOOKBEHIND(m_pos,out); NEED_OP(2);
+				*op++ = *m_pos++; *op++ = *m_pos;
+#endif /* COPY_DICT */
+				goto match_done;
+			}
+
+			/* copy match */
+#if defined(COPY_DICT)
+
+			NEED_OP(t+3-1);
+			t += 3-1; COPY_DICT(t,m_off)
+
+#else /* !COPY_DICT */
+
+			TEST_LOOKBEHIND(m_pos,out); assert(t > 0); NEED_OP(t+3-1);
+#if defined(LZO_UNALIGNED_OK_4) || defined(LZO_ALIGNED_OK_4)
+#if !defined(LZO_UNALIGNED_OK_4)
+			if (t >= 2 * 4 - (3 - 1) && PTR_ALIGNED2_4(op,m_pos))
+			{
+				assert((op - m_pos) >= 4);	/* both pointers are aligned */
+#else
+			if (t >= 2 * 4 - (3 - 1) && (op - m_pos) >= 4)
+			{
+#endif
+				* (lzo_uint32p) op = * (const lzo_uint32p) m_pos;
+				op += 4; m_pos += 4; t -= 4 - (3 - 1);
+				do {
+					* (lzo_uint32p) op = * (const lzo_uint32p) m_pos;
+					op += 4; m_pos += 4; t -= 4;
+				} while (t >= 4);
+				if (t > 0) do *op++ = *m_pos++; while (--t > 0);
+			}
+			else
+#endif
+			{
+copy_match:
+				*op++ = *m_pos++; *op++ = *m_pos++;
+				do *op++ = *m_pos++; while (--t > 0);
+			}
+
+#endif /* COPY_DICT */
+
+match_done:
+#if defined(LZO1Z)
+			t = ip[-1] & 3;
+#else
+			t = ip[-2] & 3;
+#endif
+			if (t == 0)
+				break;
+
+			/* copy literals */
+match_next:
+			assert(t > 0); NEED_OP(t); NEED_IP(t+1);
+			do *op++ = *ip++; while (--t > 0);
+			t = *ip++;
+		}
+	}
+
+#if defined(HAVE_TEST_IP) || defined(HAVE_TEST_OP)
+	/* no EOF code was found */
+	*out_len = op - out;
+	return LZO_E_EOF_NOT_FOUND;
+#endif
+
+eof_found:
+	assert(t == 1);
+	*out_len = op - out;
+	return (ip == ip_end ? LZO_E_OK :
+	       (ip < ip_end  ? LZO_E_INPUT_NOT_CONSUMED : LZO_E_INPUT_OVERRUN));
+
+
+#if defined(HAVE_NEED_IP)
+input_overrun:
+	*out_len = op - out;
+	return LZO_E_INPUT_OVERRUN;
+#endif
+
+#if defined(HAVE_NEED_OP)
+output_overrun:
+	*out_len = op - out;
+	return LZO_E_OUTPUT_OVERRUN;
+#endif
+
+#if defined(LZO_TEST_DECOMPRESS_OVERRUN_LOOKBEHIND)
+lookbehind_overrun:
+	*out_len = op - out;
+	return LZO_E_LOOKBEHIND_OVERRUN;
+#endif
+}
+
+
+/*
+vi:ts=4
+*/
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/lzo1x_d1.c linux-2.6.18.5/fs/ext2/lzo/lzo1x_d1.c
--- linux-2.6.18.5.org/fs/ext2/lzo/lzo1x_d1.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/lzo1x_d1.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,32 @@
+/* lzo1x_d1.c -- LZO1X decompression
+
+   This file is part of the LZO real-time data compression library.
+
+   Copyright (C) 1996 Markus Franz Xaver Johannes Oberhumer
+
+   The LZO library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of
+   the License, or (at your option) any later version.
+
+   The LZO library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with the LZO library; see the file COPYING.
+   If not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+
+   Markus F.X.J. Oberhumer
+   markus.oberhumer@jk.uni-linz.ac.at
+ */
+
+
+#include "config1x.h"
+
+#undef LZO_TEST_DECOMPRESS_OVERRUN
+#define DO_DECOMPRESS		lzo1x_decompress
+
+#include "lzo1x_d.ch"
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/lzo_conf.h linux-2.6.18.5/fs/ext2/lzo/lzo_conf.h
--- linux-2.6.18.5.org/fs/ext2/lzo/lzo_conf.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/lzo_conf.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,409 @@
+/* lzo_conf.h -- main internal configuration file for the the LZO library
+
+   This file is part of the LZO real-time data compression library.
+
+   Copyright (C) 1998 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1997 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1996 Markus Franz Xaver Johannes Oberhumer
+
+   The LZO library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of
+   the License, or (at your option) any later version.
+
+   The LZO library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with the LZO library; see the file COPYING.
+   If not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+
+   Markus F.X.J. Oberhumer
+   markus.oberhumer@jk.uni-linz.ac.at
+ */
+
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the library and is subject
+   to change.
+ */
+
+
+#ifndef __LZO_CONF_H
+#define __LZO_CONF_H
+
+#if !defined(__LZO_IN_MINILZO)
+#  ifndef __LZOCONF_H
+#    include <lzoconf.h>
+#  endif
+#endif
+
+
+/***********************************************************************
+// autoconf section
+************************************************************************/
+
+#if 1					// we're in the kernel, so no libc
+#  define HAVE_MEMCMP
+#  define HAVE_MEMCPY
+#  define HAVE_MEMMOVE
+#  define HAVE_MEMSET
+#endif
+
+/* #if !defined(LZO_HAVE_CONFIG_H) */
+#if 0
+#  include <stddef.h>			/* ptrdiff_t, size_t */
+#  include <string.h>			/* memcpy, memmove, memcmp, memset */
+#  if !defined(NO_STDLIB_H)
+#    include <stdlib.h>
+#  endif
+/* #else */
+#  include <sys/types.h>
+#  if defined(STDC_HEADERS)
+#    include <string.h>
+#    include <stdlib.h>
+#  endif
+#  if defined(HAVE_STDDEF_H)
+#    include <stddef.h>
+#  endif
+#  if defined(HAVE_MEMORY_H)
+#    include <memory.h>
+#  endif
+#endif
+
+#if defined(__LZO_DOS16) || defined(__LZO_WIN16)
+#  define HAVE_MALLOC_H
+#  define HAVE_HALLOC
+#endif
+
+
+#undef NDEBUG
+#if !defined(LZO_DEBUG)
+#  define NDEBUG
+#endif
+#if defined(LZO_DEBUG) || !defined(NDEBUG)
+#  if !defined(NO_STDIO_H)
+#    include <stdio.h>
+#  endif
+#endif
+
+
+#if defined(__BOUNDS_CHECKING_ON)
+#  include <unchecked.h>
+#else
+#  define BOUNDS_CHECKING_OFF_DURING(stmt)		stmt
+#  define BOUNDS_CHECKING_OFF_IN_EXPR(expr)		(expr)
+#endif
+
+
+#if !defined(LZO_UNUSED)
+#  define LZO_UNUSED(parm)	(parm = parm)
+#endif
+
+
+#if !defined(__inline__) && !defined(__GNUC__)
+#  if defined(__cplusplus)
+#    define __inline__		inline
+#  else
+#    define __inline__		/* nothing */
+#  endif
+#endif
+
+
+#if defined(NO_MEMCMP)
+#  undef HAVE_MEMCMP
+#endif
+
+#if !defined(HAVE_MEMCMP)
+#  undef memcmp
+#  define memcmp	lzo_memcmp
+#endif
+#if !defined(HAVE_MEMCPY)
+#  undef memcpy
+#  define memcpy	lzo_memcpy
+#endif
+#if !defined(HAVE_MEMMOVE)
+#  undef memmove
+#  define memmove	lzo_memmove
+#endif
+#if !defined(HAVE_MEMSET)
+#  undef memset
+#  define memset	lzo_memset
+#endif
+
+
+/***********************************************************************
+//
+************************************************************************/
+
+#if 1
+#  define LZO_BYTE(x)		((unsigned char) (x))
+#else
+#  define LZO_BYTE(x)		((unsigned char) ((x) & 0xff))
+#endif
+#if 0
+#  define LZO_USHORT(x)		((unsigned short) (x))
+#else
+#  define LZO_USHORT(x)		((unsigned short) ((x) & 0xffff))
+#endif
+
+#define LZO_MAX(a,b)		((a) >= (b) ? (a) : (b))
+#define LZO_MIN(a,b)		((a) <= (b) ? (a) : (b))
+#define LZO_MAX3(a,b,c)		((a) >= (b) ? LZO_MAX(a,c) : LZO_MAX(b,c))
+#define LZO_MIN3(a,b,c)		((a) <= (b) ? LZO_MIN(a,c) : LZO_MIN(b,c))
+
+#define lzo_sizeof(type)	((lzo_uint) (sizeof(type)))
+
+#define LZO_HIGH(array)		((lzo_uint) (sizeof(array)/sizeof(*(array))))
+
+/* this always fits into 16 bits */
+#define LZO_SIZE(bits)		(1u << (bits))
+#define LZO_MASK(bits)		(LZO_SIZE(bits) - 1)
+
+#define LZO_LSIZE(bits)		(1ul << (bits))
+#define LZO_LMASK(bits)		(LZO_LSIZE(bits) - 1)
+
+#define LZO_USIZE(bits)		((lzo_uint) 1 << (bits))
+#define LZO_UMASK(bits)		(LZO_USIZE(bits) - 1)
+
+/* Maximum value of a signed/unsigned type.
+   Do not use casts, avoid overflows ! */
+#define LZO_STYPE_MAX(b)	(((1l  << (8*(b)-2)) - 1l)  + (1l  << (8*(b)-2)))
+#define LZO_UTYPE_MAX(b)	(((1ul << (8*(b)-1)) - 1ul) + (1ul << (8*(b)-1)))
+
+
+/***********************************************************************
+//
+************************************************************************/
+
+#if !defined(SIZEOF_UNSIGNED)
+#  if (UINT_MAX == 0xffff)
+#    define SIZEOF_UNSIGNED			2
+#  elif (UINT_MAX == 0xffffffffL)
+#    define SIZEOF_UNSIGNED			4
+#  elif (UINT_MAX >= 0xffffffffL)
+#    define SIZEOF_UNSIGNED			8
+#  else
+#    error SIZEOF_UNSIGNED
+#  endif
+#endif
+
+#if !defined(SIZEOF_UNSIGNED_LONG)
+#  if (ULONG_MAX == 0xffffffffL)
+#    define SIZEOF_UNSIGNED_LONG	4
+#  elif (ULONG_MAX >= 0xffffffffL)
+#    define SIZEOF_UNSIGNED_LONG	8
+#  else
+#    error SIZEOF_UNSIGNED_LONG
+#  endif
+#endif
+
+
+#if !defined(SIZEOF_SIZE_T)
+#  define SIZEOF_SIZE_T				SIZEOF_UNSIGNED
+#endif
+#if !defined(SIZE_T_MAX)
+#  define SIZE_T_MAX				LZO_UTYPE_MAX(SIZEOF_SIZE_T)
+#endif
+
+
+/***********************************************************************
+// compiler and architecture specific stuff
+************************************************************************/
+
+/* Some defines that indicate if memory can be accessed at unaligned
+ * memory addresses. You should also test that this is actually faster
+ * even if it is allowed by your system.
+ */
+
+#if 1 && defined(__LZO_i386) && (UINT_MAX == 0xffffffffL)
+#  if !defined(LZO_UNALIGNED_OK_2) && (USHRT_MAX == 0xffff)
+#    define LZO_UNALIGNED_OK_2
+#  endif
+#  if !defined(LZO_UNALIGNED_OK_4) && (LZO_UINT32_MAX == 0xffffffffL)
+#    define LZO_UNALIGNED_OK_4
+#  endif
+#endif
+
+#if defined(LZO_UNALIGNED_OK_2) || defined(LZO_UNALIGNED_OK_4)
+#  if !defined(LZO_UNALIGNED_OK)
+#    define LZO_UNALIGNED_OK
+#  endif
+#endif
+
+#if defined(__LZO_NO_UNALIGNED)
+#  undef LZO_UNALIGNED_OK
+#  undef LZO_UNALIGNED_OK_2
+#  undef LZO_UNALIGNED_OK_4
+#endif
+
+#if defined(LZO_UNALIGNED_OK_2) && (USHRT_MAX != 0xffff)
+#  error "LZO_UNALIGNED_OK_2 must not be defined on this system"
+#endif
+#if defined(LZO_UNALIGNED_OK_4) && (LZO_UINT32_MAX != 0xffffffffL)
+#  error "LZO_UNALIGNED_OK_4 must not be defined on this system"
+#endif
+
+
+/* Many modern processors can transfer 32bit words much faster than
+ * bytes - this can significantly speed decompression.
+ */
+
+#if defined(__LZO_NO_ALIGNED)
+#  undef LZO_ALIGNED_OK_4
+#endif
+
+#if defined(LZO_ALIGNED_OK_4) && (LZO_UINT32_MAX != 0xffffffffL)
+#  error "LZO_ALIGNED_OK_4 must not be defined on this system"
+#endif
+
+
+/* Definitions for byte order, according to significance of bytes, from low
+ * addresses to high addresses. The value is what you get by putting '4'
+ * in the most significant byte, '3' in the second most significant byte,
+ * '2' in the second least significant byte, and '1' in the least
+ * significant byte.
+ * The byte order is only needed if we use LZO_UNALIGNED_OK.
+ */
+
+#define	LZO_LITTLE_ENDIAN		1234
+#define	LZO_BIG_ENDIAN			4321
+#define	LZO_PDP_ENDIAN			3412
+
+#if !defined(LZO_BYTE_ORDER)
+#  if defined(MFX_BYTE_ORDER)
+#    define LZO_BYTE_ORDER		MFX_BYTE_ORDER
+#  elif defined(__LZO_i386)
+#    define LZO_BYTE_ORDER		LZO_LITTLE_ENDIAN
+#  elif defined(BYTE_ORDER)
+#    define LZO_BYTE_ORDER		BYTE_ORDER
+#  elif defined(__BYTE_ORDER)
+#    define LZO_BYTE_ORDER		__BYTE_ORDER
+#  endif
+#endif
+
+#if defined(LZO_BYTE_ORDER)
+#  if (LZO_BYTE_ORDER != LZO_LITTLE_ENDIAN) && \
+      (LZO_BYTE_ORDER != LZO_BIG_ENDIAN)
+#    error "invalid LZO_BYTE_ORDER"
+#  endif
+#endif
+
+#if defined(LZO_UNALIGNED_OK) && !defined(LZO_BYTE_ORDER)
+#  error "LZO_BYTE_ORDER is not defined"
+#endif
+
+
+/***********************************************************************
+// optimization
+************************************************************************/
+
+/* gcc 2.6.3 and gcc 2.7.2 have a bug with 'register xxx __asm__("%yyy")' */
+#define LZO_OPTIMIZE_GNUC_i386_IS_BUGGY
+
+/* Help the gcc optimizer with register allocation. */
+#if defined(NDEBUG) && !defined(LZO_DEBUG) && !defined(__BOUNDS_CHECKING_ON)
+#  if defined(__GNUC__) && defined(__i386__)
+#    if !defined(LZO_OPTIMIZE_GNUC_i386_IS_BUGGY)
+#      define LZO_OPTIMIZE_GNUC_i386
+#    endif
+#  endif
+#endif
+
+
+/***********************************************************************
+// some globals
+************************************************************************/
+
+__LZO_EXTERN_C int __lzo_init_done;
+__LZO_EXTERN_C const lzo_byte __lzo_copyright[];
+LZO_EXTERN(const lzo_byte *) lzo_copyright(void);
+__LZO_EXTERN_C const lzo_uint32 _lzo_crc32_table[256];
+
+
+/***********************************************************************
+// ANSI C preprocessor macros
+************************************************************************/
+
+#define _LZO_STRINGIZE(x)			#x
+#define _LZO_MEXPAND(x)				_LZO_STRINGIZE(x)
+
+/* concatenate */
+#define _LZO_CONCAT2(a,b)			a ## b
+#define _LZO_CONCAT3(a,b,c)			a ## b ## c
+#define _LZO_CONCAT4(a,b,c,d)		a ## b ## c ## d
+#define _LZO_CONCAT5(a,b,c,d,e)		a ## b ## c ## d ## e
+
+/* expand and concatenate (by using one level of indirection) */
+#define _LZO_ECONCAT2(a,b)			_LZO_CONCAT2(a,b)
+#define _LZO_ECONCAT3(a,b,c)		_LZO_CONCAT3(a,b,c)
+#define _LZO_ECONCAT4(a,b,c,d)		_LZO_CONCAT4(a,b,c,d)
+#define _LZO_ECONCAT5(a,b,c,d,e)	_LZO_CONCAT5(a,b,c,d,e)
+
+
+/***********************************************************************
+// Query-interface to the algorithms
+************************************************************************/
+
+#if 0
+
+#define __LZO_IS_COMPRESS_QUERY(i,il,o,ol,w)	((lzo_voidp)(o) == (w))
+#define __LZO_QUERY_COMPRESS(i,il,o,ol,w,n,s) \
+				(*ol = (n)*(s), LZO_E_OK)
+
+#define __LZO_IS_DECOMPRESS_QUERY(i,il,o,ol,w)	((lzo_voidp)(o) == (w))
+#define __LZO_QUERY_DECOMPRESS(i,il,o,ol,w,n,s) \
+				(*ol = (n)*(s), LZO_E_OK)
+
+#define __LZO_IS_OPTIMIZE_QUERY(i,il,o,ol,w)	((lzo_voidp)(o) == (w))
+#define __LZO_QUERY_OPTIMIZE(i,il,o,ol,w,n,s) \
+				(*ol = (n)*(s), LZO_E_OK)
+
+#endif
+
+
+/***********************************************************************
+//
+************************************************************************/
+
+#include "lzo_ptr.h"
+
+
+/* Generate compressed data in a deterministic way.
+ * This is fully portable, and compression can be faster as well.
+ * A reason NOT to be deterministic is when the block size is
+ * very small (e.g. 8kB) or the dictionary is big, because
+ * then the initialization of the dictionary becomes a relevant
+ * magnitude for compression speed.
+ */
+#define LZO_DETERMINISTIC
+
+
+#define LZO_DICT_USE_PTR
+#if defined(__LZO_DOS16) || defined(__LZO_WIN16) || defined(__LZO_STRICT_16BIT)
+#  undef LZO_DICT_USE_PTR
+#endif
+
+#if defined(LZO_DICT_USE_PTR)
+#  define lzo_dict_t	const lzo_bytep
+#  define lzo_dict_p	lzo_dict_t __LZO_MMODEL *
+#else
+#  define lzo_dict_t	lzo_uint
+#  define lzo_dict_p	lzo_dict_t __LZO_MMODEL *
+#endif
+
+#if !defined(lzo_moff_t)
+/* must be unsigned */
+#define lzo_moff_t		lzo_uint
+#endif
+
+
+#endif /* already included */
+
+/*
+vi:ts=4
+*/
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/lzo_dict.h linux-2.6.18.5/fs/ext2/lzo/lzo_dict.h
--- linux-2.6.18.5.org/fs/ext2/lzo/lzo_dict.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/lzo_dict.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,302 @@
+/* lzo_dict.h -- dictionary definitions for the the LZO library
+
+   This file is part of the LZO real-time data compression library.
+
+   Copyright (C) 1998 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1997 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1996 Markus Franz Xaver Johannes Oberhumer
+
+   The LZO library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of
+   the License, or (at your option) any later version.
+
+   The LZO library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with the LZO library; see the file COPYING.
+   If not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+
+   Markus F.X.J. Oberhumer
+   markus.oberhumer@jk.uni-linz.ac.at
+ */
+
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the library and is subject
+   to change.
+ */
+
+
+#ifndef __LZO_DICT_H
+#define __LZO_DICT_H
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+
+
+/***********************************************************************
+// dictionary size
+************************************************************************/
+
+/* dictionary needed for compression */
+#if !defined(D_BITS) && defined(DBITS)
+#  define D_BITS		DBITS
+#endif
+#if !defined(D_BITS)
+#  error D_BITS is not defined
+#endif
+#if (D_BITS < 16)
+#  define D_SIZE		LZO_SIZE(D_BITS)
+#  define D_MASK		LZO_MASK(D_BITS)
+#else
+#  define D_SIZE		LZO_USIZE(D_BITS)
+#  define D_MASK		LZO_UMASK(D_BITS)
+#endif
+#define D_HIGH			((D_MASK >> 1) + 1)
+
+
+/* dictionary depth */
+#if !defined(DD_BITS)
+#  define DD_BITS		0
+#endif
+#define DD_SIZE			LZO_SIZE(DD_BITS)
+#define DD_MASK			LZO_MASK(DD_BITS)
+
+/* dictionary length */
+#if !defined(DL_BITS)
+#  define DL_BITS		(D_BITS - DD_BITS)
+#endif
+#if (DL_BITS < 16)
+#  define DL_SIZE		LZO_SIZE(DL_BITS)
+#  define DL_MASK		LZO_MASK(DL_BITS)
+#else
+#  define DL_SIZE		LZO_USIZE(DL_BITS)
+#  define DL_MASK		LZO_UMASK(DL_BITS)
+#endif
+
+
+#if (D_BITS != DL_BITS + DD_BITS)
+#  error D_BITS does not match
+#endif
+#if (D_BITS < 8 || D_BITS > 18)
+#  error invalid D_BITS
+#endif
+#if (DL_BITS < 8 || DL_BITS > 20)
+#  error invalid DL_BITS
+#endif
+#if (DD_BITS < 0 || DD_BITS > 6)
+#  error invalid DD_BITS
+#endif
+
+
+#if !defined(DL_MIN_LEN)
+#  define DL_MIN_LEN	3
+#endif
+#if !defined(DL_SHIFT)
+#  define DL_SHIFT		((DL_BITS + (DL_MIN_LEN - 1)) / DL_MIN_LEN)
+#endif
+
+
+
+/***********************************************************************
+// dictionary access
+************************************************************************/
+
+#define LZO_HASH_GZIP					1
+#define LZO_HASH_GZIP_INCREMENTAL		2
+#define LZO_HASH_LZO_INCREMENTAL_A		3
+#define LZO_HASH_LZO_INCREMENTAL_B		4
+
+#if !defined(LZO_HASH)
+#  error choose a hashing strategy
+#endif
+
+
+#if (DL_MIN_LEN == 3)
+#  define _DV2_A(p,shift1,shift2) \
+		(((( (lzo_uint32)((p)[0]) << shift1) ^ (p)[1]) << shift2) ^ (p)[2])
+#  define _DV2_B(p,shift1,shift2) \
+		(((( (lzo_uint32)((p)[2]) << shift1) ^ (p)[1]) << shift2) ^ (p)[0])
+#  define _DV3_B(p,shift1,shift2,shift3) \
+		((_DV2_B((p)+1,shift1,shift2) << (shift3)) ^ (p)[0])
+#elif (DL_MIN_LEN == 2)
+#  define _DV2_A(p,shift1,shift2) \
+		(( (lzo_uint32)(p[0]) << shift1) ^ p[1])
+#  define _DV2_B(p,shift1,shift2) \
+		(( (lzo_uint32)(p[1]) << shift1) ^ p[2])
+#else
+#  error invalid DL_MIN_LEN
+#endif
+#define _DV_A(p,shift) 		_DV2_A(p,shift,shift)
+#define _DV_B(p,shift)	 	_DV2_B(p,shift,shift)
+#define DA2(p,s1,s2) \
+		(((((lzo_uint32)((p)[2]) << (s2)) + (p)[1]) << (s1)) + (p)[0])
+#define DS2(p,s1,s2) \
+		(((((lzo_uint32)((p)[2]) << (s2)) - (p)[1]) << (s1)) - (p)[0])
+#define DX2(p,s1,s2) \
+		(((((lzo_uint32)((p)[2]) << (s2)) ^ (p)[1]) << (s1)) ^ (p)[0])
+#define DA3(p,s1,s2,s3) ((DA2((p)+1,s2,s3) << (s1)) + (p)[0])
+#define DS3(p,s1,s2,s3) ((DS2((p)+1,s2,s3) << (s1)) - (p)[0])
+#define DX3(p,s1,s2,s3) ((DX2((p)+1,s2,s3) << (s1)) ^ (p)[0])
+#define DMS(v,s)		((lzo_uint) (((v) & (D_MASK >> (s))) << (s)))
+#define DM(v)			DMS(v,0)
+
+
+
+#if (LZO_HASH == LZO_HASH_GZIP)
+   /* hash function like in gzip/zlib (deflate) */
+#  define _DINDEX(dv,p)		(_DV_A((p),DL_SHIFT))
+
+#elif (LZO_HASH == LZO_HASH_GZIP_INCREMENTAL)
+   /* incremental hash like in gzip/zlib (deflate) */
+#  define __LZO_HASH_INCREMENTAL
+#  define DVAL_FIRST(dv,p)	dv = _DV_A((p),DL_SHIFT)
+#  define DVAL_NEXT(dv,p)	dv = (((dv) << DL_SHIFT) ^ p[2])
+#  define _DINDEX(dv,p)		(dv)
+#  define DVAL_LOOKAHEAD	DL_MIN_LEN
+
+#elif (LZO_HASH == LZO_HASH_LZO_INCREMENTAL_A)
+   /* incremental LZO hash version A */
+#  define __LZO_HASH_INCREMENTAL
+#  define DVAL_FIRST(dv,p)	dv = _DV_A((p),5)
+#  define DVAL_NEXT(dv,p) \
+				dv ^= (lzo_uint32)(p[-1]) << (2*5); dv = (((dv) << 5) ^ p[2])
+#  define _DINDEX(dv,p)		((0x9f5f * (dv)) >> 5)
+#  define DVAL_LOOKAHEAD	DL_MIN_LEN
+
+#elif (LZO_HASH == LZO_HASH_LZO_INCREMENTAL_B)
+   /* incremental LZO hash version B */
+#  define __LZO_HASH_INCREMENTAL
+#  define DVAL_FIRST(dv,p)	dv = _DV_B((p),5)
+#  define DVAL_NEXT(dv,p) \
+				dv ^= p[-1]; dv = (((dv) >> 5) ^ ((lzo_uint32)(p[2]) << (2*5)))
+#  define _DINDEX(dv,p)		((0x9f5f * (dv)) >> 5)
+#  define DVAL_LOOKAHEAD	DL_MIN_LEN
+
+#else
+#  error choose a hashing strategy
+#endif
+
+
+#ifndef DINDEX
+#define DINDEX(dv,p)		((lzo_uint)((_DINDEX(dv,p)) & DL_MASK) << DD_BITS)
+#endif
+#if !defined(DINDEX1) && defined(D_INDEX1)
+#define DINDEX1				D_INDEX1
+#endif
+#if !defined(DINDEX2) && defined(D_INDEX2)
+#define DINDEX2				D_INDEX2
+#endif
+
+
+
+#if !defined(__LZO_HASH_INCREMENTAL)
+#  define DVAL_FIRST(dv,p)	((void) 0)
+#  define DVAL_NEXT(dv,p)	((void) 0)
+#  define DVAL_LOOKAHEAD	0
+#endif
+
+
+#if !defined(DVAL_ASSERT)
+#if defined(__LZO_HASH_INCREMENTAL) && !defined(NDEBUG)
+static void DVAL_ASSERT(lzo_uint32 dv, const lzo_byte *p)
+{
+	lzo_uint32 df;
+	DVAL_FIRST(df,(p));
+	assert(DINDEX(dv,p) == DINDEX(df,p));
+}
+#else
+#  define DVAL_ASSERT(dv,p)	((void) 0)
+#endif
+#endif
+
+
+
+/***********************************************************************
+// dictionary updating
+************************************************************************/
+
+#if defined(LZO_DICT_USE_PTR)
+#  define DENTRY(p,in)							(p)
+#  define GINDEX(m_pos,m_off,dict,dindex,in)	m_pos = dict[dindex]
+#else
+#  define DENTRY(p,in)							((lzo_uint) ((p)-(in)))
+#  define GINDEX(m_pos,m_off,dict,dindex,in)	m_off = dict[dindex]
+#endif
+
+
+#if (DD_BITS == 0)
+
+#  define UPDATE_D(dict,drun,dv,p,in)		dict[ DINDEX(dv,p) ] = DENTRY(p,in)
+#  define UPDATE_I(dict,drun,index,p,in)	dict[index] = DENTRY(p,in)
+#  define UPDATE_P(ptr,drun,p,in)			(ptr)[0] = DENTRY(p,in)
+
+#else
+
+#  define UPDATE_D(dict,drun,dv,p,in)	\
+		dict[ DINDEX(dv,p) + drun++ ] = DENTRY(p,in); drun &= DD_MASK
+#  define UPDATE_I(dict,drun,index,p,in)	\
+		dict[ (index) + drun++ ] = DENTRY(p,in); drun &= DD_MASK
+#  define UPDATE_P(ptr,drun,p,in)	\
+		(ptr) [ drun++ ] = DENTRY(p,in); drun &= DD_MASK
+
+#endif
+
+
+/***********************************************************************
+// test for a match
+************************************************************************/
+
+#if defined(LZO_DICT_USE_PTR)
+
+/* m_pos is either NULL or a valid pointer */
+#define LZO_CHECK_MPOS_DET(m_pos,m_off,in,ip,max_offset) \
+		(m_pos == NULL || (m_off = (lzo_moff_t) (ip - m_pos)) > max_offset)
+
+/* m_pos may point anywhere... */
+#define LZO_CHECK_MPOS_NON_DET(m_pos,m_off,in,ip,max_offset) \
+	(BOUNDS_CHECKING_OFF_IN_EXPR( \
+		(PTR_LT(m_pos,in) || \
+		 (m_off = (lzo_moff_t) PTR_DIFF(ip,m_pos)) <= 0 || \
+		  m_off > max_offset) ))
+
+#else
+
+#define LZO_CHECK_MPOS_DET(m_pos,m_off,in,ip,max_offset) \
+		(m_off == 0 || \
+		 ((m_off = (lzo_moff_t) ((ip)-(in)) - m_off) > max_offset) || \
+		 (m_pos = (ip) - (m_off), 0) )
+
+#define LZO_CHECK_MPOS_NON_DET(m_pos,m_off,in,ip,max_offset) \
+		((lzo_moff_t) ((ip)-(in)) <= m_off || \
+		 ((m_off = (lzo_moff_t) ((ip)-(in)) - m_off) > max_offset) || \
+		 (m_pos = (ip) - (m_off), 0) )
+
+#endif
+
+
+#if defined(LZO_DETERMINISTIC)
+#  define LZO_CHECK_MPOS	LZO_CHECK_MPOS_DET
+#else
+#  define LZO_CHECK_MPOS	LZO_CHECK_MPOS_NON_DET
+#endif
+
+
+
+#ifdef __cplusplus
+} /* extern "C" */
+#endif
+
+#endif /* already included */
+
+/*
+vi:ts=4
+*/
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/lzo_ptr.h linux-2.6.18.5/fs/ext2/lzo/lzo_ptr.h
--- linux-2.6.18.5.org/fs/ext2/lzo/lzo_ptr.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/lzo_ptr.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,202 @@
+/* lzo_ptr.h -- low-level pointer constructs
+
+   This file is part of the LZO real-time data compression library.
+
+   Copyright (C) 1997 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1996 Markus Franz Xaver Johannes Oberhumer
+
+   The LZO library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of
+   the License, or (at your option) any later version.
+
+   The LZO library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with the LZO library; see the file COPYING.
+   If not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+
+   Markus F.X.J. Oberhumer
+   markus.oberhumer@jk.uni-linz.ac.at
+ */
+
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the library and is subject
+   to change.
+ */
+
+
+#ifndef __LZO_PTR_H
+#define __LZO_PTR_H
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+
+/* This is the lowest part of the LZO library.
+ * It deals with pointer representations at bit level.
+ */
+
+
+/***********************************************************************
+// Includes
+************************************************************************/
+
+#if defined(__LZO_DOS16) || defined(__LZO_WIN16)
+#  include <dos.h>
+#  if 1 && defined(__WATCOMC__)
+#    include <i86.h>
+     __LZO_EXTERN_C unsigned char _HShift;
+#    define __LZO_HShift	_HShift
+#  elif 1 && defined(_MSC_VER)
+     __LZO_EXTERN_C unsigned short __near _AHSHIFT;
+#    define __LZO_HShift	((unsigned) &_AHSHIFT)
+#  elif defined(__LZO_WIN16)
+#    define __LZO_HShift	3
+#  else
+#    define __LZO_HShift	12
+#  endif
+#  if !defined(_FP_SEG) && defined(FP_SEG)
+#    define _FP_SEG			FP_SEG
+#  endif
+#  if !defined(_FP_OFF) && defined(FP_OFF)
+#    define _FP_OFF			FP_OFF
+#  endif
+#endif
+
+
+/***********************************************************************
+// Integral types
+************************************************************************/
+
+/* ptrdiff_t */
+#if (UINT_MAX >= 0xffffffffL)
+   typedef ptrdiff_t        	lzo_ptrdiff_t;
+#else
+   typedef long             	lzo_ptrdiff_t;
+#endif
+
+
+/* Unsigned type that has *exactly* the same number of bits as a lzo_voidp */
+#if !defined(__LZO_HAVE_PTR_T)
+#  if defined(lzo_ptr_t)
+#    define __LZO_HAVE_PTR_T
+#  endif
+#endif
+#if !defined(__LZO_HAVE_PTR_T)
+#  if defined(SIZEOF_CHAR_P) && defined(SIZEOF_UNSIGNED_LONG)
+#    if (SIZEOF_CHAR_P == SIZEOF_UNSIGNED_LONG)
+       typedef unsigned long  	lzo_ptr_t;
+       typedef long           	lzo_sptr_t;
+#      define __LZO_HAVE_PTR_T
+#    endif
+#  endif
+#endif
+#if !defined(__LZO_HAVE_PTR_T)
+#  if defined(SIZEOF_CHAR_P) && defined(SIZEOF_UNSIGNED)
+#    if (SIZEOF_CHAR_P == SIZEOF_UNSIGNED)
+       typedef unsigned int   	lzo_ptr_t;
+       typedef int            	lzo_sptr_t;
+#      define __LZO_HAVE_PTR_T
+#    endif
+#  endif
+#endif
+#if !defined(__LZO_HAVE_PTR_T)
+#  if defined(SIZEOF_CHAR_P) && defined(SIZEOF_UNSIGNED_SHORT)
+#    if (SIZEOF_CHAR_P == SIZEOF_UNSIGNED_SHORT)
+       typedef unsigned short 	lzo_ptr_t;
+       typedef short          	lzo_sptr_t;
+#      define __LZO_HAVE_PTR_T
+#    endif
+#  endif
+#endif
+#if !defined(__LZO_HAVE_PTR_T)
+#  if defined(LZO_HAVE_CONFIG_H) || defined(SIZEOF_CHAR_P)
+#    error "no suitable type for lzo_ptr_t"
+#  else
+     typedef unsigned long  	lzo_ptr_t;
+     typedef long           	lzo_sptr_t;
+#    define __LZO_HAVE_PTR_T
+#  endif
+#endif
+
+
+/***********************************************************************
+//
+************************************************************************/
+
+/* Always use the safe (=integral) version for pointer-comparisions.
+ * The compiler should optimize away the additional casts anyway.
+ *
+ * Note that this only works if the representation and ordering
+ * of the pointer and the integral is the same (at bit level).
+ *
+ * Most 16 bit compilers have their own view about pointers -
+ * fortunately they don't care about comparing pointers
+ * that are pointing to Nirvana.
+ */
+
+#if defined(__LZO_DOS16) || defined(__LZO_WIN16)
+#define PTR(a)				((lzo_bytep) (a))
+/* only need the low bits of the pointer -> offset is ok */
+#define PTR_ALIGNED_4(a)	((_FP_OFF(a) & 3) == 0)
+#define PTR_ALIGNED2_4(a,b)	(((_FP_OFF(a) | _FP_OFF(b)) & 3) == 0)
+#else
+#define PTR(a)				((lzo_ptr_t) (a))
+#define PTR_LINEAR(a)		PTR(a)
+#define PTR_ALIGNED_4(a)	((PTR_LINEAR(a) & 3) == 0)
+#define PTR_ALIGNED_8(a)	((PTR_LINEAR(a) & 7) == 0)
+#define PTR_ALIGNED2_4(a,b)	(((PTR_LINEAR(a) | PTR_LINEAR(b)) & 3) == 0)
+#define PTR_ALIGNED2_8(a,b)	(((PTR_LINEAR(a) | PTR_LINEAR(b)) & 7) == 0)
+#endif
+
+#define PTR_LT(a,b)			(PTR(a) < PTR(b))
+#define PTR_GE(a,b)			(PTR(a) >= PTR(b))
+#define PTR_DIFF(a,b)		((lzo_ptrdiff_t) (PTR(a) - PTR(b)))
+
+
+LZO_EXTERN(lzo_ptr_t)
+__lzo_ptr_linear(const lzo_voidp ptr);
+
+
+typedef union
+{
+	char			a_char;
+	unsigned char	a_uchar;
+	short			a_short;
+	unsigned short	a_ushort;
+	int				a_int;
+	unsigned int	a_uint;
+	long			a_long;
+	unsigned long	a_ulong;
+	lzo_int			a_lzo_int;
+	lzo_uint		a_lzo_uint;
+	lzo_int32		a_lzo_int32;
+	lzo_uint32		a_lzo_uint32;
+	ptrdiff_t		a_ptrdiff_t;
+	lzo_ptrdiff_t	a_lzo_ptrdiff_t;
+	lzo_ptr_t		a_lzo_ptr_t;
+	char *			a_charp;
+	lzo_bytep		a_lzo_bytep;
+	lzo_bytepp		a_lzo_bytepp;
+}
+lzo_align_t;
+
+
+
+#ifdef __cplusplus
+} /* extern "C" */
+#endif
+
+#endif /* already included */
+
+/*
+vi:ts=4
+*/
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/lzo_util.h linux-2.6.18.5/fs/ext2/lzo/lzo_util.h
--- linux-2.6.18.5.org/fs/ext2/lzo/lzo_util.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/lzo_util.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,192 @@
+/* lzo_util.h -- utilities for the LZO library
+
+   This file is part of the LZO real-time data compression library.
+
+   Copyright (C) 1997 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1996 Markus Franz Xaver Johannes Oberhumer
+
+   The LZO library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of
+   the License, or (at your option) any later version.
+
+   The LZO library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with the LZO library; see the file COPYING.
+   If not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+
+   Markus F.X.J. Oberhumer
+   markus.oberhumer@jk.uni-linz.ac.at
+ */
+
+
+/* WARNING: this file should *not* be used by applications. It is
+   part of the implementation of the library and is subject
+   to change.
+ */
+
+
+#ifndef __LZO_UTIL_H
+#define __LZO_UTIL_H
+
+#ifndef __LZO_CONF_H
+#  include "lzo_conf.h"
+#endif
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+
+/***********************************************************************
+// fast memcpy that copies multiples of 8 byte chunks.
+// len is the number of bytes.
+// note: all parameters must be lvalues, len >= 8
+//       dest and src advance, len is undefined afterwards
+************************************************************************/
+
+#if 1 && defined(HAVE_MEMCPY)
+#if !defined(__LZO_DOS16) && !defined(__LZO_WIN16)
+
+#define MEMCPY8_DS(dest,src,len) \
+	memcpy(dest,src,len); \
+	dest += len; \
+	src += len
+
+#endif
+#endif
+
+
+#if 0 && !defined(MEMCPY8_DS)
+
+#define MEMCPY8_DS(dest,src,len) \
+	{ do { \
+		*dest++ = *src++; \
+		*dest++ = *src++; \
+		*dest++ = *src++; \
+		*dest++ = *src++; \
+		*dest++ = *src++; \
+		*dest++ = *src++; \
+		*dest++ = *src++; \
+		*dest++ = *src++; \
+		len -= 8; \
+	} while (len > 0); }
+
+#endif
+
+
+#if !defined(MEMCPY8_DS)
+
+#define MEMCPY8_DS(dest,src,len) \
+	{ register lzo_uint __l = (len) / 8; \
+	do { \
+		*dest++ = *src++; \
+		*dest++ = *src++; \
+		*dest++ = *src++; \
+		*dest++ = *src++; \
+		*dest++ = *src++; \
+		*dest++ = *src++; \
+		*dest++ = *src++; \
+		*dest++ = *src++; \
+	} while (--__l > 0); }
+
+#endif
+
+
+/***********************************************************************
+// memcpy and pseudo-memmove
+// len is the number of bytes.
+// note: all parameters must be lvalues, len > 0
+//       dest and src advance, len is undefined afterwards
+************************************************************************/
+
+#define MEMCPY_DS(dest,src,len) \
+	do *dest++ = *src++; \
+	while (--len > 0)
+
+#define MEMMOVE_DS(dest,src,len) \
+	do *dest++ = *src++; \
+	while (--len > 0)
+
+
+/***********************************************************************
+// fast bzero that clears multiples of 8 pointers
+// n is the number of pointers.
+// note: n > 0
+//       s and n are undefined afterwards
+************************************************************************/
+
+#if 0 && defined(LZO_OPTIMIZE_GNUC_i386)
+
+#define BZERO8_PTR(s,l,n) \
+__asm__ __volatile__( \
+	"movl  %0,%%eax \n"             \
+	"movl  %1,%%edi \n"             \
+	"movl  %2,%%ecx \n"             \
+	"cld \n"                        \
+	"rep \n"                        \
+	"stosl %%eax,(%%edi) \n"        \
+	: /* no outputs */              \
+	:"g" (0),"g" (s),"g" (n)	  	\
+	:"eax","edi","ecx", "memory", "cc" \
+)
+
+#elif (LZO_UINT_MAX <= SIZE_T_MAX) && defined(HAVE_MEMSET)
+
+#define BZERO8_PTR(s,l,n) \
+	memset((lzo_voidp)(s),0,(lzo_uint)(l)*(n))
+
+#else
+
+#define BZERO8_PTR(s,l,n) \
+	lzo_memset((lzo_voidp)(s),0,(lzo_uint)(l)*(n))
+
+#endif
+
+
+/***********************************************************************
+// rotate (not used at the moment)
+************************************************************************/
+
+#if 0
+#if defined(__GNUC__) && defined(__i386__)
+
+unsigned char lzo_rotr8(unsigned char value, int shift);
+extern __inline__ unsigned char lzo_rotr8(unsigned char value, int shift)
+{
+	unsigned char result;
+
+	__asm__ __volatile__ ("movb %b1, %b0; rorb %b2, %b0"
+                      	: "=a"(result) : "g"(value), "c"(shift));
+	return result;
+}
+
+unsigned short lzo_rotr16(unsigned short value, int shift);
+extern __inline__ unsigned short lzo_rotr16(unsigned short value, int shift)
+{
+	unsigned short result;
+
+	__asm__ __volatile__ ("movw %b1, %b0; rorw %b2, %b0"
+                      	: "=a"(result) : "g"(value), "c"(shift));
+	return result;
+}
+
+#endif
+#endif
+
+
+
+#ifdef __cplusplus
+} /* extern "C" */
+#endif
+
+#endif /* already included */
+
+/*
+vi:ts=4
+*/
diff -pruN linux-2.6.18.5.org/fs/ext2/lzo/lzoconf.h linux-2.6.18.5/fs/ext2/lzo/lzoconf.h
--- linux-2.6.18.5.org/fs/ext2/lzo/lzoconf.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzo/lzoconf.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,354 @@
+/* lzoconf.h -- configuration for the LZO real-time data compression library
+
+   This file is part of the LZO real-time data compression library.
+
+   Copyright (C) 1998 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1997 Markus Franz Xaver Johannes Oberhumer
+   Copyright (C) 1996 Markus Franz Xaver Johannes Oberhumer
+
+   The LZO library is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of
+   the License, or (at your option) any later version.
+
+   The LZO library is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with the LZO library; see the file COPYING.
+   If not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+
+   Markus F.X.J. Oberhumer
+   <markus.oberhumer@jk.uni-linz.ac.at>
+   http://wildsau.idv.uni-linz.ac.at/mfx/lzo.html
+ */
+
+
+#ifndef __LZOCONF_H
+#define __LZOCONF_H
+
+#define LZO_VERSION             0x1030
+#define LZO_VERSION_STRING      "1.03"
+#define LZO_VERSION_DATE        "Jan 18 1998"
+
+/* internal Autoconf configuration file - only used when building LZO */
+#if defined(LZO_HAVE_CONFIG_H)
+#  include <config.h>
+#endif
+#include <linux/kernel.h>	/* In lieu of <limits.h> */
+#include "assert.h"		/* In lieu of <assert.h> */
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+
+/***********************************************************************
+// architecture defines
+************************************************************************/
+
+#if !defined(__LZO_WIN) && !defined(__LZO_DOS) && !defined(__LZO_OS2)
+#  if defined(__WINDOWS__) || defined(_WINDOWS) || defined(_Windows)
+#    define __LZO_WIN
+#  elif defined(__WIN32__) || defined(_WIN32) || defined(WIN32)
+#    define __LZO_WIN
+#  elif defined(__NT__) || defined(__NT_DLL__) || defined(__WINDOWS_386__)
+#    define __LZO_WIN
+#  elif defined(__DOS__) || defined(__MSDOS__) || defined(MSDOS)
+#    define __LZO_DOS
+#  elif defined(__OS2__) || defined(__OS2V2__) || defined(OS2)
+#    define __LZO_OS2
+#  elif defined(__palmos__)
+#    define __LZO_PALMOS
+#  elif defined(__TOS__) || defined(__atarist__)
+#    define __LZO_TOS
+#  endif
+#endif
+
+#if (UINT_MAX < 0xffffffffL)
+#  if defined(__LZO_WIN)
+#    define __LZO_WIN16
+#  elif defined(__LZO_DOS)
+#    define __LZO_DOS16
+#  elif defined(__LZO_PALMOS)
+#    define __LZO_PALMOS16
+#  elif defined(__LZO_TOS)
+#    define __LZO_TOS16
+#  else
+#    error "16-bit target not supported - contact me for porting hints"
+#  endif
+#endif
+
+#if !defined(__LZO_i386)
+#  if defined(__LZO_DOS) || defined(__LZO_WIN16)
+#    define __LZO_i386
+#  elif defined(__i386__) || defined(__386__) || defined(_M_IX86)
+#    define __LZO_i386
+#  endif
+#endif
+
+#if defined(__LZO_STRICT_16BIT)
+#  if (UINT_MAX < 0xffffffffL)
+#    include <lzo16bit.h>
+#  endif
+#endif
+
+
+/***********************************************************************
+// integral and pointer types
+************************************************************************/
+
+/* Integral types with 32 bits or more */
+#if !defined(LZO_UINT32_MAX)
+#  if (UINT_MAX >= 0xffffffffL)
+     typedef unsigned int       lzo_uint32;
+     typedef int                lzo_int32;
+#    define LZO_UINT32_MAX      UINT_MAX
+#    define LZO_INT32_MAX       INT_MAX
+#    define LZO_INT32_MIN       INT_MIN
+#  elif (ULONG_MAX >= 0xffffffffL)
+     typedef unsigned long      lzo_uint32;
+     typedef long               lzo_int32;
+#    define LZO_UINT32_MAX      ULONG_MAX
+#    define LZO_INT32_MAX       LONG_MAX
+#    define LZO_INT32_MIN       LONG_MIN
+#  else
+#    error "lzo_uint32"
+#  endif
+#endif
+
+/* lzo_uint is used like size_t */
+#if !defined(LZO_UINT_MAX)
+#  if (UINT_MAX >= 0xffffffffL)
+     typedef unsigned int       lzo_uint;
+     typedef int                lzo_int;
+#    define LZO_UINT_MAX        UINT_MAX
+#    define LZO_INT_MAX         INT_MAX
+#    define LZO_INT_MIN         INT_MIN
+#  elif (ULONG_MAX >= 0xffffffffL)
+     typedef unsigned long      lzo_uint;
+     typedef long               lzo_int;
+#    define LZO_UINT_MAX        ULONG_MAX
+#    define LZO_INT_MAX         LONG_MAX
+#    define LZO_INT_MIN         LONG_MIN
+#  else
+#    error "lzo_uint"
+#  endif
+#endif
+
+
+/* Memory model that allows to access memory at offsets of lzo_uint.
+ * `Huge' pointers (16-bit DOS/Windows) are somewhat slow, but they
+ * work fine and I really don't care about 16-bit compiler
+ * optimizations nowadays.
+ */
+#if !defined(__LZO_MMODEL)
+#  if (LZO_UINT_MAX <= UINT_MAX)
+#    define __LZO_MMODEL
+#  elif defined(__LZO_DOS16) || defined(__LZO_WIN16)
+#    define __LZO_MMODEL        __huge
+#    define LZO_999_UNSUPPORTED
+#  elif defined(__LZO_PALMOS16) || defined(__LZO_TOS16)
+#    define __LZO_MMODEL
+#  else
+#    error "__LZO_MMODEL"
+#  endif
+#endif
+
+/* no typedef here because of const-pointer issues */
+#define lzo_byte                unsigned char __LZO_MMODEL
+#define lzo_bytep               unsigned char __LZO_MMODEL *
+#define lzo_charp               char __LZO_MMODEL *
+#define lzo_voidp               void __LZO_MMODEL *
+#define lzo_shortp              short __LZO_MMODEL *
+#define lzo_ushortp             unsigned short __LZO_MMODEL *
+#define lzo_uint32p             lzo_uint32 __LZO_MMODEL *
+#define lzo_int32p              lzo_int32 __LZO_MMODEL *
+#define lzo_uintp               lzo_uint __LZO_MMODEL *
+#define lzo_intp                lzo_int __LZO_MMODEL *
+#define lzo_voidpp              lzo_voidp __LZO_MMODEL *
+#define lzo_bytepp              lzo_bytep __LZO_MMODEL *
+
+typedef int lzo_bool;
+
+#ifndef lzo_sizeof_dict_t
+#  define lzo_sizeof_dict_t     sizeof(lzo_bytep)
+#endif
+
+
+/***********************************************************************
+// function types
+************************************************************************/
+
+/* linkage */
+#if !defined(__LZO_EXTERN_C)
+#  ifdef __cplusplus
+#    define __LZO_EXTERN_C      extern "C"
+#  else
+#    define __LZO_EXTERN_C      extern
+#  endif
+#endif
+
+/* calling conventions */
+#if !defined(__LZO_CDECL)
+#  if defined(__LZO_DOS16) || defined(__LZO_WIN16)
+#    define __LZO_CDECL         __far __cdecl
+#  elif defined(__LZO_i386) && defined(_MSC_VER)
+#    define __LZO_CDECL         __cdecl
+#  elif defined(__LZO_i386) && defined(__WATCOMC__)
+#    define __LZO_CDECL         __near __cdecl
+#  else
+#    define __LZO_CDECL
+#  endif
+#endif
+#if !defined(__LZO_ENTRY)
+#  define __LZO_ENTRY           __LZO_CDECL
+#endif
+
+/* DLL export information */
+#if !defined(__LZO_EXPORT1)
+#  define __LZO_EXPORT1
+#endif
+#if !defined(__LZO_EXPORT2)
+#  define __LZO_EXPORT2
+#endif
+
+/* calling convention for C functions */
+#if !defined(LZO_PUBLIC)
+#  define LZO_PUBLIC(_rettype)  __LZO_EXPORT1 _rettype __LZO_EXPORT2 __LZO_ENTRY
+#endif
+#if !defined(LZO_EXTERN)
+#  define LZO_EXTERN(_rettype)  __LZO_EXTERN_C LZO_PUBLIC(_rettype)
+#endif
+#if !defined(LZO_PRIVATE)
+#  define LZO_PRIVATE(_rettype) static _rettype __LZO_ENTRY
+#endif
+
+/* cdecl calling convention for assembler functions */
+#if !defined(LZO_PUBLIC_CDECL)
+#  define LZO_PUBLIC_CDECL(_rettype) \
+                __LZO_EXPORT1 _rettype __LZO_EXPORT2 __LZO_CDECL
+#endif
+#if !defined(LZO_EXTERN_CDECL)
+#  define LZO_EXTERN_CDECL(_rettype)  __LZO_EXTERN_C LZO_PUBLIC_CDECL(_rettype)
+#endif
+
+
+typedef int
+(__LZO_ENTRY *lzo_compress_t)   ( const lzo_byte *src, lzo_uint  src_len,
+                                        lzo_byte *dst, lzo_uint *dst_len,
+                                        lzo_voidp wrkmem );
+
+typedef int
+(__LZO_ENTRY *lzo_decompress_t) ( const lzo_byte *src, lzo_uint  src_len,
+                                        lzo_byte *dst, lzo_uint *dst_len,
+                                        lzo_voidp wrkmem );
+
+typedef int
+(__LZO_ENTRY *lzo_optimize_t)   (       lzo_byte *src, lzo_uint  src_len,
+                                        lzo_byte *dst, lzo_uint *dst_len,
+                                        lzo_voidp wrkmem );
+
+typedef int
+(__LZO_ENTRY *lzo_compress_dict_t)(const lzo_byte *src, lzo_uint  src_len,
+                                        lzo_byte *dst, lzo_uint *dst_len,
+                                        lzo_voidp wrkmem,
+                                  const lzo_byte *dict, lzo_uint dict_len );
+
+typedef int
+(__LZO_ENTRY *lzo_decompress_dict_t)(const lzo_byte *src, lzo_uint  src_len,
+                                        lzo_byte *dst, lzo_uint *dst_len,
+                                        lzo_voidp wrkmem,
+                                  const lzo_byte *dict, lzo_uint dict_len );
+
+
+/* a progress indicator callback function */
+typedef void (__LZO_ENTRY *lzo_progress_callback_t) (lzo_uint, lzo_uint);
+
+
+/***********************************************************************
+// error codes and prototypes
+************************************************************************/
+
+/* Error codes for the compression/decompression functions. Negative
+ * values are errors, positive values will be used for special but
+ * normal events.
+ */
+#define LZO_E_OK                    0
+#define LZO_E_ERROR                 (-1)
+#define LZO_E_OUT_OF_MEMORY         (-2)    /* not used right now */
+#define LZO_E_NOT_COMPRESSIBLE      (-3)    /* not used right now */
+#define LZO_E_INPUT_OVERRUN         (-4)
+#define LZO_E_OUTPUT_OVERRUN        (-5)
+#define LZO_E_LOOKBEHIND_OVERRUN    (-6)
+#define LZO_E_EOF_NOT_FOUND         (-7)
+#define LZO_E_INPUT_NOT_CONSUMED    (-8)
+
+
+/* lzo_init() should be the first function you call.
+ * Check the return code !
+ *
+ * lzo_init() is a macro to allow checking that the library and the
+ * compiler's view of various types are consistent.
+ */
+#define lzo_init() __lzo_init2(LZO_VERSION,(int)sizeof(short),(int)sizeof(int),\
+    (int)sizeof(long),(int)sizeof(lzo_uint32),(int)sizeof(lzo_uint),\
+    (int)lzo_sizeof_dict_t,(int)sizeof(char *),(int)sizeof(lzo_voidp),\
+    (int)sizeof(lzo_compress_t))
+LZO_EXTERN(int) __lzo_init2(unsigned,int,int,int,int,int,int,int,int,int);
+
+/* version functions (useful for shared libraries) */
+LZO_EXTERN(unsigned) lzo_version(void);
+LZO_EXTERN(const char *) lzo_version_string(void);
+LZO_EXTERN(const char *) lzo_version_date(void);
+LZO_EXTERN(const lzo_charp) _lzo_version_string(void);
+LZO_EXTERN(const lzo_charp) _lzo_version_date(void);
+
+/* string functions */
+LZO_EXTERN(int)
+lzo_memcmp(const lzo_voidp _s1, const lzo_voidp _s2, lzo_uint _len);
+LZO_EXTERN(lzo_voidp)
+lzo_memcpy(lzo_voidp _dest, const lzo_voidp _src, lzo_uint _len);
+LZO_EXTERN(lzo_voidp)
+lzo_memmove(lzo_voidp _dest, const lzo_voidp _src, lzo_uint _len);
+LZO_EXTERN(lzo_voidp)
+lzo_memset(lzo_voidp _s, int _c, lzo_uint _len);
+
+/* checksum functions */
+LZO_EXTERN(lzo_uint32)
+lzo_adler32(lzo_uint32 _adler, const lzo_byte *_buf, lzo_uint _len);
+LZO_EXTERN(lzo_uint32)
+lzo_crc32(lzo_uint32 _c, const lzo_byte *_buf, lzo_uint _len);
+
+/* memory functions */
+LZO_EXTERN(lzo_bytep) lzo_alloc(lzo_uint _nelems, lzo_uint _size);
+LZO_EXTERN(lzo_bytep) lzo_malloc(lzo_uint _size);
+LZO_EXTERN(void) lzo_free(lzo_voidp _ptr);
+
+extern lzo_bytep (__LZO_ENTRY *lzo_alloc_f) (lzo_uint,lzo_uint);
+extern void (__LZO_ENTRY *lzo_free_f) (lzo_voidp);
+
+/* misc. */
+LZO_EXTERN(lzo_bool) lzo_assert(int _expr);
+LZO_EXTERN(int) _lzo_config_check(void);
+typedef union { lzo_bytep p; lzo_uint u; } __lzo_pu_u;
+typedef union { lzo_bytep p; lzo_uint32 u32; } __lzo_pu32_u;
+
+/* align a char pointer on a boundary that is a multiple of `size' */
+LZO_EXTERN(unsigned) __lzo_align_gap(const lzo_voidp _ptr, lzo_uint _size);
+#define LZO_PTR_ALIGN_UP(_ptr,_size) \
+    ((_ptr) + (lzo_uint) __lzo_align_gap((const lzo_voidp)(_ptr),(lzo_uint)(_size)))
+
+/* depracted - backward compatibility */
+#define LZO_ALIGN(_ptr,_size) LZO_PTR_ALIGN_UP(_ptr,_size)
+
+
+#ifdef __cplusplus
+} /* extern "C" */
+#endif
+
+#endif /* already included */
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzrw3a/Makefile linux-2.6.18.5/fs/ext2/lzrw3a/Makefile
--- linux-2.6.18.5.org/fs/ext2/lzrw3a/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzrw3a/Makefile	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,19 @@
+#
+# Makefile for the linux compression routines.
+#
+# Note! Dependencies are done automagically by 'make dep', which also
+# removes any old dependencies. DON'T put your own dependencies here
+# unless it's something special (ie not a .c file).
+#
+# Note 2! The CFLAGS definitions are now in the main makefile...
+
+.S.o:
+	$(CC) -D__ASSEMBLY__ -traditional -c $< -o $*.o
+
+obj-$(CONFIG_EXT2_HAVE_LZRW3A) := ext2-compr-lzrw3a.o
+
+ifeq ($(CONFIG_EXT2_COMPR_X86_CODE),y)
+ext2-compr-lzrw3a-y := e2compr-lzrw3a.o rlzrw3a.o wlzrw3a.o
+else
+ext2-compr-lzrw3a-y := e2compr-lzrw3a.o lzrw3a.o
+endif
diff -pruN linux-2.6.18.5.org/fs/ext2/lzrw3a/e2compr-lzrw3a.c linux-2.6.18.5/fs/ext2/lzrw3a/e2compr-lzrw3a.c
--- linux-2.6.18.5.org/fs/ext2/lzrw3a/e2compr-lzrw3a.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzrw3a/e2compr-lzrw3a.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,81 @@
+#include <linux/fs.h>
+#include <linux/ext2_fs_c.h>
+#include <linux/types.h>
+#include <linux/module.h>
+
+#include "lzrw.h"
+
+#ifdef MODULE
+MODULE_AUTHOR("Ross Williams and Antoine de Maricourt");
+MODULE_DESCRIPTION("Lev-Zimpel Ross Williams 3A algorithm for EXT2 file compression");
+MODULE_LICENSE("GPL");
+#endif
+
+#define U(X)	((unsigned long) X)
+#define MEM_REQ	(U(4096)*(U(sizeof(unsigned char *))) + (U(16)))
+
+extern int ext2_LZRW3A_compress(UBYTE *p_src_first, UBYTE *p_dst_first,
+				UBYTE *heap, int  src_len, int p_dst_len);
+extern int ext2_LZRW3A_decompress(UBYTE *p_src_first, UBYTE *p_dst,
+				UBYTE *heap, int src_len, int dst_len);
+
+size_t ext2_iLZRW3A(int action)
+{
+	switch (action) {
+		case EXT2_ALG_INIT_COMPRESS:
+			return MEM_REQ;
+		case EXT2_ALG_INIT_DECOMPRESS:
+			return MEM_REQ;
+		default:
+			return 0;
+	}
+}
+
+size_t ext2_wLZRW3A (unsigned char *p_src_first, unsigned char *p_dst_first,
+		void *heap, size_t src_len, size_t p_dst_len, int xarg)
+{
+	int ret_code;
+
+	if (!try_module_get(THIS_MODULE))
+		return 0;
+	ret_code = ext2_LZRW3A_compress(p_src_first, p_dst_first, heap, src_len, p_dst_len);
+	module_put(THIS_MODULE);
+	return ret_code;
+}
+
+size_t ext2_rLZRW3A (unsigned char *p_src_first, unsigned char *p_dst,
+		void *heap, size_t src_len, size_t dst_len, int xarg)
+{
+	int ret_code;
+
+	if (!try_module_get(THIS_MODULE))
+		return 0;
+	ret_code = ext2_LZRW3A_decompress(p_src_first, p_dst, heap, src_len, dst_len);
+	module_put(THIS_MODULE);
+	return ret_code;
+}
+
+#ifdef MODULE
+
+int init_module(void)
+{
+        struct ext2_algorithm lzrw3a_alg;
+
+        lzrw3a_alg.name = NULL;
+        lzrw3a_alg.avail = 1;
+        lzrw3a_alg.init = ext2_iLZRW3A;
+        lzrw3a_alg.compress = ext2_wLZRW3A;
+        lzrw3a_alg.decompress = ext2_rLZRW3A;
+
+        return ext2_register_compression_module(EXT2_LZRW3A_ALG,
+                        MEM_REQ, MEM_REQ,
+                        &lzrw3a_alg);
+}
+
+void cleanup_module(void)
+{
+        ext2_unregister_compression_module(EXT2_LZRW3A_ALG);
+}
+
+#endif
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzrw3a/lzrw.h linux-2.6.18.5/fs/ext2/lzrw3a/lzrw.h
--- linux-2.6.18.5.org/fs/ext2/lzrw3a/lzrw.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzrw3a/lzrw.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,106 @@
+/* Author : Ross Williams.                                                    */
+/* Date   : December 1989.                                                    */
+/* This header file and the C functions have been modified in order to        */
+/* include them in the DouBle device driver by Jean-Marc Verbavatz, Feb 1994. */
+/*                                                                            */
+/* This header file defines the interface to a set of lzrw functions          */
+/* each member of which implements a particular data compression              */
+/* algorithm.                                                                 */
+/*                                                                            */
+/* Normally in C programming, for each .H file, there is a corresponding .C   */
+/* file that implements the functions promised in the .H file.                */
+/* Here, there are many .C files corresponding to this header file.           */
+/* Each comforming implementation file contains a single function             */
+/* called that implements a single data compression                           */
+/* algorithm that conforms with the interface specified in this header file.  */
+/*                                                                            */
+/******************************************************************************/
+/* See the formal C definition later for a description of the parameters.     */
+/*                                                                            */
+/* Although compression algorithms usually compress data, there will always   */
+/* be data that a given compressor will expand (this can be proven).          */
+/*                                                                            */
+/* Unfortunately, in general, the only way to tell if an algorithm will       */
+/* expand a particular block of data is to run the algorithm on the data.     */
+/* If the algorithm does not continuously monitor how many output bytes it    */
+/* has written, it might write an output block far larger than the input      */
+/* block before realizing that it has done so.                                */
+/* On the other hand, continuous checks on output length are inefficient.     */
+/*                                                                            */
+/* The problem does not arise for decompression.                              */
+/*                                                                            */
+/******************************************************************************/
+/* Macro definitions and types that are likely to change between computers.   */
+/*                                                                            */
+#ifndef DONE_PORT       /* Only do this if not previously done.               */
+   #ifndef TRUE
+	#define TRUE 1
+	#define FALSE 0
+   #endif
+   #ifndef UBYTE
+      #define UBYTE unsigned char      /* Unsigned byte                       */
+      #define UWORD unsigned short     /* Unsigned word (2 bytes)             */
+      #define ULONG unsigned long      /* Unsigned word (4 bytes)             */
+  #endif
+
+   #define DONE_PORT                   /* Don't do all this again.            */
+   #define LOCAL static                /* For non-exported routines.          */
+   #define EXPORT                      /* Signals exported function.          */
+
+#endif
+
+#if 0
+extern void *ext2_lzrw3a_htab;
+#endif
+
+int  rlzrw2(        /* function for decompression algorithm. */
+UBYTE   *src_adr,      /* Address of input  data.            */
+UBYTE   *dst_adr,      /* Address of output data.            */
+int      src_len,      /* Length  of output data.            */
+int      dst_len       /* Length  of output data.            */
+);
+
+void  wlzrw2(        /* function for compression algorithm. */
+UBYTE   *src_adr,      /* Address of input  data.           */
+int      src_len,      /* Length  of input  data.           */
+UBYTE   *dst_adr,      /* Address of output data.           */
+int   *p_dst_len       /* Pointer to a longword for         */
+                       /*    Length of output data. */
+);
+
+int  rlzrw3(        /* function for decompression algorithm. */
+UBYTE   *src_adr,      /* Address of input  data.                             */
+UBYTE   *dst_adr,      /* Address of output data.                             */
+ULONG    dst_len       /* Length  of output data.                             */
+);
+
+void  wlzrw3(        /* function for compression algorithm. */
+UBYTE   *src_adr,      /* Address of input  data.                             */
+ULONG    src_len,      /* Length  of input  data.                             */
+UBYTE   *dst_adr,      /* Address of output data.                             */
+ULONG *p_dst_len       /* Pointer to a longword where routine will write:     */
+                       /*    Length of output data. */
+);
+/*
+ * NOTE: lzrw3 is not implemented in the DouBle driver version 0.2
+ */
+
+int  rlzrw3a(        /* function for decompression algorithm. */
+UBYTE   *src_adr,      /* Address of input  data.             */
+UBYTE   *dst_adr,      /* Address of output data.             */
+int      src_len,      /* Length  of input data.              */
+int      dst_len       /* Length  of output data.             */
+);
+
+void  wlzrw3a(        /* function for compression algorithm. */
+UBYTE   *src_adr,      /* Address of input  data.            */
+int      src_len,      /* Length  of input  data.            */
+UBYTE   *dst_adr,      /* Address of output data.            */
+int   *p_dst_len       /* Pointer to a longword for          */
+                       /*    Length of output data.          */
+);
+
+/******************************************************************************/
+/*                             End of LZRW.H                                  */
+/******************************************************************************/
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzrw3a/lzrw3a.c linux-2.6.18.5/fs/ext2/lzrw3a/lzrw3a.c
--- linux-2.6.18.5.org/fs/ext2/lzrw3a/lzrw3a.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzrw3a/lzrw3a.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,795 @@
+/*****************************************************************************/
+/*                                                                           */
+/*                                  LZRW3-A.C                                */
+/*                                                                           */
+/*****************************************************************************/
+/*                                                                           */
+/* Author  : Ross Williams.                                                  */
+/* Date    : 15-Jul-1991.                                                    */
+/* Release : 1.                                                              */
+/*                                                                           */
+/*****************************************************************************/
+/*                                                                           */
+/* This file contains an implementation of the LZRW3-A data compression      */
+/* algorithm in the C programming language.                                  */
+/*                                                                           */
+/* The LZRW3-A algorithm has the following features:                         */
+/*                                                                           */
+/*    1 Requires only 16K of memory (for both compression and decompression).*/
+/*    2 The compressor   runs about two   times faster than Unix compress's. */
+/*    3 The decompressor runs about three times faster than Unix compress's. */
+/*    4 Yields a few percent better compression than Unix compress for       */
+/*      most files.                                                          */
+/*    5 Allows you to dial up extra compression at a speed cost in the       */
+/*      compressor. The speed of the decompressor is not affected.           */
+/*    6 Algorithm is deterministic.                                          */
+/*    8 This implementation in C is in the public domain.                    */
+/*                                                                           */
+/* (Timing tests for the speed comparison were performed on a Pyramid 9820.) */
+/*                                                                           */
+/* LZRW3-A is LZRW3 with a deepened hash table. This simple change yields    */
+/* about a 6% (absolute) improvement in compression.                         */
+/*                                                                           */
+/* Here are the results of applying this code, compiled under THINK C 4.0    */
+/* and running on a Mac-SE (8MHz 68000), to the standard calgary corpus.     */
+/*                                                                           */
+/*     +----------------------------------------------------------------+    */
+/*     | DATA COMPRESSION TEST                                          |    */
+/*     | =====================                                          |    */
+/*     | Time of run     : Mon 15-Jul-1991 05:29PM                      |    */
+/*     | Timing accuracy : One part in 100                              |    */
+/*     | Context length  : 262144 bytes (= 256.0000K)                   |    */
+/*     | Test suite      : Calgary Corpus Suite                         |    */
+/*     | Files in suite  : 14                                           |    */
+/*     | Algorithm       : LZRW3-A                                      |    */
+/*     | Note: All averages are calculated from the un-rounded values.  |    */
+/*     +----------------------------------------------------------------+    */
+/*     | File Name   Length  CxB  ComLen  %Remn  Bits  Com K/s  Dec K/s |    */
+/*     | ----------  ------  ---  ------  -----  ----  -------  ------- |    */
+/*     | rpus:Bib.D  111261    1   49044   44.1  3.53     8.47    31.19 |    */
+/*     | us:Book1.D  768771    3  420464   54.7  4.38     7.27    30.07 |    */
+/*     | us:Book2.D  610856    3  277955   45.5  3.64     8.51    33.40 |    */
+/*     | rpus:Geo.D  102400    1   84218   82.2  6.58     4.23    15.04 |    */
+/*     | pus:News.D  377109    2  192880   51.1  4.09     7.08    25.89 |    */
+/*     | pus:Obj1.D   21504    1   12651   58.8  4.71     5.23    17.44 |    */
+/*     | pus:Obj2.D  246814    1  108044   43.8  3.50     8.01    28.11 |    */
+/*     | s:Paper1.D   53161    1   24526   46.1  3.69     8.11    30.24 |    */
+/*     | s:Paper2.D   82199    1   39483   48.0  3.84     8.11    32.04 |    */
+/*     | rpus:Pic.D  513216    2  111622   21.7  1.74    10.64    49.31 |    */
+/*     | us:Progc.D   39611    1   17923   45.2  3.62     8.06    29.01 |    */
+/*     | us:Progl.D   71646    1   24362   34.0  2.72    10.74    39.51 |    */
+/*     | us:Progp.D   49379    1   16805   34.0  2.72    10.64    37.58 |    */
+/*     | us:Trans.D   93695    1   30296   32.3  2.59    11.02    38.06 |    */
+/*     +----------------------------------------------------------------+    */
+/*     | Average     224401    1  100733   45.8  3.67     8.29    31.21 |    */
+/*     +----------------------------------------------------------------+    */
+/*                                                                           */
+/*****************************************************************************/
+
+#include "lzrw.h"
+/******************************************************************************/
+
+/*
+  The following structure is returned by the "compress" function below when 
+  the user asks the function to return identifying information.             
+  The most important field in the record is the working memory field which  
+  tells the calling program how much working memory should be passed to     
+  "compress" when it is called to perform a compression or decompression.   
+  LZRW3-A uses the same amount of memory during compression and             
+  decompression. For more information on this structure see "lzrw.h".       
+  The alignment fudge below really only needs to be 4 (but I play it safe!).
+  The id looks non-random, but it really was generated by coin tossing!     
+  */
+
+#define U(X)            ((ULONG) X)
+#define SIZE_P_BYTE     (U(sizeof(UBYTE *)))
+#define ALIGNMENT_FUDGE (U(16))
+#define MEM_REQ ( U(4096)*(SIZE_P_BYTE) + ALIGNMENT_FUDGE )
+
+/*
+   static struct compress_identity identity =
+   {
+     U(0x01B90B91),                         * Algorithm identification number. * 
+     MEM_REQ,                               * Working memory (bytes) required. * 
+     "LZRW3-A",                             * Name of algorithm.               * 
+     "1.0",                                 * Version number of algorithm.     * 
+     "15-Jul-1990",                         * Date of algorithm.               * 
+     "Ross N. Williams",                    * Author of algorithm.             * 
+     "Renaissance Software",                * Affiliation of author.           * 
+   };
+*/
+
+/******************************************************************************/
+/*                                                                            */
+/* BRIEF DESCRIPTION OF THE LZRW3-A ALGORITHM                                 */
+/* ==========================================                                 */
+/* Note: Before attempting to understand this algorithm, you should first     */
+/* understand the LZRW3 algorithm from which this algorithm is derived.       */
+/*                                                                            */
+/* The LZRW3-A algorithm is identical to the LZRW3 algorithm except that the  */
+/* hash table has been "deepened". The LZRW3 algorithm has a hash table of    */
+/* 4096 pointers which point to strings in the buffer. LZRW3-A generalizes    */
+/* this to 4096/(2^n) partitions each of which contains (2^n) pointers.       */
+/* In LZRW3-A, the hash function hashes to a partition number.                */
+/*                                                                            */
+/* During the processing of each phrase, LZRW3 overwrites the pointer in the  */
+/* position selected by the hash function. LZRW3-A overwrites one of the      */
+/* pointers in the partition that was selected by the hash function.          */
+/*                                                                            */
+/* When searching for a match, LZRW3-A matches against all (2^n) strings      */
+/* pointed to by the pointers in the target partition.                        */
+/*                                                                            */
+/* Deep hash tables were used in early versions of LZRW1 in late 1989, but    */
+/* were discarded in an effort to increase speed (which was the primary       */
+/* requirement for LZRW1). They were revived for use in LZRW3-A in order to   */
+/* produce an algorithm with compression performance competitive with Unix    */
+/* compress.                                                                  */
+/*                                                                            */
+/* Until 14-Jul-1991, deep hash tables used in prototype LZRW* algorithms     */
+/* used a queue discipline within each partition. Upon the arrival of a new   */
+/* pointer, the pointers in the partition would be block copied back one      */
+/* position (with the oldest pointer being overwritten) and the new pointer   */
+/* being inserted in the space at the front (the youngest position).          */
+/* This meant that pointers to the (2^n) most recent phrases corresponding to */
+/* each hash was kept. The only flaw in this system was the time-consuming    */
+/* block copy operation which was cheap for shallow tables but expensive for  */
+/* deep tables.                                                               */
+/*                                                                            */
+/* The traditional solution to ring buffer block copy problems is to maintain */
+/* a cyclic counter which points to the "head" of the queue. However, this    */
+/* would have required one counter to be stored for each partition and would  */
+/* have been slightly messy. After some thought (on 14-Jul-1991) a better     */
+/* solution was found. Instead of maintaining a counter for each partition,   */
+/* LZRW3-A maintains a single counter for all partitions! This counter is     */
+/* maintained in both the compressor and decompressor and means that the      */
+/* algorithm (effectively) overwrites a RANDOM element of the partition to be */
+/* updated. The result was to increase the speed of the compressor and        */
+/* decompressor, to make the decompressor's speed independent from whatever   */
+/* depth was selected, and to impair compression by less than 1% absolute.    */
+/*                                                                            */
+/* Setting the depth is a speed/compression tradeoff. The table below gives   */
+/* the tradeoff observed for a typical 50K text file on a Mac-SE.             */
+/* Note: %Rem=Percentage Remaining (after compression).                       */
+/*                                                                            */
+/*      Depth    %Rem    CmpK/s  DecK/s                                       */
+/*          1    45.2    14.77   32.24                                        */
+/*          2    42.6    12.12   31.26                                        */
+/*          4    40.9    10.28   31.91                                        */
+/*          8    40.0     7.81   32.36                                        */
+/*         16    39.5     5.30   32.47                                        */
+/*         32    39.0     3.23   32.59                                        */
+/*                                                                            */
+/* I have chosen a depth of 8 as the "default" depth for LZRW3-A. If you use  */
+/* a depth different to this (e.g. 4), you should use the name LZRW3-A(4) to  */
+/* indicate that a different depth is being used. LZRW3-A(8) is an acceptable */
+/* longhand for LZRW3-A.                                                      */
+/*                                                                            */
+/* To change the depth, search for "HERE IT IS" in the rest of this file.     */
+/*                                                                            */
+/*                                  +---+                                     */
+/*                                  |___|4095                                 */
+/*                                  |===|                                     */
+/*              +---------------------*_|<---+   /----+---\                   */
+/*              |                   |___|    +---|Hash    |                   */
+/*              |    512 partitions |___|        |Function|                   */
+/*              |    of 8 pointers  |===|        \--------/                   */
+/*              |    each (or any   |___|0            ^                       */
+/*              |    a*b=4096)      +---+             |                       */
+/*              |                   Hash        +-----+                       */
+/*              |                   Table       |                             */
+/*              |                              ---                            */
+/*              v                              ^^^                            */
+/*      +-------------------------------------|----------------+              */
+/*      ||||||||||||||||||||||||||||||||||||||||||||||||||||||||              */
+/*      +-------------------------------------|----------------+              */
+/*      |                                     |1......18|      |              */
+/*      |<------- Lempel=History ------------>|<--Ziv-->|      |              */
+/*      |     (=bytes already processed)      |<-Still to go-->|              */
+/*      |<-------------------- INPUT BLOCK ------------------->|              */
+/*                                                                            */
+/*                                                                            */
+/******************************************************************************/
+/*                                                                            */
+/*                     DEFINITION OF COMPRESSED FILE FORMAT                   */
+/*                     ====================================                   */
+/*    The file consists of zero  */
+/*    or more GROUPS, each of which represents one or more bytes.             */
+/*  * Each group consists of two bytes of CONTROL information followed by     */
+/*    sixteen ITEMs except for the last group which can contain from one      */
+/*    to sixteen items.                                                       */
+/*  * An item can be either a LITERAL item or a COPY item.                    */
+/*  * Each item corresponds to a bit in the control bytes.                    */
+/*  * The first control byte corresponds to the first 8 items in the group    */
+/*    with bit 0 corresponding to the first item in the group and bit 7 to    */
+/*    the eighth item in the group.                                           */
+/*  * The second control byte corresponds to the second 8 items in the group  */
+/*    with bit 0 corresponding to the ninth item in the group and bit 7 to    */
+/*    the sixteenth item in the group.                                        */
+/*  * A zero bit in a control word means that the corresponding item is a     */
+/*    literal item. A one bit corresponds to a copy item.                     */
+/*  * A literal item consists of a single byte which represents itself.       */
+/*  * A copy item consists of two bytes that represent from 3 to 18 bytes.    */
+/*  * The first  byte in a copy item will be denoted C1.                      */
+/*  * The second byte in a copy item will be denoted C2.                      */
+/*  * Bits will be selected using square brackets.                            */
+/*    For example: C1[0..3] is the low nibble of the first control byte.      */
+/*    of copy item C1.                                                        */
+/*  * The LENGTH of a copy item is defined to be C1[0..3]+3 which is a number */
+/*    in the range [3,18].                                                    */
+/*  * The INDEX of a copy item is defined to be C1[4..7]*256+C2[0..8] which   */
+/*    is a number in the range [0,4095].                                      */
+/*  * A copy item represents the sequence of bytes                            */
+/*       text[POS-OFFSET..POS-OFFSET+LENGTH-1] where                          */
+/*          text   is the entire text of the uncompressed string.             */
+/*          POS    is the index in the text of the character following the    */
+/*                   string represented by all the items preceeding the item  */
+/*                   being defined.                                           */
+/*          OFFSET is obtained from INDEX by looking up the hash table.       */
+/*                                                                            */
+/******************************************************************************/
+
+/* When I first started to get concerned about the portability of my C code,  */
+/* I switched over to using only macro defined types UBYTE, UWORD, ULONG and  */
+/* one or two others. While, these are useful for most purposes, they impair  */
+/* efficiency as, if I have a variable whose range will be [0,1000], I will   */
+/* declare it as a UWORD. This will translate into (say) "short int" and      */
+/* hence may be less efficient than just an "int" which represents the        */
+/* natural size of the machine. Before releasing LZRW3-A, I realized this     */
+/* mistake. Unfortunately, I can't access the ftp archive with my portability */
+/* header in it in time for this algorithm's release and so I am including an */
+/* extra definition. The definition UCARD stands for an unsigned (cardinal)   */
+/* type that can hold values in the range [0,32767]. This is within the ANSI  */
+/* range of a standard int or unsigned. No assumption about overflow of this  */
+/* type is made in the code (i.e. all usages are within range and I do not    */
+/* use the value -1 to detect the end of loops.).                             */
+/* You can use either "unsigned" or just "int" here depending on which is     */
+/* more efficient in your environment (both the same probably).               */
+#define UCARD unsigned
+
+/* The following constant defines the maximum length of an uncompressed item. */
+/* This definition must not be changed; its value is hardwired into the code. */
+/* The longest number of bytes that can be spanned by a single item is 18     */
+/* for the longest copy item.                                                 */
+#define MAX_RAW_ITEM (18)
+
+/* The following constant defines the maximum length of a compressed group.   */
+/* This definition must not be changed; its value is hardwired into the code. */
+/* A compressed group consists of two control bytes followed by up to 16      */
+/* compressed items each of which can have a maximum length of two bytes.     */
+#define MAX_CMP_GROUP (2+16*2)
+
+/* This constant defines the number of pointers in the hash table. The number */
+/* of partitions multiplied by the number of pointers in each partition must  */
+/* multiply out to this value of 4096. In LZRW1, LZRW1-A, and LZRW2, this     */
+/* table length value can be changed. However, in LZRW3-A (and LZRW3), the    */
+/* table length cannot be changed because it is connected directly to the     */
+/* coding scheme which is hardwired (the table index of a single pointer is   */
+/* transmitted in the 12-bit index field). So don't change this constant!     */
+#define HASH_TABLE_LENGTH (4096)
+
+/* HERE IT IS: THE PLACE TO CHANGE THE HASH TABLE DEPTH!                      */
+/* The following definition is the log_2 of the depth of the hash table. This */
+/* constant can be in the range [0,1,2,3,...,12]. Increasing the depth        */
+/* increases compression at the expense of speed. However, you are not likely */
+/* to see much of a compression improvement (e.g. not more than 0.5%) above a */
+/* value of 6 and the algorithm will start to get very slow. See the table in */
+/* the earlier comments block for an idea of the trade-off involved.          */
+/* Note: The parentheses are to avoid macro substitution funnies.             */
+/* Note: The LZRW3-A default is a value of (3).                               */
+/* Note: If you end up choosing a value of 0, you should use LZRW3 instead.   */
+/* Note: Changing the value of HASH_TABLE_DEPTH_BITS is the ONLY thing you    */
+/* have to do to change the depth, so go ahead and recompile now!             */
+/* Note: I have tested LZRW3-A for DEPTH_BITS=0,1,2,3,4 and a few other       */
+/* values. However, I have not tested it for 12 as I can't wait that long!    */
+#define HASH_TABLE_DEPTH_BITS (3)      /* Must be in range [0,12].            */
+
+/* The following definitions are all self-explanatory and follow from the     */
+/* definition of HASH_TABLE_DEPTH_BITS and the hardwired requirement that the */
+/* hash table contain exactly 4096 pointers.                                  */
+#define PARTITION_LENGTH_BITS (12-HASH_TABLE_DEPTH_BITS)
+#define PARTITION_LENGTH      (1<<PARTITION_LENGTH_BITS)
+#define HASH_TABLE_DEPTH      (1<<HASH_TABLE_DEPTH_BITS )
+#define HASH_MASK             (PARTITION_LENGTH-1)
+#define DEPTH_MASK            (HASH_TABLE_DEPTH-1)
+
+/* LZRW3-A, unlike LZRW1(-A), must initialize its hash table so as to enable  */
+/* the compressor and decompressor to stay in step maintaining identical hash */
+/* tables. In an early version of LZRW3, the tables were simply               */
+/* initialized to zero and a check for zero was included just before the      */
+/* matching code. However, this test costs time. A better solution is to      */
+/* initialize all the entries in the hash table to point to a constant        */
+/* string. The decompressor does the same. This solution requires no extra    */
+/* test. The contents of the string do not matter so long as the string is    */
+/* the same for the compressor and decompressor and contains at least         */
+/* MAX_RAW_ITEM bytes. I chose consecutive decimal digits because they do not */
+/* have white space problems (e.g. there is no chance that the compiler will  */
+/* replace more than one space by a TAB) and because they make the length of  */
+/* the string obvious by inspection.                                          */
+#define START_STRING_18 ((UBYTE *) "123456789012345678")
+/* fixme: I (pjm) am not at all sure that the above string cannot be written to. */
+
+/* The following macro accepts a pointer PTR to three consecutive bytes in    */
+/* memory and hashes them into an integer that is a hash table index that     */
+/* points to the zeroth (first) element of a partition. Thus, the hash        */
+/* function really hashes to a partition number but, for convenience,         */
+/* multiplies it up to yield a hash table index. From all this, we see that   */
+/* the resultant number is in the range [0,HASH_TABLE_LENGTH-1] and is a      */
+/* multiple of HASH_TABLE_DEPTH.                                              */
+/* A macro is used, because in LZRW3-A we have to hash more than once.        */
+#if 0
+#define HASH(PTR) \
+ ( \
+     (((40543*(((*(PTR))<<8)^((*((PTR)+1))<<4)^(*((PTR)+2))))>>4) & HASH_MASK) \
+  << HASH_TABLE_DEPTH_BITS \
+ )
+
+#else
+#define HASH(PTR) \
+ ( \
+     ( ((((*(PTR)<<3)^(*((PTR)+1)))<<3)^(*((PTR)+2))) & HASH_MASK) \
+  << HASH_TABLE_DEPTH_BITS \
+ )
+#endif
+
+/* Another operation that is performed more than once is the updating of the  */
+/* hash table. Here two macros are defined to simplify update operations.     */
+/* Updating consists of identifying and overwriting a pointer in a partition  */
+/* with a newer pointer and then updating the global cycle value.             */
+/* These macros accept the new pointer (NEWPTR) and either a pointer to       */
+/* (P_BASE) or the index of (I_BASE) the zeroth (first, or base) pointer in   */
+/* the partition that is to be updated. The macros use the 'cycle' variable   */
+/* to locate and overwrite a pointer and then update the cycle value.         */
+/* Note: Hardcoding 'cycle' in this macro is naughty (it should really be a   */
+/* macro parameter), but I have done so because it neatens up the code.       */
+#define UPDATE_P(P_BASE,NEWPTR) \
+{(P_BASE)[cycle++]=(NEWPTR); cycle&=DEPTH_MASK;}
+
+#define UPDATE_I(I_BASE,NEWPTR) \
+{hash[(I_BASE)+cycle++]=(NEWPTR); cycle&=DEPTH_MASK;}
+
+/* This constant supplies a legal (in-range) hash table index for use when    */
+/* a legal-but-don't-care index is required.                                  */
+#define ANY_HASH_INDEX (0)
+
+/******************************************************************************/
+#if 0
+void *ext2_lzrw3a_htab;
+#endif
+
+EXPORT int ext2_LZRW3A_compress
+           (UBYTE *p_src_first, UBYTE *p_dst_first, void *heap, int src_len,
+	    int p_dst_len)
+/* Input  : Specify input block using p_src_first and src_len.                */
+/* Input  : Point p_dst_first to the start of the output zone (OZ).           */
+/* Input  : Point p_dst_len to a ULONG to receive the output length.          */
+/* Input  : Input block and output zone must not overlap.                     */
+/* Output : Length of output block written to *p_dst_len.                     */
+/* Output : Output block in Mem[p_dst_first..p_dst_first+*p_dst_len-1]. May   */
+/* Output : write in OZ=Mem[p_dst_first..p_dst_first+src_len+MAX_CMP_GROUP-1].*/
+/* Output : Upon completion *p_dst_len contains final size or 0               */
+{
+  /* p_src and p_dst step through the source and destination blocks.           */
+  UBYTE const *p_src = p_src_first;
+  UBYTE *p_dst = p_dst_first;
+
+  /* The following variables are never modified and are used in the            */
+  /* calculations that determine when the main loop terminates.                */
+  UBYTE const * const p_src_post  = p_src_first + src_len;
+  UBYTE       * const p_dst_post  = p_dst_first + p_dst_len;
+  UBYTE const * const p_src_max1  = p_src_first + src_len - MAX_RAW_ITEM;
+  UBYTE const * const p_src_max16 = p_src_first + src_len - (MAX_RAW_ITEM * 16);
+
+  /* The variables 'p_control' and 'control' are used to buffer control bits.  */
+  /* Before each group is processed, the next two bytes of the output block    */
+  /* are set aside for the control word for the group about to be processed.   */
+  /* 'p_control' is set to point to the first byte of that word. Meanwhile,    */
+  /* 'control' buffers the control bits being generated during the processing  */
+  /* of the group. Instead of having a counter to keep track of how many items */
+  /* have been processed (=the number of bits in the control word), at the     */
+  /* start of each group, the top word of 'control' is filled with 1 bits.     */
+  /* As 'control' is shifted for each item, the 1 bits in the top word are     */
+  /* absorbed or destroyed. When they all run out (i.e. when the top word is   */
+  /* all zero bits, we know that we are at the end of a group.                 */
+  #define TOPWORD 0xFFFF0000
+  UBYTE *p_control;
+  ULONG control=TOPWORD;
+
+  /* The variable 'hash' always points to the first element of the hash table. */
+#if 0
+  UBYTE **hash= (UBYTE **) ext2_lzrw3a_htab;
+#else
+  UBYTE **hash= (UBYTE **) heap;
+#endif
+
+  /* The following two variables represent the literal buffer. p_h1 points to  */
+  /* the partition (i.e. the zero'th (first) element of the partition)         */
+  /* corresponding to the youngest literal. p_h2 points to the partition       */
+  /* corresponding to the second youngest literal.                             */
+  /* The value zero denotes an "empty" buffer value with p_h1=0 => p_h2=0.     */
+  UBYTE **p_h1=0;
+  UBYTE **p_h2=0;
+
+  /* The following variable holds the current 'cycle' value. This value cycles */
+  /* through the range [0,HASH_TABLE_DEPTH-1], being incremented every time    */
+  /* the hash table is updated. The value gives the within-partition number of */
+  /* the next pointer to be overwritten. The decompressor maintains a cycle    */
+  /* value in synchrony.                                                       */
+  UCARD cycle=0;
+
+  /* Reserve the first two bytes of output as the control word for the first group. */
+  /* Note: This is undone at the end if the input block is empty.              */
+  p_control = p_dst;
+  p_dst += 2;
+
+  /* Initialize all elements of the hash table to point to a constant string.  */
+  /* Use of an unrolled loop speeds this up considerably.                      */
+  /* These variables should really be declared "register", but I am worried    */
+  /* about the possibility that extra register declarations will tempt stupid  */
+  /* compilers to allocate all registers before they get to the innermostloop. */
+  {
+   UCARD i;
+   UBYTE **p_h = hash;
+   #define ZH *p_h++=START_STRING_18
+   for (i = 256; i--;)     /* 256=HASH_TABLE_LENGTH/16. */
+     {ZH;ZH;ZH;ZH;
+      ZH;ZH;ZH;ZH;
+      ZH;ZH;ZH;ZH;
+      ZH;ZH;ZH;ZH;}
+   #undef ZH
+  }
+
+  /* The main loop processes either 1 or 16 items per iteration. As its        */
+  /* termination logic is complicated, I have opted for an infinite loop       */
+  /* structure containing 'break' and 'goto' statements.                       */
+  while (TRUE)
+    {/* Begin main processing loop. */
+
+     /* Note: All the variables here except unroll should be defined within    */
+     /*       the inner loop. Unfortunately the loop hasn't got a block.       */
+      UBYTE const *p_ziv = (void *) 0; /* Points to first byte of current Ziv.       */
+      UCARD unroll;              /* Loop counter for unrolled inner loop.      */
+      UCARD index;               /* Index of current partition.                */
+      UBYTE **p_h0;              /* Pointer to current partition.              */
+      register UCARD d;          /* Depth looping variable.                    */
+      register UCARD bestlen;    /* Holds the best length seen so far.         */
+      register UCARD bestpos;    /* Holds number of best pointer seen so far.  */
+
+     /* Test for overrun and jump to overrun code if necessary.                */
+     if (p_dst > p_dst_post)	/* tbi: This looks wrong.  Why not '>=' ? */
+	goto overrun;
+
+     /* The following cascade of if statements efficiently catches and deals   */
+     /* with varying degrees of closeness to the end of the input block.       */
+     /* When we get very close to the end, we stop updating the table and      */
+     /* code the remaining bytes as literals. This makes the code simpler.     */
+     unroll=16;
+     if (p_src>p_src_max16)
+       {
+	unroll=1;
+	if (p_src>p_src_max1)
+	  {
+	   if (p_src==p_src_post)
+	      break;
+	   else
+	      {p_h0=&hash[ANY_HASH_INDEX]; /* Avoid undefined pointer. */
+	       goto literal;}
+	  }
+       }
+
+     /* This inner unrolled loop processes 'unroll' (whose value is either 1   */
+     /* or 16) items. I have chosen to implement this loop with labels and     */
+     /* gotos to heighten the ease with which the loop may be implemented with */
+     /* a single decrement and branch instruction in assembly language and     */
+     /* also because the labels act as highly readable place markers.          */
+     /* (Also because we jump into the loop for endgame literals (see above)). */
+
+     begin_unrolled_loop:
+
+	p_ziv=p_src;
+
+	/* To process the next phrase, we hash the next three bytes to obtain  */
+	/* an index to the zeroth (first) pointer in a target partition. We    */
+	/* get the pointer.                                                    */
+	index=HASH(p_src);
+	p_h0=&hash[index];
+
+	/* This next part runs through the pointers in the partition matching  */
+	/* the bytes they point to in the Lempel with the bytes in the Ziv.    */
+	/* The length (bestlen) and within-partition pointer number (bestpos)  */
+	/* of the longest match so far is maintained and is the output of this */
+	/* segment of code. The s[bestlen]==... is an optimization only.       */
+	bestlen=0;
+	bestpos=0;
+	for (d=0;d<HASH_TABLE_DEPTH;d++)
+	  {
+	   register UBYTE const *s=p_src;
+	   register UBYTE *p=p_h0[d];
+	   register UCARD len;
+	   if (s[bestlen] == p[bestlen])
+	     {
+	      #define PS *p++!=*s++
+	      (void) (PS || PS || PS || PS || PS || PS || PS || PS || PS ||
+		      PS || PS || PS || PS || PS || PS || PS || PS || PS || s++);
+	      len = s - p_src - 1;
+	      if (len>bestlen)
+		{
+		 bestpos=d;
+		 bestlen=len;
+		}
+	     }
+	  }
+
+	/* The length of the longest match determines whether we code a */
+	/* literal item or a copy item.                                 */
+
+	if (bestlen<3)
+	  {
+	   /* Literal. */
+
+	   /* Code the literal byte as itself and a zero control bit.          */
+	   literal: *p_dst++=*p_src++; control&=0xFFFEFFFF;
+
+	   /* We have just coded a literal. If we had two pending ones, that   */
+	   /* makes three and we can update the hash table.                    */
+	   if (p_h2!=0)
+	      {UPDATE_P(p_h2, (UBYTE *)p_ziv - 2);}
+
+	   /* In any case, rotate the hash table pointers for next time. */
+	   p_h2=p_h1;
+	   p_h1=p_h0;
+
+	  }
+	else
+	  {
+	   /* Copy */
+
+	   /* To code a copy item, we construct a hash table index of the      */
+	   /* winning pointer (index+=bestpos) and code it and the best length */
+	   /* into a 2 byte code word. Bump up p_src.                          */
+	   index+=bestpos;
+	   *p_dst++=((index&0xF00)>>4)|(bestlen-3);
+	   *p_dst++=index&0xFF;
+	   p_src+=bestlen;
+
+	   /* As we have just coded three bytes, we are now in a position to   */
+	   /* update the hash table with the literal bytes that were pending   */
+	   /* upon the arrival of extra context bytes.                         */
+	   if (p_h1!=0)
+	     {
+	      if (p_h2!=0)
+		{UPDATE_P(p_h2,(UBYTE *)p_ziv-2); p_h2=0;}
+	      UPDATE_P(p_h1,(UBYTE *)p_ziv-1); p_h1=0;
+	     }
+
+	   /* In any case, we can update the hash table based on the current   */
+	   /* position as we just coded at least three bytes in a copy items.  */
+	   UPDATE_P(p_h0,(UBYTE *)p_ziv);
+	  }
+	control>>=1;
+
+	/* This loop is all set up for a decrement and jump instruction! */
+     if (--unroll) goto begin_unrolled_loop;
+
+     /* At this point it will nearly always be the end of a group in which     */
+     /* case, we have to do some control-word processing. However, near the    */
+     /* end of the input block, the inner unrolled loop is only executed once. */
+     /* This necessitates the 'if' test.                                       */
+     if ((control&TOPWORD)==0)
+       {
+	/* Write the control word to the place we saved for it in the output. */
+	*p_control++=  control     &0xFF;
+	*p_control  = (control>>8) &0xFF;
+
+	/* Reserve the next word in the output block for the control word */
+	/* for the group about to be processed.                           */
+	p_control=p_dst; p_dst+=2;
+
+	/* Reset the control bits buffer. */
+	control=TOPWORD;
+       }
+
+    } /* End main processing loop. */
+
+  /* After the main processing loop has executed, all the input bytes have     */
+  /* been processed. However, the control word has still to be written to the  */
+  /* word reserved for it in the output at the start of the most recent group. */
+  /* Before writing, the control word has to be shifted so that all the bits   */
+  /* are in the right place. The "empty" bit positions are filled with 1s      */
+  /* which partially fill the top word.                                        */
+  while(control&TOPWORD) control>>=1;
+  *p_control++= control     &0xFF;
+  *p_control++=(control>>8) &0xFF;
+
+  /* If the last group contained no items, delete the control word too.        */
+  if (p_control==p_dst) p_dst-=2;
+
+  /* Write the length of the output block to the dst_len parameter and return. */
+  return p_dst - p_dst_first;
+
+  /* Jump here as soon as an overrun is detected. An overrun is defined to     */
+  /* have occurred if p_dst>p_dst_first+*p_dst_len as set by caller. */
+  /* The algorithm checks for overruns at least at the end of each group       */
+  /* which means that the maximum overrun is MAX_CMP_GROUP bytes.              */
+  /* Once an overrun occurs, the only thing to do is to return an error        */
+  overrun:
+  return 0;
+}
+
+/******************************************************************************/
+
+#ifndef LZRW3A_ASM
+EXPORT int ext2_LZRW3A_decompress
+           (UBYTE *p_src_first, UBYTE *p_dst, void *heap, int src_len,
+	    int dst_len)
+/* Input  : Specify input block using p_src_first.                */
+/* Input  : Point p_dst to the start of the output zone.                */
+/* Input  : dst_len contains output length */
+/* Input  : Input block and output zone must not overlap. User knows          */
+/* Input  : upperbound on output block length from earlier compression.       */
+/* Input  : In any case, maximum expansion possible is nine times.            */
+/* Input  : src_len contains max input length */
+/* Output : Writes only  in Mem[p_dst..p_dst+dst_len-1].       */
+{
+ UBYTE *p_dst_first = p_dst;
+
+ /* Byte pointers scan through the input and output blocks.   */
+ register UBYTE *p_src = p_src_first;
+
+ /* The following two variables are never modified and are used to control    */
+ /* the main loop.                                                            */
+ UBYTE * const p_dst_post  = p_dst+dst_len;
+ UBYTE * const p_dst_max16 = p_dst+dst_len-MAX_RAW_ITEM*16;
+
+ /* The hash table is the only resident of the working memory. The hash table */
+ /* contains HASH_TABLE_LENGTH=4096 pointers to positions in the history. To  */
+ /* keep Macintoshes happy, it is longword aligned.                           */
+#if 0
+ UBYTE **hash = (UBYTE **) ext2_lzrw3a_htab;
+#else
+ UBYTE **hash = (UBYTE **) heap;
+#endif
+
+ /* The variable 'control' is used to buffer the control bits which appear in */
+ /* groups of 16 bits (control words) at the start of each compressed group.  */
+ /* When each group is read, bit 16 of the register is set to one. Whenever   */
+ /* a new bit is needed, the register is shifted right. When the value of the */
+ /* register becomes 1, we know that we have reached the end of a group.      */
+ /* Initializing the register to 1 thus instructs the code to follow that it  */
+ /* should read a new control word immediately.                               */
+ register ULONG control=1;
+
+ /* The value of 'literals' is always in the range 0..3. It is the number of  */
+ /* consecutive literal items just seen. We have to record this number so as  */
+ /* to know when to update the hash table. When literals gets to 3, there     */
+ /* have been three consecutive literals and we can update at the position of */
+ /* the oldest of the three.                                                  */
+ register UCARD literals=0;
+
+ /* The following variable holds the current 'cycle' value. This value cycles */
+ /* through the range [0,HASH_TABLE_DEPTH-1], being incremented every time    */
+ /* the hash table is updated. The value give the within-partition number of  */
+ /* the next pointer to be overwritten. The compressor maintains a cycle      */
+ /* value in synchrony.                                                       */
+ UCARD cycle=0;
+
+ /* Initialize all elements of the hash table to point to a constant string.  */
+ /* Use of an unrolled loop speeds this up considerably.                      */
+ /* The comment about register declarations above similar code in the         */
+ /* compressor applies here too.                                              */
+ {
+  UCARD i; 
+  UBYTE **p_h = hash;
+
+  #define ZJ *p_h++=START_STRING_18
+  for (i=0;i<256;i++)     /* 256=HASH_TABLE_LENGTH/16. */
+    {ZJ;ZJ;ZJ;ZJ;
+     ZJ;ZJ;ZJ;ZJ;
+     ZJ;ZJ;ZJ;ZJ;
+     ZJ;ZJ;ZJ;ZJ;}
+  #undef ZJ
+ }
+
+ /* The outer loop processes either 1 or 16 items per iteration depending on  */
+ /* how close p_dst is to the end of the output block.                         */
+ while (p_dst < p_dst_post && src_len >= 0)
+   {/* Start of outer loop */
+
+    register UCARD unroll;   /* Counts unrolled loop executions.              */
+
+    /* When 'control' has the value 1, it means that the 16 buffered control  */
+    /* bits that were read in at the start of the current group have all been */
+    /* shifted out and that all that is left is the 1 bit that was injected   */
+    /* into bit 16 at the start of the current group. When we reach the end   */
+    /* of a group, we have to load a new control word and inject a new 1 bit. */
+    if (control==1)
+      {
+       control=0x10000|*p_src++;
+       control|=(*p_src++)<<8;
+       src_len -= 2;
+      }
+
+    /* If it is possible that we are within 16 groups from the end of the     */
+    /* input, execute the unrolled loop only once, else process a whole group */
+    /* of 16 items by looping 16 times.                                       */
+    unroll= p_dst<=p_dst_max16 ? 16 : 1;
+
+    /* This inner loop processes one phrase (item) per iteration. */
+    while (unroll--)
+      { /* Begin unrolled inner loop. */
+
+       /* Process a literal or copy item depending on the next control bit. */
+       if (control&1)
+         {
+          /* Copy item. */
+
+          register UBYTE *p;           /* Points to place from which to copy. */
+          register UCARD lenmt;        /* Length of copy item minus three.    */
+          register UBYTE *p_ziv = p_dst; /* Pointer to start of current Ziv.    */
+          register UCARD index;        /* Index of hash table copy pointer.   */
+
+          /* Read and dismantle the copy word. Work out from where to copy.   */
+          lenmt=*p_src++;
+          index=((lenmt&0xF0)<<4)|*p_src++;
+	  src_len -= 2;
+          p=hash[index];
+          lenmt&=0xF;
+
+          /* Now perform the copy using a half unrolled loop. */
+          *p_dst++=*p++;
+          *p_dst++=*p++;
+          *p_dst++=*p++;
+          while (lenmt--)
+             *p_dst++=*p++;
+
+          /* Because we have just received 3 or more bytes in a copy item     */
+          /* (whose bytes we have just installed in the output), we are now   */
+          /* in a position to flush all the pending literal hashings that had */
+          /* been postponed for lack of bytes.                                */
+          if (literals>0)
+            {
+             register UBYTE *r = p_ziv - literals;;
+             UPDATE_I(HASH(r),r);
+             if (literals==2)
+                {r++; UPDATE_I(HASH(r),r);}
+             literals=0;
+            }
+
+          /* In any case, we can immediately update the hash table with the   */
+          /* current position. We don't need to do a HASH(...) to work out    */
+          /* where to put the pointer, as the compressor just told us!!!      */
+          UPDATE_I(index&(~DEPTH_MASK),p_ziv);
+         }
+       else
+         {
+          /* Literal item. */
+
+          /* Copy over the literal byte. */
+          *p_dst++=*p_src++;
+	  src_len--;
+
+          /* If we now have three literals waiting to be hashed into the hash */
+          /* table, we can do one of them now (because there are three).      */
+          if (++literals == 3)
+             {register UBYTE *p=p_dst-3;
+              UPDATE_I(HASH(p),p); literals=2;}
+         }
+
+       /* Shift the control buffer so the next control bit is in bit 0. */
+       control>>=1;
+
+      } /* End unrolled inner loop. */
+
+   } /* End of outer loop */
+
+#if 0
+   if(src_len < 0) return 0;
+   return (p_src-p_src_first);
+#else
+   return p_dst - p_dst_first;
+#endif
+}
+#endif
+
+/******************************************************************************/
+/*                              End of LZRW3-A.C                              */
+/******************************************************************************/
diff -pruN linux-2.6.18.5.org/fs/ext2/lzrw3a/rlzrw3a.S linux-2.6.18.5/fs/ext2/lzrw3a/rlzrw3a.S
--- linux-2.6.18.5.org/fs/ext2/lzrw3a/rlzrw3a.S	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzrw3a/rlzrw3a.S	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,350 @@
+/*	
+ *	LZRW3A decompression written in assembly.
+ *
+ *	Written by Antoine de Maricourt (dumesnil@etca.fr)
+ *	                17 Mar 1995
+ *
+ *	NOTE: The use of this algorithm may be restricted by some
+ *	      patent. Please check if this is the case in your 
+ *	      country before using it. As far as I know LZRW3A is
+ *	      patended in the USA.
+ *
+ *	In order to be used, the compression routine (file lrzw3a.c)
+ *	must be updated to use the following hash function :
+ *
+ *	#define HASH(PTR) (\
+ *	  (((((*(PTR)<<3)^(*((PTR)+1)))<<3)^(*((PTR)+2)))&HASH_MASK)\
+ *	  << HASH_TABLE_DEPTH_BITS)
+ *
+ *	The hash table depth is hardwired to 8.
+ *
+ *	Interface :
+ *	-----------
+ *
+ *	   int rLZRW3A (char *ib, char *ob, int il, int ol);
+ *
+ *              ib = input buffer (compressed data)
+ *              ob = output buffer (uncompressed data)
+ *		il = number of bytes in the input buffer
+ *		ol = number of bytes to be uncompressed
+ *
+ *	The routine may write more (i.e., ???) bytes than requested
+ *	The returned value is the number of bytes written to output
+ *	buffer.
+ *
+ *	The algorithm and the coding method have been taken from
+ *	file lzrw3a.c originaly written by Ross Williams.
+ *
+ *	-----
+ *	Copyright (c) 1995 Antoine de Maricourt. Redistribution
+ *	of this file is permitted under the GNU Public License.
+ */
+
+#include <linux/linkage.h>
+
+#define A0	32
+#define A1	36
+#if 0
+#define A2	40
+#define A3	44
+#else
+#define A2	44
+#define A3	48
+#endif
+#define L0
+
+#if 0
+#define HTAB	SYMBOL_NAME(ext2_lzrw3a_htab)
+#else
+#define HTAB	40(%esp)
+#endif
+
+#if 0
+.comm HTAB, 4				# hash table
+#endif
+
+LC0:	.ascii "123456789012345678\0"
+
+	/* for i386 */
+#define bswap rol $16,
+
+/*
+ *	Initialize the hash table. Some might want to do
+ *	it with an unrolled loop. It would not be faster
+ *	on my system.
+ */
+
+#define InitTable()			\
+	movl HTAB, %edi;		\
+	movl $4096, %ecx;		\
+	movl $LC0, %eax;		\
+	rep; stosl
+
+#define CopyOneByte			\
+	movb (%esi), %dh;		\
+	movb %dh, 3(%edi)
+
+#define CopyByte(o1,o2)			\
+	movb o1(%esi), %dh;		\
+	movb %dh, o2(%edi)
+
+/*
+ *	Update the hash key with the given byte. Note that
+ *	the key is shifted after the xor, and not before.
+ *	This will make it easier to add the cycle count.
+ */
+
+#define UpdateHashKey			\
+	bswap %ebx;			\
+	xorb %dh, %bl;			\
+	andw $0x1ff, %bx;		\
+	shlw $3, %bx;			\
+	bswap %ebx
+
+/*
+ *	Update the hashtable with the current position. The
+ *	cycle count is incremented.
+ */
+
+#define UpdateHashTable			\
+	andl $7, %edx;			\
+	bswap %ebx;			\
+	addw %bx, %dx;			\
+	movl %edi, (%ebp,%edx,4);	\
+	incb %dl;			\
+	bswap %ebx
+
+/*
+ *	Copy a byte from the input buffer to the output
+ *	and remember the value. This will be used instead
+ *	of movsb when we need the value to update the hash
+ *	key.
+ */
+
+#define CopyLiteral			\
+	CopyOneByte;			\
+	incl %esi;			\
+	incl %edi;			\
+	UpdateHashKey
+
+	.text
+
+ENTRY(ext2_LZRW3A_decompress)
+	pushl %ebp
+	pushl %edi
+	pushl %esi
+	pushl %ebx			# be safe
+	pushl %ecx
+	pushl %edx
+	subl $4, %esp
+
+	InitTable ()
+
+	movl A0(%esp), %esi		# esi = input buffer
+	movl A1(%esp), %edi		# edi = output buffer
+	subl $3, %edi
+	addl %esi, A2(%esp)		# input limit
+	movl A3(%esp), %eax
+	addl %edi, %eax	
+	movl %eax, A3(%esp)		# output limit
+	subl $288, %eax
+	movl %eax, L0(%esp)
+
+	/*
+	 *	eax = control word
+	 *	ebx = hash key (high) + literal count (bl) + loop count (bh)
+	 *	ecx = tmp data
+	 *	edx = cycle count (dl) + tmp data (dh)
+	 *	esi = source pointer
+	 *	edi = destination pointer
+	 *	ebp = hash table address
+	 */
+
+	xorl %eax, %eax			# we need a control word
+	movl %eax, %ebx			# hash key is 0
+	movl %eax, %edx			# cycle is 0
+	movl HTAB, %ebp			# get pointer to hash table
+	movb $3, %bl	
+
+	/*
+	 *	Load a fresh control word if needed
+	 */
+
+L10:	cmpl %esi, A2(%esp)		# check for input overflow
+	jna L40
+
+	cmp $1, %eax
+	ja L11
+	movb (%esi), %al
+	movb 1(%esi), %ah
+	lea 2(%esi), %esi
+	orl $0x10000, %eax
+
+L11:	movb $16, %bh
+	cmpl L0(%esp), %edi
+	jbe L12
+	movb $1, %bh
+
+L12:	shrl $1, %eax
+	jc L20				# test control bit
+
+	/*
+	 *	This is a literal
+	 */
+
+	CopyLiteral
+
+	decb %bl
+	jne L13
+	UpdateHashTable
+	incb %bl
+
+L13:	decb %bh			# loop 
+	jne L12
+
+	cmpl A3(%esp), %edi		# check for output overflow
+	jb L10				# and continue
+	jmp L40
+
+	/*
+	 *	This is a copy item
+	 */
+
+
+	ALIGN
+L20:
+	/*
+	 *	Load the index into ecx, and the len into dh
+	 */
+
+	xorl %ecx, %ecx			# clear high part of ecx
+	movb (%esi), %ch		# load 4 high bits of index
+	movb 1(%esi), %cl		# load 8 low bits of index
+	shrb $4, %ch			# adjust index
+
+	movl (%ebp,%ecx,4), %ecx	# get pointer to item
+	xchg %ecx, %esi			# put it into esi, save esi into ecx
+
+	/*
+	 *	Copy first byte
+         */
+
+	CopyLiteral
+	decb %bl
+	jne L22
+	UpdateHashTable
+	incb %bl
+
+	/*
+	 *	Copy second byte
+         */
+
+L22:	CopyLiteral
+	decb %bl
+	jne L23
+	UpdateHashTable
+
+	/*
+	 *	Copy third byte
+	 */
+
+L23:	CopyLiteral
+	UpdateHashTable			# do not need to test bl.
+	movb $3, %bl
+
+	/*
+	 *	Copy the remaining bytes
+	 */
+	
+	testb $1, (%ecx)
+	je L24
+	CopyOneByte
+	incl %esi
+	incl %edi
+
+L24:	testb $2, (%ecx)
+	je L25
+
+	CopyByte (0, 3)
+	CopyByte (1, 4)
+
+	lea 2(%esi), %esi
+	lea 2(%edi), %edi
+
+L25:	cmp %esi, %edi
+	ja L30
+
+	testb $4, (%ecx)
+	je L26
+
+	CopyByte (0, 3)
+	CopyByte (1, 4)
+	CopyByte (2, 5)
+	CopyByte (3, 6)
+
+	lea 4(%esi), %esi
+	lea 4(%edi), %edi
+
+L26:	testb $8, (%ecx)
+	je L27
+
+	CopyByte (0, 3)
+	CopyByte (1, 4)
+	CopyByte (2, 5)
+	CopyByte (3, 6)
+	CopyByte (4, 7)
+	CopyByte (5, 8)
+	CopyByte (6, 9)
+	CopyByte (7, 10)
+
+	lea 8(%esi), %esi
+	lea 8(%edi), %edi
+
+L27:	lea 2(%ecx), %esi		# restore input pointer
+
+	decb %bh			# loop 
+	jne L12
+
+	cmpl A3(%esp), %edi		# check for output overflow
+	jb L10				# and continue
+	jmp L40
+
+	/*
+	 *	Copy the remaining bytes, but using movsl
+	 *	when we know the strings does not overlap.
+	 */
+	
+	ALIGN
+
+L30:	lea 3(%edi), %edi
+	testb $4, (%ecx)
+	je L31
+	movsl
+L31:	testb $8, (%ecx)
+	je L32
+	movsl
+	movsl
+
+L32:	lea -3(%edi), %edi
+	lea  2(%ecx), %esi		# restore input pointer
+	decb %bh			# loop 
+	jne L12
+
+	cmpl A3(%esp), %edi		# check for output overflow
+	jb L10				# and continue
+	
+	/*
+	 *	The end.
+	 */
+
+L40:	lea 3(%edi), %eax
+	subl A1(%esp),%eax
+
+	addl $4, %esp
+	popl %edx
+	popl %ecx
+	popl %ebx
+	popl %esi
+	popl %edi
+	popl %ebp
+	ret
diff -pruN linux-2.6.18.5.org/fs/ext2/lzrw3a/wlzrw3a.c linux-2.6.18.5/fs/ext2/lzrw3a/wlzrw3a.c
--- linux-2.6.18.5.org/fs/ext2/lzrw3a/wlzrw3a.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzrw3a/wlzrw3a.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,776 @@
+/******************************************************************************/
+/*                                                                            */
+/*                                   LZRW3-A.C                                */
+/*                                                                            */
+/******************************************************************************/
+/*                                                                            */
+/* Author  : Ross Williams.                                                   */
+/* Date    : 15-Jul-1991.                                                     */
+/* Release : 1.                                                               */
+/*                                                                            */
+/******************************************************************************/
+/*                                                                            */
+/* This file contains an implementation of the LZRW3-A data compression       */
+/* algorithm in the C programming language.                                   */
+/*                                                                            */
+/* The LZRW3-A algorithm has the following features:                          */
+/*                                                                            */
+/*    1 Requires only 16K of memory (for both compression and decompression). */
+/*    2 The compressor   runs about two   times faster than Unix compress's.  */
+/*    3 The decompressor runs about three times faster than Unix compress's.  */
+/*    4 Yields a few percent better compression than Unix compress for        */
+/*      most files.                                                           */
+/*    5 Allows you to dial up extra compression at a speed cost in the        */
+/*      compressor. The speed of the decompressor is not affected.            */
+/*    6 Algorithm is deterministic.                                           */
+/*    8 This implementation in C is in the public domain.                     */
+/*                                                                            */
+/* (Timing tests for the speed comparison were performed on a Pyramid 9820.)  */
+/*                                                                            */
+/* LZRW3-A is LZRW3 with a deepened hash table. This simple change yields     */
+/* about a 6% (absolute) improvement in compression.                          */
+/*                                                                            */
+/* Here are the results of applying this code, compiled under THINK C 4.0     */
+/* and running on a Mac-SE (8MHz 68000), to the standard calgary corpus.      */
+/*                                                                            */
+/*     +----------------------------------------------------------------+     */
+/*     | DATA COMPRESSION TEST                                          |     */
+/*     | =====================                                          |     */
+/*     | Time of run     : Mon 15-Jul-1991 05:29PM                      |     */
+/*     | Timing accuracy : One part in 100                              |     */
+/*     | Context length  : 262144 bytes (= 256.0000K)                   |     */
+/*     | Test suite      : Calgary Corpus Suite                         |     */
+/*     | Files in suite  : 14                                           |     */
+/*     | Algorithm       : LZRW3-A                                      |     */
+/*     | Note: All averages are calculated from the un-rounded values.  |     */
+/*     +----------------------------------------------------------------+     */
+/*     | File Name   Length  CxB  ComLen  %Remn  Bits  Com K/s  Dec K/s |     */
+/*     | ----------  ------  ---  ------  -----  ----  -------  ------- |     */
+/*     | rpus:Bib.D  111261    1   49044   44.1  3.53     8.47    31.19 |     */
+/*     | us:Book1.D  768771    3  420464   54.7  4.38     7.27    30.07 |     */
+/*     | us:Book2.D  610856    3  277955   45.5  3.64     8.51    33.40 |     */
+/*     | rpus:Geo.D  102400    1   84218   82.2  6.58     4.23    15.04 |     */
+/*     | pus:News.D  377109    2  192880   51.1  4.09     7.08    25.89 |     */
+/*     | pus:Obj1.D   21504    1   12651   58.8  4.71     5.23    17.44 |     */
+/*     | pus:Obj2.D  246814    1  108044   43.8  3.50     8.01    28.11 |     */
+/*     | s:Paper1.D   53161    1   24526   46.1  3.69     8.11    30.24 |     */
+/*     | s:Paper2.D   82199    1   39483   48.0  3.84     8.11    32.04 |     */
+/*     | rpus:Pic.D  513216    2  111622   21.7  1.74    10.64    49.31 |     */
+/*     | us:Progc.D   39611    1   17923   45.2  3.62     8.06    29.01 |     */
+/*     | us:Progl.D   71646    1   24362   34.0  2.72    10.74    39.51 |     */
+/*     | us:Progp.D   49379    1   16805   34.0  2.72    10.64    37.58 |     */
+/*     | us:Trans.D   93695    1   30296   32.3  2.59    11.02    38.06 |     */
+/*     +----------------------------------------------------------------+     */
+/*     | Average     224401    1  100733   45.8  3.67     8.29    31.21 |     */
+/*     +----------------------------------------------------------------+     */
+/*                                                                            */
+/******************************************************************************/
+
+#include "lzrw.h"
+/******************************************************************************/
+
+/* The following structure is returned by the "compress" function below when  */
+/* the user asks the function to return identifying information.              */
+/* The most important field in the record is the working memory field which   */
+/* tells the calling program how much working memory should be passed to      */
+/* "compress" when it is called to perform a compression or decompression.    */
+/* LZRW3-A uses the same amount of memory during compression and              */
+/* decompression. For more information on this structure see "lzrw.h".        */
+/* The alignment fudge below really only needs to be 4 (but I play it safe!). */
+/* The id looks non-random, but it really was generated by coin tossing!      */
+
+#define U(X)            ((ULONG) X)
+#define SIZE_P_BYTE     (U(sizeof(UBYTE *)))
+#define ALIGNMENT_FUDGE (U(16))
+#define MEM_REQ ( U(4096)*(SIZE_P_BYTE) + ALIGNMENT_FUDGE )
+
+/*static struct compress_identity identity =
+{
+ U(0x01B90B91),                            * Algorithm identification number. * 
+ MEM_REQ,                                  * Working memory (bytes) required. * 
+ "LZRW3-A",                                * Name of algorithm.               * 
+ "1.0",                                    * Version number of algorithm.     * 
+ "15-Jul-1990",                            * Date of algorithm.               * 
+ "Ross N. Williams",                       * Author of algorithm.             * 
+ "Renaissance Software",                   * Affiliation of author.           * 
+};*/
+
+/******************************************************************************/
+/*                                                                            */
+/* BRIEF DESCRIPTION OF THE LZRW3-A ALGORITHM                                 */
+/* ==========================================                                 */
+/* Note: Before attempting to understand this algorithm, you should first     */
+/* understand the LZRW3 algorithm from which this algorithm is derived.       */
+/*                                                                            */
+/* The LZRW3-A algorithm is identical to the LZRW3 algorithm except that the  */
+/* hash table has been "deepened". The LZRW3 algorithm has a hash table of    */
+/* 4096 pointers which point to strings in the buffer. LZRW3-A generalizes    */
+/* this to 4096/(2^n) partitions each of which contains (2^n) pointers.       */
+/* In LZRW3-A, the hash function hashes to a partition number.                */
+/*                                                                            */
+/* During the processing of each phrase, LZRW3 overwrites the pointer in the  */
+/* position selected by the hash function. LZRW3-A overwrites one of the      */
+/* pointers in the partition that was selected by the hash function.          */
+/*                                                                            */
+/* When searching for a match, LZRW3-A matches against all (2^n) strings      */
+/* pointed to by the pointers in the target partition.                        */
+/*                                                                            */
+/* Deep hash tables were used in early versions of LZRW1 in late 1989, but    */
+/* were discarded in an effort to increase speed (which was the primary       */
+/* requirement for LZRW1). They were revived for use in LZRW3-A in order to   */
+/* produce an algorithm with compression performance competitive with Unix    */
+/* compress.                                                                  */
+/*                                                                            */
+/* Until 14-Jul-1991, deep hash tables used in prototype LZRW* algorithms     */
+/* used a queue discipline within each partition. Upon the arrival of a new   */
+/* pointer, the pointers in the partition would be block copied back one      */
+/* position (with the oldest pointer being overwritten) and the new pointer   */
+/* being inserted in the space at the front (the youngest position).          */
+/* This meant that pointers to the (2^n) most recent phrases corresponding to */
+/* each hash was kept. The only flaw in this system was the time-consuming    */
+/* block copy operation which was cheap for shallow tables but expensive for  */
+/* deep tables.                                                               */
+/*                                                                            */
+/* The traditional solution to ring buffer block copy problems is to maintain */
+/* a cyclic counter which points to the "head" of the queue. However, this    */
+/* would have required one counter to be stored for each partition and would  */
+/* have been slightly messy. After some thought (on 14-Jul-1991) a better     */
+/* solution was found. Instead of maintaining a counter for each partition,   */
+/* LZRW3-A maintains a single counter for all partitions! This counter is     */
+/* maintained in both the compressor and decompressor and means that the      */
+/* algorithm (effectively) overwrites a RANDOM element of the partition to be */
+/* updated. The result was to increase the speed of the compressor and        */
+/* decompressor, to make the decompressor's speed independent from whatever   */
+/* depth was selected, and to impair compression by less than 1% absolute.    */
+/*                                                                            */
+/* Setting the depth is a speed/compression tradeoff. The table below gives   */
+/* the tradeoff observed for a typical 50K text file on a Mac-SE.             */
+/* Note: %Rem=Percentage Remaining (after compression).                       */
+/*                                                                            */
+/*      Depth    %Rem    CmpK/s  DecK/s                                       */
+/*          1    45.2    14.77   32.24                                        */
+/*          2    42.6    12.12   31.26                                        */
+/*          4    40.9    10.28   31.91                                        */
+/*          8    40.0     7.81   32.36                                        */
+/*         16    39.5     5.30   32.47                                        */
+/*         32    39.0     3.23   32.59                                        */
+/*                                                                            */
+/* I have chosen a depth of 8 as the "default" depth for LZRW3-A. If you use  */
+/* a depth different to this (e.g. 4), you should use the name LZRW3-A(4) to  */
+/* indicate that a different depth is being used. LZRW3-A(8) is an acceptable */
+/* longhand for LZRW3-A.                                                      */
+/*                                                                            */
+/* To change the depth, search for "HERE IT IS" in the rest of this file.     */
+/*                                                                            */
+/*                                  +---+                                     */
+/*                                  |___|4095                                 */
+/*                                  |===|                                     */
+/*              +---------------------*_|<---+   /----+---\                   */
+/*              |                   |___|    +---|Hash    |                   */
+/*              |    512 partitions |___|        |Function|                   */
+/*              |    of 8 pointers  |===|        \--------/                   */
+/*              |    each (or any   |___|0            ^                       */
+/*              |    a*b=4096)      +---+             |                       */
+/*              |                   Hash        +-----+                       */
+/*              |                   Table       |                             */
+/*              |                              ---                            */
+/*              v                              ^^^                            */
+/*      +-------------------------------------|----------------+              */
+/*      ||||||||||||||||||||||||||||||||||||||||||||||||||||||||              */
+/*      +-------------------------------------|----------------+              */
+/*      |                                     |1......18|      |              */
+/*      |<------- Lempel=History ------------>|<--Ziv-->|      |              */
+/*      |     (=bytes already processed)      |<-Still to go-->|              */
+/*      |<-------------------- INPUT BLOCK ------------------->|              */
+/*                                                                            */
+/*                                                                            */
+/******************************************************************************/
+/*                                                                            */
+/*                     DEFINITION OF COMPRESSED FILE FORMAT                   */
+/*                     ====================================                   */
+/*    The file consists of zero  */
+/*    or more GROUPS, each of which represents one or more bytes.             */
+/*  * Each group consists of two bytes of CONTROL information followed by     */
+/*    sixteen ITEMs except for the last group which can contain from one      */
+/*    to sixteen items.                                                       */
+/*  * An item can be either a LITERAL item or a COPY item.                    */
+/*  * Each item corresponds to a bit in the control bytes.                    */
+/*  * The first control byte corresponds to the first 8 items in the group    */
+/*    with bit 0 corresponding to the first item in the group and bit 7 to    */
+/*    the eighth item in the group.                                           */
+/*  * The second control byte corresponds to the second 8 items in the group  */
+/*    with bit 0 corresponding to the ninth item in the group and bit 7 to    */
+/*    the sixteenth item in the group.                                        */
+/*  * A zero bit in a control word means that the corresponding item is a     */
+/*    literal item. A one bit corresponds to a copy item.                     */
+/*  * A literal item consists of a single byte which represents itself.       */
+/*  * A copy item consists of two bytes that represent from 3 to 18 bytes.    */
+/*  * The first  byte in a copy item will be denoted C1.                      */
+/*  * The second byte in a copy item will be denoted C2.                      */
+/*  * Bits will be selected using square brackets.                            */
+/*    For example: C1[0..3] is the low nibble of the first control byte.      */
+/*    of copy item C1.                                                        */
+/*  * The LENGTH of a copy item is defined to be C1[0..3]+3 which is a number */
+/*    in the range [3,18].                                                    */
+/*  * The INDEX of a copy item is defined to be C1[4..7]*256+C2[0..8] which   */
+/*    is a number in the range [0,4095].                                      */
+/*  * A copy item represents the sequence of bytes                            */
+/*       text[POS-OFFSET..POS-OFFSET+LENGTH-1] where                          */
+/*          text   is the entire text of the uncompressed string.             */
+/*          POS    is the index in the text of the character following the    */
+/*                   string represented by all the items preceeding the item  */
+/*                   being defined.                                           */
+/*          OFFSET is obtained from INDEX by looking up the hash table.       */
+/*                                                                            */
+/******************************************************************************/
+
+/* When I first started to get concerned about the portability of my C code,  */
+/* I switched over to using only macro defined types UBYTE, UWORD, ULONG and  */
+/* one or two others. While, these are useful for most purposes, they impair  */
+/* efficiency as, if I have a variable whose range will be [0,1000], I will   */
+/* declare it as a UWORD. This will translate into (say) "short int" and      */
+/* hence may be less efficient than just an "int" which represents the        */
+/* natural size of the machine. Before releasing LZRW3-A, I realized this     */
+/* mistake. Unfortunately, I can't access the ftp archive with my portability */
+/* header in it in time for this algorithm's release and so I am including an */
+/* extra definition. The definition UCARD stands for an unsigned (cardinal)   */
+/* type that can hold values in the range [0,32767]. This is within the ANSI  */
+/* range of a standard int or unsigned. No assumption about overflow of this  */
+/* type is made in the code (i.e. all usages are within range and I do not    */
+/* use the value -1 to detect the end of loops.).                             */
+/* You can use either "unsigned" or just "int" here depending on which is     */
+/* more efficient in your environment (both the same probably).               */
+#define UCARD unsigned
+
+/* The following constant defines the maximum length of an uncompressed item. */
+/* This definition must not be changed; its value is hardwired into the code. */
+/* The longest number of bytes that can be spanned by a single item is 18     */
+/* for the longest copy item.                                                 */
+#define MAX_RAW_ITEM (18)
+
+/* The following constant defines the maximum length of a compressed group.   */
+/* This definition must not be changed; its value is hardwired into the code. */
+/* A compressed group consists of two control bytes followed by up to 16      */
+/* compressed items each of which can have a maximum length of two bytes.     */
+#define MAX_CMP_GROUP (2+16*2)
+
+/* This constant defines the number of pointers in the hash table. The number */
+/* of partitions multiplied by the number of pointers in each partition must  */
+/* multiply out to this value of 4096. In LZRW1, LZRW1-A, and LZRW2, this     */
+/* table length value can be changed. However, in LZRW3-A (and LZRW3), the    */
+/* table length cannot be changed because it is connected directly to the     */
+/* coding scheme which is hardwired (the table index of a single pointer is   */
+/* transmitted in the 12-bit index field). So don't change this constant!     */
+#define HASH_TABLE_LENGTH (4096)
+
+/* HERE IT IS: THE PLACE TO CHANGE THE HASH TABLE DEPTH!                      */
+/* The following definition is the log_2 of the depth of the hash table. This */
+/* constant can be in the range [0,1,2,3,...,12]. Increasing the depth        */
+/* increases compression at the expense of speed. However, you are not likely */
+/* to see much of a compression improvement (e.g. not more than 0.5%) above a */
+/* value of 6 and the algorithm will start to get very slow. See the table in */
+/* the earlier comments block for an idea of the trade-off involved.          */
+/* Note: The parentheses are to avoid macro substitution funnies.             */
+/* Note: The LZRW3-A default is a value of (3).                               */
+/* Note: If you end up choosing a value of 0, you should use LZRW3 instead.   */
+/* Note: Changing the value of HASH_TABLE_DEPTH_BITS is the ONLY thing you    */
+/* have to do to change the depth, so go ahead and recompile now!             */
+/* Note: I have tested LZRW3-A for DEPTH_BITS=0,1,2,3,4 and a few other       */
+/* values. However, I have not tested it for 12 as I can't wait that long!    */
+#define HASH_TABLE_DEPTH_BITS (3)      /* Must be in range [0,12].            */
+
+/* The following definitions are all self-explanatory and follow from the     */
+/* definition of HASH_TABLE_DEPTH_BITS and the hardwired requirement that the */
+/* hash table contain exactly 4096 pointers.                                  */
+#define PARTITION_LENGTH_BITS (12-HASH_TABLE_DEPTH_BITS)
+#define PARTITION_LENGTH      (1<<PARTITION_LENGTH_BITS)
+#define HASH_TABLE_DEPTH      (1<<HASH_TABLE_DEPTH_BITS )
+#define HASH_MASK             (PARTITION_LENGTH-1)
+#define DEPTH_MASK            (HASH_TABLE_DEPTH-1)
+
+/* LZRW3-A, unlike LZRW1(-A), must initialize its hash table so as to enable  */
+/* the compressor and decompressor to stay in step maintaining identical hash */
+/* tables. In an early version of LZRW3, the tables were simply               */
+/* initialized to zero and a check for zero was included just before the      */
+/* matching code. However, this test costs time. A better solution is to      */
+/* initialize all the entries in the hash table to point to a constant        */
+/* string. The decompressor does the same. This solution requires no extra    */
+/* test. The contents of the string do not matter so long as the string is    */
+/* the same for the compressor and decompressor and contains at least         */
+/* MAX_RAW_ITEM bytes. I chose consecutive decimal digits because they do not */
+/* have white space problems (e.g. there is no chance that the compiler will  */
+/* replace more than one space by a TAB) and because they make the length of  */
+/* the string obvious by inspection.                                          */
+#define START_STRING_18 ((UBYTE *) "123456789012345678")
+
+/* The following macro accepts a pointer PTR to three consecutive bytes in    */
+/* memory and hashes them into an integer that is a hash table index that     */
+/* points to the zeroth (first) element of a partition. Thus, the hash        */
+/* function really hashes to a partition number but, for convenience,         */
+/* multiplies it up to yield a hash table index. From all this, we see that   */
+/* the resultant number is in the range [0,HASH_TABLE_LENGTH-1] and is a      */
+/* multiple of HASH_TABLE_DEPTH.                                              */
+/* A macro is used, because in LZRW3-A we have to hash more than once.        */
+#if 0
+#define HASH(PTR) \
+ ( \
+     (((40543*(((*(PTR))<<8)^((*((PTR)+1))<<4)^(*((PTR)+2))))>>4) & HASH_MASK) \
+  << HASH_TABLE_DEPTH_BITS \
+ )
+
+#else
+#define HASH(PTR) \
+ ( \
+     ( ((((*(PTR)<<3)^(*((PTR)+1)))<<3)^(*((PTR)+2))) & HASH_MASK) \
+  << HASH_TABLE_DEPTH_BITS \
+ )
+#endif
+
+/* Another operation that is performed more than once is the updating of the  */
+/* hash table. Here two macros are defined to simplify update operations.     */
+/* Updating consists of identifying and overwriting a pointer in a partition  */
+/* with a newer pointer and then updating the global cycle value.             */
+/* These macros accept the new pointer (NEWPTR) and either a pointer to       */
+/* (P_BASE) or the index of (I_BASE) the zeroth (first, or base) pointer in   */
+/* the partition that is to be updated. The macros use the 'cycle' variable   */
+/* to locate and overwrite a pointer and then update the cycle value.         */
+/* Note: Hardcoding 'cycle' in this macro is naughty (it should really be a   */
+/* macro parameter), but I have done so because it neatens up the code.       */
+#define UPDATE_P(P_BASE,NEWPTR) \
+{(P_BASE)[cycle++]=(NEWPTR); cycle&=DEPTH_MASK;}
+
+#define UPDATE_I(I_BASE,NEWPTR) \
+{hash[(I_BASE)+cycle++]=(NEWPTR); cycle&=DEPTH_MASK;}
+
+/* This constant supplies a legal (in-range) hash table index for use when    */
+/* a legal-but-don't-care index is required.                                  */
+#define ANY_HASH_INDEX (0)
+
+/******************************************************************************/
+#if 0
+void *ext2_lzrw3a_htab;
+#endif
+
+EXPORT int ext2_LZRW3A_compress
+           (UBYTE *p_src_first, UBYTE *p_dst_first, void *heap, int src_len,
+	    int p_dst_len)
+/* Input  : Specify input block using p_src_first and src_len.                */
+/* Input  : Point p_dst_first to the start of the output zone (OZ).           */
+/* Input  : Point p_dst_len to a ULONG to receive the output length.          */
+/* Input  : Input block and output zone must not overlap.                     */
+/* Output : Length of output block written to *p_dst_len.                     */
+/* Output : Output block in Mem[p_dst_first..p_dst_first+*p_dst_len-1]. May   */
+/* Output : write in OZ=Mem[p_dst_first..p_dst_first+src_len+MAX_CMP_GROUP-1].*/
+/* Output : Upon completion *p_dst_len contains final size or 0               */
+{
+ /* p_src and p_dst step through the source and destination blocks.           */
+ UBYTE *p_src = p_src_first;
+ UBYTE *p_dst = p_dst_first;
+
+ /* The following variables are never modified and are used in the            */
+ /* calculations that determine when the main loop terminates.                */
+ UBYTE *p_src_post  = p_src_first+src_len;
+ UBYTE *p_dst_post  = p_dst_first+p_dst_len;
+ UBYTE *p_src_max1  = p_src_first+src_len-MAX_RAW_ITEM;
+ UBYTE *p_src_max16 = p_src_first+src_len-MAX_RAW_ITEM*16;
+
+ /* The variables 'p_control' and 'control' are used to buffer control bits.  */
+ /* Before each group is processed, the next two bytes of the output block    */
+ /* are set aside for the control word for the group about to be processed.   */
+ /* 'p_control' is set to point to the first byte of that word. Meanwhile,    */
+ /* 'control' buffers the control bits being generated during the processing  */
+ /* of the group. Instead of having a counter to keep track of how many items */
+ /* have been processed (=the number of bits in the control word), at the     */
+ /* start of each group, the top word of 'control' is filled with 1 bits.     */
+ /* As 'control' is shifted for each item, the 1 bits in the top word are     */
+ /* absorbed or destroyed. When they all run out (i.e. when the top word is   */
+ /* all zero bits, we know that we are at the end of a group.                 */
+ #define TOPWORD 0xFFFF0000
+ UBYTE *p_control;
+ ULONG control=TOPWORD;
+
+ /* The variable 'hash' always points to the first element of the hash table. */
+#if 0
+ UBYTE **hash= (UBYTE **) ext2_lzrw3a_htab;
+#else
+ UBYTE **hash= (UBYTE **) heap;
+#endif
+
+ /* The following two variables represent the literal buffer. p_h1 points to  */
+ /* the partition (i.e. the zero'th (first) element of the partition)         */
+ /* corresponding to the youngest literal. p_h2 points to the partition       */
+ /* corresponding to the second youngest literal.                             */
+ /* The value zero denotes an "empty" buffer value with p_h1=0 => p_h2=0.     */
+ UBYTE **p_h1=0;
+ UBYTE **p_h2=0;
+
+ /* The following variable holds the current 'cycle' value. This value cycles */
+ /* through the range [0,HASH_TABLE_DEPTH-1], being incremented every time    */
+ /* the hash table is updated. The value gives the within-partition number of */
+ /* the next pointer to be overwritten. The decompressor maintains a cycle    */
+ /* value in synchrony.                                                       */
+ UCARD cycle=0;
+
+ /* Reserve the first word of output as the control word for the first group. */
+ /* Note: This is undone at the end if the input block is empty.              */
+ p_control=p_dst; p_dst+=2;
+
+ /* Initialize all elements of the hash table to point to a constant string.  */
+ /* Use of an unrolled loop speeds this up considerably.                      */
+ /* These variables should really be declared "register", but I am worried    */
+ /* about the possibility that extra register declarations will tempt stupid  */
+ /* compilers to allocate all registers before they get to the innermostloop. */
+ {UCARD i; UBYTE **p_h=hash;
+  #define ZH *p_h++=START_STRING_18
+  for (i=0;i<256;i++)     /* 256=HASH_TABLE_LENGTH/16. */
+    {ZH;ZH;ZH;ZH;
+     ZH;ZH;ZH;ZH;
+     ZH;ZH;ZH;ZH;
+     ZH;ZH;ZH;ZH;}
+ }
+
+ /* The main loop processes either 1 or 16 items per iteration. As its        */
+ /* termination logic is complicated, I have opted for an infinite loop       */
+ /* structure containing 'break' and 'goto' statements.                       */
+ while (TRUE)
+   {/* Begin main processing loop. */
+
+    /* Note: All the variables here except unroll should be defined within    */
+    /*       the inner loop. Unfortunately the loop hasn't got a block.       */
+     UBYTE *p_ziv;              /* Points to first byte of current Ziv.       */
+     UCARD unroll;              /* Loop counter for unrolled inner loop.      */
+     UCARD index;               /* Index of current partition.                */
+     UBYTE **p_h0;              /* Pointer to current partition.              */
+     register UCARD d;          /* Depth looping variable.                    */
+     register UCARD bestlen;    /* Holds the best length seen so far.         */
+     register UCARD bestpos;    /* Holds number of best pointer seen so far.  */
+
+    /* Test for overrun and jump to overrun code if necessary.                */
+    if (p_dst>p_dst_post)
+       goto overrun;
+
+    /* The following cascade of if statements efficiently catches and deals   */
+    /* with varying degrees of closeness to the end of the input block.       */
+    /* When we get very close to the end, we stop updating the table and      */
+    /* code the remaining bytes as literals. This makes the code simpler.     */
+    unroll=16;
+    if (p_src>p_src_max16)
+      {
+       unroll=1;
+       if (p_src>p_src_max1)
+         {
+          if (p_src==p_src_post)
+             break;
+          else
+             {p_h0=&hash[ANY_HASH_INDEX]; /* Avoid undefined pointer. */
+              goto literal;}
+         }
+      }
+
+    /* This inner unrolled loop processes 'unroll' (whose value is either 1   */
+    /* or 16) items. I have chosen to implement this loop with labels and     */
+    /* gotos to heighten the ease with which the loop may be implemented with */
+    /* a single decrement and branch instruction in assembly language and     */
+    /* also because the labels act as highly readable place markers.          */
+    /* (Also because we jump into the loop for endgame literals (see above)). */
+
+    begin_unrolled_loop:
+
+       p_ziv=p_src;
+
+       /* To process the next phrase, we hash the next three bytes to obtain  */
+       /* an index to the zeroth (first) pointer in a target partition. We    */
+       /* get the pointer.                                                    */
+       index=HASH(p_src);
+       p_h0=&hash[index];
+
+       /* This next part runs through the pointers in the partition matching  */
+       /* the bytes they point to in the Lempel with the bytes in the Ziv.    */
+       /* The length (bestlen) and within-partition pointer number (bestpos)  */
+       /* of the longest match so far is maintained and is the output of this */
+       /* segment of code. The s[bestlen]==... is an optimization only.       */
+       bestlen=0;
+       bestpos=0;
+       for (d=0;d<HASH_TABLE_DEPTH;d++)
+         {
+          register UBYTE *s=p_src;
+          register UBYTE *p=p_h0[d];
+          register UCARD len;
+          if (s[bestlen] == p[bestlen])
+            {
+             #define PS *p++!=*s++
+             (void) (PS || PS || PS || PS || PS || PS || PS || PS || PS ||
+                     PS || PS || PS || PS || PS || PS || PS || PS || PS || s++);
+             len=s-p_src-1;
+             if (len>bestlen)
+               {
+                bestpos=d;
+                bestlen=len;
+               }
+            }
+         }
+
+       /* The length of the longest match determines whether we code a */
+       /* literal item or a copy item.                                 */
+
+       if (bestlen<3)
+         {
+          /* Literal. */
+
+          /* Code the literal byte as itself and a zero control bit.          */
+          literal: *p_dst++=*p_src++; control&=0xFFFEFFFF;
+
+          /* We have just coded a literal. If we had two pending ones, that   */
+          /* makes three and we can update the hash table.                    */
+          if (p_h2!=0)
+             {UPDATE_P(p_h2,p_ziv-2);}
+
+          /* In any case, rotate the hash table pointers for next time. */
+          p_h2=p_h1; p_h1=p_h0;
+
+         }
+       else
+         {
+          /* Copy */
+
+          /* To code a copy item, we construct a hash table index of the      */
+          /* winning pointer (index+=bestpos) and code it and the best length */
+          /* into a 2 byte code word. Bump up p_src.                          */
+          index+=bestpos;
+          *p_dst++=((index&0xF00)>>4)|(bestlen-3);
+          *p_dst++=index&0xFF;
+          p_src+=bestlen;
+
+          /* As we have just coded three bytes, we are now in a position to   */
+          /* update the hash table with the literal bytes that were pending   */
+          /* upon the arrival of extra context bytes.                         */
+          if (p_h1!=0)
+            {
+             if (p_h2!=0)
+               {UPDATE_P(p_h2,p_ziv-2); p_h2=0;}
+             UPDATE_P(p_h1,p_ziv-1); p_h1=0;
+            }
+
+          /* In any case, we can update the hash table based on the current   */
+          /* position as we just coded at least three bytes in a copy items.  */
+          UPDATE_P(p_h0,p_ziv);
+         }
+       control>>=1;
+
+       /* This loop is all set up for a decrement and jump instruction! */
+    if (--unroll) goto begin_unrolled_loop;
+
+    /* At this point it will nearly always be the end of a group in which     */
+    /* case, we have to do some control-word processing. However, near the    */
+    /* end of the input block, the inner unrolled loop is only executed once. */
+    /* This necessitates the 'if' test.                                       */
+    if ((control&TOPWORD)==0)
+      {
+       /* Write the control word to the place we saved for it in the output. */
+       *p_control++=  control     &0xFF;
+       *p_control  = (control>>8) &0xFF;
+
+       /* Reserve the next word in the output block for the control word */
+       /* for the group about to be processed.                           */
+       p_control=p_dst; p_dst+=2;
+
+       /* Reset the control bits buffer. */
+       control=TOPWORD;
+      }
+
+   } /* End main processing loop. */
+
+ /* After the main processing loop has executed, all the input bytes have     */
+ /* been processed. However, the control word has still to be written to the  */
+ /* word reserved for it in the output at the start of the most recent group. */
+ /* Before writing, the control word has to be shifted so that all the bits   */
+ /* are in the right place. The "empty" bit positions are filled with 1s      */
+ /* which partially fill the top word.                                        */
+ while(control&TOPWORD) control>>=1;
+ *p_control++= control     &0xFF;
+ *p_control++=(control>>8) &0xFF;
+
+ /* If the last group contained no items, delete the control word too.        */
+ if (p_control==p_dst) p_dst-=2;
+
+ /* Write the length of the output block to the dst_len parameter and return. */
+ return p_dst-p_dst_first;
+
+ /* Jump here as soon as an overrun is detected. An overrun is defined to     */
+ /* have occurred if p_dst>p_dst_first+*p_dst_len as set by caller. */
+ /* The algorithm checks for overruns at least at the end of each group       */
+ /* which means that the maximum overrun is MAX_CMP_GROUP bytes.              */
+ /* Once an overrun occurs, the only thing to do is to return an error        */
+ overrun:
+ return 0;
+}
+
+/******************************************************************************/
+
+#if 0
+EXPORT int ext2_LZRW3A_decompress
+           (UBYTE *p_src_first, UBYTE *p_dst, int src_len, int dst_len)
+/* Input  : Specify input block using p_src_first.                */
+/* Input  : Point p_dst to the start of the output zone.                */
+/* Input  : dst_len contains output length */
+/* Input  : Input block and output zone must not overlap. User knows          */
+/* Input  : upperbound on output block length from earlier compression.       */
+/* Input  : In any case, maximum expansion possible is nine times.            */
+/* Input  : src_len contains max input length */
+/* Output : Writes only  in Mem[p_dst..p_dst+dst_len-1].       */
+{
+ UBYTE *p_dst_first = p_dst;
+
+ /* Byte pointers scan through the input and output blocks.   */
+ register UBYTE *p_src = p_src_first;
+
+ /* The following two variables are never modified and are used to control    */
+ /* the main loop.                                                            */
+ UBYTE *p_dst_post  = p_dst+dst_len;
+ UBYTE *p_dst_max16 = p_dst+dst_len-MAX_RAW_ITEM*16;
+
+ /* The hash table is the only resident of the working memory. The hash table */
+ /* contains HASH_TABLE_LENGTH=4096 pointers to positions in the history. To  */
+ /* keep Macintoshes happy, it is longword aligned.                           */
+ UBYTE **hash = (UBYTE **) ext2_lzrw3a_htab;
+
+ /* The variable 'control' is used to buffer the control bits which appear in */
+ /* groups of 16 bits (control words) at the start of each compressed group.  */
+ /* When each group is read, bit 16 of the register is set to one. Whenever   */
+ /* a new bit is needed, the register is shifted right. When the value of the */
+ /* register becomes 1, we know that we have reached the end of a group.      */
+ /* Initializing the register to 1 thus instructs the code to follow that it  */
+ /* should read a new control word immediately.                               */
+ register ULONG control=1;
+
+ /* The value of 'literals' is always in the range 0..3. It is the number of  */
+ /* consecutive literal items just seen. We have to record this number so as  */
+ /* to know when to update the hash table. When literals gets to 3, there     */
+ /* have been three consecutive literals and we can update at the position of */
+ /* the oldest of the three.                                                  */
+ register UCARD literals=0;
+
+ /* The following variable holds the current 'cycle' value. This value cycles */
+ /* through the range [0,HASH_TABLE_DEPTH-1], being incremented every time    */
+ /* the hash table is updated. The value give the within-partition number of  */
+ /* the next pointer to be overwritten. The compressor maintains a cycle      */
+ /* value in synchrony.                                                       */
+ UCARD cycle=0;
+
+ /* Initialize all elements of the hash table to point to a constant string.  */
+ /* Use of an unrolled loop speeds this up considerably.                      */
+ /* The comment about register declarations above similar code in the         */
+ /* compressor applies here too.                                              */
+ {UCARD i; UBYTE **p_h=hash;
+  #define ZJ *p_h++=START_STRING_18
+  for (i=0;i<256;i++)     /* 256=HASH_TABLE_LENGTH/16. */
+    {ZJ;ZJ;ZJ;ZJ;
+     ZJ;ZJ;ZJ;ZJ;
+     ZJ;ZJ;ZJ;ZJ;
+     ZJ;ZJ;ZJ;ZJ;}
+ }
+
+ /* The outer loop processes either 1 or 16 items per iteration depending on  */
+ /* how close p_dst is to the end of the output block.                         */
+ while (p_dst < p_dst_post && src_len >= 0)
+   {/* Start of outer loop */
+
+    register UCARD unroll;   /* Counts unrolled loop executions.              */
+
+    /* When 'control' has the value 1, it means that the 16 buffered control  */
+    /* bits that were read in at the start of the current group have all been */
+    /* shifted out and that all that is left is the 1 bit that was injected   */
+    /* into bit 16 at the start of the current group. When we reach the end   */
+    /* of a group, we have to load a new control word and inject a new 1 bit. */
+    if (control==1)
+      {
+       control=0x10000|*p_src++;
+       control|=(*p_src++)<<8;
+       src_len -= 2;
+      }
+
+    /* If it is possible that we are within 16 groups from the end of the     */
+    /* input, execute the unrolled loop only once, else process a whole group */
+    /* of 16 items by looping 16 times.                                       */
+    unroll= p_dst<=p_dst_max16 ? 16 : 1;
+
+    /* This inner loop processes one phrase (item) per iteration. */
+    while (unroll--)
+      { /* Begin unrolled inner loop. */
+
+       /* Process a literal or copy item depending on the next control bit. */
+       if (control&1)
+         {
+          /* Copy item. */
+
+          register UBYTE *p;           /* Points to place from which to copy. */
+          register UCARD lenmt;        /* Length of copy item minus three.    */
+          register UBYTE *p_ziv=p_dst; /* Pointer to start of current Ziv.    */
+          register UCARD index;        /* Index of hash table copy pointer.   */
+
+          /* Read and dismantle the copy word. Work out from where to copy.   */
+          lenmt=*p_src++;
+          index=((lenmt&0xF0)<<4)|*p_src++;
+	  src_len -= 2;
+          p=hash[index];
+          lenmt&=0xF;
+
+          /* Now perform the copy using a half unrolled loop. */
+          *p_dst++=*p++;
+          *p_dst++=*p++;
+          *p_dst++=*p++;
+          while (lenmt--)
+             *p_dst++=*p++;
+
+          /* Because we have just received 3 or more bytes in a copy item     */
+          /* (whose bytes we have just installed in the output), we are now   */
+          /* in a position to flush all the pending literal hashings that had */
+          /* been postponed for lack of bytes.                                */
+          if (literals>0)
+            {
+             register UBYTE *r=p_ziv-literals;;
+             UPDATE_I(HASH(r),r);
+             if (literals==2)
+                {r++; UPDATE_I(HASH(r),r);}
+             literals=0;
+            }
+
+          /* In any case, we can immediately update the hash table with the   */
+          /* current position. We don't need to do a HASH(...) to work out    */
+          /* where to put the pointer, as the compressor just told us!!!      */
+          UPDATE_I(index&(~DEPTH_MASK),p_ziv);
+         }
+       else
+         {
+          /* Literal item. */
+
+          /* Copy over the literal byte. */
+          *p_dst++=*p_src++;
+	  src_len--;
+
+          /* If we now have three literals waiting to be hashed into the hash */
+          /* table, we can do one of them now (because there are three).      */
+          if (++literals == 3)
+             {register UBYTE *p=p_dst-3;
+              UPDATE_I(HASH(p),p); literals=2;}
+         }
+
+       /* Shift the control buffer so the next control bit is in bit 0. */
+       control>>=1;
+
+      } /* End unrolled inner loop. */
+
+   } /* End of outer loop */
+
+#if 0
+   if(src_len < 0) return 0;
+   return (p_src-p_src_first);
+#else
+   return p_dst - p_dst_first;
+#endif
+}
+#endif
+
+/******************************************************************************/
+/*                              End of LZRW3-A.C                              */
+/******************************************************************************/
diff -pruN linux-2.6.18.5.org/fs/ext2/lzv1/Makefile linux-2.6.18.5/fs/ext2/lzv1/Makefile
--- linux-2.6.18.5.org/fs/ext2/lzv1/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzv1/Makefile	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,19 @@
+#
+# Makefile for the linux compression routines.
+#
+# Note! Dependencies are done automagically by 'make dep', which also
+# removes any old dependencies. DON'T put your own dependencies here
+# unless it's something special (ie not a .c file).
+#
+# Note 2! The CFLAGS definitions are now in the main makefile...
+
+.S.o:
+	$(CC) $(AFLAGS) -traditional -c $< -o $*.o
+
+obj-$(CONFIG_EXT2_HAVE_LZV1) := ext2-compr-lzv1.o
+
+ifeq ($(CONFIG_EXT2_COMPR_X86_CODE),y)
+ext2-compr-lzv1-y   := e2compr_lzv.o rlzv1.o wlzv1.o
+else
+ext2-compr-lzv1-y   := e2compr_lzv.o lzv1.o
+endif
diff -pruN linux-2.6.18.5.org/fs/ext2/lzv1/e2compr_lzv.c linux-2.6.18.5/fs/ext2/lzv1/e2compr_lzv.c
--- linux-2.6.18.5.org/fs/ext2/lzv1/e2compr_lzv.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzv1/e2compr_lzv.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,71 @@
+#include <linux/fs.h>
+#include <linux/ext2_fs_c.h>
+#include <linux/types.h>
+#include <linux/module.h>
+#include "lzv1.h"
+
+#ifdef MODULE
+MODULE_AUTHOR("Hermann Vogt");
+MODULE_DESCRIPTION("Lev-Zimpel-Vogt algorithm for EXT2 file compression");
+MODULE_LICENSE("GPL");
+#endif
+
+size_t ext2_iLZV1 (int action)
+{
+	switch (action) {
+		case EXT2_ALG_INIT_COMPRESS:
+			return HSIZE * sizeof (unsigned short);
+		case EXT2_ALG_INIT_DECOMPRESS:
+			return 0;
+		default:
+			return 0;
+	}
+}
+
+size_t ext2_wLZV1 (unsigned char *in, unsigned char *out, void *heap, size_t len, size_t out_len, int xarg)
+{
+	int ret_code;
+
+	if (!try_module_get(THIS_MODULE))
+		return 0;
+	ret_code = ext2_LZV1_compress(in, out, heap, len, out_len);
+	module_put(THIS_MODULE);
+	return ret_code;
+}
+
+size_t ext2_rLZV1 (unsigned char *in, unsigned char *out, void *heap, size_t ilen, size_t len, int xarg)
+{
+	int ret_code;
+
+	if (!try_module_get(THIS_MODULE))
+		return 0;
+	ret_code = ext2_LZV1_decompress(in, out, ilen, len);
+	module_put(THIS_MODULE);
+	return ret_code;
+}
+
+#ifdef MODULE
+
+int init_module(void)
+{
+	struct ext2_algorithm lzv_alg;
+
+	lzv_alg.name = NULL;
+	lzv_alg.avail = 1;
+	lzv_alg.init = ext2_iLZV1;
+	lzv_alg.compress = ext2_wLZV1;
+	lzv_alg.decompress = ext2_rLZV1;
+
+	return ext2_register_compression_module(EXT2_LZV1_ALG,
+			HSIZE * sizeof(unsigned short),
+			0,
+			&lzv_alg);
+}
+
+void cleanup_module(void)
+{
+	ext2_unregister_compression_module(EXT2_LZV1_ALG);
+}
+
+#endif
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzv1/lzv1.c linux-2.6.18.5/fs/ext2/lzv1/lzv1.c
--- linux-2.6.18.5.org/fs/ext2/lzv1/lzv1.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzv1/lzv1.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,245 @@
+/***********************************************************************
+**
+**	lzv.c -- an extremly fast compression/decompression-method
+**
+**	Written by Hermann Vogt
+**
+**		v 0.3 -- 94/03/08	aCembler version of rLZV built in.
+**		v 0.2 -- 94/03/04	Changes for usage with DouBle 0.2 built in.
+**		v 0.1 -- 94/03/01	Intensivly tested, removed all known bugs.
+**		v 0.0 -- 94/02/21	First Version.
+**
+**	Copyright (c) 1994 Hermann Vogt. Redistribution of this file is
+**	permitted under the GNU Public Licence.
+**
+**	The method presented here is faster and compresses better
+**	than lzrw1 and lzrw1-a. I named it lzv for "Lev-Zimpel-Vogt".
+**	It uses ideas introduced by Ross Williams in his algorithm lzrw1
+**	[R. N. Williams (1991): "An Extremly Fast ZIV-Lempel Data
+**	Compression Algorithm", Proceedings IEEE Data Compression
+**	Conference, Snowbird, Utah, 362-371] and by Fiala and Green in their
+**	algorithm a1 [E. R. Fiala, D. H. Greene (1989): "Data Compression
+**	with Finite Windows", Communications of the ACM, 4, 490-505].
+**	Because lzv differs strongly from both, I hope there will be no
+**	patent problems. The hashing-method has been stolen from Jean-loup
+**	Gailly's (patent free) gzip.
+**
+**	KNOWN PROBLEMS:
+**	-	My english is very bad.
+**  -	Badly commented. (I hope this will be better in the next
+**		version.)
+**	-	I'm not sure if lzv is free from patent problems.
+**
+***********************************************************************/
+
+#define	MLL	32		/*	Maximum len of chain of literals	*/
+#define	MML	(8+256)		/*	Maximum len of match			*/
+#define	MOFF	8191		/*	Maximum offset				*/
+#define	HSIZ	16384		/*	Size of Hashtable			*/
+
+typedef	unsigned char	uch;
+typedef	unsigned short	ush;
+typedef	unsigned int	uit;
+
+#if 0
+unsigned short *ext2_lzv1_htab = ((unsigned short*) 0L);
+#endif
+
+#ifdef __KERNEL__
+# include <linux/linkage.h>
+# include <linux/sched.h>
+# include <asm/param.h>
+static unsigned long next_brk = 0;
+extern unsigned long volatile jiffies;
+#endif
+
+int ext2_LZV1_compress (uch *in, uch *out, ush *heap, int len, int out_len)
+{
+	uit hval, op, ip, l_len, m_pos, m_off, m_len, maxlen;
+	ush *ext2_lzv1_htab = heap;
+
+	maxlen = out_len;
+	hval = ((in[0] << 5) ^ in[1]) & (HSIZ - 1);
+	ip = op = l_len = 0;
+	do {
+		hval = ((hval << 5) ^ in[ip+2]) & (HSIZ - 1);
+		m_pos = ext2_lzv1_htab[hval];
+		ext2_lzv1_htab[hval] = ip;
+
+#if 0
+		/* 
+		 *	If you want to compress more than 64K, uncomment
+		 *	the following lines.
+		 */
+
+		m_pos = (ip &~0xffff) + m_pos;
+		if (m_pos >= ip && m_pos >= 0x10000)
+		  m_pos -= 0x10000;
+#endif
+
+#ifdef __KERNEL__
+		/* Time slice of a fifth of a second. */
+		if (jiffies > next_brk)
+		  {
+		    schedule();
+		    next_brk = jiffies + HZ / 5;  
+		  }
+#endif
+
+		if (m_pos < ip &&
+				in[m_pos] == in[ip] &&
+				(m_off = ip - m_pos - 1) <= MOFF &&
+				ip + 4 < len &&
+				*(ush *)(in+m_pos+1) == *(ush *)(in+ip+1)) {
+			/*	We have found a match	*/
+			uit look = len - ip - 2;
+			if (look > MML) look = MML;
+			m_len = 2;
+			do {
+				m_len++;
+			} while(m_len != look && in[ip+m_len] == in[m_pos+m_len]);
+
+			if (op + 2 + l_len + 3 >= maxlen) return 0;
+			if (l_len != 0) {
+				out[op++] = (l_len - 1) << 3;
+				do {
+					out[op++] = in[ip-l_len--];
+				} while (l_len != 0);
+			}
+			m_len -= 2;
+			if (m_len <= 6) {
+				out[op++] = m_len | ((m_off >> 5) & 0xf8);
+			}
+			else {
+				out[op++] = 0x07 | ((m_off >> 5) & 0xf8);
+				out[op++] = m_len - 7;
+			}
+			out[op++] = m_off & 0xff;
+			ip++;
+			hval = ((hval << 5) ^ in[ip+2]) & (HSIZ - 1);
+			ext2_lzv1_htab[hval] = ip;
+			ip++;
+			do {
+				hval = ((hval << 5) ^ in[ip+2]) & (HSIZ - 1);
+				ext2_lzv1_htab[hval] = ip;
+				ip++;
+				m_len--;
+			} while (0 != m_len);
+		}
+		else {
+			/*	No match found	*/
+			ip++;
+			l_len++;
+			if (MLL == l_len) {
+				if (op + 2 + MLL >= maxlen) return 0;
+				out[op++] = 0xf8;
+				do {
+					out[op++] = in[ip-l_len--];
+				} while (l_len != 0);
+			}
+		}
+	} while (ip < len);
+	if (l_len != 0) {
+		if (op + l_len + 3 >= maxlen) return 0;
+		out[op++] = (l_len - 1) << 3;
+		do {
+			out[op++] = in[ip-l_len--];
+		} while (l_len != 0);
+	}
+	return op;
+}
+
+#ifndef LZV_ASM
+
+int ext2_LZV1_decompress (uch const * const in, uch * const out, int ilen, int len)
+{
+	register uit tbuf, c_len;
+	uch * const out_end = out + len;
+	register uch *op = out;
+	uch const * const in_end = in + ilen;
+	register uch const *ip = in;
+
+	do {
+		tbuf = *ip++;
+ 		c_len = tbuf & 0x07;
+		if (0 == c_len) {
+			c_len = (tbuf >> 3) + 1;
+			do *op++ = *ip++;
+			while (--c_len); /* effic: memcpy()? */
+		} else {
+			register uch *m_pos;
+
+			if (0x07 == c_len) c_len = *ip++ + 7;
+			m_pos = op - 1 - (((uit) (tbuf & 0xf8) << 5) | *ip++);
+#ifndef EXT2_NO_ERRCHK
+			/* If we don't check this then we segfault (if in user
+                           space) or leave process in uninteruptible state (if
+                           in kernel) if the data is corrupt. */
+			if (m_pos < out)
+			  return 0; /* Compression error. */
+#endif
+			*op++ = *m_pos++;
+			*op++ = *m_pos++;
+			do *op++ = *m_pos++;
+			while (--c_len);
+		}
+	} while (op < out_end && ip < in_end);
+	return op - out;
+#if 0
+	if(ip > in_end) return 0;
+	return (ip - in);
+#endif
+}
+
+#else
+
+/*
+**	aCembler version
+**
+**	IMPORTANT: Use gcc -O2 -fomit-frame-pointer!!!
+*/
+
+int ext2_LZV1_decompress (uch *in, uch *out, int ilen, int len)
+{
+	register uit tbuf;
+	register uch *ip;
+	register uch *op asm ("%edi");
+	register uch *m_pos asm ("%esi");
+	uch *out_end, *in_end;
+	register uit c_len asm ("%ecx");
+
+	ip = in;
+	op = out;
+	out_end = out + len;
+	in_end = in + ilen;
+	asm volatile ("cld");
+	do {
+		tbuf = *ip++;
+		c_len = tbuf & 0x07;
+		if (0 == c_len) {
+			c_len = (tbuf >> 3) + 1;
+			m_pos = ip;
+			ip += c_len;
+			asm volatile ("rep; movsb"
+				: /* no output */
+				: "g" (m_pos), "g" (op), "g" (c_len));
+		}
+		else {
+			if (0x07 == c_len) c_len = *ip++ + 9;
+			else c_len += 2;
+			m_pos = op - 1 - (((uit) (tbuf & 0xf8) << 5) | *ip++);
+			asm volatile ("rep; movsb"
+				: /* no output */
+				: "g" (m_pos), "g" (op), "g" (c_len));
+		}
+	} while (op < out_end && ip < in_end);
+
+	return op-out;
+#if 0
+	if(ip > in_end) return 0;
+	return (ip-in);
+#endif
+}
+
+#endif
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzv1/lzv1.h linux-2.6.18.5/fs/ext2/lzv1/lzv1.h
--- linux-2.6.18.5.org/fs/ext2/lzv1/lzv1.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzv1/lzv1.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,21 @@
+#define HSIZE	0x4000
+#define HMASK	0x3fff
+#define HSHIFT	5
+
+#define MOFF	8192
+#define	MLL	32		/* Maximum literal length */
+#define	MML	261		/* Maximum match length   */
+
+#ifndef __ASSEMBLY__
+#if 0
+extern unsigned short *ext2_lzv1_htab;
+#endif
+
+typedef	unsigned char	uch;
+typedef	unsigned short	ush;
+typedef	unsigned int	uit;
+
+int ext2_LZV1_compress (uch *in, uch *out, ush *heap, int len, int out_len);
+int ext2_LZV1_decompress (uch *in, uch *out, int ilen, int len);
+#endif
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzv1/rlzv1.S linux-2.6.18.5/fs/ext2/lzv1/rlzv1.S
--- linux-2.6.18.5.org/fs/ext2/lzv1/rlzv1.S	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzv1/rlzv1.S	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,168 @@
+/*	
+ *	LZV decompression written in assembly.
+ *
+ *	Written by Antoine de Maricourt (dumesnil@etca.fr)
+ *	                15 Mar 1995
+ *
+ *	NOTE: The use of this algorithm may be restricted by some
+ *	      patent. Please check if this is the case in your 
+ *	      country before using it.
+ *
+ *	Interface :
+ *	-----------
+ *
+ *	   int rLZV1 (char *ib, char *ob, int il, int ol);
+ *
+ *              ib = input buffer (compressed data)
+ *              ob = output buffer (uncompressed data)
+ *		il = number of bytes in the input buffer
+ *		ol = number of bytes to be uncompressed
+ *
+ *	The routine may write more (i.e., 264) bytes than requested
+ *	(and could be modified in order to write exactly the requested
+ *	number if this is really important). However, it will not write
+ *	more than the original number of bytes in the uncompressed
+ *	data. The returned value is the number of bytes written to
+ *	output buffer.
+ *
+ *	The algorithm and the coding method have been taken from
+ *	file lzv.c originaly written by Hermann Vogt, and whose
+ *	original copyright notice follows:
+ *
+ *	-------
+ *	Copyright (c) 1994 Hermann Vogt. Redistribution of this file is
+ *	permitted under the GNU Public Licence.
+ *
+ *	The method presented here is faster and compresses better
+ *	than lzrw1 and lzrw1-a. I named it lzv for "Lev-Zimpel-Vogt".
+ *	It uses ideas introduced by Ross Williams in his algorithm 
+ *	lzrw1 [R. N. Williams (1991): "An Extremly Fast ZIV-Lempel Data
+ *	Compression Algorithm", Proceedings IEEE Data Compression
+ *	Conference, Snowbird, Utah, 362-371] and by Fiala and Green
+ *	in their algorithm a1 [E. R. Fiala, D. H. Greene (1989): 
+ *	"Data Compression with Finite Windows", Communications of the
+ *	ACM, 4, 490-505]. Because lzv differs strongly from both,
+ *	I hope there will be no	patent problems. The hashing-method 
+ *	has been stolen from Jean-loup Gailly's (patent free) gzip.
+ *	-------
+ *	Copyright (c) 1995 Antoine de Maricourt. Redistribution
+ *	of this file is permitted under the GNU Public License.
+ */
+
+#include <linux/linkage.h>
+
+#define A0	28
+#define A1	32
+#define A2	36
+#define A3	40
+
+	.text
+ENTRY(ext2_LZV1_decompress)
+	pushl %ebp
+	pushl %edi
+	pushl %esi
+	pushl %ebx			# be safe
+	pushl %ecx
+	pushl %edx
+	
+	movl A0(%esp), %esi		# esi = input buffer
+	movl A1(%esp), %edi		# edi = output buffer
+	movl %esi, %ebx
+	addl A2(%esp), %ebx		# ebx = input limit
+	movl %edi, %edx
+	addl A3(%esp), %edx		# edx = output limit
+	xorl %ecx, %ecx
+	xorl %eax, %eax
+	cld
+
+	ALIGN
+
+L10:	cmpl %esi, %ebx			# check for input overflow
+	jna L13
+
+	/*
+	 *	Get the opcode
+	 */
+
+	movb (%esi), %al		# load opcode 
+	testb $7, %al			# test 3 lower bits
+	jne L11
+
+	/*
+	 *	This is a literal
+	 */
+
+	shrb $3, %al			# store its len into ecx
+	incb %al			
+	movb %al, %cl
+	incl %esi
+	rep; movsb
+	cmpl %edx, %edi			# check for output overflow
+	jb L10				# and continue
+	jmp L13
+
+	/*
+	 *	This is a match
+	 */
+
+	ALIGN
+
+L11:	movb %al, %cl
+	andb $7, %cl			# store its len into ecx
+	cmpb $7, %cl	
+	je L12
+
+	/*
+	 *	This is a small match
+	 */
+
+	addl $2, %ecx
+	shll $5, %eax			# store the offset into eax
+	movb 1(%esi), %al
+	notl %eax
+	lea 2(%esi), %ebp		# save next value of esi
+	lea (%edi,%eax), %esi		# load pointer to match
+	cmpl A1(%esp), %esi		# safety check
+	jb L13
+	rep; movsb			# copy the match
+	xorl %eax, %eax
+	movl %ebp, %esi			# restore esi
+	cmpl %edx, %edi			# check for output overflow
+	jb L10				# and continue
+	jmp L13
+
+	/*
+	 *	This is a big match
+	 */
+
+	ALIGN
+
+L12:	movb 1(%esi), %cl
+	addl $9, %ecx
+	shll $5, %eax			# store the offset into eax
+	movb 2(%esi), %al
+	notl %eax
+	lea 3(%esi), %ebp		# save next value of esi
+	lea (%edi,%eax), %esi		# load pointer to match
+	cmpl A1(%esp), %esi		# safety check
+	jb L13
+	rep; movsb			# copy the match
+	xorl %eax, %eax
+	movl %ebp, %esi			# restore esi
+	cmpl %edx, %edi			# check for output overflow
+	jb L10				# and continue
+
+	/*
+	 *	The end.
+	 */
+
+L13:	movl %edi, %eax
+	subl A1(%esp),%eax
+
+	popl %edx
+	popl %ecx
+	popl %ebx
+	popl %esi
+	popl %edi
+	popl %ebp
+	ret
diff -pruN linux-2.6.18.5.org/fs/ext2/lzv1/rlzv2.S linux-2.6.18.5/fs/ext2/lzv1/rlzv2.S
--- linux-2.6.18.5.org/fs/ext2/lzv1/rlzv2.S	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzv1/rlzv2.S	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,178 @@
+/*	
+ *	LZV decompression written in assembly.
+ *
+ *	Written by Antoine de Maricourt (dumesnil@etca.fr)
+ *	                12 Apr 1995
+ *
+ *	NOTE: The use of this algorithm may be restricted by some
+ *	      patent. Please check if this is the case in your 
+ *	      country before using it.
+ *
+ *	Interface :
+ *	-----------
+ *
+ *	   int rLZV2 (char *ib, char *ob, int il, int ol);
+ *
+ *              ib = input buffer (compressed data)
+ *              ob = output buffer (uncompressed data)
+ *		il = number of bytes in the input buffer
+ *		ol = number of bytes to be uncompressed
+ *
+ *	The routine may write more (i.e., 65) bytes than requested
+ *	(and could be modified in order to write exactly the requested
+ *	number if this is really important). However, it will not write
+ *	more than the original number of bytes in the uncompressed
+ *	data. The returned value is the number of bytes written to
+ *	output buffer.
+ *
+ *	The coding method is different from the original one. This
+ *	allows matches that can be more distant. This also reduces
+ *	the maximum match length from 264 to 65.
+ *
+ *	The algorithm and the coding method have been taken from
+ *	file lzv.c originaly written by Hermann Vogt, and whose
+ *	original copyright notice follows:
+ *
+ *	-------
+ *	Copyright (c) 1994 Hermann Vogt. Redistribution of this file is
+ *	permitted under the GNU Public Licence.
+ *
+ *	The method presented here is faster and compresses better
+ *	than lzrw1 and lzrw1-a. I named it lzv for "Lev-Zimpel-Vogt".
+ *	It uses ideas introduced by Ross Williams in his algorithm 
+ *	lzrw1 [R. N. Williams (1991): "An Extremly Fast ZIV-Lempel Data
+ *	Compression Algorithm", Proceedings IEEE Data Compression
+ *	Conference, Snowbird, Utah, 362-371] and by Fiala and Green
+ *	in their algorithm a1 [E. R. Fiala, D. H. Greene (1989): 
+ *	"Data Compression with Finite Windows", Communications of the
+ *	ACM, 4, 490-505]. Because lzv differs strongly from both,
+ *	I hope there will be no	patent problems. The hashing-method 
+ *	has been stolen from Jean-loup Gailly's (patent free) gzip.
+ *	-------
+ *	Copyright (c) 1995 Antoine de Maricourt. Redistribution
+ *	of this file is permitted under the GNU Public License.
+ */
+
+#include <linux/linkage.h>
+
+#define A0	28
+#define A1	32
+#define A2	36
+#define A3	40
+
+	.text
+ENTRY(ext2_LZV2_decompress)
+	pushl %ebp
+	pushl %edi
+	pushl %esi
+	pushl %ebx			# be safe
+	pushl %ecx
+	pushl %edx
+
+	movl A0(%esp), %esi		# esi = input buffer
+	movl A1(%esp), %edi		# edi = output buffer
+	movl %esi, %ebx
+	addl A2(%esp), %ebx		# ebx = input limit
+	movl %edi, %edx
+	addl A3(%esp), %edx		# edx = output limit
+	xorl %ecx, %ecx
+	xorl %eax, %eax
+	cld
+
+	ALIGN
+
+L10:	cmpl %esi, %ebx			# check for input overflow
+	jna L13
+
+	/*
+	 *	Get the opcode
+	 */
+
+	movb (%esi), %al		# load opcode 
+	testb $7, %al			# test 3 lower bits
+	jne L11
+
+	/*
+	 *	This is a literal
+	 */
+
+	shrb $3, %al			# store its len into ecx
+	incb %al			
+	movb %al, %cl
+	incl %esi
+	rep; movsb
+	cmpl %edx, %edi			# check for output overflow
+	jb L10				# and continue
+	jmp L13
+
+	/*
+	 *	This is a match
+	 */
+
+	ALIGN
+
+L11:	movb %al, %cl
+	andb $7, %cl			# store its len into ecx
+	cmpb $7, %cl	
+	je L12
+
+	/*
+	 *	This is a small match
+	 */
+
+	addl $2, %ecx
+	shll $5, %eax
+	movb 1(%esi), %al
+	notl %eax
+	lea 2(%esi), %ebp		# save next value of esi
+	lea (%edi,%eax), %esi		# load pointer to match
+	cmpl A1(%esp), %esi		# safety check
+	jb L13
+	rep; movsb			# copy the match
+	xorl %eax, %eax
+	movl %ebp, %esi			# restore esi
+	cmpl %edx, %edi			# check for output overflow
+	jb L10				# and continue
+	jmp L13
+
+	/*
+	 *	This is a big match or a far match
+	 */
+
+	ALIGN
+
+L12:	movb 1(%esi), %cl
+	shll $7, %eax			# store the offset into eax
+	movb %cl, %al
+	andb $0xfc, %ah
+	andb $0x03, %al
+	orb %al, %ah
+	movb 2(%esi), %al
+	notl %eax
+	shrb $2, %cl			# adjust match len
+	addl $2, %ecx
+	lea 3(%esi), %ebp		# save next value of esi
+	lea (%edi,%eax), %esi		# load pointer to match
+	cmpl A1(%esp), %esi		# safety check
+	jb L13
+	rep; movsb			# copy the match
+	xorl %eax, %eax
+	movl %ebp, %esi			# restore esi
+	cmpl %edx, %edi			# check for output overflow
+	jb L10				# and continue
+
+	/*
+	 *	The end.
+	 */
+
+L13:	movl %edi, %eax
+	subl A1(%esp),%eax
+
+	popl %edx
+	popl %ecx
+	popl %ebx
+	popl %esi
+	popl %edi
+	popl %ebp
+	ret
+
diff -pruN linux-2.6.18.5.org/fs/ext2/lzv1/wlzv1.S linux-2.6.18.5/fs/ext2/lzv1/wlzv1.S
--- linux-2.6.18.5.org/fs/ext2/lzv1/wlzv1.S	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/lzv1/wlzv1.S	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,301 @@
+/*	
+ *	LZV compression written in assembly.
+ *
+ *	Written by Antoine de Maricourt (dumesnil@etca.fr)
+ *	                21 Mar 1995
+ *
+ *	NOTE: The use of this algorithm may be restricted by some
+ *	      patent. Please check if this is the case in your 
+ *	      country before using it.
+ *
+ *	Interface :
+ *	-----------
+ *
+ *	   int wLZV1 (char *ib, char *ob, int il, int ol);
+ *
+ *              ib = input buffer (uncompressed data)
+ *              ob = output buffer (compressed data)
+ *		il = number of bytes in the input buffer
+ *		ol = maximum number of bytes for compressed data
+ *
+ *	The routine may read more (i.e., 3) bytes than the specified
+ *	size for the input buffer. It may also write more (i.e., 264)
+ *	bytes than the specified size for the output buffer before it
+ *	realizes it must stop. The caller should be ready for that.
+ *	Returned value is the size in byte of the compressed data, or
+ *	0 if the data can not be compressed into less bytes than ol.
+ *
+ *	The algorithm and the coding method have been taken from
+ *	file lzv.c originaly written by Hermann Vogt, and whose
+ *	original copyright notice follows:
+ *
+ *	-------
+ *	Copyright (c) 1994 Hermann Vogt. Redistribution of this file is
+ *	permitted under the GNU Public Licence.
+ *
+ *	The method presented here is faster and compresses better
+ *	than lzrw1 and lzrw1-a. I named it lzv for "Lev-Zimpel-Vogt".
+ *	It uses ideas introduced by Ross Williams in his algorithm 
+ *	lzrw1 [R. N. Williams (1991): "An Extremly Fast ZIV-Lempel Data
+ *	Compression Algorithm", Proceedings IEEE Data Compression
+ *	Conference, Snowbird, Utah, 362-371] and by Fiala and Green
+ *	in their algorithm a1 [E. R. Fiala, D. H. Greene (1989): 
+ *	"Data Compression with Finite Windows", Communications of the
+ *	ACM, 4, 490-505]. Because lzv differs strongly from both,
+ *	I hope there will be no	patent problems. The hashing-method 
+ *	has been stolen from Jean-loup Gailly's (patent free) gzip.
+ *	-------
+ *	Copyright (c) 1995 Antoine de Maricourt. Redistribution
+ *	of this file is permitted under the GNU Public License.
+ */
+
+#include <linux/linkage.h>
+#include "lzv1.h"
+
+#if 0
+#define HTAB	SYMBOL_NAME(ext2_lzv1_htab)
+#else
+#define HTAB	40(%esp)
+#endif
+
+#define A0	32
+#define A1	36
+#if 0
+#define A2	40
+#define A3	44
+#else
+#define A2	44
+#define A3	48
+#endif
+#define L0
+
+	/* for i386 */
+#define bswap rol $16,
+
+#define OutputLiteral(_b)	\
+	movb _b, (%edi);	\
+	incl %edi;		\
+	andl $255, %ecx;	\
+	subl %ecx, %esi;	\
+	rep; movsb
+
+#define UpdateHashKey(_b)	\
+	shll $HSHIFT, %ebx;	\
+	xorb _b, %bl
+
+#define UpdateHashTable(_w)	\
+	andl $HMASK, %ebx;	\
+	movw _w, (%ebp,%ebx,2)
+
+#if 0
+.comm HTAB, 4				# hash table
+#endif
+
+	.text
+
+ENTRY(ext2_LZV1_compress)
+	pushl %ebp
+	pushl %edi
+	pushl %esi
+	pushl %ebx			# be safe
+	pushl %ecx
+	pushl %edx
+	subl $4, %esp
+
+	movl A0(%esp), %esi		# esi = input buffer
+	movl A1(%esp), %edi		# edi = output buffer
+	movl A2(%esp), %ecx
+	addl %esi, %ecx
+	movl %ecx, A2(%esp)		# input limit
+	subl $3, %ecx
+	movl %ecx, L0(%esp)
+	addl %edi, A3(%esp)		# output limit
+
+	movl HTAB, %ebp			# get pointer to hash table
+	xorl %ecx, %ecx			# clear literal len
+	xorl %edx, %edx			# clear input position
+	cld
+
+	movzbl (%esi), %ebx		# initialize hash key
+	UpdateHashKey (1(%esi))
+
+	/*
+	 *	eax = 
+	 *	ebx = hash key (bx)
+	 *	ecx = literal len (cl) + tmp data (ch)
+	 *	edx = 
+	 *	esi = source pointer
+	 *	edi = destination pointer
+	 *	ebp = hash table address
+	 */
+
+L10:	cmpl A2(%esp), %esi		# check for input overflow
+	jae L40
+
+	UpdateHashKey (2(%esi))		# update hash key
+
+	/*
+	 *	Test for potential match
+	 */
+
+	andl $HMASK, %ebx
+	movl %edx, %eax
+	movw (%ebp,%ebx,2), %ax		# eax = Hi(ip)|Lo(m_pos)
+	movw %dx, (%ebp,%ebx,2)		# edx = ip
+	cmpl %edx, %eax
+	jb L11
+	cmpl $0x10000, %eax
+	jb L30
+	subl $0x10000, %eax
+
+L11:	subl %edx, %eax			# eax = m_pos - ip [= -m_off - 1 < 0]
+	cmpl $-MOFF, %eax
+	jb L30
+	cmpl L0(%esp), %esi
+	jae L30
+
+	movb (%esi), %ch		# compare first byte
+	cmpb %ch, (%esi,%eax)
+	jne L30
+
+	movb 1(%esi), %ch		# compare second byte
+	cmpb %ch, 1(%esi,%eax)
+	jne L30
+
+	movb 2(%esi), %ch		# compare third byte
+	cmpb %ch, 2(%esi,%eax)
+	jne L30
+
+	/*
+	 *	Flush pending literal on output first,
+	 *	so that we do not need anymore its len.
+	 */
+
+	testb %cl, %cl
+	je L12
+	movb %cl, %ch
+	decb %ch
+	shlb $3, %ch
+	OutputLiteral (%ch)
+
+	/*
+	 *	Find length of current match
+	 */
+
+L12:	lea 2(%esi), %esi
+
+	movl A2(%esp), %ecx		# find maximum match len
+	subl %esi, %ecx
+	cmpl $MML, %ecx
+	jbe L13
+	movl $MML, %ecx
+
+L13:	bswap %ebx			# save hash key
+	movw %cx, %bx			# save max match len
+
+	xchg %eax, %edi			# save edi
+	lea (%esi,%edi), %edi		# get pointer to potential match
+
+	repe; cmpsb
+	jz L14
+	incl %ecx
+	decl %esi
+	decl %edi
+
+L14:	subw %cx, %bx			# match len
+
+	xchg %eax, %edi			# restore edi
+	subl %esi, %eax			# restore the offset
+
+	/*
+	 *	Output offset and length of match.
+	 */
+
+	notl %eax
+	movzwl %bx, %ecx
+
+	cmpw $7, %bx
+	jae L17
+
+	shlb $3, %ah			# output 2 bytes (len < 7)
+	orb %bl, %ah
+	movb %ah, (%edi)
+	jmp L18
+
+L17:	shlb $3, %ah			# output 3 bytes (len >= 7)
+	orb $7, %ah
+	movb %ah, (%edi)
+	subw $7, %bx
+	incl %edi
+	movb %bl, (%edi)
+
+L18:	movb %al, 1(%edi)
+	lea 2(%edi), %edi
+
+	/*
+	 *	Update hash table
+	 */
+
+	bswap %ebx
+	subl %ecx, %esi
+
+L16:	incl %edx
+	incl %esi
+	UpdateHashKey ((%esi))
+	UpdateHashTable (%dx)
+	decl %ecx
+	jne L16
+
+	incl %edx
+	UpdateHashKey (1(%esi))
+	UpdateHashTable (%dx)
+	incl %edx
+
+	xorl %ecx, %ecx			# reset literal len
+
+	cmpl A3(%esp), %edi		# check for output overflow
+	jbe L10				# and continue
+	jmp L40
+	
+	/*
+	 *	Not a match.
+	 */
+
+	ALIGN
+
+L30:	incl %edx; incl %esi
+	incb %cl			# update literal len
+
+	cmpb $MLL, %cl			# flush pending literal
+	jb L32				# if needed
+	OutputLiteral ($0xf8)
+	xorb %cl, %cl
+
+L32:	cmpl A3(%esp), %edi		# check for output overflow
+	jbe L10				# and continue
+	
+	/*
+	 *	The end.
+	 */
+
+L40:	testb %cl, %cl			# flush pending literal
+	je L41				# if needed
+	movb %cl, %ch
+	decb %ch
+	shlb $3, %ch
+	OutputLiteral (%ch)
+
+L41:	xorl %eax, %eax			# return 0 if output overflow
+	cmpl A3(%esp), %edi
+	jae L42 
+	movl %edi, %eax
+	subl A1(%esp),%eax
+
+L42:	addl $4, %esp
+	popl %edx
+	popl %ecx	
+	popl %ebx	
+	popl %esi
+	popl %edi
+	popl %ebp
+	ret
diff -pruN linux-2.6.18.5.org/fs/ext2/namei.c linux-2.6.18.5/fs/ext2/namei.c
--- linux-2.6.18.5.org/fs/ext2/namei.c	2006-12-04 13:31:58.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/namei.c	2007-04-01 21:18:38.000000000 -0700
@@ -30,10 +30,19 @@
  *        David S. Miller (davem@caip.rutgers.edu), 1995
  */
 
+/*
+ *  Copyright (C) 2001 Alcatel Business Systems - R&D Illkirch
+ *  Added by Pierre Peiffer (pierre.peiffer@sxb.bsf.alcatel.fr) (2001-08-09)
+ *  (Copied from pjm e2compr-0.4.39-patch-2.2.18 patch)
+ *    For e2compress: GZ_HACK: Do not compress file with .gz or .bz2
+ *    extension (option of kernel config)
+ */
+
 #include <linux/pagemap.h>
 #include "ext2.h"
 #include "xattr.h"
 #include "acl.h"
+#include "debug.h"
 #include "xip.h"
 
 static inline int ext2_add_nondir(struct dentry *dentry, struct inode *inode)
@@ -108,6 +117,42 @@ static int ext2_create (struct inode * d
 	struct inode * inode = ext2_new_inode (dir, mode);
 	int err = PTR_ERR(inode);
 	if (!IS_ERR(inode)) {
+#ifdef CONFIG_GZ_HACK
+	  /* pjm 1997-12-05: Quick hack not to compress files whose
+	     name ends in `gz' or `.bz2'.  There are much better ways of achieving
+	     this sort of thing, but this will do for now. */
+	  if ((dentry->d_name.len >= 4)
+	      && (((dentry->d_name.name[dentry->d_name.len - 2] == 'g')
+		   && (dentry->d_name.name[dentry->d_name.len - 1] == 'z')
+		   && ((dentry->d_name.name[dentry->d_name.len - 3] == '.')
+		       || (dentry->d_name.name[dentry->d_name.len - 4] == '.')))
+		  || ((dentry->d_name.name[dentry->d_name.len - 3] == 'b')
+		      && (dentry->d_name.name[dentry->d_name.len - 2] == 'z')
+		      && (dentry->d_name.name[dentry->d_name.len - 1] == '2')
+		      && (dentry->d_name.name[dentry->d_name.len - 4] == '.')
+		      && (dentry->d_name.len >= 5)))
+#if 0
+	  /* TLL 2007-3-25: Quick hack not to compress .Z files */
+		  || ((dentry->d_name.len >= 3)
+		   && (((dentry->d_name.name[dentry->d_name.len - 1] | ' ' == 'z')
+		   && ((dentry->d_name.name[dentry->d_name.len - 2] == '.'))))
+#endif
+		   )
+	    EXT2_I(inode)->i_flags &= ~EXT2_COMPR_FL;
+	  /* If you want to add other rules like this:
+	     - Don't forget to test dentry->d_name.len;
+	     - See ext2_new_inode() in ialloc.c for some other things
+	     that can be changed here.  The compression method would be:
+	     inode->u.ext2_i.i_compr_method = EXT2_blah_ID;
+	     The cluster size requires changing 2 fields, where the
+	     *size fields are 1 << *bits. */
+	  
+	  /* (In a perfect world, we'd choose when to compress based on
+	     amount of free disk space and CPU utilisation, and we'd
+	     choose compression parameters based on much more than the
+	     name, and we wouldn't have to recompile the kernel just to
+	     change the setup.) */
+#endif
 		inode->i_op = &ext2_file_inode_operations;
 		if (ext2_use_xip(inode->i_sb)) {
 			inode->i_mapping->a_ops = &ext2_aops_xip;
diff -pruN linux-2.6.18.5.org/fs/ext2/new-method-howto.c linux-2.6.18.5/fs/ext2/new-method-howto.c
--- linux-2.6.18.5.org/fs/ext2/new-method-howto.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/new-method-howto.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,249 @@
+/* 
+   (This document isn't finished, but it does include the most 
+   important information.)
+
+   This file hopes to give some documentation on how to add a new 
+   compression method to e2compr.
+
+   LZO looks like a good method to add, for example.  (Good for files
+   that are usually accessed when CPU already fully loaded.)
+
+
+   Requirements
+   ============
+
+   You need to know how much memory is needed.  (You get a pointer to
+   a large area of memory before you start to compress or decompress.
+   You need to know in advance -- i.e. at compile time -- how big you
+   need this area to be.)
+
+   You need to check that your routines don't use recursion or
+   anything else likely to use large amounts of stack space.
+
+   For future compatibility, you should try to make the routines re-entrant.
+   E.g. for all variables that are either at file scope or are declared static,
+   expect that they can be clobbered whenever you call schedule().
+   One way of dealing with this would be to move the variables to the heap
+   (remembering to increase your heap size requirement declaration);
+   another is to store local (i.e. stack space) copies of such vars.
+   At the moment, the most likely sort of reentrancy requirement is that
+   compression and decompression must be able to happen simultaneously.
+   So the most important thing is vars that are common to your compression
+   and decompression routines.
+
+   Write wrapper functions that conform to the interface that the
+   kernel expects.  Namely, you're given a pointers to two 32KB areas
+   (one for the input data, one for the output data; or put another
+   way, one for compressed data and one for uncompressed data), a
+   pointer to a large area of memory for your own use, you're told how
+   long the input data is, how long the output data is expected to be.
+   You must never write past the end of the 32KB output buffer, or
+   write to the input buffer.
+   
+   (If, for performance reasons, you think you may overrun the output
+   buffer by no more than a known amount: the heap and buffers are
+   next to each other, so you may be able to organise that overruns go
+   into the heap, and not use the first n bytes of the heap for
+   anything else.)
+   
+   Return zero if there is an error, or if the data can't be compressed; 
+   otherwise return the length of the output data.
+
+   More details follow.
+
+
+   Memory allocation
+   -----------------
+
+   There are several sources of memory:
+
+   + A large area (about 256KB, or however much hou say you need) of 
+     contiguous memory is made available to you.  This area is generally 
+     known as the "heap".
+     
+     You are encouraged to use the heap for most of your memory requirements, 
+     as it remains allocated whether or not you use it.  
+     
+     Data put here is not retained between invocations of your 
+     functions.
+     
+   + Stack space.  (This is what C uses for its automatic
+     (i.e. non-static) variables.)  Do not use much of this!  In
+     kernel space, there is only a page or two (I think that in 2.1
+     and later, it\'s two pages less ~1.5KB; and one page or less in
+     2.0 and earlier; a page is 4096 bytes on most machines) of stack
+     space per process.  This effectively rules out recursion in any
+     of your routines.  It also means that you can't have large arrays
+     as automatic (non-static) variables.
+
+
+
+   Interface:
+
+   There are three entry functions: a simple initialisation function,
+   a decompression function, and a compression function.
+
+   The initialisation function is to organise an area of memory for
+   you.
+
+     int ext2_iFOO (void *mem_start, int action);
+
+   The first time it's called is by fs/ext2/super.c, the first time an
+   ext2 filesystem is mounted (which is usually at boot time).
+   super.c wants to know how much memory (measured in bytes) you'll
+   need in this area.  (gzip requires 200KB; I think one of the others
+   requires 256KB.)  super.c passes NULL for mem_start.
+
+   Subsequently it's called by fs/ext2/compress.c (search for
+   `.init'), which passes you a pointer to an area of memory at least
+   as big as you told super.c that you need.  Assign this value to a
+   static variable (at file scope) so that your compression and
+   decompression routines know where the heap is.
+   
+   The 'int action' parameter tells your function what's going on, i.e.
+   if it has to initialize algorithm for compression or decompression -
+   this is important for simultaneous compression/decompression.
+
+   Changes to rest of e2compr code:
+
+   + Decide on a short identifier.  In these instructions I've used `foo'.
+     Check that your chosen identifier isn't used elsewhere by doing:
+
+       find /usr/src/linux/. -name '*.[ch]'|xargs grep -i foo
+
+     or some such.
+
+   + If you only have one new file to add to the source tree, call it
+     `fs/ext2/foo.c'.  Add foo.c to the definition of COMPRESS_STUFF
+     in fs/ext2/Makefile.
+
+     Otherwise `mkdir fs/ext2/foo', and put a Makefile in it.  If you
+     have .S files, lzrw/Makefile is a good starting point, otherwise
+     gzip/Makefile.  Add foo to the definition of SUB_DIRS in
+     fs/ext2/Makefile.
+
+   + Edit fs/Config.in.  Search for `LZV' (there should be two such
+     lines), duplicate each line, and change LZV to FOO.
+
+   + You will need three entry functions.  I suggest you call them
+     ext2_iFOO(), ext2_rFOO() and ext2_wFOO().
+     
+     The `i' function is the initialisation function.  It is called by
+     fs/ext2/super.c (which wants to know how much memory to allocate
+     for you, and which passes NULL as the parameter) and it is called
+     before your de/compression routines are called, to give you the
+     starting address of the heap.
+
+   * A good way of testing your routines is to link them into
+     e2compress before into the kernel, just so that you get the
+     benefit of memory- and crash-protection and of not having to
+     reboot the machine to change your code.  Change the Makefile in
+     src/e2compress, edit e2compress.c, search for `gzip' (ignoring
+     case), follow the pattern.
+
+   + Edit fs/ext2/compress.c.  Search for `GZIP'.  Follow the pattern
+     for the the other algorithms there.  (I.e. #undef FOO, #define
+     FOO to 1 or 0 according to whether or not CONFIG_EXT2_HAVE_FOO is
+     defined.)
+
+     Add an entry to ext2_algorithm_table[].  (Or rather, change one
+     of the entries with `0' in the second column.)  Again, follow the
+     existing pattern.
+
+   + Edit include/linux/ext2_fs.h and include/linux/ext2_fs_c.h.
+     Search for `GZIP' and follow the pattern.
+
+
+
+   */
+
+#include <linux/types.h>
+
+/* Your code needs to be able to be compiled into e2compress(1), so make sure
+   that kernel-specific things are wrapped in `#ifdef __KERNEL__'. */
+#ifdef __KERNEL__
+# include <linux/linkage.h>
+# include <asm/param.h>
+/* If you want your code to be able to be compiled as module add those
+ * two includes
+ */
+# include <linux/ext2_fs_c.h>
+# include <linux/module.h>
+asmlinkage void schedule(void);
+static unsigned long next_brk = 0;
+extern unsigned long volatile jiffies;
+#endif
+
+size_t ext2_iFOO (void *ptr, int action)
+{
+	switch (action) {
+		case EXT2_ALG_INIT_COMPRESS:
+			return 0;
+		case EXT2_ALG_INIT_DECOMPRESS:
+			return 0;
+		default:
+			return 0;
+	}
+}
+
+size_t ext2_rFOO (__u8 *ibuf, __u8 *obuf, size_t ilen, size_t olen, int xarg)
+{
+  /* I've never noticed decompression to be a speed problem, so
+     I don't bother calling schedule() from here. */
+
+  /* If you want your code to be able to be compiled as module add this code */
+  MOD_INC_USE_COUNT;
+  /* do decompression */
+  /* If you want your code to be able to be compiled as module add this code */
+  MOD_DEC_USE_COUNT;
+  return 0;
+}
+
+size_t ext2_wFOO (__u8 *ibuf, __u8 *obuf, size_t ilen, size_t olen, int xarg)
+{
+  /* If you want your code to be able to be compiled as module add this code */
+  MOD_INC_USE_COUNT;
+  for(;;)
+    {
+      /* Preferably put this bit in a place that's only executed
+         rarely rather than once per byte processed... */
+#ifdef __KERNEL__
+      /* Time slice of a fifth of a second. */
+      if (jiffies > next_brk)
+	{
+	  schedule();
+	  next_brk = jiffies + HZ / 5;
+	}
+#endif
+    }
+  /* If you want your code to be able to be compiled as module add this code */
+  MOD_DEC_USE_COUNT;
+  return 0; /* never reached */
+}
+
+/* If you want your code to be able to be compiled as module add this code */
+#ifdef MODULE
+
+int init_module(void)
+{
+	struct ext2_algorithm foo_alg;
+
+	foo_alg.name = NULL;
+	foo_alg.avail = 1;
+	foo_alg.init = ext2_iFOO;
+	foo_alg.compress = ext2_wFOO;
+	foo_alg.decompress = ext2_rFOO;
+
+	return ext2_register_compression_module(EXT2_FOO_ALG,
+			0, /* AMOUNT OF REQUIRED MEMORY FOR COMPRESSION */
+			0, /* AMOUNT OF REQUIRED MEMORY FOR DECOMPRESSION */
+			&foo_alg);
+}
+
+void cleanup_module(void)
+{
+	ext2_unregister_compression_module(EXT2_FOO_ALG);
+}
+
+#endif
+
diff -pruN linux-2.6.18.5.org/fs/ext2/none.c linux-2.6.18.5/fs/ext2/none.c
--- linux-2.6.18.5.org/fs/ext2/none.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/none.c	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,5 @@
+#include <linux/types.h>
+
+size_t ext2_iNONE (int action) { return 0; }
+size_t ext2_rNONE (__u8 *ibuf, __u8 *obuf, void *wa, size_t ilen, size_t olen, int xarg) { return 0; }
+size_t ext2_wNONE (__u8 *ibuf, __u8 *obuf, void *wa, size_t ilen, size_t olen, int xarg) { return 0; }
diff -pruN linux-2.6.18.5.org/fs/ext2/super.c linux-2.6.18.5/fs/ext2/super.c
--- linux-2.6.18.5.org/fs/ext2/super.c	2006-12-04 13:31:58.000000000 -0800
+++ linux-2.6.18.5/fs/ext2/super.c	2007-03-23 02:23:01.000000000 -0700
@@ -16,6 +16,19 @@
  *        David S. Miller (davem@caip.rutgers.edu), 1995
  */
 
+/*
+ *  Copyright (C) 1995  Antoine Dumesnil de Maricourt (dumesnil@etca.fr)
+ *    (transparent compression code)
+ */
+
+/*
+ *  Copyright (C) 2001 Alcatel Business Systems - R&D Illkirch
+ *    (transparent compression code)
+ *  Pierre Peiffer (pierre.peiffer@sxb.bsf.alcatel.fr),
+ *  Denis Richard (denis.richard@sxb.bsf.alcatel.fr)
+ *  Adapted from patch e2compr-0.4.39-patch-2.2.18 .
+ */
+
 #include <linux/module.h>
 #include <linux/string.h>
 #include <linux/fs.h>
@@ -30,7 +43,8 @@
 #include <linux/seq_file.h>
 #include <linux/mount.h>
 #include <asm/uaccess.h>
-#include "ext2.h"
+#include <linux/vmalloc.h>
+#include <linux/ext2_fs_c.h>
 #include "xattr.h"
 #include "acl.h"
 #include "xip.h"
@@ -106,6 +120,14 @@ void ext2_update_dynamic_rev(struct supe
 	 */
 }
 
+#ifdef CONFIG_EXT2_COMPRESS
+/*
+ *    A reference count for the working area used by
+ *    compression code.
+ */
+static int ext2_wa_ref = 0;
+#endif
+
 static void ext2_put_super (struct super_block * sb)
 {
 	int db_count;
@@ -132,6 +154,21 @@ static void ext2_put_super (struct super
 	sb->s_fs_info = NULL;
 	kfree(sbi);
 
+#ifdef CONFIG_EXT2_COMPRESS
+	if (--ext2_wa_ref == 0) {
+		if (ext2_rd_wa != NULL) {
+			vfree (ext2_rd_wa);
+			ext2_rd_wa = NULL;
+		}
+#ifdef CONFIG_EXT2_SEPARATE_WORK_AREAS
+		if (ext2_wr_wa != NULL) {
+			vfree (ext2_wr_wa);
+			ext2_wr_wa = NULL;
+		}
+# endif
+	}
+#endif
+
 	return;
 }
 
@@ -324,7 +361,11 @@ enum {
 	Opt_resgid, Opt_resuid, Opt_sb, Opt_err_cont, Opt_err_panic,
 	Opt_err_ro, Opt_nouid32, Opt_nocheck, Opt_debug,
 	Opt_oldalloc, Opt_orlov, Opt_nobh, Opt_user_xattr, Opt_nouser_xattr,
-	Opt_acl, Opt_noacl, Opt_xip, Opt_ignore, Opt_err, Opt_quota,
+	Opt_acl, Opt_noacl,
+#ifdef CONFIG_EXT2_COMPRESS
+	Opt_force_compat,
+#endif
+	Opt_xip, Opt_ignore, Opt_err, Opt_quota,
 	Opt_usrquota, Opt_grpquota
 };
 
@@ -357,6 +398,9 @@ static match_table_t tokens = {
 	{Opt_ignore, "noquota"},
 	{Opt_quota, "quota"},
 	{Opt_usrquota, "usrquota"},
+#ifdef CONFIG_EXT2_COMPRESS
+	{Opt_force_compat, "force-compat"},
+#endif
 	{Opt_err, NULL}
 };
 
@@ -483,7 +527,11 @@ static int parse_options (char * options
 
 			break;
 #endif
-
+#ifdef CONFIG_EXT2_COMPRESS
+		case Opt_force_compat:
+			set_opt(sbi->s_mount_opt, FORCE_COMPAT);
+			break;
+#endif
 		case Opt_ignore:
 			break;
 		default:
@@ -507,7 +555,11 @@ static int ext2_setup_super (struct supe
 		res = MS_RDONLY;
 	}
 	if (read_only)
+#ifdef CONFIG_EXT2_COMPRESS
+		goto readonly;
+#else /* CONFIG_EXT2_COMPRESS */
 		return res;
+#endif /* CONFIG_EXT2_COMPRESS */
 	if (!(sbi->s_mount_state & EXT2_VALID_FS))
 		printk ("EXT2-fs warning: mounting unchecked fs, "
 			"running e2fsck is recommended\n");
@@ -536,6 +588,99 @@ static int ext2_setup_super (struct supe
 			EXT2_BLOCKS_PER_GROUP(sb),
 			EXT2_INODES_PER_GROUP(sb),
 			sbi->s_mount_opt);
+
+#ifdef CONFIG_EXT2_COMPRESS
+	/* Try to allocate the working area for compression. */
+	/* (Maybe this should be moved to compress.c.) */
+# include "e2compr_version.h"
+#ifdef CONFIG_EXT2_COMPRESS
+ readonly:
+#endif /* CONFIG_EXT2_COMPRESS */
+# ifdef CONFIG_EXT2_SEPARATE_WORK_AREAS
+	if ((ext2_rd_wa == NULL) || (ext2_wr_wa == NULL)) {
+		size_t rsize = 0;
+		size_t wsize = 0;
+		int	 i;
+
+		printk (KERN_INFO "e2compr " E2COMPR_VERSION " loading.\n");
+		for (i = 0; i < EXT2_N_ALGORITHMS; i++) {
+			if (ext2_algorithm_table[i].avail) {
+				size_t hsize;
+
+				hsize = ext2_algorithm_table[i].init(
+						EXT2_ALG_INIT_COMPRESS);
+				if (wsize < hsize)
+					wsize = hsize;
+				hsize = ext2_algorithm_table[i].init(
+						EXT2_ALG_INIT_DECOMPRESS);
+				if (rsize < hsize)
+					rsize = hsize;
+			}
+		}
+		rsize += 2 * EXT2_MAX_CLUSTER_BYTES;
+		ext2_rd_wa = vmalloc (rsize);
+		if (ext2_rd_wa == NULL)
+			printk ("EXT2-fs: can't allocate working area; "
+				"compression turned off.\n");
+		else {
+			ext2_rd_wa_size = rsize - 2 * EXT2_MAX_CLUSTER_BYTES;
+#  ifdef EXT2_COMPR_REPORT_WA
+			printk (KERN_INFO "EXT2-fs: rd_wa=%p--%p (%d)\n",
+				ext2_rd_wa, (char *)ext2_rd_wa + rsize, rsize);
+#  endif
+		}
+		wsize += 2 * EXT2_MAX_CLUSTER_BYTES;
+		ext2_wr_wa = vmalloc (wsize);
+		if (ext2_wr_wa == NULL)
+			printk ("EXT2-fs: can't allocate working area; "
+				"compression turned off.\n");
+		else {
+			ext2_wr_wa_size = wsize - 2 * EXT2_MAX_CLUSTER_BYTES;
+#  ifdef EXT2_COMPR_REPORT_WA
+			printk (KERN_INFO "EXT2-fs: wr_wa=%p--%p (%d)\n",
+				ext2_wr_wa, (char *)ext2_wr_wa + wsize, wsize);
+#  endif
+		}
+	}
+# else /* ! CONFIG_EXT2_SEPARATE_WORK_AREAS */
+	if (ext2_rd_wa == NULL) {
+		size_t max_hsize = 0;
+		int	 i;
+
+		printk (KERN_INFO "e2compr " E2COMPR_VERSION " loading.\n");
+		for (i = 0; i < EXT2_N_ALGORITHMS; i++) {
+			if (ext2_algorithm_table[i].avail) {
+				size_t hsize;
+
+				hsize = ext2_algorithm_table[i].init(
+						EXT2_ALG_INIT_COMPRESS);
+				if (max_hsize < hsize)
+					max_hsize = hsize;
+				hsize = ext2_algorithm_table[i].init(
+						EXT2_ALG_INIT_DECOMPRESS);
+				if (max_hsize < hsize)
+					max_hsize = hsize;
+			}
+		}
+		max_hsize += 2 * EXT2_MAX_CLUSTER_BYTES;
+		ext2_rd_wa = vmalloc (max_hsize);
+		if (ext2_rd_wa == NULL)
+			printk ("EXT2-fs: can't allocate working area; "
+				"compression turned off.\n");
+		else {
+			ext2_rd_wa_size = (max_hsize
+					   - 2 * EXT2_MAX_CLUSTER_BYTES);
+#  ifdef EXT2_COMPR_REPORT
+			printk (KERN_INFO "EXT2-fs: wa=%p--%p (%d)\n",
+				ext2_rd_wa, (char *)ext2_rd_wa + max_hsize,
+				max_hsize);
+#  endif
+		}
+	}
+# endif
+	ext2_wa_ref++;
+#endif
+
 	return res;
 }
 
@@ -737,18 +882,107 @@ static int ext2_fill_super(struct super_
 	 */
 	features = EXT2_HAS_INCOMPAT_FEATURE(sb, ~EXT2_FEATURE_INCOMPAT_SUPP);
 	if (features) {
-		printk("EXT2-fs: %s: couldn't mount because of "
-		       "unsupported optional features (%x).\n",
-		       sb->s_id, le32_to_cpu(features));
-		goto failed_mount;
+		if (test_opt(sb, FORCE_COMPAT)) {
+			printk("EXT2-fs: %s: has incompatibilities %#x "
+			    "but ignoring as you request.\n",
+			    sb->s_id, (le32_to_cpu(es->s_feature_incompat)
+			    & ~EXT2_FEATURE_INCOMPAT_SUPP));
+			/*
+			 * todo: Maybe convert the number to a list of
+			 * strings, e.g. "compression".
+			 */
+		} else {
+			printk("EXT2-fs: %s: couldn't mount because of "
+			    "unsupported optional features (%x).\n", sb->s_id,
+			    le32_to_cpu(features));
+			goto failed_mount;
+		}
 	}
+#ifdef CONFIG_EXT2_COMPRESS
+	/* Check that required algorithms are available. */
+	/* todo: Provide a mount option to override this. */
+	/*
+	 * Philosophical bug: we assume that an algorithm's
+	 * module is available if and only if this kernel was
+	 * compiled with that algorithm as a module.  This is
+	 * untrue, but it is unclear what the right thing to
+	 * do is.
+	 */
+	j = 0; /* error flag */
+	if ((es->s_feature_incompat
+	    & cpu_to_le32(EXT2_FEATURE_INCOMPAT_COMPRESSION))
+	    && (es->s_algorithm_usage_bitmap
+	    & ~cpu_to_le32(EXT2_ALGORITHMS_SUPP))) {
+		/*
+		 * The filesystem employs an algorithm not
+		 * supported by this filesystem.  Issue warning or
+		 * error.
+		 */
+		for (i = 0; i < 32; i++) {
+			if (!(es->s_algorithm_usage_bitmap
+			    & cpu_to_le32(1 << i))
+			    || ((EXT2_ALGORITHMS_SUPP
+			    & (1 << i))))
+				continue;
+		/*
+		 * TODO: Can't this message be moved outside
+		 * of the for loop?
+		 */
+		if (!j) {
+			if (test_opt(sb, FORCE_COMPAT))
+				printk(KERN_WARNING
+				    "EXT2-fs: %s: "
+				    "uses unsupported "
+				    "compression algorithms",
+				    sb->s_id);
+			else
+				printk("EXT2-fs: %s: couldn't mount "
+				    "because of unsupported "
+				    "compression algorithms",
+				    sb->s_id);
+			j = 1;
+		}
+		if (i < EXT2_N_ALGORITHMS)
+			printk(" %s", ext2_algorithm_table[i].name);
+		else
+			printk(" %u", i);
+		}
+	}
+	if (j) {
+		if (test_opt(sb, FORCE_COMPAT))
+			printk(" but ignoring as you request.\n");
+		else {
+			printk(".\n");
+			goto failed_mount;
+		}
+	}
+#endif /* CONFIG_EXT2_COMPRESS */
 	if (!(sb->s_flags & MS_RDONLY) &&
 	    (features = EXT2_HAS_RO_COMPAT_FEATURE(sb, ~EXT2_FEATURE_RO_COMPAT_SUPP))){
-		printk("EXT2-fs: %s: couldn't mount RDWR because of "
-		       "unsupported optional features (%x).\n",
-		       sb->s_id, le32_to_cpu(features));
-		goto failed_mount;
+		if (test_opt(sb, FORCE_COMPAT)) {
+			printk(KERN_WARNING
+			    "EXT2-fs: %s: contains unsupported optional "
+			    "features %#x, but ignoring as you request.\n",
+			    sb->s_id,
+			    (le32_to_cpu(es->s_feature_ro_compat)
+			    & ~EXT2_FEATURE_RO_COMPAT_SUPP));
+			/*
+			 * todo: Maybe print a list of strings
+			 * (e.g. "sparse_super") rather than
+			 * the bitmap.
+			 */
+		} else {
+			printk("EXT2-fs: %s: couldn't mount RDWR because of "
+			    "unsupported optional features (%x).\n",
+			    sb->s_id, le32_to_cpu(features));
+			goto failed_mount;
+		}
 	}
+#if 0
+	/* Standard ext2 doesn't set s_blocksize_bits!  Is this a bug? */
+	sb->s_blocksize_bits = BLOCK_SIZE_BITS +
+	    le32_to_cpu(sbi->s_es->s_log_block_size);
+#endif
 
 	blocksize = BLOCK_SIZE << le32_to_cpu(sbi->s_es->s_log_block_size);
 
diff -pruN linux-2.6.18.5.org/include/linux/ext2_fs.h linux-2.6.18.5/include/linux/ext2_fs.h
--- linux-2.6.18.5.org/include/linux/ext2_fs.h	2006-12-04 13:34:16.000000000 -0800
+++ linux-2.6.18.5/include/linux/ext2_fs.h	2007-02-04 09:37:35.000000000 -0800
@@ -17,12 +17,22 @@
 #define _LINUX_EXT2_FS_H
 
 #include <linux/types.h>
+#ifdef __KERNEL__
+# include <linux/config.h>  /* For e2compr stuff. */
+#endif
 
 /*
  * The second extended filesystem constants/structures
  */
 
 /*
+ *  Copyright (C) 2001 Alcatel Business Systems - R&D Illkirch
+ *  Added by Pierre Peiffer (pierre.peiffer@sxb.bsf.alcatel.fr) (2001-08-09)
+ *  (Copied from pjm e2compr-0.4.39-patch-2.2.18 patch)
+ *      e2compress stuff.
+ */
+
+/*
  * Define EXT2FS_DEBUG to produce debug messages
  */
 #undef EXT2FS_DEBUG
@@ -59,6 +69,13 @@
 #define EXT2_ROOT_INO		 2	/* Root inode */
 #define EXT2_BOOT_LOADER_INO	 5	/* Boot loader inode */
 #define EXT2_UNDEL_DIR_INO	 6	/* Undelete directory inode */
+#if 0 /* Neither of these are used yet.  They might never be. */
+/* There are two copies of the ext2_compress_ino because
+   one is a backup.  The data is stored in compressed form,
+   and we don't want to keep all our eggs in the one basket. */
+# define EXT2_COMPRESS1_INO	 7	/* Extra inode info (copy 1). */
+# define EXT2_COMPRESS2_INO	 8	/* Extra inode info (copy 2). */
+#endif
 
 /* First non-reserved inode for old ext2 filesystems */
 #define EXT2_GOOD_OLD_FIRST_INO	11
@@ -89,6 +106,10 @@ static inline struct ext2_sb_info *EXT2_
 /*
  * Macro-instructions used to manage several block sizes
  */
+#define EXT2_GRAIN_SIZE 		1024
+/* Minimum allocation unit.  This is used in fs/ext2/compress.c to
+   check compr_len validity wrt (uncompressed) len.  This definition
+   will probably need to be changed when fragments are implemented. */
 #define EXT2_MIN_BLOCK_SIZE		1024
 #define	EXT2_MAX_BLOCK_SIZE		4096
 #define EXT2_MIN_BLOCK_LOG_SIZE		  10
@@ -171,6 +192,8 @@ struct ext2_group_desc
 /*
  * Inode flags
  */
+/* Note: if you overload any of these flags then you may need to change
+   code in ext2_read_inode() and ext2_update_inode(). */
 #define	EXT2_SECRM_FL			0x00000001 /* Secure deletion */
 #define	EXT2_UNRM_FL			0x00000002 /* Undelete */
 #define	EXT2_COMPR_FL			0x00000004 /* Compress file */
@@ -180,26 +203,31 @@ struct ext2_group_desc
 #define EXT2_NODUMP_FL			0x00000040 /* do not dump file */
 #define EXT2_NOATIME_FL			0x00000080 /* do not update atime */
 /* Reserved for compression usage... */
-#define EXT2_DIRTY_FL			0x00000100
+#define EXT2_DIRTY_FL			0x00000100 /* Needs compressing; see Readme.e2compr */
 #define EXT2_COMPRBLK_FL		0x00000200 /* One or more compressed clusters */
-#define EXT2_NOCOMP_FL			0x00000400 /* Don't compress */
+#define EXT2_NOCOMPR_FL			0x00000400 /* Access raw data */
 #define EXT2_ECOMPR_FL			0x00000800 /* Compression error */
 /* End compression flags --- maybe not all used */	
 #define EXT2_BTREE_FL			0x00001000 /* btree format dir */
 #define EXT2_INDEX_FL			0x00001000 /* hash-indexed directory */
 #define EXT2_IMAGIC_FL			0x00002000 /* AFS directory */
-#define EXT2_JOURNAL_DATA_FL		0x00004000 /* Reserved for ext3 */
+#define EXT2_JOURNAL_DATA_FL	0x00004000 /* Reserved for ext3 */
 #define EXT2_NOTAIL_FL			0x00008000 /* file tail should not be merged */
 #define EXT2_DIRSYNC_FL			0x00010000 /* dirsync behaviour (directories only) */
 #define EXT2_TOPDIR_FL			0x00020000 /* Top of directory hierarchies*/
 #define EXT2_RESERVED_FL		0x80000000 /* reserved for ext2 lib */
 
 #define EXT2_FL_USER_VISIBLE		0x0003DFFF /* User visible flags */
-#define EXT2_FL_USER_MODIFIABLE		0x000380FF /* User modifiable flags */
+#ifdef CONFIG_EXT2_COMPRESS
+# define EXT2_FL_USER_MODIFIABLE	0x00038CFF /* User modifiable flags */
+#else
+# define EXT2_FL_USER_MODIFIABLE	0x000380FF /* User modifiable flags */
+#endif
 
 /*
  * ioctl commands
  */
+#include <linux/ioctl.h>
 #define	EXT2_IOC_GETFLAGS		_IOR('f', 1, long)
 #define	EXT2_IOC_SETFLAGS		_IOW('f', 2, long)
 #define	EXT2_IOC_GETVERSION		_IOR('v', 1, long)
@@ -310,6 +338,7 @@ struct ext2_inode {
 #define EXT2_MOUNT_MINIX_DF		0x000080  /* Mimics the Minix statfs */
 #define EXT2_MOUNT_NOBH			0x000100  /* No buffer_heads */
 #define EXT2_MOUNT_NO_UID32		0x000200  /* Disable 32-bit UIDs */
+#define EXT2_MOUNT_FORCE_COMPAT		0x0400  /* Mount despite incompatibilities */
 #define EXT2_MOUNT_XATTR_USER		0x004000  /* Extended user attributes */
 #define EXT2_MOUNT_POSIX_ACL		0x008000  /* POSIX Access Control Lists */
 #define EXT2_MOUNT_XIP			0x010000  /* Execute in place */
@@ -474,8 +503,14 @@ struct ext2_super_block {
 #define EXT2_FEATURE_INCOMPAT_ANY		0xffffffff
 
 #define EXT2_FEATURE_COMPAT_SUPP	EXT2_FEATURE_COMPAT_EXT_ATTR
-#define EXT2_FEATURE_INCOMPAT_SUPP	(EXT2_FEATURE_INCOMPAT_FILETYPE| \
+#ifdef CONFIG_EXT2_COMPRESS
+# define EXT2_FEATURE_INCOMPAT_SUPP	(EXT2_FEATURE_INCOMPAT_COMPRESSION| \
+					 EXT2_FEATURE_INCOMPAT_FILETYPE| \
 					 EXT2_FEATURE_INCOMPAT_META_BG)
+#else
+# define EXT2_FEATURE_INCOMPAT_SUPP	(EXT2_FEATURE_INCOMPAT_FILETYPE| \
+					 EXT2_FEATURE_INCOMPAT_META_BG)
+#endif
 #define EXT2_FEATURE_RO_COMPAT_SUPP	(EXT2_FEATURE_RO_COMPAT_SPARSE_SUPER| \
 					 EXT2_FEATURE_RO_COMPAT_LARGE_FILE| \
 					 EXT2_FEATURE_RO_COMPAT_BTREE_DIR)
@@ -554,4 +589,16 @@ enum {
 #define EXT2_DIR_REC_LEN(name_len)	(((name_len) + 8 + EXT2_DIR_ROUND) & \
 					 ~EXT2_DIR_ROUND)
 
+#ifndef __KERNEL__
+/* This simplifies things for user programs (notably e2fsprogs) that
+   must compile whether or not <linux/ext2_fs_c.h> is present, but
+   would prefer to include it.  Presumably the file is present if the
+   user has this version of ext2_fs.h. */
+
+# /* Do not remove this comment. */ include <linux/ext2_fs_c.h>
+
+/* The comment between `#' and `include' prevents mkdep from generating
+   a dependency on ext2_fs_c.h. */
+#endif
+
 #endif	/* _LINUX_EXT2_FS_H */
diff -pruN linux-2.6.18.5.org/include/linux/ext2_fs_c.h linux-2.6.18.5/include/linux/ext2_fs_c.h
--- linux-2.6.18.5.org/include/linux/ext2_fs_c.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-2.6.18.5/include/linux/ext2_fs_c.h	2007-01-23 11:37:02.000000000 -0800
@@ -0,0 +1,472 @@
+/*
+ *  Copyright (C) 2001 Alcatel Business Systems - R&D Illkirch
+ *  	(transparent compression code)
+ *  Pierre Peiffer (pierre.peiffer@sxb.bsf.alcatel.fr) - Denis Richard (denis.richard@sxb.bsf.alcatel.fr)
+ *  Adapted from patch e2compr-0.4.39-patch-2.2.18 .
+ */
+
+#ifndef EXT2_FS_C_H
+#define EXT2_FS_C_H
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+#include <linux/ext2_fs.h>
+#include "../../fs/ext2/ext2.h"
+
+#define EXT2_IOC_GETCLUSTERSIZE 	_IOR('c', 0, long)
+#define EXT2_IOC_SETCLUSTERSIZE 	_IOW('c', 0, long)
+#define EXT2_IOC_GETCOMPRMETHOD 	_IOR('c', 1, long)
+#define EXT2_IOC_SETCOMPRMETHOD 	_IOW('c', 1, long)
+#define EXT2_IOC_GETFIRSTCLUSTERSIZE 	_IOR('c', 2, long)
+#define EXT2_IOC_RECOGNIZE_COMPRESSED	_IOW('c', 2, long)
+#define EXT2_IOC_GETCLUSTERBIT		_IOR('c', 3, long)
+#define EXT2_IOC_GETCOMPRRATIO		_IOR('c', 4, long)
+/* Don't use _IOW('c', {5,6}, long), as these are used by old
+   e2compress binaries as SETCLUSTERBIT and CLRCLUSTERBIT
+   respectively. */
+
+/* EXT2_xxxx_ALG is an index into ext2_algorithm_table[] defined in
+   fs/ext2/compress.c. */
+/* N.B. Don't change these without also changing the table in
+   compress.c.  Be careful not to break binary compatibility.
+   (EXT2_NONE_ALG and EXT2_UNDEF_ALG are safe from binary
+   compatibility problems, though, so they can safely be renumbered --
+   and indeed probably should be if you do add another algorithm.) */
+#define EXT2_LZV1_ALG 0
+#define EXT2_LZRW3A_ALG 1
+#define EXT2_GZIP_ALG 2
+#define EXT2_BZIP2_ALG 3
+#define EXT2_LZO_ALG 4
+#define EXT2_NONE_ALG 5
+#define EXT2_UNDEF_ALG 6
+#define EXT2_N_ALGORITHMS 5 /* Count of "real" algorithms.  Excludes
+			       `none' and `undef'. */
+
+/* EXT2_xxxx_METH is an index into ext2_method_table[] defined in
+   fs/ext2/compress.c. */
+/* N.B. Don't change these without also changing the table in
+   compress.c. */
+#define EXT2_LZV1_METH 0
+#define EXT2_AUTO_METH 1
+#define EXT2_DEFER_METH 2
+#define EXT2_NEVER_METH 3
+#define EXT2_BZIP2_METH 4
+#define EXT2_LZRW3A_METH 8
+#define EXT2_LZO1X_1_METH 10
+#define EXT2_GZIP_1_METH 16
+#define EXT2_GZIP_2_METH 17
+#define EXT2_GZIP_3_METH 18
+#define EXT2_GZIP_4_METH 19
+#define EXT2_GZIP_5_METH 20
+#define EXT2_GZIP_6_METH 21
+#define EXT2_GZIP_7_METH 22
+#define EXT2_GZIP_8_METH 23
+#define EXT2_GZIP_9_METH 24
+#define EXT2_N_METHODS 32 /* Don't change this unless you know what
+			      you're doing.  In particular, it's tied
+			      to the width of the algorithm field
+			      in i_flags.*/
+
+#define EXT2_MAX_CLUSTER_BYTES		(32*1024)
+#define EXT2_LOG2_MAX_CLUSTER_BYTES	(5 + 10)
+
+#define EXT2_COMPRESS_MAGIC_04X	0x9ec7
+#define EXT2_MAX_CLUSTER_BLOCKS	32
+#define EXT2_MAX_CLUSTER_PAGES		EXT2_MAX_CLUSTER_BYTES >> PAGE_CACHE_SHIFT
+#define EXT2_ECOMPR			EIO
+/* A cluster is considered compressed iff the block number for the
+   last block of that cluster is EXT2_COMPRESSED_BLKADDR.  If this
+   changes then check if there's anywhere that needs a cpu_to_le32()
+   conversion. */
+#define EXT2_COMPRESSED_BLKADDR	0xffffffff 
+
+/* I like these names better. */
+#define EXT2_MAX_CLU_NBYTES EXT2_MAX_CLUSTER_BYTES
+#define EXT2_LOG2_MAX_CLU_NBYTES EXT2_LOG2_MAX_CLUSTER_BYTES
+#define EXT2_MAX_CLU_NBLOCKS EXT2_MAX_CLUSTER_BLOCKS
+
+
+#ifndef __KERNEL__
+
+/* Cluster head on disk, for e2compr versions before 0.4.0.  I'm
+   leaving this here so tht as I may make e2compress able to read
+   old-style e2compr files. */
+struct ext2_cluster_head_03x {
+  __u16 magic;			/* == EXT2_COMPRESS_MAGIC_03X */
+  __u16 len;			/* size of uncompressed data */
+  __u16 compr_len;		/* size of compressed data */
+  __u8  method;			/* compress method */
+  __u8  reserved_0;
+  __u32 bitmap;			/* block bitmap */
+  __u32 reserved_2;		/* 0 or adler32 checksum of
+				   _compressed_ data */
+};
+# define EXT2_COMPRESS_MAGIC_03X	0x8ec7 /* Head magic number
+						  for e2compr versions
+						  before 0.4.0. */
+#endif /* !__KERNEL__ */
+
+
+
+#ifdef __KERNEL__
+# include <linux/config.h>
+# ifdef  CONFIG_EXT2_COMPRESS
+
+/* If defined, compress each cluster as soon as we get to the end of a
+   whole cluster, when writing.  (If undefined, we wait until
+   ext2_release_file() or the like.) */
+#  define EXT2_COMPRESS_WHEN_CLU
+
+/* Some working area details are yet to be worked out, so I'm using this
+   set of wrappers. */
+/* # define ext2_rd_wa (ext2_wa[0]) */
+#  define ext2_rd_wa_lock (ext2_wa_lock[0])
+#  define ext2_lock_rd_wa() ext2_lock_wa(0)
+#  define ext2_unlock_rd_wa() ext2_unlock_wa(0)
+#  define ext2_rd_wa_ix 0
+#  ifdef CONFIG_EXT2_SEPARATE_WORK_AREAS
+#   define EXT2_N_WA 2
+#   define ext2_wr_wa_ix 1
+#   define ext2_wr_wa_lock (ext2_wa_lock[1])
+#   define ext2_lock_wr_wa() ext2_lock_wa(1)
+#   define ext2_unlock_wr_wa() ext2_unlock_wa(1)
+#  else
+#   define EXT2_N_WA 1
+#   define ext2_wr_wa_ix 0
+#   define ext2_wr_wa_lock (ext2_wa_lock[0])
+#   define ext2_lock_wr_wa() ext2_lock_wa(0)
+#   define ext2_unlock_wr_wa() ext2_unlock_wa(0)
+#  endif
+
+#  ifdef CONFIG_EXT2_DEFAULT_COMPR_METHOD_DEFER
+#   define EXT2_DEFAULT_COMPR_METHOD	EXT2_DEFER_METH
+#  elif defined (CONFIG_EXT2_DEFAULT_COMPR_METHOD_LZO)
+#   define EXT2_DEFAULT_COMPR_METHOD	EXT2_LZO1X_1_METH
+#   ifndef CONFIG_EXT2_HAVE_LZO
+#    error "Default algorithm (lzo) is not compiled in."
+#   endif
+#  elif defined (CONFIG_EXT2_DEFAULT_COMPR_METHOD_LZV1)
+#   define EXT2_DEFAULT_COMPR_METHOD	EXT2_LZV1_METH
+#   ifndef CONFIG_EXT2_HAVE_LZV1
+#    error "Default algorithm (lzv1) is not compiled in."
+#   endif
+#  elif defined (CONFIG_EXT2_DEFAULT_COMPR_METHOD_LZRW3A)
+#   define EXT2_DEFAULT_COMPR_METHOD	EXT2_LZRW3A_METH
+#   ifndef CONFIG_EXT2_HAVE_LZRW3A
+#    error "Default algorithm (lzrw3a) is not compiled in."
+#   endif
+#  elif defined (CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP1)
+#   define EXT2_DEFAULT_COMPR_METHOD	EXT2_GZIP_1_METH
+#  elif defined (CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP2)
+#   define EXT2_DEFAULT_COMPR_METHOD	EXT2_GZIP_2_METH
+#  elif defined (CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP3)
+#   define EXT2_DEFAULT_COMPR_METHOD	EXT2_GZIP_3_METH
+#  elif defined (CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP4)
+#   define EXT2_DEFAULT_COMPR_METHOD	EXT2_GZIP_4_METH
+#  elif defined (CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP5)
+#   define EXT2_DEFAULT_COMPR_METHOD	EXT2_GZIP_5_METH
+#  elif defined (CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP6)
+#   define EXT2_DEFAULT_COMPR_METHOD	EXT2_GZIP_6_METH
+#  elif defined (CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP7)
+#   define EXT2_DEFAULT_COMPR_METHOD	EXT2_GZIP_7_METH
+#  elif defined (CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP8)
+#   define EXT2_DEFAULT_COMPR_METHOD	EXT2_GZIP_8_METH
+#  elif defined (CONFIG_EXT2_DEFAULT_COMPR_METHOD_GZIP9)
+#   define EXT2_DEFAULT_COMPR_METHOD	EXT2_GZIP_9_METH
+#  elif defined (CONFIG_EXT2_DEFAULT_COMPR_METHOD_BZIP2)
+#   define EXT2_DEFAULT_COMPR_METHOD	EXT2_BZIP2_METH
+#   ifndef CONFIG_EXT2_HAVE_BZIP2
+#    error "Default algorithm (bzip2) is not compiled in."
+#   endif
+#  else
+#   error "No default compression algorithm."
+#  endif
+#  if EXT2_DEFAULT_COMPR_METHOD >= EXT2_GZIP_1_METH && EXT2_DEFAULT_COMPR_METHOD <= EXT2_GZIP_9_METH
+#   ifndef CONFIG_EXT2_HAVE_GZIP
+#    error "Default algorithm (gzip) is not compiled in."
+#   endif
+#  endif
+
+#  if defined (CONFIG_EXT2_DEFAULT_CLUSTER_BITS_2)
+#   define EXT2_DEFAULT_LOG2_CLU_NBLOCKS	2
+#  elif defined (CONFIG_EXT2_DEFAULT_CLUSTER_BITS_3)
+#   define EXT2_DEFAULT_LOG2_CLU_NBLOCKS	3
+#  elif defined (CONFIG_EXT2_DEFAULT_CLUSTER_BITS_4)
+#   define EXT2_DEFAULT_LOG2_CLU_NBLOCKS	4
+#  elif defined (CONFIG_EXT2_DEFAULT_CLUSTER_BITS_5)
+#   define EXT2_DEFAULT_LOG2_CLU_NBLOCKS	5
+#  else
+#   error "No default cluster size."
+#  endif
+
+#  define EXT2_DEFAULT_CLU_NBLOCKS	(1 << EXT2_DEFAULT_LOG2_CLU_NBLOCKS)
+
+#  if (EXT2_LZV1_ALG != 0) || (EXT2_BZIP2_ALG != 3) || (EXT2_LZO_ALG != 4) || (EXT2_N_ALGORITHMS != 5)
+#   error "this code needs changing; but then, you shouldn't be messing with algorithm ids anyway unless you are very careful to protect disk format compatibility"
+#  endif
+#  ifdef CONFIG_EXT2_HAVE_LZV1
+#   define _ext2_lzv1_builtin (1 << EXT2_LZV1_ALG)
+#  else
+#   define _ext2_lzv1_builtin 0
+#  endif
+#  ifdef CONFIG_EXT2_HAVE_LZRW3A
+#   define _ext2_lzrw3a_builtin (1 << EXT2_LZRW3A_ALG)
+#  else
+#   define _ext2_lzrw3a_builtin 0
+#  endif
+#  ifdef CONFIG_EXT2_HAVE_GZIP
+#   define _ext2_gzip_builtin (1 << EXT2_GZIP_ALG)
+#  else
+#   define _ext2_gzip_builtin 0
+#  endif
+#  ifdef CONFIG_EXT2_HAVE_BZIP2
+#   define _ext2_bzip2_builtin (1 << EXT2_BZIP2_ALG)
+#  else
+#   define _ext2_bzip2_builtin 0
+#  endif
+#  ifdef CONFIG_EXT2_HAVE_LZO
+#   define _ext2_lzo_builtin (1 << EXT2_LZO_ALG)
+#  else
+#   define _ext2_lzo_builtin 0
+#  endif
+
+#  ifdef CONFIG_EXT2_HAVE_LZV1_MODULE
+#   define _ext2_lzv1_module (1 << EXT2_LZV1_ALG)
+#  else
+#   define _ext2_lzv1_module 0
+#  endif
+#  ifdef CONFIG_EXT2_HAVE_LZRW3A_MODULE
+#   define _ext2_lzrw3a_module (1 << EXT2_LZRW3A_ALG)
+#  else
+#   define _ext2_lzrw3a_module 0
+#  endif
+#  ifdef CONFIG_EXT2_HAVE_GZIP_MODULE
+#   define _ext2_gzip_module (1 << EXT2_GZIP_ALG)
+#  else
+#   define _ext2_gzip_module 0
+#  endif
+#  ifdef CONFIG_EXT2_HAVE_BZIP2_MODULE
+#   define _ext2_bzip2_module (1 << EXT2_BZIP2_ALG)
+#  else
+#   define _ext2_bzip2_module 0
+#  endif
+#  ifdef CONFIG_EXT2_HAVE_LZO_MODULE
+#   define _ext2_lzo_module (1 << EXT2_LZO_ALG)
+#  else
+#   define _ext2_lzo_module 0
+#  endif
+
+#  define EXT2_ALGORITHMS_MODULE  (_ext2_lzv1_module | _ext2_lzrw3a_module | _ext2_gzip_module | _ext2_bzip2_module | _ext2_lzo_module)
+#  define EXT2_ALGORITHMS_BUILTIN  (_ext2_lzv1_builtin | _ext2_lzrw3a_builtin | _ext2_gzip_builtin | _ext2_bzip2_builtin | _ext2_lzo_builtin)
+
+#  if EXT2_ALGORITHMS_MODULE & EXT2_ALGORITHMS_BUILTIN
+#   error "Arithmetic error?  Some algorithm appears to be both built-in and a module."
+#  endif
+
+/* EXT2_ALGORITHMS_SUPP is what we test when mounting a filesystem.
+   See fs/ext2/super.c. */
+#  define EXT2_ALGORITHMS_SUPP (EXT2_ALGORITHMS_MODULE | EXT2_ALGORITHMS_BUILTIN)
+#  if EXT2_ALGORITHMS_SUPP == 0
+#   error "You must select at least one compression algorithm."
+#  endif
+
+/* Cluster head on disk.  Little-endian. */
+struct ext2_cluster_head {
+  __u16 magic;		/* == EXT2_COMPRESS_MAGIC_04X. */
+  __u8  method;		/* compression method id. */
+  __u8  holemap_nbytes;	/* length of holemap[] array */
+  __u32 checksum;	/* adler32 checksum.  Checksum covers all fields
+			   below this one, and the compressed data. */
+  __u32 ulen;		/* size of uncompressed data */
+  __u32 clen;		/* size of compressed data (excluding cluster head) */
+  __u8  holemap[0];     /* bitmap describing where to put holes. */
+};
+
+
+
+struct ext2_wa_S {
+  __u8 u[EXT2_MAX_CLUSTER_BYTES];  /* Uncompressed data. */
+  __u8 c[EXT2_MAX_CLUSTER_BYTES];  /* Compressed data. */
+  __u8 heap[1];  /* Heap: working space for de/compression routines. */
+};
+extern struct ext2_wa_S *ext2_rd_wa;
+extern size_t ext2_rd_wa_size;
+#  ifdef CONFIG_EXT2_SEPARATE_WORK_AREAS
+extern struct ext2_wa_S *ext2_wr_wa;
+extern size_t ext2_wr_wa_size;
+#  else
+#   define ext2_wr_wa ext2_rd_wa
+#   define ext2_wr_wa_size ext2_rd_wa_size
+#  endif
+
+#  define EXT2_CLEANUP_FL		0x40 /* See Readme.e2compr */
+#  define EXT2_OSYNC_INODE		0x20 /* sync of inode running	*/
+#  define ROUNDUP_DIV(_n, _d) ((_n) ? 1 + (((_n) - 1) / (_d)) : 0)
+#  define ROUNDUP_RSHIFT(_n, _b) ((_n) ? 1 + (((_n) - 1) >> (_b)) : 0)
+
+#  if defined(EXT2_NDIR_BLOCKS) && (EXT2_NDIR_BLOCKS != 12)
+#   error "e2compr currently assumes that EXT2_NDIR_BLOCKS is 12."
+/* If EXT2_NDIR_BLOCKS changes then change the definitions of
+   ext2_first_cluster_nblocks() and friends, and search the patch for
+   anywhere where 12 is hard-coded.  (At the time of writing, it's
+   only hard-coded in ext2_first_cluster_nblocks().)  What we want to
+   achieve is for clusters not to straddle address blocks.  Apart from
+   performance, some code in compress.c (search for `straddle')
+   assumes this. */
+#  endif
+
+#  include <linux/fs.h>
+
+#  define EXT2_ALG_INIT_COMPRESS  	1
+#  define EXT2_ALG_INIT_DECOMPRESS	2
+
+extern int    ext2_get_cluster_pages (struct inode*, u32, struct page**, struct page *, int);
+extern int    ext2_get_cluster_extra_pages (struct inode*, u32, struct page**, struct page**);
+extern int    ext2_get_cluster_blocks (struct inode*, u32, struct buffer_head**, struct page**, struct page**, int);
+extern int    ext2_decompress_cluster (struct inode*, u32);
+extern int    ext2_decompress_pages(struct inode*, u32, struct page**);
+extern int    ext2_compress_cluster (struct inode*, u32);
+extern int    ext2_decompress_inode (struct inode*);
+extern int    ext2_cleanup_compressed_inode (struct inode*);
+extern void   ext2_update_comprblk (struct inode *);
+
+extern void   ext2_lock_rd_wa_uninterruptible(void);
+extern int    ext2_lock_wa (unsigned);
+extern void   ext2_unlock_wa (unsigned);
+
+extern size_t ext2_decompress_blocks    (struct inode*, struct buffer_head**, int, size_t);
+extern int    ext2_count_blocks		(struct inode*);
+extern int    ext2_recognize_compressed (struct inode *, unsigned cluster);
+extern unsigned long ext2_adler32	(unsigned long, unsigned char*, int);
+
+extern size_t ext2_iLZV1   (int);
+extern size_t ext2_iLZV2   (int);
+extern size_t ext2_iNONE   (int);
+extern size_t ext2_iGZIP   (int);
+extern size_t ext2_iBZIP2  (int);
+extern size_t ext2_iLZO    (int);
+extern size_t ext2_iLZRW3A (int);
+
+extern size_t ext2_wLZV1   (__u8*, __u8*, void*, size_t, size_t, int);
+extern size_t ext2_wLZV2   (__u8*, __u8*, void*, size_t, size_t, int);
+extern size_t ext2_wNONE   (__u8*, __u8*, void*, size_t, size_t, int);
+extern size_t ext2_wGZIP   (__u8*, __u8*, void*, size_t, size_t, int);
+extern size_t ext2_wBZIP2  (__u8*, __u8*, void*, size_t, size_t, int);
+extern size_t ext2_wLZO    (__u8*, __u8*, void*, size_t, size_t, int);
+extern size_t ext2_wLZRW3A (__u8*, __u8*, void*, size_t, size_t, int);
+
+extern size_t ext2_rLZV1   (__u8*, __u8*, void*, size_t, size_t, int);
+extern size_t ext2_rLZV2   (__u8*, __u8*, void*, size_t, size_t, int);
+extern size_t ext2_rNONE   (__u8*, __u8*, void*, size_t, size_t, int);
+extern size_t ext2_rGZIP   (__u8*, __u8*, void*, size_t, size_t, int);
+extern size_t ext2_rBZIP2  (__u8*, __u8*, void*, size_t, size_t, int);
+extern size_t ext2_rLZO    (__u8*, __u8*, void*, size_t, size_t, int);
+extern size_t ext2_rLZRW3A (__u8*, __u8*, void*, size_t, size_t, int);
+
+struct ext2_algorithm { 
+	char	*name; 
+	int	avail;
+	size_t (*init)		(int); 
+	size_t (*compress)	(__u8*, __u8*, void*, size_t, size_t, int); 
+	size_t (*decompress)	(__u8*, __u8*, void*, size_t, size_t, int);
+};
+
+struct ext2_method {
+	unsigned  alg;
+	int	  xarg;
+};
+
+extern int ext2_register_compression_module(unsigned alg,
+				size_t compress_memory,
+				size_t decompress_memory,
+		                struct ext2_algorithm *new_algorithm);
+extern void ext2_unregister_compression_module(unsigned alg);
+
+#  define ext2_first_cluster_nblocks(_i) ((EXT2_I(_i))->i_clu_nblocks > 4 && (_i)->i_sb->s_blocksize < 4096 ? 12 : 4)
+#  define ext2_block_to_cluster(_i,_b)	((_b) < ext2_first_cluster_nblocks(_i) ? 0 : (((_b) - ext2_first_cluster_nblocks(_i)) >> (EXT2_I(_i))->i_log2_clu_nblocks) + 1)
+#  define ext2_offset_to_cluster(_i,_o)	ext2_block_to_cluster((_i), ((_o) >> (_i)->i_sb->s_blocksize_bits))
+#  define ext2_n_clusters(_i)	((_i)->i_size ? ext2_offset_to_cluster((_i), (_i)->i_size - 1) + 1 : 0)
+#  define ext2_cluster_block0(_i,_c)	((_c) ? ext2_first_cluster_nblocks(_i) + (((_c) - 1) << (EXT2_I(_i))->i_log2_clu_nblocks) : 0)
+#  define ext2_cluster_nblocks(_i,_c)	((_c) ? (EXT2_I(_i))->i_clu_nblocks : ext2_first_cluster_nblocks(_i))
+#  define ext2_cluster_offset(_i,_c)	((_c) ? ext2_cluster_block0((_i), (_c)) << (_i)->i_sb->s_blocksize_bits : 0)
+
+#  define ext2_first_cluster_npages(_i) ((EXT2_I(_i))->i_clu_nblocks > 4 && (_i)->i_sb->s_blocksize < 4096 ? 12 >> (PAGE_CACHE_SHIFT - (_i)->i_sb->s_blocksize_bits) : 4 >> (PAGE_CACHE_SHIFT - (_i)->i_sb->s_blocksize_bits))
+#  define ext2_page_to_cluster(_i,_p)	((_p) < ext2_first_cluster_npages(_i) ? 0 : (((_p) - ext2_first_cluster_npages(_i)) >> (((EXT2_I(_i))->i_log2_clu_nblocks)+(_i)->i_sb->s_blocksize_bits-PAGE_CACHE_SHIFT)) + 1)
+#  define ext2_cluster_page0(_i,_c)	((_c) ? ext2_cluster_block0(_i, _c) >> (PAGE_CACHE_SHIFT - (_i)->i_sb->s_blocksize_bits) : 0)
+#  define ext2_cluster_npages(_i,_c)	((_c) ? (EXT2_I(_i))->i_clu_nblocks >> (PAGE_CACHE_SHIFT - (_i)->i_sb->s_blocksize_bits) : ext2_first_cluster_npages(_i))
+
+extern inline int
+ext2_offset_is_clu_boundary(struct inode *inode, u32 off)
+{
+	if (off & (inode->i_sb->s_blocksize - 1))
+		return 0;
+	if (off == 0)
+		return 1;
+	off >>= inode->i_sb->s_blocksize_bits;
+	if (off < ext2_first_cluster_nblocks(inode))
+		return 0;
+	off -= ext2_first_cluster_nblocks(inode);
+	return !(off & (EXT2_I(inode)->i_clu_nblocks - 1));
+}
+
+struct ext2_wa_contents_S {
+	ino_t ino;
+	dev_t dev;
+	unsigned cluster;
+};
+extern struct ext2_wa_contents_S ext2_rd_wa_ucontents;
+#  ifdef CONFIG_EXT2_SEPARATE_WORK_AREAS
+extern struct ext2_wa_contents_S ext2_wr_wa_ucontents;
+#  else
+#   define ext2_wr_wa_ucontents ext2_rd_wa_ucontents
+#  endif
+
+extern struct ext2_algorithm ext2_algorithm_table[];
+extern struct ext2_method ext2_method_table[];
+
+/* Both of these return -errno if error, 0 if not compressed, positive
+   if compressed.  (You should use the macro unless you've already
+   tested COMPRBLK.) */
+extern int ext2_cluster_is_compressed_fn (struct inode *inode, __u32 cluster);
+extern inline int ext2_cluster_is_compressed (struct inode *inode, __u32 cluster)
+{
+	if ((EXT2_I(inode)->i_flags & EXT2_COMPRBLK_FL) == 0)
+		return 0;
+	return ext2_cluster_is_compressed_fn (inode, cluster);
+}
+extern unsigned ext2_calc_free_ix (unsigned , u8 const *, unsigned );
+extern int ext2_unpack_blkaddrs(struct inode *, struct buffer_head **, int, unsigned , u8 const *, unsigned , unsigned , unsigned , unsigned );
+
+#  define ext2_compression_error(_i)	((EXT2_I(_i))->i_flags & EXT2_ECOMPR_FL)
+#  define ext2_compression_disabled(_i)	((EXT2_I(_i))->i_flags & EXT2_NOCOMPR_FL)
+#  define ext2_compression_enabled(_i)	(! ext2_compression_disabled (_i))
+
+#  define HOLE_BLKADDR(_b) \
+	(((_b) == 0) \
+	 || ((_b) == EXT2_COMPRESSED_BLKADDR))
+# else /* !CONFIG_EXT2_COMPRESS */
+#  define HOLE_BLKADDR(_b) ((_b) == 0)
+# endif
+
+/* For some reason or other, I see code like `if (le32_to_cpu(tmp) !=
+   0)' around in the kernel.  So far I haven't checked whether or not
+   the compiler knows that the swab can be dropped. */
+# if defined(EXT2_COMPRESSED_BLKADDR) && EXT2_COMPRESSED_BLKADDR != 0xffffffff
+/* This may be a false positive; the "correct" test would be `if
+   defined(CONFIG_EXT2_COMPRESS)', but if this test does succeed, then
+   there is at least cause to have a look around. */
+#  error "Next bit of code is wrong."
+# endif
+
+# define HOLE_BLKADDR_SWAB32(_b) HOLE_BLKADDR(_b)
+
+#ifdef EXT2_COMPR_REPORT
+#define trace_e2c(format, args...) printk(KERN_DEBUG format, ## args)
+#else
+#define trace_e2c(format, args...) do {} while(0)
+#endif
+
+#endif /* __KERNEL__ */
+
+
+#endif /* EXT2_FS_C_H */
diff -pruN linux-2.6.18.5.org/include/linux/mm.h linux-2.6.18.5/include/linux/mm.h
--- linux-2.6.18.5.org/include/linux/mm.h	2006-12-04 13:34:17.000000000 -0800
+++ linux-2.6.18.5/include/linux/mm.h	2007-01-23 11:37:02.000000000 -0800
@@ -388,6 +388,8 @@ void split_page(struct page *page, unsig
  *   to swap space and (later) to be read back into memory.
  */
 
+extern wait_queue_head_t *page_waitqueue(struct page *page);
+
 /*
  * The zone field is never updated after free_area_init_core()
  * sets it, so none of the operations on it need to be atomic.
diff -pruN linux-2.6.18.5.org/include/linux/pagemap.h linux-2.6.18.5/include/linux/pagemap.h
--- linux-2.6.18.5.org/include/linux/pagemap.h	2006-12-04 13:34:17.000000000 -0800
+++ linux-2.6.18.5/include/linux/pagemap.h	2007-01-23 11:37:02.000000000 -0800
@@ -11,6 +11,7 @@
 #include <linux/compiler.h>
 #include <asm/uaccess.h>
 #include <linux/gfp.h>
+#include <linux/pagevec.h>
 
 /*
  * Bits in mapping->flags.  The lower __GFP_BITS_SHIFT bits are the page
@@ -83,6 +84,9 @@ unsigned find_get_pages_contig(struct ad
 unsigned find_get_pages_tag(struct address_space *mapping, pgoff_t *index,
 			int tag, unsigned int nr_pages, struct page **pages);
 
+extern struct page *__grab_cache_page(struct address_space *, unsigned long,
+					struct page **, struct pagevec *);
+
 /*
  * Returns locked page at given index in given cache, creating it if needed.
  */
diff -pruN linux-2.6.18.5.org/mm/filemap.c linux-2.6.18.5/mm/filemap.c
--- linux-2.6.18.5.org/mm/filemap.c	2006-12-04 13:32:05.000000000 -0800
+++ linux-2.6.18.5/mm/filemap.c	2007-02-23 07:34:38.000000000 -0800
@@ -44,6 +44,10 @@ static ssize_t
 generic_file_direct_IO(int rw, struct kiocb *iocb, const struct iovec *iov,
 	loff_t offset, unsigned long nr_segs);
 
+#ifdef CONFIG_EXT2_COMPRESS
+# include <linux/ext2_fs_c.h>
+#endif
+
 /*
  * Shared mappings implemented 30.11.1994. It's not fully working yet,
  * though.
@@ -260,7 +264,18 @@ int wait_on_page_writeback_range(struct 
 			PAGECACHE_TAG_WRITEBACK,
 			min(end - index, (pgoff_t)PAGEVEC_SIZE-1) + 1)) != 0) {
 		unsigned i;
+#ifdef CONFIG_EXT2_COMPRESS
+/*
+ * I'm not sure that this is right.  It has been reworked considerably since
+ * 2.6.5. - whitpa
+ */
+		struct inode *inode = mapping->host;
 
+		if ((strcmp(inode->i_sb->s_type->name, "ext2") != 0)
+			    || (atomic_read(&inode->i_mutex.count) > 0)
+			    || (EXT2_I(inode)->i_compr_flags &
+				EXT2_OSYNC_INODE))
+#endif
 		for (i = 0; i < nr_pages; i++) {
 			struct page *page = pvec.pages[i];
 
@@ -498,12 +513,13 @@ EXPORT_SYMBOL(page_cache_alloc_cold);
  * at a cost of "thundering herd" phenomena during rare hash
  * collisions.
  */
-static wait_queue_head_t *page_waitqueue(struct page *page)
+wait_queue_head_t *page_waitqueue(struct page *page)
 {
 	const struct zone *zone = page_zone(page);
 
 	return &zone->wait_table[hash_ptr(page, zone->wait_table_bits)];
 }
+EXPORT_SYMBOL(page_waitqueue);
 
 static inline void wake_up_page(struct page *page, int bit)
 {
@@ -1854,7 +1870,7 @@ EXPORT_SYMBOL(read_cache_page);
  * caller's lru-buffering pagevec.  This function is specifically for
  * generic_file_write().
  */
-static inline struct page *
+struct page *
 __grab_cache_page(struct address_space *mapping, unsigned long index,
 			struct page **cached_page, struct pagevec *lru_pvec)
 {
@@ -1883,6 +1899,8 @@ repeat:
 	return page;
 }
 
+EXPORT_SYMBOL(__grab_cache_page);
+
 /*
  * The logic we want is
  *
diff -pruN linux-2.6.18.5.org/mm/memory.c linux-2.6.18.5/mm/memory.c
--- linux-2.6.18.5.org/mm/memory.c	2006-12-04 13:32:06.000000000 -0800
+++ linux-2.6.18.5/mm/memory.c	2007-01-24 10:41:56.000000000 -0800
@@ -59,6 +59,10 @@
 #include <linux/swapops.h>
 #include <linux/elf.h>
 
+#ifdef CONFIG_EXT2_COMPRESS
+#include <linux/ext2_fs_c.h>
+#endif
+
 #ifndef CONFIG_NEED_MULTIPLE_NODES
 /* use the per-pgdat data instead for discontigmem - mbligh */
 unsigned long max_mapnr;
@@ -1815,6 +1819,11 @@ int vmtruncate(struct inode * inode, lof
 		goto out_busy;
 	i_size_write(inode, offset);
 	unmap_mapping_range(mapping, offset + PAGE_SIZE - 1, 0, 1);
+#ifdef CONFIG_EXT2_COMPRESS
+	if ((inode->i_op && inode->i_op->truncate) &&
+	    ((strcmp(inode->i_sb->s_type->name, "ext2") != 0) ||
+	    (!(EXT2_I(inode)->i_flags & EXT2_COMPRBLK_FL))))
+#endif
 	truncate_inode_pages(mapping, offset);
 	goto out_truncate;
 
diff -pruN linux-2.6.18.5.org/mm/page_alloc.c linux-2.6.18.5/mm/page_alloc.c
--- linux-2.6.18.5.org/mm/page_alloc.c	2006-12-04 13:32:05.000000000 -0800
+++ linux-2.6.18.5/mm/page_alloc.c	2007-01-23 11:37:02.000000000 -0800
@@ -1101,6 +1101,8 @@ void __pagevec_free(struct pagevec *pvec
 		free_hot_cold_page(pvec->pages[i], pvec->cold);
 }
 
+EXPORT_SYMBOL(__pagevec_free);
+
 fastcall void __free_pages(struct page *page, unsigned int order)
 {
 	if (put_page_testzero(page)) {
diff -pruN linux-2.6.18.5.org/mm/swapfile.c linux-2.6.18.5/mm/swapfile.c
--- linux-2.6.18.5.org/mm/swapfile.c	2006-12-04 13:32:06.000000000 -0800
+++ linux-2.6.18.5/mm/swapfile.c	2007-01-24 10:43:54.000000000 -0800
@@ -5,6 +5,13 @@
  *  Swap reorganised 29.12.95, Stephen Tweedie
  */
 
+/*
+ *  Copyright (C) 2001 Alcatel Business Systems - R&D Illkirch
+ *  Added by Pierre Peiffer  (pierre.peiffer@sxb.bsf.alcatel.fr) (2001-08-09)
+ *  (Copied from pjm e2compr-0.4.39-patch-2.2.18 patch)
+ *      For e2compress: Swapping not supported for e2compressed files.
+ */
+
 #include <linux/mm.h>
 #include <linux/hugetlb.h>
 #include <linux/mman.h>
@@ -27,6 +34,9 @@
 #include <linux/mutex.h>
 #include <linux/capability.h>
 #include <linux/syscalls.h>
+#ifdef CONFIG_EXT2_COMPRESS
+#include <linux/ext2_fs_c.h>
+#endif
 
 #include <asm/pgtable.h>
 #include <asm/tlbflush.h>
@@ -1457,6 +1467,18 @@ asmlinkage long sys_swapon(const char __
 			goto bad_swap;
 		p->bdev = bdev;
 	} else if (S_ISREG(inode->i_mode)) {
+#ifdef CONFIG_EXT2_COMPRESS
+		/*
+		 * Swapping not supported for e2compressed files.
+		 * (Actually, this code is pretty useless because we
+		 * should get an error later anyway because of the
+		 * holes.)  Yes, this is pretty horrible code... I'll
+		 * improve it later.
+		 */
+		if ((strcmp(inode->i_sb->s_type->name, "ext2") == 0)
+		    && (EXT2_I(inode)->i_flags & EXT2_COMPRBLK_FL))
+			goto bad_swap;
+#endif
 		p->bdev = inode->i_sb->s_bdev;
 		mutex_lock(&inode->i_mutex);
 		did_down = 1;
