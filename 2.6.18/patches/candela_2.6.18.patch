diff --git a/arch/i386/kernel/tsc.c b/arch/i386/kernel/tsc.c
index 7e0d8da..5609853 100644
--- a/arch/i386/kernel/tsc.c
+++ b/arch/i386/kernel/tsc.c
@@ -124,6 +124,7 @@ #endif
 	/* return the value in ns */
 	return cycles_2_ns(this_offset);
 }
+EXPORT_SYMBOL(sched_clock);
 
 static unsigned long calculate_cpu_khz(void)
 {
diff --git a/arch/x86_64/kernel/time.c b/arch/x86_64/kernel/time.c
index ffd1cb8..74b2b85 100644
--- a/arch/x86_64/kernel/time.c
+++ b/arch/x86_64/kernel/time.c
@@ -510,6 +510,7 @@ #endif
 	rdtscll(a);
 	return cycles_2_ns(a);
 }
+EXPORT_SYMBOL(sched_clock);
 
 static unsigned long get_cmos_time(void)
 {
diff --git a/drivers/net/e100.c b/drivers/net/e100.c
index ce850f1..fb5c305 100644
--- a/drivers/net/e100.c
+++ b/drivers/net/e100.c
@@ -1,4 +1,4 @@
-/*******************************************************************************
+/**************************************************************
 
 
   Copyright(c) 1999 - 2005 Intel Corporation. All rights reserved.
@@ -383,6 +383,7 @@ enum cb_command {
 	cb_ucode  = 0x0005,
 	cb_dump   = 0x0006,
 	cb_tx_sf  = 0x0008,
+	cb_tx_nc  = 0x0010, /* 0 == controler does CRC, ie normal.  1 == CRC from memory */
 	cb_cid    = 0x1f00,
 	cb_i      = 0x2000,
 	cb_s      = 0x4000,
@@ -419,7 +420,7 @@ struct config {
 /*5*/	u8 X(tx_dma_max_count:7, dma_max_count_enable:1);
 /*6*/	u8 X(X(X(X(X(X(X(late_scb_update:1, direct_rx_dma:1),
 	   tno_intr:1), cna_intr:1), standard_tcb:1), standard_stat_counter:1),
-	   rx_discard_overruns:1), rx_save_bad_frames:1);
+	   rx_save_overruns:1), rx_save_bad_frames:1);
 /*7*/	u8 X(X(X(X(X(rx_discard_short_frames:1, tx_underrun_retry:2),
 	   pad7:2), rx_extended_rfd:1), tx_two_frames_in_fifo:1),
 	   tx_dynamic_tbd:1);
@@ -549,6 +550,8 @@ struct nic {
 		multicast_all      = (1 << 2),
 		wol_magic          = (1 << 3),
 		ich_10h_workaround = (1 << 4),
+		accept_all_frames  = (1 << 5),
+		save_fcs           = (1 << 6),
 	} flags					____cacheline_aligned;
 
 	enum mac mac;
@@ -1009,6 +1012,16 @@ static void e100_configure(struct nic *n
 		config->promiscuous_mode = 0x1;		/* 1=on, 0=off */
 	}
 
+	if(nic->flags & accept_all_frames) {
+		config->rx_save_overruns = 0x1;	/* 1=save, 0=discard */
+		config->rx_save_bad_frames = 0x1;	/* 1=save, 0=discard */
+		config->rx_discard_short_frames = 0x0;	/* 1=discard, 0=save */
+	}
+
+	if(nic->flags & save_fcs) {
+		config->rx_crc_transfer = 0x1;	/* 1=save, 0=discard */
+	}
+
 	if(nic->flags & multicast_all)
 		config->multicast_all = 0x1;		/* 1=accept, 0=no */
 
@@ -1467,6 +1480,16 @@ static void e100_set_multicast_list(stru
 	else
 		nic->flags &= ~promiscuous;
 
+	if(netdev->flags & IFF_ACCEPT_ALL_FRAMES)
+		nic->flags |= accept_all_frames;
+	else
+		nic->flags &= ~accept_all_frames;
+
+	if(netdev->flags & IFF_SAVE_FCS)
+		nic->flags |= save_fcs;
+	else
+		nic->flags &= ~save_fcs;
+
 	if(netdev->flags & IFF_ALLMULTI ||
 		netdev->mc_count > E100_MAX_MULTICAST_ADDRS)
 		nic->flags |= multicast_all;
@@ -1608,6 +1631,19 @@ static void e100_xmit_prepare(struct nic
 	struct sk_buff *skb)
 {
 	cb->command = nic->tx_command;
+
+#ifdef CONFIG_SUPPORT_SEND_BAD_CRC
+	/* Use the last 4 bytes of the SKB payload packet as the CRC, used for
+	 * testing, ie sending frames with bad CRC.
+	 */
+	if (unlikely(skb->use_specified_ether_crc)) {
+		cb->command |= __constant_cpu_to_le16(cb_tx_nc);
+	}
+	else {
+		cb->command &= ~__constant_cpu_to_le16(cb_tx_nc);
+	}
+#endif
+	
 	/* interrupt every 16 packets regardless of delay */
 	if((nic->cbs_avail & ~15) == nic->cbs_avail)
 		cb->command |= cpu_to_le16(cb_i);
@@ -1837,7 +1873,21 @@ static int e100_rx_indicate(struct nic *
 	skb_reserve(skb, sizeof(struct rfd));
 	skb_put(skb, actual_size);
 	skb->protocol = eth_type_trans(skb, nic->netdev);
-
+	/* NOTE:  The config step turns on acceptance of various bogus frames
+	 * when in loopback or promisc mode, but this code will still throw
+	 * them away unless you also set the new 'accept_all_frames' flag.
+	 * Perhaps the implementors meant to accept the bogus frames in
+	 * promisc mode here?? --Ben
+	 */
+	if(unlikely(!(nic->flags & accept_all_frames))) {
+		if(actual_size > nic->netdev->mtu + VLAN_ETH_HLEN) {
+			/* Received oversized frame */
+			nic->net_stats.rx_over_errors++;
+		}
+		/* We're accepting all, so pass the bogons on up the stack. */
+		goto process_skb;
+	}
+ 
 	if(unlikely(!(rfd_status & cb_ok))) {
 		/* Don't indicate if hardware indicates errors */
 		dev_kfree_skb_any(skb);
@@ -1846,6 +1896,7 @@ static int e100_rx_indicate(struct nic *
 		nic->rx_over_length_errors++;
 		dev_kfree_skb_any(skb);
 	} else {
+	process_skb:
 		nic->net_stats.rx_packets++;
 		nic->net_stats.rx_bytes += actual_size;
 		nic->netdev->last_rx = jiffies;
@@ -2209,6 +2260,63 @@ static int e100_set_settings(struct net_
 	return err;
 }
 
+static int e100_set_rxall(struct net_device *netdev, u32 data)
+{
+	struct nic *nic = netdev->priv;
+	if (data) {
+		netdev->priv_flags |= IFF_ACCEPT_ALL_FRAMES;
+		nic->flags |= accept_all_frames;
+	}
+	else {
+		netdev->priv_flags &= ~(IFF_ACCEPT_ALL_FRAMES);
+		nic->flags &= ~accept_all_frames;
+	}
+	
+	e100_exec_cb(nic, NULL, e100_configure);
+
+	return 0;
+}
+
+static int e100_get_rxall(struct net_device *netdev, u32* data)
+{
+	struct nic *nic = netdev->priv;
+	if (nic->flags & accept_all_frames) {
+		*data = 1;
+	}
+	else {
+		*data = 0;
+	}
+	
+	return 0;
+}
+
+static int e100_set_save_fcs(struct net_device *netdev, u32 data)
+{
+	struct nic *nic = netdev->priv;
+	if (data) {
+		nic->flags |= save_fcs;
+	}
+	else {
+		nic->flags &= ~save_fcs;
+	}
+	e100_exec_cb(nic, NULL, e100_configure);
+
+	return 0;
+}
+
+static int e100_get_save_fcs(struct net_device *netdev, u32* data)
+{
+	struct nic *nic = netdev->priv;
+	if (nic->flags & save_fcs) {
+		*data = 1;
+	}
+	else {
+		*data = 0;
+	}
+	
+	return 0;
+}
+
 static void e100_get_drvinfo(struct net_device *netdev,
 	struct ethtool_drvinfo *info)
 {
@@ -2506,6 +2614,10 @@ static struct ethtool_ops e100_ethtool_o
 	.get_stats_count	= e100_get_stats_count,
 	.get_ethtool_stats	= e100_get_ethtool_stats,
 	.get_perm_addr		= ethtool_op_get_perm_addr,
+	.set_rx_all             = e100_set_rxall,
+	.get_rx_all             = e100_get_rxall,
+	.set_save_fcs           = e100_set_save_fcs,
+	.get_save_fcs           = e100_get_save_fcs,
 };
 
 static int e100_do_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)
diff --git a/drivers/net/e1000/e1000.h b/drivers/net/e1000/e1000.h
index d304297..6de5129 100644
--- a/drivers/net/e1000/e1000.h
+++ b/drivers/net/e1000/e1000.h
@@ -366,6 +366,7 @@ int e1000_set_spd_dplx(struct e1000_adap
 
 /*  e1000_ethtool.c  */
 void e1000_set_ethtool_ops(struct net_device *netdev);
+void e1000_set_multi(struct net_device *netdev);
 
 /*  e1000_param.c  */
 void e1000_check_options(struct e1000_adapter *adapter);
diff --git a/drivers/net/e1000/e1000_ethtool.c b/drivers/net/e1000/e1000_ethtool.c
index 88a82ba..d825b19 100644
--- a/drivers/net/e1000/e1000_ethtool.c
+++ b/drivers/net/e1000/e1000_ethtool.c
@@ -1,4 +1,4 @@
-/*******************************************************************************
+/*****************************************************************
 
   
   Copyright(c) 1999 - 2006 Intel Corporation. All rights reserved.
@@ -1887,6 +1887,58 @@ e1000_get_strings(struct net_device *net
 	}
 }
 
+static int e1000_ethtool_setrxall(struct net_device *netdev, uint32_t val) {
+	unsigned short old_flags = netdev->priv_flags;
+	if (val) {
+		netdev->priv_flags |= IFF_ACCEPT_ALL_FRAMES;
+	}
+	else {
+		netdev->priv_flags &= ~(IFF_ACCEPT_ALL_FRAMES);
+	}
+
+	/* printk("e1000_ethtool_setrxall (%s) val: %d\n",
+	   netdev->name, val); */
+	if (old_flags != netdev->priv_flags) {
+		netif_tx_lock_bh(netdev);
+		if (netif_running(netdev)) {
+			/*printk("Kicking e1000 for setrxall..\n");*/
+			e1000_set_multi(netdev);
+		} else {
+			/* Value will be flushed into the hardware when the device is
+			 * brought up.
+			 */
+		}
+		netif_tx_unlock_bh(netdev);
+	}
+	return 0;
+}
+
+static int e1000_ethtool_set_save_fcs(struct net_device *netdev, uint32_t val) {
+	netif_tx_lock_bh(netdev);
+	if (val) {
+		netdev->priv_flags |= IFF_SAVE_FCS;
+	}
+	else {
+		netdev->priv_flags &= ~IFF_SAVE_FCS;
+	}
+	netif_tx_unlock_bh(netdev);
+	return 0;
+}
+
+static int e1000_ethtool_get_save_fcs(struct net_device *netdev, uint32_t* val) {
+	*val = !!(netdev->priv_flags & IFF_SAVE_FCS);
+	/*printk("GETRXALL, data: %d  priv_flags: %hx\n",
+	  edata.data, netdev->priv_flags);*/
+	return 0;
+}
+
+static int e1000_ethtool_getrxall(struct net_device *netdev, uint32_t* val) {
+	*val = !!(netdev->priv_flags & IFF_ACCEPT_ALL_FRAMES);
+	/*printk("GETRXALL, data: %d  priv_flags: %hx\n",
+	  edata.data, netdev->priv_flags);*/
+	return 0;
+}		
+
 static struct ethtool_ops e1000_ethtool_ops = {
 	.get_settings           = e1000_get_settings,
 	.set_settings           = e1000_set_settings,
@@ -1923,6 +1975,10 @@ #endif
 	.get_stats_count        = e1000_get_stats_count,
 	.get_ethtool_stats      = e1000_get_ethtool_stats,
 	.get_perm_addr		= ethtool_op_get_perm_addr,
+	.get_rx_all             = e1000_ethtool_getrxall,
+	.set_rx_all             = e1000_ethtool_setrxall,
+	.set_save_fcs           = e1000_ethtool_set_save_fcs,
+	.get_save_fcs           = e1000_ethtool_get_save_fcs,
 };
 
 void e1000_set_ethtool_ops(struct net_device *netdev)
diff --git a/drivers/net/e1000/e1000_main.c b/drivers/net/e1000/e1000_main.c
index 98ef9f8..f4ff677 100644
--- a/drivers/net/e1000/e1000_main.c
+++ b/drivers/net/e1000/e1000_main.c
@@ -1,4 +1,4 @@
-/*******************************************************************************
+/*****************************************************************
 
   
   Copyright(c) 1999 - 2006 Intel Corporation. All rights reserved.
@@ -137,7 +137,7 @@ static void e1000_clean_tx_ring(struct e
                                 struct e1000_tx_ring *tx_ring);
 static void e1000_clean_rx_ring(struct e1000_adapter *adapter,
                                 struct e1000_rx_ring *rx_ring);
-static void e1000_set_multi(struct net_device *netdev);
+void e1000_set_multi(struct net_device *netdev);
 static void e1000_update_phy_info(unsigned long data);
 static void e1000_watchdog(unsigned long data);
 static void e1000_82547_tx_fifo_stall(unsigned long data);
@@ -822,6 +822,9 @@ #endif
 	if (pci_using_dac)
 		netdev->features |= NETIF_F_HIGHDMA;
 
+	/* Has ability to receive all frames (even bad CRCs and such) */
+ 	netdev->features |= NETIF_F_RX_ALL | NETIF_F_SAVE_CRC;
+	
 	netdev->features |= NETIF_F_LLTX;
 
 	adapter->en_mng_pt = e1000_enable_mng_pass_thru(&adapter->hw);
@@ -2194,7 +2197,7 @@ e1000_set_mac(struct net_device *netdev,
  * promiscuous mode, and all-multi behavior.
  **/
 
-static void
+void
 e1000_set_multi(struct net_device *netdev)
 {
 	struct e1000_adapter *adapter = netdev_priv(netdev);
@@ -2229,6 +2232,35 @@ e1000_set_multi(struct net_device *netde
 
 	E1000_WRITE_REG(hw, RCTL, rctl);
 
+
+	/* This is useful for using ethereal or tcpdump to sniff 
+        * packets in promiscuous mode without stripping VLAN/priority 
+        * information, and also letting bad packets through. 
+        * 
+        * THIS IS NOT PRODUCTION CODE - FOR INTERNAL USE ONLY!!! 
+        * 
+        */ 
+        if (netdev->priv_flags & IFF_ACCEPT_ALL_FRAMES) {
+		uint32_t ctrl; 
+		/*printk("%s: Enabling acceptance of ALL frames (bad CRC too).\n",
+		  netdev->name); */
+		/* store bad packets, promisc/multicast all, no VLAN
+		 * filter */ 
+		rctl = E1000_READ_REG(hw, RCTL); 
+		rctl |= (E1000_RCTL_SBP | E1000_RCTL_UPE | E1000_RCTL_MPE); 
+		rctl &= ~(E1000_RCTL_VFE | E1000_RCTL_CFIEN); 
+		E1000_WRITE_REG(hw, RCTL, rctl); 
+		/* disable VLAN tagging/striping */ 
+		ctrl = E1000_READ_REG(hw, CTRL); 
+		ctrl &= ~E1000_CTRL_VME; 
+		E1000_WRITE_REG(hw, CTRL, ctrl); 
+	}
+	else {
+		/* TODO:  Do we need a way to explicitly turn this off if it was
+		 * previously enabled, or will it magically go back to normal??? --Ben
+		 */
+	}
+	
 	/* 82542 2.0 needs to be in reset to write receive address registers */
 
 	if (hw->mac_type == e1000_82542_rev2_0)
@@ -2508,6 +2540,7 @@ #define E1000_TX_FLAGS_CSUM		0x00000001
 #define E1000_TX_FLAGS_VLAN		0x00000002
 #define E1000_TX_FLAGS_TSO		0x00000004
 #define E1000_TX_FLAGS_IPV4		0x00000008
+#define E1000_TX_FLAGS_NO_FCS		0x00000010
 #define E1000_TX_FLAGS_VLAN_MASK	0xffff0000
 #define E1000_TX_FLAGS_VLAN_SHIFT	16
 
@@ -2764,6 +2797,13 @@ e1000_tx_queue(struct e1000_adapter *ada
 		txd_upper |= (tx_flags & E1000_TX_FLAGS_VLAN_MASK);
 	}
 
+#ifdef CONFIG_SUPPORT_SEND_BAD_CRC
+	if (unlikely(tx_flags & E1000_TX_FLAGS_NO_FCS)) {
+		txd_lower &= ~(E1000_TXD_CMD_IFCS);
+		/* printk("Disabling CRC in tx_queue, txd_lower: 0x%x\n", txd_lower); */
+	}
+#endif
+	
 	i = tx_ring->next_to_use;
 
 	while (count--) {
@@ -2778,6 +2818,14 @@ e1000_tx_queue(struct e1000_adapter *ada
 
 	tx_desc->lower.data |= cpu_to_le32(adapter->txd_cmd);
 
+#ifdef CONFIG_SUPPORT_SEND_BAD_CRC
+	/* txd_cmd re-enables FCS, so we'll re-disable it here as desired. */
+	if (unlikely(tx_flags & E1000_TX_FLAGS_NO_FCS)) {
+		tx_desc->lower.data &= ~(cpu_to_le32(E1000_TXD_CMD_IFCS));
+		/* printk("Disabling2 CRC in tx_queue, txd_lower: 0x%x\n", tx_desc->lower.data); */
+	}
+#endif
+	
 	/* Force memory writes to complete before letting h/w
 	 * know there are new descriptors to fetch.  (Only
 	 * applicable for weak-ordered memory model archs,
@@ -3015,6 +3063,12 @@ #endif
 	if (likely(skb->protocol == htons(ETH_P_IP)))
 		tx_flags |= E1000_TX_FLAGS_IPV4;
 
+#ifdef CONFIG_SUPPORT_SEND_BAD_CRC
+	if (unlikely(skb->use_specified_ether_crc)) {
+		tx_flags |= E1000_TX_FLAGS_NO_FCS;
+	}
+#endif	
+
 	e1000_tx_queue(adapter, tx_ring, tx_flags,
 	               e1000_tx_map(adapter, tx_ring, skb, first,
 	                            max_per_txd, nr_frags, mss));
@@ -3684,7 +3738,11 @@ #endif
 			goto next_desc;
 		}
 
-		if (unlikely(rx_desc->errors & E1000_RXD_ERR_FRAME_ERR_MASK)) {
+		/* If we are accepting all frames, then do not pay attention to the
+		 * framing errors.
+		 */
+ 		if (unlikely(rx_desc->errors & E1000_RXD_ERR_FRAME_ERR_MASK) &&
+		    !(netdev->priv_flags & IFF_ACCEPT_ALL_FRAMES)) {
 			last_byte = *(skb->data + length - 1);
 			if (TBI_ACCEPT(&adapter->hw, status,
 			              rx_desc->errors, length, last_byte)) {
@@ -3702,6 +3760,16 @@ #endif
 			}
 		}
 
+
+		// This may not be needed now. --Ben
+		//if (netdev->priv_flags & IFF_SAVE_FCS) {
+		//		skb_put(skb, length);
+		//}
+		//else {
+		//	skb_put(skb, length - ETHERNET_FCS_SIZE);
+		//}
+
+		
 		/* code added for copybreak, this should improve
 		 * performance for small packets with large amounts
 		 * of reassembly being done in the stack */
@@ -3839,7 +3907,8 @@ #endif
 			goto next_desc;
 		}
 
-		if (unlikely(staterr & E1000_RXDEXT_ERR_FRAME_ERR_MASK)) {
+		if ((unlikely(staterr & E1000_RXDEXT_ERR_FRAME_ERR_MASK)) &&
+		    !(netdev->priv_flags & IFF_ACCEPT_ALL_FRAMES)) {
 			dev_kfree_skb_irq(skb);
 			goto next_desc;
 		}
diff --git a/fs/cifs/cifsglob.h b/fs/cifs/cifsglob.h
index b24006c..0ed7e16 100644
--- a/fs/cifs/cifsglob.h
+++ b/fs/cifs/cifsglob.h
@@ -120,6 +120,7 @@ struct TCP_Server_Info {
 		struct sockaddr_in sockAddr;
 		struct sockaddr_in6 sockAddr6;
 	} addr;
+	u32 ip4_local_ip; /* if != 0, will bind locally to this IP */
 	wait_queue_head_t response_q; 
 	wait_queue_head_t request_q; /* if more than maxmpx to srvr must block*/
 	struct list_head pending_mid_q;
diff --git a/fs/cifs/connect.c b/fs/cifs/connect.c
index 5d394c7..0327e0c 100644
--- a/fs/cifs/connect.c
+++ b/fs/cifs/connect.c
@@ -88,13 +88,15 @@ struct smb_vol {
 	unsigned int rsize;
 	unsigned int wsize;
 	unsigned int sockopt;
+	u32 local_ip; /* allow binding to a local IP address if != 0 */
 	unsigned short int port;
 };
 
 static int ipv4_connect(struct sockaddr_in *psin_server, 
 			struct socket **csocket,
 			char * netb_name,
-			char * server_netb_name);
+			char * server_netb_name,
+			u32 local_ip);
 static int ipv6_connect(struct sockaddr_in6 *psin_server, 
 			struct socket **csocket);
 
@@ -189,7 +191,8 @@ cifs_reconnect(struct TCP_Server_Info *s
 			rc = ipv4_connect(&server->addr.sockAddr, 
 					&server->ssocket,
 					server->workstation_RFC1001_name,
-					server->server_RFC1001_name);
+					  server->server_RFC1001_name,
+					  server->ip4_local_ip);
 		}
 		if(rc) {
 			cFYI(1,("reconnect error %d",rc));
@@ -993,6 +996,18 @@ #endif
 				printk(KERN_WARNING "CIFS: domain name too long\n");
 				return 1;
 			}
+		} else if (strnicmp(data, "local_ip", 8) == 0) {
+			if (!value || !*value) {
+				printk(KERN_WARNING "CIFS: local_ip value not specified.\n");
+				return 1;	/* needs_arg; */
+			}
+			i = cifs_inet_pton(AF_INET, value, &(vol->local_ip));
+			if (i < 0) {
+				vol->local_ip = 0;
+				printk(KERN_WARNING "CIFS:  Could not parse local_ip: %s\n",
+				       value);
+				return 1;
+			}
 		} else if (strnicmp(data, "iocharset", 9) == 0) {
 			if (!value || !*value) {
 				printk(KERN_WARNING "CIFS: invalid iocharset specified\n");
@@ -1231,7 +1246,8 @@ #endif
 static struct cifsSesInfo *
 cifs_find_tcp_session(struct in_addr * target_ip_addr, 
 		struct in6_addr *target_ip6_addr,
-		 char *userName, struct TCP_Server_Info **psrvTcp)
+		      char *userName, struct TCP_Server_Info **psrvTcp,
+		      u32 local_ip)
 {
 	struct list_head *tmp;
 	struct cifsSesInfo *ses;
@@ -1241,7 +1257,11 @@ cifs_find_tcp_session(struct in_addr * t
 	list_for_each(tmp, &GlobalSMBSessionList) {
 		ses = list_entry(tmp, struct cifsSesInfo, cifsSessionList);
 		if (ses->server) {
-			if((target_ip_addr && 
+			if((target_ip_addr &&
+			    /* If binding to a local IP, do not re-use sessions bound to different
+			     * local IP addresses.
+			     */
+			    (local_ip == ses->server->ip4_local_ip) &&
 				(ses->server->addr.sockAddr.sin_addr.s_addr
 				  == target_ip_addr->s_addr)) || (target_ip6_addr
 				&& memcmp(&ses->server->addr.sockAddr6.sin6_addr,
@@ -1264,7 +1284,7 @@ cifs_find_tcp_session(struct in_addr * t
 }
 
 static struct cifsTconInfo *
-find_unc(__be32 new_target_ip_addr, char *uncName, char *userName)
+find_unc(__be32 new_target_ip_addr, char *uncName, char *userName, u32 local_ip)
 {
 	struct list_head *tmp;
 	struct cifsTconInfo *tcon;
@@ -1279,8 +1299,9 @@ find_unc(__be32 new_target_ip_addr, char
 				     ("old ip addr: %x == new ip %x ?",
 				      tcon->ses->server->addr.sockAddr.sin_addr.
 				      s_addr, new_target_ip_addr));
-				if (tcon->ses->server->addr.sockAddr.sin_addr.
-				    s_addr == new_target_ip_addr) {
+				if ((local_ip == tcon->ses->server->ip4_local_ip) &&
+				    (tcon->ses->server->addr.sockAddr.sin_addr.
+				     s_addr == new_target_ip_addr)) {
 	/* BB lock tcon, server and tcp session and increment use count here? */
 					/* found a match on the TCP session */
 					/* BB check if reconnection needed */
@@ -1382,7 +1403,8 @@ static void rfc1002mangle(char * target,
 
 static int
 ipv4_connect(struct sockaddr_in *psin_server, struct socket **csocket, 
-	     char * netbios_name, char * target_name)
+	     char * netbios_name, char * target_name,
+	     u32 local_ip /* in network byte order */)
 {
 	int rc = 0;
 	int connected = 0;
@@ -1401,6 +1423,24 @@ ipv4_connect(struct sockaddr_in *psin_se
 		}
 	}
 
+	/* Bind to the local IP address if specified */
+	if (local_ip) {
+		struct sockaddr_in myaddr = {
+			.sin_family = AF_INET,
+		};
+		myaddr.sin_addr.s_addr = local_ip;
+		myaddr.sin_port = 0; /* any */
+		rc = (*csocket)->ops->bind(*csocket, (struct sockaddr *) &myaddr,
+					   sizeof(myaddr));
+		if (rc < 0) {
+			printk("Tried to bind to local ip: 0x%x, but failed with error: %d\n",
+			       local_ip, rc);
+		}
+		else {
+			printk("CIFS:  Successfully bound to local ip: 0x%x\n", local_ip);
+		}
+	}
+	
 	psin_server->sin_family = AF_INET;
 	if(psin_server->sin_port) { /* user overrode default port */
 		rc = (*csocket)->ops->connect(*csocket,
@@ -1680,11 +1720,12 @@ cifs_mount(struct super_block *sb, struc
 	if(address_type == AF_INET)
 		existingCifsSes = cifs_find_tcp_session(&sin_server.sin_addr,
 			NULL /* no ipv6 addr */,
-			volume_info.username, &srvTcp);
+			volume_info.username, &srvTcp,
+			volume_info.local_ip);
 	else if(address_type == AF_INET6)
 		existingCifsSes = cifs_find_tcp_session(NULL /* no ipv4 addr */,
 			&sin_server6.sin6_addr,
-			volume_info.username, &srvTcp);
+			volume_info.username, &srvTcp, 0);
 	else {
 		kfree(volume_info.UNC);
 		kfree(volume_info.password);
@@ -1702,7 +1743,8 @@ cifs_mount(struct super_block *sb, struc
 			sin_server.sin_port = 0;
 		rc = ipv4_connect(&sin_server,&csocket,
 				  volume_info.source_rfc1001_name,
-				  volume_info.target_rfc1001_name);
+				  volume_info.target_rfc1001_name,
+				  volume_info.local_ip);
 		if (rc < 0) {
 			cERROR(1,
 			       ("Error connecting to IPv4 socket. Aborting operation"));
@@ -1729,6 +1771,7 @@ cifs_mount(struct super_block *sb, struc
 			/* BB Add code for ipv6 case too */
 			srvTcp->ssocket = csocket;
 			srvTcp->protocolType = IPV4;
+			srvTcp->ip4_local_ip = volume_info.local_ip;
 			init_waitqueue_head(&srvTcp->response_q);
 			init_waitqueue_head(&srvTcp->request_q);
 			INIT_LIST_HEAD(&srvTcp->pending_mid_q);
@@ -1862,7 +1905,7 @@ cifs_mount(struct super_block *sb, struc
 
 		tcon =
 		    find_unc(sin_server.sin_addr.s_addr, volume_info.UNC,
-			     volume_info.username);
+			     volume_info.username, volume_info.local_ip);
 		if (tcon) {
 			cFYI(1, ("Found match on UNC path"));
 			/* we can have only one retry value for a connection
diff --git a/fs/nfs/super.c b/fs/nfs/super.c
index e8a9bee..5ac99b6 100644
--- a/fs/nfs/super.c
+++ b/fs/nfs/super.c
@@ -702,6 +702,11 @@ nfs_create_client(struct nfs_server *ser
 				__FUNCTION__, PTR_ERR(xprt));
 		return (struct rpc_clnt *)xprt;
 	}
+	if (data->local_ip != 0) {
+		printk("nfs: Configuring local ip address as: 0x%x\n",
+		       data->local_ip);
+	}
+	xprt->local_address = data->local_ip; /* specify local IP address */
 	clnt = rpc_create_client(xprt, server->hostname, &nfs_program,
 				 server->rpc_ops->version, data->pseudoflavor);
 	if (IS_ERR(clnt)) {
@@ -955,6 +960,11 @@ static int nfs_compare_super(struct supe
 		return 0;
 	if (old->addr.sin_port != server->addr.sin_port)
 		return 0;
+	if (old->local_ip != server->local_ip) {
+		/*printk("nfs_compare_super, old->ip: %x  server->ip: %x\n",
+		         old->local_ip, server->local_ip); */
+		return 0;
+	}
 	return !nfs_compare_fh(&old->fh, &server->fh);
 }
 
@@ -1037,6 +1047,10 @@ #endif /* CONFIG_NFS_V3 */
 		goto out_err;
 	}
 
+	server->local_ip = data->local_ip; /* Allow unique local mounts when
+					    * binding to local IP addresses.
+					    */
+
 	/* Fire up rpciod if not yet running */
 	error = rpciod_up();
 	if (error < 0) {
@@ -1272,6 +1286,11 @@ static int nfs4_compare_super(struct sup
 		return 0;
 	if (strcmp(server->mnt_path, old->mnt_path) != 0)
 		return 0;
+	if (old->local_ip != server->local_ip) {
+		/*printk("nfs_compare_super, old->ip: %x  server->ip: %x\n",
+		         old->local_ip, server->local_ip); */
+		return 0;
+	}
 	return 1;
 }
 
diff --git a/include/asm-i386/socket.h b/include/asm-i386/socket.h
index 5755d57..3c7c7d2 100644
--- a/include/asm-i386/socket.h
+++ b/include/asm-i386/socket.h
@@ -50,4 +50,8 @@ #define SO_ACCEPTCONN		30
 #define SO_PEERSEC		31
 #define SO_PASSSEC		34
 
+/* Instruct lower device to not calculate the frame
+ * checksum.  Useful only for testing, afaik. --Ben */
+#define SO_NOFCS               50
+
 #endif /* _ASM_SOCKET_H */
diff --git a/include/asm-x86_64/socket.h b/include/asm-x86_64/socket.h
index b467026..cffefd4 100644
--- a/include/asm-x86_64/socket.h
+++ b/include/asm-x86_64/socket.h
@@ -50,4 +50,9 @@ #define SO_ACCEPTCONN		30
 #define SO_PEERSEC             31
 #define SO_PASSSEC		34
 
+/* Instruct lower device to not calculate the frame
+ * checksum.  Useful only for testing, afaik. --Ben */
+#define SO_NOFCS               50
+
+
 #endif /* _ASM_SOCKET_H */
diff --git a/include/linux/compat_ioctl.h b/include/linux/compat_ioctl.h
index bea0255..d5ceec2 100644
--- a/include/linux/compat_ioctl.h
+++ b/include/linux/compat_ioctl.h
@@ -266,6 +266,10 @@ COMPATIBLE_IOCTL(SIOCGMIIREG)
 COMPATIBLE_IOCTL(SIOCSMIIREG)
 COMPATIBLE_IOCTL(SIOCGIFVLAN)
 COMPATIBLE_IOCTL(SIOCSIFVLAN)
+COMPATIBLE_IOCTL(SIOCSIFMACVLAN)
+COMPATIBLE_IOCTL(SIOCGIFMACVLAN)
+COMPATIBLE_IOCTL(SIOCGIFREDIRDEV)
+COMPATIBLE_IOCTL(SIOCSIFREDIRDEV)
 COMPATIBLE_IOCTL(SIOCBRADDBR)
 COMPATIBLE_IOCTL(SIOCBRDELBR)
 /* SG stuff */
diff --git a/include/linux/ethtool.h b/include/linux/ethtool.h
index c6310ae..8ed7f16 100644
--- a/include/linux/ethtool.h
+++ b/include/linux/ethtool.h
@@ -1,4 +1,4 @@
-/*
+/* -*-linux-c-*-
  * ethtool.h: Defines for Linux ethtool.
  *
  * Copyright (C) 1998 David S. Miller (davem@redhat.com)
@@ -309,7 +309,11 @@ int ethtool_op_set_ufo(struct net_device
  * phys_id: Identify the device
  * get_stats: Return statistics about the device
  * get_perm_addr: Gets the permanent hardware address
- * 
+ * set_rx_all: Set or clear IFF_ACCEPT_ALL_FRAMES, see if.h
+ * get_rx_all: Return 1 if set, 0 if not.
+ * set_save_fcs: Set or clear IFF_SAVE_FCS, see if.h
+ * get_save_fcs: Return 1 if set, 0 if not.
+ *
  * Description:
  *
  * get_settings:
@@ -368,6 +372,10 @@ struct ethtool_ops {
 	int	(*get_stats_count)(struct net_device *);
 	void	(*get_ethtool_stats)(struct net_device *, struct ethtool_stats *, u64 *);
 	int	(*get_perm_addr)(struct net_device *, struct ethtool_perm_addr *, u8 *);
+	int      (*set_rx_all)(struct net_device *, u32);
+	int      (*get_rx_all)(struct net_device *, u32 *);
+	int      (*set_save_fcs)(struct net_device *, u32);
+	int      (*get_save_fcs)(struct net_device *, u32 *);	
 	int	(*begin)(struct net_device *);
 	void	(*complete)(struct net_device *);
 	u32     (*get_ufo)(struct net_device *);
@@ -375,6 +383,13 @@ struct ethtool_ops {
 };
 #endif /* __KERNEL__ */
 
+/* for dumping net-device statistics */
+struct ethtool_ndstats {
+	u32	cmd;		/* ETHTOOL_GNDSTATS */
+	u8	data[0];        /* sizeof(struct net_device_stats) */
+};
+
+
 /* CMDs currently supported */
 #define ETHTOOL_GSET		0x00000001 /* Get settings. */
 #define ETHTOOL_SSET		0x00000002 /* Set settings. */
@@ -414,6 +429,15 @@ #define ETHTOOL_SUFO		0x00000022 /* Set 
 #define ETHTOOL_GGSO		0x00000023 /* Get GSO enable (ethtool_value) */
 #define ETHTOOL_SGSO		0x00000024 /* Set GSO enable (ethtool_value) */
 
+
+#define ETHTOOL_GNDSTATS         0x00000070 /* get standard net-device statistics */
+#define ETHTOOL_GETRXALL         0x00000071 /* Retrieve whether or not
+					     * IFF_ACCEPT_ALL_FRAMES is set. */
+#define ETHTOOL_SETRXALL         0x00000072 /* Set IFF_ACCEPT_ALL_FRAMES */
+#define ETHTOOL_GETRXFCS         0x00000073 /* Set IFF_SAVE_FCS */
+#define ETHTOOL_SETRXFCS         0x00000074 /* Set IFF_SAVE_FCS */
+
+
 /* compatibility with older code */
 #define SPARC_ETH_GSET		ETHTOOL_GSET
 #define SPARC_ETH_SSET		ETHTOOL_SSET
diff --git a/include/linux/if.h b/include/linux/if.h
index 374e20a..019717c 100644
--- a/include/linux/if.h
+++ b/include/linux/if.h
@@ -60,6 +60,19 @@ #define IFF_SLAVE_INACTIVE	0x4	/* bondin
 #define IFF_MASTER_8023AD	0x8	/* bonding master, 802.3ad. 	*/
 #define IFF_MASTER_ALB	0x10		/* bonding master, balance-alb.	*/
 
+#define IFF_ACCEPT_LOCAL_ADDRS  0x0200     /**  Accept pkts even if they come from a local
+                                         * address.  This lets use send pkts to ourselves
+                                         * over external interfaces (when used in conjunction
+                                         * with SO_BINDTODEVICE
+                                         */
+#define IFF_ACCEPT_ALL_FRAMES  0x0400     /** Accept all frames, even ones with bad CRCs.
+                                         * Should only be used in debugging/testing situations
+                                         * Do NOT enable this unless you understand the
+                                         * consequences! */
+#define IFF_SAVE_FCS           0x0800     /** Save the Frame Check Sum (FCS) on receive, if
+                                         * possible. */
+#define IFF_MAC_VLAN           0x1000     /* MAC VLAN device.          */
+
 #define IF_GET_IFACE	0x0001		/* for querying only */
 #define IF_GET_PROTO	0x0002
 
diff --git a/include/linux/if_macvlan.h b/include/linux/if_macvlan.h
new file mode 100644
index 0000000..0f56ae5
--- /dev/null
+++ b/include/linux/if_macvlan.h
@@ -0,0 +1,58 @@
+/* -*- linux-c -*- */
+#ifndef _LINUX_IF_MACVLAN_H
+#define _LINUX_IF_MACVLAN_H
+
+/* the ioctl commands */
+
+/* actions */
+#define MACVLAN_ENABLE         1
+#define MACVLAN_DISABLE        2
+#define MACVLAN_ADD            3
+#define MACVLAN_DEL            4
+//#define MACVLAN_BIND           5
+//#define MACVLAN_UNBIND         6
+
+/* informative */
+#define MACVLAN_GET_NUM_PORTS  7
+#define MACVLAN_GET_PORT_NAME  8
+#define MACVLAN_GET_NUM_VLANS  9
+#define MACVLAN_GET_VLAN_NAME  10
+//#define MACVLAN_GET_NUM_MACS   11
+//#define MACVLAN_GET_MAC_NAME   12
+
+#define MACVLAN_SET_PORT_FLAGS 13
+#define MACVLAN_GET_PORT_FLAGS 14
+
+/* If this IOCTL succeedes, we are a MAC-VLAN interface, otherwise, we are not. */
+#define MACVLAN_IS_MACVLAN     15
+#define MACVLAN_IS_MACVLAN2    16 /* new ioctl API */
+
+
+#ifdef __KERNEL__
+#include <linux/if.h>
+#include <linux/netdevice.h>
+extern int (*macvlan_ioctl_hook)(unsigned long arg);
+
+/* Returns >= 0 if it consumed the packet, otherwise let the pkt
+ * be processed by the netif_rx method, as if macvlan's didn't
+ * exist.
+ */
+extern int (*macvlan_handle_frame_hook)(struct sk_buff *skb);
+#endif
+
+struct macvlan_ioctl_reply {
+        int32_t num;
+        char name[IFNAMSIZ];
+};
+
+struct macvlan_ioctl {
+        int32_t cmd;
+        int32_t portidx;
+        int32_t ifidx; /* flags when setting port flags */
+        int32_t macaddridx;
+        char ifname[IFNAMSIZ];
+        unsigned char macaddr[8];
+        struct macvlan_ioctl_reply reply;
+};
+
+#endif
diff --git a/include/linux/if_redirdev.h b/include/linux/if_redirdev.h
new file mode 100644
index 0000000..2acdd77
--- /dev/null
+++ b/include/linux/if_redirdev.h
@@ -0,0 +1,34 @@
+/* -*- linux-c -*- */
+#ifndef _LINUX_IF_REDIRDEV_H
+#define _LINUX_IF_REDIRDEV_H
+
+/* the ioctl commands */
+
+#define REDIRDEV_ADD           2090
+#define REDIRDEV_DEL           2091
+/* If this IOCTL succeedes, we are a Redirect-Device
+   interface, otherwise, we are not. */
+#define REDIRDEV_IS_REDIRDEV   2092
+#define REDIRDEV_GET_BY_IDX    2093
+#define REDIRDEV_GET_BY_NAME   2094
+
+#ifdef __KERNEL__
+#include <linux/if.h>
+#include <linux/netdevice.h>
+extern int (*redirdev_ioctl_hook)(void*);
+
+#endif
+
+/* Request and response */
+struct redirdev_ioctl {
+	u32  cmd;
+	u32 ifidx; /* when getting info by idx */
+
+#define RDD_ASSOCIATED (1<<0)
+	u32 flags; /* 1<<0:  Is the interface associated with tx-dev or not */
+	u32 not_used; /* explicitly align 64-bit */
+	char ifname[IFNAMSIZ];
+	char txifname[IFNAMSIZ];
+};
+
+#endif
diff --git a/include/linux/netdevice.h b/include/linux/netdevice.h
index 50a4719..61c60b1 100644
--- a/include/linux/netdevice.h
+++ b/include/linux/netdevice.h
@@ -158,6 +158,7 @@ #include <linux/skbuff.h>
 struct neighbour;
 struct neigh_parms;
 struct sk_buff;
+struct pktgen_dev;
 
 struct netif_rx_stats
 {
@@ -310,6 +311,11 @@ #define NETIF_F_HW_VLAN_FILTER	512	/* Re
 #define NETIF_F_VLAN_CHALLENGED	1024	/* Device cannot handle VLAN packets */
 #define NETIF_F_GSO		2048	/* Enable software GSO. */
 #define NETIF_F_LLTX		4096	/* LockLess TX */
+#define NETIF_F_RX_ALL          16384   /* Can be configured to receive all packets, even
+					 * ones with busted CRC.  May disable VLAN filtering
+					 * in the NIC, users should NOT enable this feature
+					 * unless they understand the consequences. */
+#define NETIF_F_SAVE_CRC        32768 /* Can save FCS in skb, last 4 bytes for ethernet */
 
 	/* Segmentation offload features */
 #define NETIF_F_GSO_SHIFT	16
@@ -513,9 +519,22 @@ #ifdef CONFIG_NET_POLL_CONTROLLER
 	void                    (*poll_controller)(struct net_device *dev);
 #endif
 
+        /* Callback for when the queue is woken, used by pktgen currently */
+        int			(*notify_queue_woken)(struct net_device *dev);
+        void* nqw_data; /* To be used by the method above as needed */
+
+	struct pktgen_dev* pkt_dev; /* to quickly find the pkt-gen dev registered with this
+				     * interface, if any.
+				     */
+	
 	/* bridge stuff */
 	struct net_bridge_port	*br_port;
 
+#if defined(CONFIG_MACVLAN) || defined(CONFIG_MACVLAN_MODULE)
+        struct macvlan_port *macvlan_priv;
+#endif
+
+
 #ifdef CONFIG_NET_DIVERT
 	/* this will get initialized at each interface type init routine */
 	struct divert_blk	*divert;
@@ -642,8 +661,13 @@ #ifdef CONFIG_NETPOLL_TRAP
 	if (netpoll_trap())
 		return;
 #endif
-	if (test_and_clear_bit(__LINK_STATE_XOFF, &dev->state))
+	if (test_and_clear_bit(__LINK_STATE_XOFF, &dev->state)) {
 		__netif_schedule(dev);
+
+                if (dev->notify_queue_woken) {
+                   dev->notify_queue_woken(dev);
+                }
+        }
 }
 
 static inline void netif_stop_queue(struct net_device *dev)
diff --git a/include/linux/nfs_fs_sb.h b/include/linux/nfs_fs_sb.h
index 6b4a13c..cfa089e 100644
--- a/include/linux/nfs_fs_sb.h
+++ b/include/linux/nfs_fs_sb.h
@@ -37,6 +37,8 @@ struct nfs_server {
 	struct sockaddr_in	addr;
 	struct nfs_fsid		fsid;
 	unsigned long		mount_time;	/* when this fs was mounted */
+	u32			local_ip; /* Allow local binding in .v3 */
+	
 #ifdef CONFIG_NFS_V4
 	/* Our own IP address, as a null-terminated string.
 	 * This is used to generate the clientid, and the callback address.
diff --git a/include/linux/nfs_mount.h b/include/linux/nfs_mount.h
index 659c754..77700de 100644
--- a/include/linux/nfs_mount.h
+++ b/include/linux/nfs_mount.h
@@ -20,7 +20,7 @@ #include <linux/nfs3.h>
  * mount-to-kernel version compatibility.  Some of these aren't used yet
  * but here they are anyway.
  */
-#define NFS_MOUNT_VERSION	6
+#define NFS_MOUNT_VERSION	7
 #define NFS_MAX_CONTEXT_LEN	256
 
 struct nfs_mount_data {
@@ -43,6 +43,8 @@ struct nfs_mount_data {
 	struct nfs3_fh	root;			/* 4 */
 	int		pseudoflavor;		/* 5 */
 	char		context[NFS_MAX_CONTEXT_LEN + 1];	/* 6 */
+	char            pad[3];                 /* 7  Align the context above */
+	unsigned int    local_ip;               /* 7 */
 };
 
 /* bits in the flags field */
diff --git a/include/linux/rtnetlink.h b/include/linux/rtnetlink.h
index facd9ee..c1217e4 100644
--- a/include/linux/rtnetlink.h
+++ b/include/linux/rtnetlink.h
@@ -238,9 +238,8 @@ enum rt_class_t
 	RT_TABLE_DEFAULT=253,
 	RT_TABLE_MAIN=254,
 	RT_TABLE_LOCAL=255,
-	__RT_TABLE_MAX
 };
-#define RT_TABLE_MAX (__RT_TABLE_MAX - 1)
+#define RT_TABLE_MAX 0xFFFFFFFF
 
 
 
@@ -263,6 +262,7 @@ enum rtattr_type_t
 	RTA_CACHEINFO,
 	RTA_SESSION,
 	RTA_MP_ALGO,
+	RTA_TABLE,
 	__RTA_MAX
 };
 
@@ -1065,6 +1065,13 @@ #define BUG_TRAP(x) do { \
 	} \
 } while(0)
 
+static inline u32 rtm_get_table(struct rtmsg *rtm, struct rtattr **rta)
+{
+	return RTA_GET_U32(rta[RTA_TABLE-1]);
+rtattr_failure:
+	return rtm->rtm_table;
+}
+
 #endif /* __KERNEL__ */
 
 
diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h
index 755e9cd..73d06aa 100644
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -282,7 +282,11 @@ struct sk_buff {
 				nfctinfo:3;
 	__u8			pkt_type:3,
 				fclone:2,
-				ipvs_property:1;
+				ipvs_property:1,
+	/* Use the last 4 bytes of the payload for the ethernet CRC.  Only supported on some
+	 * NICs, such as e1000. --Ben
+	 */
+				use_specified_ether_crc:1;
 	__be16			protocol;
 
 	void			(*destructor)(struct sk_buff *skb);
diff --git a/include/linux/sockios.h b/include/linux/sockios.h
index e6b9d1d..f6c8e31 100644
--- a/include/linux/sockios.h
+++ b/include/linux/sockios.h
@@ -94,6 +94,13 @@ #define SIOCDRARP	0x8960		/* delete RARP
 #define SIOCGRARP	0x8961		/* get RARP table entry		*/
 #define SIOCSRARP	0x8962		/* set RARP table entry		*/
 
+/* MAC address based VLAN control calls */
+#define SIOCGIFMACVLAN	0x8965		/* Mac address multiplex/demultiplex support */
+#define SIOCSIFMACVLAN	0x8966		/* Set macvlan options 	*/
+
+#define SIOCGIFREDIRDEV	0x8967		/* Redirect device get ioctl */
+#define SIOCSIFREDIRDEV	0x8968		/* Set redirect dev options */
+
 /* Driver configuration calls */
 
 #define SIOCGIFMAP	0x8970		/* Get device parameters	*/
@@ -122,6 +129,15 @@ #define SIOCBRDELBR     0x89a1		/* remov
 #define SIOCBRADDIF	0x89a2		/* add interface to bridge      */
 #define SIOCBRDELIF	0x89a3		/* remove interface from bridge */
 
+/* Ben's little hack land */
+#define SIOCSACCEPTLOCALADDRS  0x89ba   /*  Allow interfaces to accept pkts from
+                                         * local interfaces...use with SO_BINDTODEVICE
+                                         */
+#define SIOCGACCEPTLOCALADDRS  0x89bb   /*  Allow interfaces to accept pkts from
+                                         * local interfaces...use with SO_BINDTODEVICE
+                                         */
+
+
 /* Device private ioctl calls */
 
 /*
diff --git a/include/linux/sunrpc/xprt.h b/include/linux/sunrpc/xprt.h
index 3a0cca2..927bbc6 100644
--- a/include/linux/sunrpc/xprt.h
+++ b/include/linux/sunrpc/xprt.h
@@ -171,7 +171,8 @@ struct rpc_xprt {
 				reestablish_timeout;
 	struct work_struct	connect_worker;
 	unsigned short		port;
-
+	u32                     local_address; /* local IP address to bind to */
+	
 	/*
 	 * Disconnection of idle transports
 	 */
diff --git a/include/net/dn_fib.h b/include/net/dn_fib.h
index a15dcf0..9464f48 100644
--- a/include/net/dn_fib.h
+++ b/include/net/dn_fib.h
@@ -94,7 +94,7 @@ #define DN_FIB_INFO(f) ((f)->fn_info)
 
 
 struct dn_fib_table {
-	int n;
+	u32 n;
 
 	int (*insert)(struct dn_fib_table *t, struct rtmsg *r, 
 			struct dn_kern_rta *rta, struct nlmsghdr *n, 
@@ -137,7 +137,7 @@ extern int dn_fib_sync_up(struct net_dev
 /*
  * dn_tables.c
  */
-extern struct dn_fib_table *dn_fib_get_table(int n, int creat);
+extern struct dn_fib_table *dn_fib_get_table(u32 n, int creat);
 extern struct dn_fib_table *dn_fib_empty_table(void);
 extern void dn_fib_table_init(void);
 extern void dn_fib_table_cleanup(void);
diff --git a/include/net/ip_fib.h b/include/net/ip_fib.h
index a095d1d..4b764e2 100644
--- a/include/net/ip_fib.h
+++ b/include/net/ip_fib.h
@@ -149,7 +149,8 @@ #define FIB_RES_NETMASK(res)	        (0)
 #endif /* CONFIG_IP_ROUTE_MULTIPATH_WRANDOM */
 
 struct fib_table {
-	unsigned char	tb_id;
+	struct hlist_node tb_hlist;
+	u32		tb_id;
 	unsigned	tb_stamp;
 	int		(*tb_lookup)(struct fib_table *tb, const struct flowi *flp, struct fib_result *res);
 	int		(*tb_insert)(struct fib_table *table, struct rtmsg *r,
@@ -172,14 +173,14 @@ #ifndef CONFIG_IP_MULTIPLE_TABLES
 extern struct fib_table *ip_fib_local_table;
 extern struct fib_table *ip_fib_main_table;
 
-static inline struct fib_table *fib_get_table(int id)
+static inline struct fib_table *fib_get_table(u32 id)
 {
 	if (id != RT_TABLE_LOCAL)
 		return ip_fib_main_table;
 	return ip_fib_local_table;
 }
 
-static inline struct fib_table *fib_new_table(int id)
+static inline struct fib_table *fib_new_table(u32 id)
 {
 	return fib_get_table(id);
 }
@@ -199,30 +200,14 @@ static inline void fib_select_default(co
 }
 
 #else /* CONFIG_IP_MULTIPLE_TABLES */
-#define ip_fib_local_table (fib_tables[RT_TABLE_LOCAL])
-#define ip_fib_main_table (fib_tables[RT_TABLE_MAIN])
+#define ip_fib_local_table fib_get_table(RT_TABLE_LOCAL)
+#define ip_fib_main_table fib_get_table(RT_TABLE_MAIN)
 
-extern struct fib_table * fib_tables[RT_TABLE_MAX+1];
 extern int fib_lookup(const struct flowi *flp, struct fib_result *res);
-extern struct fib_table *__fib_new_table(int id);
+extern struct fib_table *fib_new_table(u32 id);
+extern struct fib_table *fib_get_table(u32 id);
 extern void fib_rule_put(struct fib_rule *r);
 
-static inline struct fib_table *fib_get_table(int id)
-{
-	if (id == 0)
-		id = RT_TABLE_MAIN;
-
-	return fib_tables[id];
-}
-
-static inline struct fib_table *fib_new_table(int id)
-{
-	if (id == 0)
-		id = RT_TABLE_MAIN;
-
-	return fib_tables[id] ? : __fib_new_table(id);
-}
-
 extern void fib_select_default(const struct flowi *flp, struct fib_result *res);
 
 #endif /* CONFIG_IP_MULTIPLE_TABLES */
@@ -248,7 +233,7 @@ extern int fib_convert_rtentry(int cmd, 
 extern u32  __fib_res_prefsrc(struct fib_result *res);
 
 /* Exported by fib_hash.c */
-extern struct fib_table *fib_hash_init(int id);
+extern struct fib_table *fib_hash_init(u32 id);
 
 #ifdef CONFIG_IP_MULTIPLE_TABLES
 /* Exported by fib_rules.c */
diff --git a/include/net/sock.h b/include/net/sock.h
index 324b3ea..1d9c9e6 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -391,6 +391,10 @@ enum sock_flags {
 	SOCK_RCVTSTAMP, /* %SO_TIMESTAMP setting */
 	SOCK_LOCALROUTE, /* route locally only, %SO_DONTROUTE setting */
 	SOCK_QUEUE_SHRUNK, /* write queue has been shrunk recently */
+        SOCK_DONT_DO_LL_FCS, /* Tell NIC not to do the ethernet FCS.  Will use
+                              * last 4 bytes of packet sent from user-space
+                              * instead.
+                              */
 };
 
 static inline void sock_copy_flags(struct sock *nsk, struct sock *osk)
diff --git a/net/8021q/vlan_dev.c b/net/8021q/vlan_dev.c
index da9cfe9..e49e252 100644
--- a/net/8021q/vlan_dev.c
+++ b/net/8021q/vlan_dev.c
@@ -439,6 +439,11 @@ int vlan_dev_hard_start_xmit(struct sk_b
 	struct net_device_stats *stats = vlan_dev_get_stats(dev);
 	struct vlan_ethhdr *veth = (struct vlan_ethhdr *)(skb->data);
 
+	/* Please note, dev_queue_xmit consumes the pkt regardless of the
+	 * return value.  So, will copy the skb first and free if successful.
+	 */
+	struct sk_buff* skb2 = skb_get(skb);
+
 	/* Handle non-VLAN frames if they are sent to us, for example by DHCP.
 	 *
 	 * NOTE: THIS ASSUMES DIX ETHERNET, SPECIFICALLY NOT SUPPORTING
@@ -468,6 +473,10 @@ #endif
 		skb = __vlan_put_tag(skb, veth_TCI);
 		if (!skb) {
 			stats->tx_dropped++;
+			/* Free the extra copy, assuming this is a non-recoverable
+			 * issue and we don't want calling code to retry.
+			 */
+			kfree_skb(skb2);
 			return 0;
 		}
 
@@ -485,13 +494,24 @@ #ifdef VLAN_DEBUG
 	       veth->h_vlan_proto, veth->h_vlan_TCI, veth->h_vlan_encapsulated_proto);
 #endif
 
-	stats->tx_packets++; /* for statics only */
-	stats->tx_bytes += skb->len;
-
 	skb->dev = VLAN_DEV_INFO(dev)->real_dev;
-	dev_queue_xmit(skb);
 
-	return 0;
+	{
+		int rv = dev_queue_xmit(skb);
+		if (rv == 0) {
+			/* Was success, need to free the skb reference since
+			 * we bumped up the user count above.  If there was an
+			 * error instead, then the skb2 will not be freed, and so
+			 * the calling code will be able to re-send it.
+			 */
+
+			stats->tx_packets++; /* for statics only */
+			stats->tx_bytes += skb2->len;
+
+			kfree_skb(skb2);
+		}
+		return rv;
+	}
 }
 
 int vlan_dev_hwaccel_hard_start_xmit(struct sk_buff *skb, struct net_device *dev)
diff --git a/net/Kconfig b/net/Kconfig
index 4959a4e..d345e4b 100644
--- a/net/Kconfig
+++ b/net/Kconfig
@@ -168,6 +168,8 @@ source "net/tipc/Kconfig"
 source "net/atm/Kconfig"
 source "net/bridge/Kconfig"
 source "net/8021q/Kconfig"
+source "net/macvlan/Kconfig"
+source "net/redir/Kconfig"
 source "net/decnet/Kconfig"
 source "net/llc/Kconfig"
 source "net/ipx/Kconfig"
@@ -237,6 +239,14 @@ config NET_TCPPROBE
 	To compile this code as a module, choose M here: the
 	module will be called tcp_probe.
 
+config SUPPORT_SEND_BAD_CRC
+	bool "Support Send Bad CRC (USE WITH CAUTION)"
+	---help---
+	  When enabled, one can send a specially crafted packet to the ethernet
+          device via a raw socket and it will be sent with the last 4 bytes of
+          the packet as the ethernet CRC.  Requires driver support.  Current driver
+          support is limited to e100 and e1000.
+
 endmenu
 
 endmenu
diff --git a/net/Makefile b/net/Makefile
index 065796f..0fdf59d 100644
--- a/net/Makefile
+++ b/net/Makefile
@@ -46,6 +46,8 @@ obj-$(CONFIG_IP_DCCP)		+= dccp/
 obj-$(CONFIG_IP_SCTP)		+= sctp/
 obj-$(CONFIG_IEEE80211)		+= ieee80211/
 obj-$(CONFIG_TIPC)		+= tipc/
+obj-$(CONFIG_MACVLAN)		+= macvlan/
+obj-$(CONFIG_REDIRDEV)		+= redir/
 
 ifeq ($(CONFIG_NET),y)
 obj-$(CONFIG_SYSCTL)		+= sysctl_net.o
diff --git a/net/core/dev.c b/net/core/dev.c
index a34f7c6..4d12e94 100644
--- a/net/core/dev.c
+++ b/net/core/dev.c
@@ -89,6 +89,7 @@ #include <linux/errno.h>
 #include <linux/interrupt.h>
 #include <linux/if_ether.h>
 #include <linux/netdevice.h>
+#include <linux/ethtool.h>
 #include <linux/etherdevice.h>
 #include <linux/notifier.h>
 #include <linux/skbuff.h>
@@ -118,6 +119,22 @@ #include <linux/dmaengine.h>
 #include <linux/err.h>
 #include <linux/ctype.h>
 
+#if defined(CONFIG_NET_PKTGEN) || defined(CONFIG_NET_PKTGEN_MODULE)
+#include "pktgen.h"
+
+#warning "Compiling dev.c for pktgen.";
+
+int (*handle_pktgen_hook)(struct sk_buff *skb) = NULL;
+EXPORT_SYMBOL(handle_pktgen_hook);
+
+static __inline__ int handle_pktgen_rcv(struct sk_buff* skb) {
+	if (handle_pktgen_hook) {
+		return handle_pktgen_hook(skb);
+	}
+	return -1;
+}
+#endif
+
 /*
  *	The list of packet types we will receive (as opposed to discard)
  *	and the routines to invoke.
@@ -1760,6 +1777,23 @@ static int ing_filter(struct sk_buff *sk
 }
 #endif
 
+
+#if defined(CONFIG_MACVLAN) || defined(CONFIG_MACVLAN_MODULE)
+/* Returns >= 0 if we consume the packet.  Otherwise, let
+ * it fall through the rest of the packet processing.
+ */
+int (*macvlan_handle_frame_hook)(struct sk_buff *skb) = NULL;
+EXPORT_SYMBOL(macvlan_handle_frame_hook);
+#endif
+
+/* Returns >= 0 if we consume the packet.  Otherwise, let
+ * it fall through the rest of the packet processing.
+ */
+static __inline__ int handle_macvlan(struct sk_buff *skb) {
+	return macvlan_handle_frame_hook(skb);
+}
+
+
 int netif_receive_skb(struct sk_buff *skb)
 {
 	struct packet_type *ptype, *pt_prev;
@@ -1830,6 +1864,32 @@ #endif
 	if (handle_bridge(&skb, &pt_prev, &ret, orig_dev))
 		goto out;
 
+#if defined(CONFIG_MACVLAN) || defined(CONFIG_MACVLAN_MODULE)
+	if (skb->dev->macvlan_priv != NULL &&
+	    macvlan_handle_frame_hook != NULL) {
+		if (handle_macvlan(skb) >= 0) {
+			/* consumed by mac-vlan...it would have been
+			 * re-sent to this method with a different
+			 * device...
+			 */
+			goto out;
+		}
+		else {
+			/* Let it fall through and be processed normally */
+		}
+	}
+#endif
+	
+#if defined(CONFIG_NET_PKTGEN) || defined(CONFIG_NET_PKTGEN_MODULE)
+	if ((skb->dev->pkt_dev) &&
+	    (handle_pktgen_rcv(skb) >= 0)) {
+		/* Pktgen may consume the packet, no need to send
+		 * to further protocols.
+		 */
+		goto out;
+	}
+#endif
+ 	
 	type = skb->protocol;
 	list_for_each_entry_rcu(ptype, &ptype_base[ntohs(type)&15], list) {
 		if (ptype->type == type &&
@@ -2591,6 +2651,24 @@ static int dev_ifsioc(struct ifreq *ifr,
 			ifr->ifr_newname[IFNAMSIZ-1] = '\0';
 			return dev_change_name(dev, ifr->ifr_newname);
 
+                case SIOCSACCEPTLOCALADDRS:
+                        if (ifr->ifr_flags) {
+                                dev->priv_flags |= IFF_ACCEPT_LOCAL_ADDRS;
+                        }
+                        else {
+                                dev->priv_flags &= ~IFF_ACCEPT_LOCAL_ADDRS;
+                        }
+                        return 0;
+
+                case SIOCGACCEPTLOCALADDRS:
+                        if (dev->priv_flags & IFF_ACCEPT_LOCAL_ADDRS) {
+                                ifr->ifr_flags = 1;
+                        }
+                        else {
+                                ifr->ifr_flags = 0;
+                        }
+                        return 0;
+
 		/*
 		 *	Unknown or private ioctl
 		 */
@@ -2689,6 +2767,7 @@ int dev_ioctl(unsigned int cmd, void __u
 		case SIOCGIFMAP:
 		case SIOCGIFINDEX:
 		case SIOCGIFTXQLEN:
+	        case SIOCGACCEPTLOCALADDRS:
 			dev_load(ifr.ifr_name);
 			read_lock(&dev_base_lock);
 			ret = dev_ifsioc(&ifr, cmd);
@@ -2763,6 +2842,7 @@ int dev_ioctl(unsigned int cmd, void __u
 		case SIOCBONDCHANGEACTIVE:
 		case SIOCBRADDIF:
 		case SIOCBRDELIF:
+		case SIOCSACCEPTLOCALADDRS:
 			if (!capable(CAP_NET_ADMIN))
 				return -EPERM;
 			/* fall through */
@@ -3585,6 +3665,10 @@ EXPORT_SYMBOL(net_enable_timestamp);
 EXPORT_SYMBOL(net_disable_timestamp);
 EXPORT_SYMBOL(dev_get_flags);
 
+#if defined(CONFIG_NET_PKTGEN) || defined(CONFIG_NET_PKTGEN_MODULE)
+EXPORT_SYMBOL(handle_pktgen_rcv);
+#endif
+
 #if defined(CONFIG_BRIDGE) || defined(CONFIG_BRIDGE_MODULE)
 EXPORT_SYMBOL(br_handle_frame_hook);
 EXPORT_SYMBOL(br_fdb_get_hook);
diff --git a/net/core/ethtool.c b/net/core/ethtool.c
index 2797e28..9507c5c 100644
--- a/net/core/ethtool.c
+++ b/net/core/ethtool.c
@@ -1,4 +1,4 @@
-/*
+/* -*- linux-c -*-
  * net/core/ethtool.c - Ethtool ioctl handler
  * Copyright (c) 2003 Matthew Wilcox <matthew@wil.cx>
  *
@@ -33,6 +33,12 @@ u32 ethtool_op_get_tx_csum(struct net_de
 	return (dev->features & NETIF_F_ALL_CSUM) != 0;
 }
 
+u32 ethtool_op_get_rx_all(struct net_device *dev, u32* retval)
+{
+	*retval = ((dev->priv_flags & IFF_ACCEPT_ALL_FRAMES) != 0);
+	return 0;
+}
+
 int ethtool_op_set_tx_csum(struct net_device *dev, u32 data)
 {
 	if (data)
@@ -796,6 +802,88 @@ static int ethtool_get_perm_addr(struct 
 	return ret;
 }
 
+
+static int ethtool_get_rx_all(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_value edata = { ETHTOOL_GSG };
+	int rv = 0;
+	
+	if (!dev->ethtool_ops->get_rx_all)
+		return -EOPNOTSUPP;
+
+	if ((rv = dev->ethtool_ops->get_rx_all(dev, &edata.data)) < 0) {
+		return rv;
+	}
+
+	if (copy_to_user(useraddr, &edata, sizeof(edata)))
+		return -EFAULT;
+	return 0;
+}
+
+
+static int ethtool_set_rx_all(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_value id;
+
+	if (!dev->ethtool_ops->set_rx_all)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&id, useraddr, sizeof(id)))
+		return -EFAULT;
+
+	return dev->ethtool_ops->set_rx_all(dev, id.data);
+}
+
+static int ethtool_get_rx_fcs(struct net_device *dev, char *useraddr)
+{
+	struct ethtool_value edata = { ETHTOOL_GSG };
+	int rv = 0;
+	
+	if (!dev->ethtool_ops->get_save_fcs)
+		return -EOPNOTSUPP;
+
+	if ((rv = dev->ethtool_ops->get_save_fcs(dev, &edata.data)) < 0) {
+		return rv;
+	}
+
+	if (copy_to_user(useraddr, &edata, sizeof(edata)))
+		return -EFAULT;
+	return 0;
+}
+
+
+static int ethtool_set_rx_fcs(struct net_device *dev, void *useraddr)
+{
+	struct ethtool_value id;
+
+	if (!dev->ethtool_ops->set_save_fcs)
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&id, useraddr, sizeof(id)))
+		return -EFAULT;
+
+	return dev->ethtool_ops->set_save_fcs(dev, id.data);
+}
+
+
+/* Handle some generic ethtool commands here */
+static int ethtool_get_netdev_stats(struct net_device *dev, void *useraddr) {
+	
+	struct ethtool_ndstats* nds = (struct ethtool_ndstats*)(useraddr);
+      
+	struct net_device_stats *stats = dev->get_stats(dev);
+	if (stats) {
+		if (copy_to_user(nds->data, stats, sizeof(*stats))) {
+			return -EFAULT;
+		}
+	}
+	else {
+		return -EOPNOTSUPP;
+	}
+	return 0;
+}
+
+
 /* The main entry point in this file.  Called from net/core/dev.c */
 
 int dev_ethtool(struct ifreq *ifr)
@@ -816,18 +904,28 @@ int dev_ethtool(struct ifreq *ifr)
 	if (!dev || !netif_device_present(dev))
 		return -ENODEV;
 
-	if (!dev->ethtool_ops)
-		goto ioctl;
-
 	if (copy_from_user(&ethcmd, useraddr, sizeof (ethcmd)))
 		return -EFAULT;
 
-	if(dev->ethtool_ops->begin)
+	if(dev->ethtool_ops && dev->ethtool_ops->begin)
 		if ((rc = dev->ethtool_ops->begin(dev)) < 0)
 			return rc;
 
 	old_features = dev->features;
-
+	
+	/* Handle some generic operations that do not require specific
+ 	 * ethtool handlers.
+ 	 */
+	switch (ethcmd) {
+	case ETHTOOL_GNDSTATS:
+		return ethtool_get_netdev_stats(dev, useraddr);
+	default:
+		break;
+	}
+ 	
+ 	if (!dev->ethtool_ops)
+ 		goto ioctl;
+	
 	switch (ethcmd) {
 	case ETHTOOL_GSET:
 		rc = ethtool_get_settings(dev, useraddr);
@@ -916,6 +1014,18 @@ int dev_ethtool(struct ifreq *ifr)
 	case ETHTOOL_PHYS_ID:
 		rc = ethtool_phys_id(dev, useraddr);
 		break;
+	case ETHTOOL_SETRXALL:
+		rc = ethtool_set_rx_all(dev, useraddr);
+		break;
+	case ETHTOOL_GETRXALL:
+		rc = ethtool_get_rx_all(dev, useraddr);
+		break;
+	case ETHTOOL_SETRXFCS:
+		rc = ethtool_set_rx_fcs(dev, useraddr);
+		break;
+	case ETHTOOL_GETRXFCS:
+		rc = ethtool_get_rx_fcs(dev, useraddr);
+		break;
 	case ETHTOOL_GSTATS:
 		rc = ethtool_get_stats(dev, useraddr);
 		break;
diff --git a/net/core/pktgen.c b/net/core/pktgen.c
index 6a7320b..7ebc5e4 100644
--- a/net/core/pktgen.c
+++ b/net/core/pktgen.c
@@ -156,44 +156,28 @@ #include <asm/dma.h>
 #include <asm/uaccess.h>
 #include <asm/div64.h>		/* do_div */
 #include <asm/timex.h>
+#include <linux/sched.h> /* sched_clock() */
+#include "pktgen.h"
+
+#define USE_NQW_CALLBACK
+
+#ifdef USE_NQW_CALLBACK
+#include <linux/if_vlan.h>
+
+#if defined(CONFIG_MACVLAN) || defined(CONFIG_MACVLAN_MODULE)
+#include <linux/if_macvlan.h>
+#include "../macvlan/macvlan.h"
+#endif
+#endif
+
+#define VERSION  "pktgen v2.68: Packet Generator for packet performance testing.\n"
+
+#define PG_DEBUG(a) a
+//#define PG_DEBUG(a)
 
-#define VERSION  "pktgen v2.67: Packet Generator for packet performance testing.\n"
-
-/* #define PG_DEBUG(a) a */
-#define PG_DEBUG(a)
-
-/* The buckets are exponential in 'width' */
-#define LAT_BUCKETS_MAX 32
-#define IP_NAME_SZ 32
-#define MAX_MPLS_LABELS 16 /* This is the max label stack depth */
-#define MPLS_STACK_BOTTOM __constant_htonl(0x00000100)
-
-/* Device flag bits */
-#define F_IPSRC_RND   (1<<0)	/* IP-Src Random  */
-#define F_IPDST_RND   (1<<1)	/* IP-Dst Random  */
-#define F_UDPSRC_RND  (1<<2)	/* UDP-Src Random */
-#define F_UDPDST_RND  (1<<3)	/* UDP-Dst Random */
-#define F_MACSRC_RND  (1<<4)	/* MAC-Src Random */
-#define F_MACDST_RND  (1<<5)	/* MAC-Dst Random */
-#define F_TXSIZE_RND  (1<<6)	/* Transmit size is random */
-#define F_IPV6        (1<<7)	/* Interface in IPV6 Mode */
-#define F_MPLS_RND    (1<<8)	/* Random MPLS labels */
-
-/* Thread control flag bits */
-#define T_TERMINATE   (1<<0)
-#define T_STOP        (1<<1)	/* Stop run */
-#define T_RUN         (1<<2)	/* Start run */
-#define T_REMDEVALL   (1<<3)	/* Remove all devs */
-#define T_REMDEV      (1<<4)	/* Remove one dev */
 
 /* If lock -- can be removed after some work */
-#define   if_lock(t)           spin_lock(&(t->if_lock));
-#define   if_unlock(t)           spin_unlock(&(t->if_lock));
 
-/* Used to help with determining the pkts on receive */
-#define PKTGEN_MAGIC 0xbe9be955
-#define PG_PROC_DIR "pktgen"
-#define PGCTRL	    "pgctrl"
 static struct proc_dir_entry *pg_proc_dir = NULL;
 
 #define MAX_CFLOWS  65536
@@ -203,158 +187,43 @@ struct flow_state {
 	int count;
 };
 
-struct pktgen_dev {
-
-	/*
-	 * Try to keep frequent/infrequent used vars. separated.
-	 */
-
-	char ifname[IFNAMSIZ];
-	char result[512];
-
-	struct pktgen_thread *pg_thread;	/* the owner */
-	struct list_head list;		/* Used for chaining in the thread's run-queue */
-
-	int running;		/* if this changes to false, the test will stop */
-
-	/* If min != max, then we will either do a linear iteration, or
-	 * we will do a random selection from within the range.
-	 */
-	__u32 flags;
-	int removal_mark;	/* non-zero => the device is marked for
-				 * removal by worker thread */
-
-	int min_pkt_size;	/* = ETH_ZLEN; */
-	int max_pkt_size;	/* = ETH_ZLEN; */
-	int nfrags;
-	__u32 delay_us;		/* Default delay */
-	__u32 delay_ns;
-	__u64 count;		/* Default No packets to send */
-	__u64 sofar;		/* How many pkts we've sent so far */
-	__u64 tx_bytes;		/* How many bytes we've transmitted */
-	__u64 errors;		/* Errors when trying to transmit, pkts will be re-sent */
-
-	/* runtime counters relating to clone_skb */
-	__u64 next_tx_us;	/* timestamp of when to tx next */
-	__u32 next_tx_ns;
-
-	__u64 allocated_skbs;
-	__u32 clone_count;
-	int last_ok;		/* Was last skb sent?
-				 * Or a failed transmit of some sort?  This will keep
-				 * sequence numbers in order, for example.
-				 */
-	__u64 started_at;	/* micro-seconds */
-	__u64 stopped_at;	/* micro-seconds */
-	__u64 idle_acc;		/* micro-seconds */
-	__u32 seq_num;
-
-	int clone_skb;		/* Use multiple SKBs during packet gen.  If this number
-				 * is greater than 1, then that many copies of the same
-				 * packet will be sent before a new packet is allocated.
-				 * For instance, if you want to send 1024 identical packets
-				 * before creating a new packet, set clone_skb to 1024.
-				 */
-
-	char dst_min[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
-	char dst_max[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
-	char src_min[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
-	char src_max[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
-
-	struct in6_addr in6_saddr;
-	struct in6_addr in6_daddr;
-	struct in6_addr cur_in6_daddr;
-	struct in6_addr cur_in6_saddr;
-	/* For ranges */
-	struct in6_addr min_in6_daddr;
-	struct in6_addr max_in6_daddr;
-	struct in6_addr min_in6_saddr;
-	struct in6_addr max_in6_saddr;
-
-	/* If we're doing ranges, random or incremental, then this
-	 * defines the min/max for those ranges.
-	 */
-	__u32 saddr_min;	/* inclusive, source IP address */
-	__u32 saddr_max;	/* exclusive, source IP address */
-	__u32 daddr_min;	/* inclusive, dest IP address */
-	__u32 daddr_max;	/* exclusive, dest IP address */
-
-	__u16 udp_src_min;	/* inclusive, source UDP port */
-	__u16 udp_src_max;	/* exclusive, source UDP port */
-	__u16 udp_dst_min;	/* inclusive, dest UDP port */
-	__u16 udp_dst_max;	/* exclusive, dest UDP port */
-
-	/* MPLS */
-	unsigned nr_labels;	/* Depth of stack, 0 = no MPLS */
-	__be32 labels[MAX_MPLS_LABELS];
-
-	__u32 src_mac_count;	/* How many MACs to iterate through */
-	__u32 dst_mac_count;	/* How many MACs to iterate through */
-
-	unsigned char dst_mac[ETH_ALEN];
-	unsigned char src_mac[ETH_ALEN];
+static char version[] __initdata = VERSION;
 
-	__u32 cur_dst_mac_offset;
-	__u32 cur_src_mac_offset;
-	__u32 cur_saddr;
-	__u32 cur_daddr;
-	__u16 cur_udp_dst;
-	__u16 cur_udp_src;
-	__u32 cur_pkt_size;
-
-	__u8 hh[14];
-	/* = {
-	   0x00, 0x80, 0xC8, 0x79, 0xB3, 0xCB,
-
-	   We fill in SRC address later
-	   0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
-	   0x08, 0x00
-	   };
-	 */
-	__u16 pad;		/* pad out the hh struct to an even 16 bytes */
 
-	struct sk_buff *skb;	/* skb we are to transmit next, mainly used for when we
-				 * are transmitting the same one multiple times
-				 */
-	struct net_device *odev;	/* The out-going device.  Note that the device should
-					 * have it's pg_info pointer pointing back to this
-					 * device.  This will be set when the user specifies
-					 * the out-going device name (not when the inject is
-					 * started as it used to do.)
-					 */
-	struct flow_state *flows;
-	unsigned cflows;	/* Concurrent flows (config) */
-	unsigned lflow;		/* Flow length  (config) */
-	unsigned nflows;	/* accumulated flows (stats) */
-};
+#define REMOVE 1
+#define FIND   0
 
-struct pktgen_hdr {
-	__u32 pgh_magic;
-	__u32 seq_num;
-	__u32 tv_sec;
-	__u32 tv_usec;
-};
+static struct pktgen_dev *__pktgen_NN_threads(const char *ifname, int remove);
+static int pktgen_remove_device(struct pktgen_thread *t, struct pktgen_dev *i);
+static int pktgen_add_device(struct pktgen_thread *t, const char *ifname);
+static struct pktgen_dev *pktgen_find_dev(struct pktgen_thread *t,
+					  const char *ifname);
+static int pktgen_device_event(struct notifier_block *, unsigned long, void *);
+static void pktgen_run_all_threads(int background);
+static void pktgen_stop_all_threads_ifs(void);
+static int pktgen_stop_device(struct pktgen_dev *pkt_dev);
+static void pktgen_stop(struct pktgen_thread *t);
+static void pktgen_clear_counters(struct pktgen_dev *pkt_dev, int seq_too);
+static int pktgen_mark_device(const char *ifname);
+static unsigned int scan_ip6(const char *s, char ip[16]);
+static unsigned int fmt_ip6(char *s, const char ip[16]);
+static void clear_nqw_hook(struct pktgen_thread* t, struct net_device* dev);
+static int set_nqw_hook(struct pktgen_thread* t, struct net_device* dev, int gfp);
 
-struct pktgen_thread {
-	spinlock_t if_lock;
-	struct list_head if_list;	/* All device here */
-	struct list_head th_list;
-	int removed;
-	char name[32];
-	char result[512];
-	u32 max_before_softirq;	/* We'll call do_softirq to prevent starvation. */
+/* Module parameters, defaults. */
+static int pg_count_d = 1000;	/* 1000 pkts by default */
+static int pg_delay_d = 0x7FFFFFFF; /* Don't run until someone sets a different delay. */
 
-	/* Field for thread to receive "posted" events terminate, stop ifs etc. */
+static int pg_clone_skb_d;
+static int debug;
 
-	u32 control;
-	int pid;
-	int cpu;
+static DEFINE_MUTEX(pktgen_thread_lock);
+static LIST_HEAD(pktgen_threads);
 
-	wait_queue_head_t queue;
+static struct notifier_block pktgen_notifier_block = {
+	.notifier_call = pktgen_device_event,
 };
 
-#define REMOVE 1
-#define FIND   0
 
 /*  This code works around the fact that do_div cannot handle two 64-bit
     numbers, and regular 64-bit division doesn't work on x86 kernels.
@@ -457,13 +326,6 @@ #else
 #endif
 }
 
-static inline __u64 getCurMs(void)
-{
-	struct timeval tv;
-	do_gettimeofday(&tv);
-	return tv_to_ms(&tv);
-}
-
 static inline __u64 getCurUs(void)
 {
 	struct timeval tv;
@@ -476,36 +338,17 @@ static inline __u64 tv_diff(const struct
 	return tv_to_us(a) - tv_to_us(b);
 }
 
-/* old include end */
-
-static char version[] __initdata = VERSION;
-
-static int pktgen_remove_device(struct pktgen_thread *t, struct pktgen_dev *i);
-static int pktgen_add_device(struct pktgen_thread *t, const char *ifname);
-static struct pktgen_dev *pktgen_find_dev(struct pktgen_thread *t,
-					  const char *ifname);
-static int pktgen_device_event(struct notifier_block *, unsigned long, void *);
-static void pktgen_run_all_threads(void);
-static void pktgen_stop_all_threads_ifs(void);
-static int pktgen_stop_device(struct pktgen_dev *pkt_dev);
-static void pktgen_stop(struct pktgen_thread *t);
-static void pktgen_clear_counters(struct pktgen_dev *pkt_dev);
-static int pktgen_mark_device(const char *ifname);
-static unsigned int scan_ip6(const char *s, char ip[16]);
-static unsigned int fmt_ip6(char *s, const char ip[16]);
-
-/* Module parameters, defaults. */
-static int pg_count_d = 1000;	/* 1000 pkts by default */
-static int pg_delay_d;
-static int pg_clone_skb_d;
-static int debug;
+/* Since the machine booted. */
+static inline __u64 getRelativeCurUs(void) {
+	return pg_div(sched_clock(), 1000);
+}
 
-static DEFINE_MUTEX(pktgen_thread_lock);
-static LIST_HEAD(pktgen_threads);
+/* Since the machine booted. */
+static inline __u64 getRelativeCurNs(void) {
+	return sched_clock();
+}
 
-static struct notifier_block pktgen_notifier_block = {
-	.notifier_call = pktgen_device_event,
-};
+/* old include end */
 
 /*
  * /proc handling functions 
@@ -542,10 +385,14 @@ static ssize_t pgctrl_write(struct file 
 		pktgen_stop_all_threads_ifs();
 
 	else if (!strcmp(data, "start"))
-		pktgen_run_all_threads();
+		pktgen_run_all_threads(0);
+
+	/* Run in the background. */
+	else if (!strcmp(data, "bg_start"))
+		pktgen_run_all_threads(1);
 
 	else
-		printk("pktgen: Unknown command: %s\n", data);
+		printk("pktgen: Unknown command: \"%s\"\n", data);
 
 	err = count;
 
@@ -558,6 +405,137 @@ static int pgctrl_open(struct inode *ino
 	return single_open(file, pgctrl_show, PDE(inode)->data);
 }
 
+static int pg_populate_report(struct pktgen_dev_report* rpt, struct pktgen_dev* pkt_dev) {
+	int i;
+	
+	memset(rpt, 0, sizeof(*rpt));
+	rpt->api_version = 1;
+	rpt->flags = pkt_dev->flags;
+	strncpy(rpt->thread_name, pkt_dev->pg_thread->name, 32);
+	strncpy(rpt->interface_name, pkt_dev->ifname, 32);
+	rpt->min_pkt_size = pkt_dev->min_pkt_size;
+	rpt->max_pkt_size = pkt_dev->max_pkt_size;
+	rpt->clone_skb = pkt_dev->clone_skb;
+	rpt->peer_clone_skb = pkt_dev->peer_clone_skb;
+	rpt->nfrags = pkt_dev->nfrags;
+	
+	strncpy(rpt->dst_min, pkt_dev->dst_min, IP_NAME_SZ);
+	strncpy(rpt->dst_max, pkt_dev->dst_max, IP_NAME_SZ);
+	strncpy(rpt->src_min, pkt_dev->src_min, IP_NAME_SZ);
+	strncpy(rpt->src_max, pkt_dev->src_max, IP_NAME_SZ);
+
+	memcpy(&rpt->in6_saddr, &pkt_dev->in6_saddr, sizeof(struct in6_addr));
+	memcpy(&rpt->in6_daddr, &pkt_dev->in6_daddr, sizeof(struct in6_addr));
+
+	/* For ranges */
+	memcpy(&rpt->min_in6_daddr, &pkt_dev->min_in6_daddr, sizeof(struct in6_addr));
+	memcpy(&rpt->max_in6_daddr, &pkt_dev->max_in6_daddr, sizeof(struct in6_addr));
+	memcpy(&rpt->min_in6_saddr, &pkt_dev->min_in6_saddr, sizeof(struct in6_addr));
+	memcpy(&rpt->max_in6_saddr, &pkt_dev->max_in6_saddr, sizeof(struct in6_addr));
+
+	/* If we're doing ranges, random or incremental, then this
+	 * defines the min/max for those ranges.
+	 */
+	rpt->saddr_min = pkt_dev->saddr_min;
+	rpt->saddr_max = pkt_dev->saddr_max;
+	rpt->daddr_min = pkt_dev->daddr_min;
+	rpt->daddr_max = pkt_dev->daddr_max;
+
+	rpt->udp_src_min = pkt_dev->udp_src_min;
+	rpt->udp_src_max = pkt_dev->udp_src_max;
+	rpt->udp_dst_min = pkt_dev->udp_dst_min;
+	rpt->udp_dst_max = pkt_dev->udp_dst_max;
+	
+	/* MPLS */
+	rpt->nr_labels = pkt_dev->nr_labels;	/* Depth of stack, 0 = no MPLS */
+	for (i = 0; i<MAX_MPLS_LABELS; i++) {
+		rpt->labels[i] = pkt_dev->labels[i];
+	}
+
+	rpt->src_mac_count = pkt_dev->src_mac_count;
+	rpt->dst_mac_count = pkt_dev->dst_mac_count;
+
+	memcpy(&rpt->dst_mac, &pkt_dev->dst_mac, ETH_ALEN);
+	memcpy(&rpt->src_mac, &pkt_dev->src_mac, ETH_ALEN);
+
+	rpt->nflows = pkt_dev->nflows;
+	rpt->cflows = pkt_dev->cflows;
+	rpt->lflow = pkt_dev->lflow;
+
+	rpt->delay_ns = pkt_dev->delay_ns;
+	rpt->count = pkt_dev->count;  /* Default No packets to send */
+	rpt->sofar = pkt_dev->sofar;  /* How many pkts we've sent so far */
+	rpt->tx_bytes = pkt_dev->tx_bytes; /* How many bytes we've transmitted */
+	rpt->errors = pkt_dev->errors;    /* Errors when trying to transmit, pkts will be re-sent */
+
+	/* Fields relating to receiving pkts */
+	rpt->avg_latency = pkt_dev->avg_latency; /* in micro-seconds */
+	rpt->min_latency = pkt_dev->min_latency;
+	rpt->max_latency = pkt_dev->max_latency;
+	for (i = 0; i<LAT_BUCKETS_MAX; i++) {
+		rpt->latency_bkts[i] = pkt_dev->latency_bkts[i];
+	}
+	rpt->pkts_rcvd_since_clear = pkt_dev->pkts_rcvd_since_clear;
+	
+        rpt->ooo_rcvd = pkt_dev->ooo_rcvd;
+        rpt->pkts_rcvd = pkt_dev->pkts_rcvd;
+        rpt->dup_rcvd = pkt_dev->dup_rcvd;
+        rpt->bytes_rcvd = pkt_dev->bytes_rcvd;
+        rpt->seq_gap_rcvd = pkt_dev->seq_gap_rcvd;
+	rpt->non_pg_pkts_rcvd = pkt_dev->non_pg_pkts_rcvd;
+	return 0;
+}; /* populate report */
+
+
+int pktgen_proc_ioctl(struct inode* inode, struct file* file, unsigned int cmd,
+                      unsigned long arg) {
+        int err = 0;
+        struct pktgen_ioctl_info args;
+        struct pktgen_dev* pkt_dev = NULL;
+
+        if (copy_from_user(&args, (void*)arg, sizeof(args))) {
+                return -EFAULT;
+        }
+
+        /* Null terminate the names */
+        args.thread_name[31] = 0;
+        args.interface_name[31] = 0;
+
+        /* printk("pktgen:  thread_name: %s  interface_name: %s\n",
+         *        args.thread_name, args.interface_name);
+         */
+        
+        switch (cmd) {
+         case GET_PKTGEN_INTERFACE_INFO: {
+		 mutex_lock(&pktgen_thread_lock);
+                 pkt_dev = __pktgen_NN_threads(args.interface_name, FIND);
+                 if (pkt_dev) {
+			 pg_populate_report(&(args.report), pkt_dev);
+			 if (copy_to_user((void*)(arg), &args, sizeof(args))) {
+				 printk("ERROR: pktgen:  copy_to_user failed.\n");
+				 err = -EFAULT;
+			 }
+			 else {
+				 err = 0;
+			 }
+		 }
+		 else {
+			 printk("ERROR: pktgen:  Could not find interface -:%s:-\n",
+				args.interface_name);
+			 err = -ENODEV;
+		 }
+		 mutex_unlock(&pktgen_thread_lock);
+                 break;
+         }
+         default:
+		printk("%s: Unknown pktgen IOCTL: %x \n", __FUNCTION__,
+			cmd);
+		return -EINVAL;
+        }
+        
+        return err;
+}/* pktgen_proc_ioctl */
+
 static struct file_operations pktgen_fops = {
 	.owner   = THIS_MODULE,
 	.open    = pgctrl_open,
@@ -565,6 +543,7 @@ static struct file_operations pktgen_fop
 	.llseek  = seq_lseek,
 	.write   = pgctrl_write,
 	.release = single_release,
+        .ioctl   = pktgen_proc_ioctl,
 };
 
 static int pktgen_if_show(struct seq_file *seq, void *v)
@@ -581,10 +560,11 @@ static int pktgen_if_show(struct seq_fil
 		   pkt_dev->max_pkt_size);
 
 	seq_printf(seq,
-		   "     frags: %d  delay: %u  clone_skb: %d  ifname: %s\n",
+		   "     frags: %d  delay: %lluns  clone_skb: %d  peer_clone_skb: %d ifname: %s\n",
 		   pkt_dev->nfrags,
-		   1000 * pkt_dev->delay_us + pkt_dev->delay_ns,
-		   pkt_dev->clone_skb, pkt_dev->ifname);
+		   (unsigned long long)pkt_dev->delay_ns,
+		   pkt_dev->clone_skb, pkt_dev->peer_clone_skb,
+		   pkt_dev->ifname);
 
 	seq_printf(seq, "     flows: %u flowlen: %u\n", pkt_dev->cflows,
 		   pkt_dev->lflow);
@@ -681,11 +661,29 @@ static int pktgen_if_show(struct seq_fil
 		stopped = now;	/* not really stopped, more like last-running-at */
 
 	seq_printf(seq,
-		   "Current:\n     pkts-sofar: %llu  errors: %llu\n     started: %lluus  stopped: %lluus idle: %lluus\n",
+		   "Current:\n     tx-pkts: %llu  tx-errors: %llu  tx-bytes: %llu\n",
 		   (unsigned long long)pkt_dev->sofar,
-		   (unsigned long long)pkt_dev->errors, (unsigned long long)sa,
+		   (unsigned long long)pkt_dev->errors,
+		   (unsigned long long)pkt_dev->tx_bytes);
+	seq_printf(seq,
+		   "     rx-pkts: %llu  rx-bytes: %llu\n",
+		   (unsigned long long)pkt_dev->pkts_rcvd,
+		   (unsigned long long)pkt_dev->bytes_rcvd);
+
+	seq_printf(seq,
+		   "     blocked: %s  next-tx-ns: %llu (%lli)  started: %lluus  stopped: %lluus idle: %lluns\n",
+		   pkt_dev->tx_blocked ? "TRUE" : "false",
+		   (unsigned long long)pkt_dev->next_tx_ns,
+		   (long long)(pkt_dev->next_tx_ns - getRelativeCurNs()),
+		   (unsigned long long)sa,
 		   (unsigned long long)stopped,
-		   (unsigned long long)pkt_dev->idle_acc);
+		   (unsigned long long)pkt_dev->idle_acc_ns);
+	seq_printf(seq,
+		   "     nanodelays: %llu  sleeps: %llu  queue_stopped: %llu  tx-early: %llu\n",
+		   (unsigned long long)pkt_dev->nanodelays,
+		   (unsigned long long)pkt_dev->sleeps,
+		   (unsigned long long)pkt_dev->queue_stopped,
+		   (unsigned long long)pkt_dev->req_tx_early);
 
 	seq_printf(seq,
 		   "     seq_num: %d  cur_dst_mac_offset: %d  cur_src_mac_offset: %d\n",
@@ -969,15 +967,11 @@ static ssize_t pktgen_if_write(struct fi
 			return len;
 		}
 		i += len;
-		if (value == 0x7FFFFFFF) {
-			pkt_dev->delay_us = 0x7FFFFFFF;
-			pkt_dev->delay_ns = 0;
-		} else {
-			pkt_dev->delay_us = value / 1000;
-			pkt_dev->delay_ns = value % 1000;
+		pkt_dev->delay_ns = value;
+		if ((getRelativeCurNs() + pkt_dev->delay_ns) > pkt_dev->next_tx_ns) {
+			pkt_dev->next_tx_ns = getRelativeCurNs() + pkt_dev->delay_ns;
 		}
-		sprintf(pg_result, "OK: delay=%u",
-			1000 * pkt_dev->delay_us + pkt_dev->delay_ns);
+		sprintf(pg_result, "OK: delay=%lluns", (unsigned long long)pkt_dev->delay_ns);
 		return count;
 	}
 	if (!strcmp(name, "udp_src_min")) {
@@ -1043,6 +1037,17 @@ static ssize_t pktgen_if_write(struct fi
 		sprintf(pg_result, "OK: clone_skb=%d", pkt_dev->clone_skb);
 		return count;
 	}
+	if (!strcmp(name, "peer_clone_skb")) {
+		len = num_arg(&user_buffer[i], 10, &value);
+		if (len < 0) {
+			return len;
+		}
+		i += len;
+		pkt_dev->peer_clone_skb = value;
+
+		sprintf(pg_result, "OK: peer_clone_skb=%d", pkt_dev->peer_clone_skb);
+		return count;
+	}
 	if (!strcmp(name, "count")) {
 		len = num_arg(&user_buffer[i], 10, &value);
 		if (len < 0) {
@@ -1141,6 +1146,7 @@ static ssize_t pktgen_if_write(struct fi
 			pkt_dev->flags &= ~F_MPLS_RND;
 
 		else {
+			printk("pktgen: Flag -:%s:- unknown\n", f);
 			sprintf(pg_result,
 				"Flag -:%s:- unknown\nAvailable flags, (prepend ! to un-set flag):\n%s",
 				f,
@@ -1405,7 +1411,7 @@ static ssize_t pktgen_if_write(struct fi
 	}
 
 	if (!strcmp(name, "clear_counters")) {
-		pktgen_clear_counters(pkt_dev);
+		pktgen_clear_counters(pkt_dev, 0);
 		sprintf(pg_result, "OK: Clearing counters.\n");
 		return count;
 	}
@@ -1448,6 +1454,7 @@ static ssize_t pktgen_if_write(struct fi
 		return count;
 	}
 
+	printk("pktgen: No such parameter \"%s\"\n", name);
 	sprintf(pkt_dev->result, "No such parameter \"%s\"", name);
 	return -EINVAL;
 }
@@ -1464,6 +1471,7 @@ static struct file_operations pktgen_if_
 	.llseek  = seq_lseek,
 	.write   = pktgen_if_write,
 	.release = single_release,
+        .ioctl   = pktgen_proc_ioctl,
 };
 
 static int pktgen_thread_show(struct seq_file *seq, void *v)
@@ -1473,12 +1481,17 @@ static int pktgen_thread_show(struct seq
 
 	BUG_ON(!t);
 
-	seq_printf(seq, "Name: %s  max_before_softirq: %d\n",
-		   t->name, t->max_before_softirq);
+	mutex_lock(&pktgen_thread_lock);
+	
+	/* versioning info.  CFG_RT means we do not busy-spin, so can be configured for
+	 * real-time scheduling if user-space so desires. */
+	seq_printf(seq, "VERSION-2 CFG_RT\n");
+	
+	seq_printf(seq, "PID: %d Name: %s  max_before_softirq: %d\n",
+		   t->pid, t->name, t->max_before_softirq);
 
 	seq_printf(seq, "Running: ");
 
-	if_lock(t);
 	list_for_each_entry(pkt_dev, &t->if_list, list)
 		if (pkt_dev->running)
 			seq_printf(seq, "%s ", pkt_dev->ifname);
@@ -1494,8 +1507,7 @@ static int pktgen_thread_show(struct seq
 	else
 		seq_printf(seq, "\nResult: NA\n");
 
-	if_unlock(t);
-
+	mutex_unlock(&pktgen_thread_lock);
 	return 0;
 }
 
@@ -1563,18 +1575,42 @@ static ssize_t pktgen_thread_write(struc
 			return -EFAULT;
 		i += len;
 		mutex_lock(&pktgen_thread_lock);
-		pktgen_add_device(t, f);
+		t->control_arg = f;
+		t->control |= T_ADD_DEV;
+		while (t->control & T_ADD_DEV) {
+			schedule_timeout_interruptible(msecs_to_jiffies(10));
+		}
+		t->control_arg = 0;
 		mutex_unlock(&pktgen_thread_lock);
 		ret = count;
 		sprintf(pg_result, "OK: add_device=%s", f);
 		goto out;
 	}
 
+	if (!strcmp(name, "rem_device")) {
+		char f[32];
+		memset(f, 0, 32);
+		len = strn_len(&user_buffer[i], sizeof(f) - 1);
+		if (len < 0) {
+			ret = len;
+			goto out;
+		}
+		if (copy_from_user(f, &user_buffer[i], len))
+			return -EFAULT;
+		i += len;
+		pktgen_mark_device(f);
+		ret = count;
+		sprintf(pg_result, "OK: rem_device=%s", f);
+		goto out;
+	}
+
 	if (!strcmp(name, "rem_device_all")) {
 		mutex_lock(&pktgen_thread_lock);
 		t->control |= T_REMDEVALL;
 		mutex_unlock(&pktgen_thread_lock);
-		schedule_timeout_interruptible(msecs_to_jiffies(125));	/* Propagate thread->control  */
+		while (t->control & T_REMDEVALL) {
+			schedule_timeout_interruptible(msecs_to_jiffies(10));
+		}
 		ret = count;
 		sprintf(pg_result, "OK: rem_device_all");
 		goto out;
@@ -1590,6 +1626,8 @@ static ssize_t pktgen_thread_write(struc
 		goto out;
 	}
 
+	printk("pktgen:  un-known command to pktgen_thread: -:%s:-\n", name);
+	
 	ret = -EINVAL;
 out:
 	return ret;
@@ -1607,8 +1645,10 @@ static struct file_operations pktgen_thr
 	.llseek  = seq_lseek,
 	.write   = pktgen_thread_write,
 	.release = single_release,
+        .ioctl   = pktgen_proc_ioctl,
 };
 
+
 /* Think find or remove for NN */
 static struct pktgen_dev *__pktgen_NN_threads(const char *ifname, int remove)
 {
@@ -1619,14 +1659,14 @@ static struct pktgen_dev *__pktgen_NN_th
 		pkt_dev = pktgen_find_dev(t, ifname);
 		if (pkt_dev) {
 			if (remove) {
-				if_lock(t);
 				pkt_dev->removal_mark = 1;
 				t->control |= T_REMDEV;
-				if_unlock(t);
 			}
 			break;
 		}
 	}
+	/*PG_DEBUG(printk("pktgen_NN_threads, ifname: %s  remove: %d  pkt_dev: 0x%p\n",
+	  ifname, remove, pkt_dev)); */
 	return pkt_dev;
 }
 
@@ -1698,13 +1738,21 @@ static int pktgen_device_event(struct no
 
 /* Associate pktgen_dev with a device. */
 
-static struct net_device *pktgen_setup_dev(struct pktgen_dev *pkt_dev)
+static struct net_device *pktgen_setup_dev(struct pktgen_dev *pkt_dev, struct pktgen_thread* t)
 {
 	struct net_device *odev;
 
 	/* Clean old setups */
 
 	if (pkt_dev->odev) {
+
+#ifdef USE_NQW_CALLBACK
+		/* Set the nqw callback hooks */
+		rtnl_lock();
+		clear_nqw_hook(t, pkt_dev->odev);
+		rtnl_unlock();
+#endif
+		pkt_dev->odev->pkt_dev = NULL;
 		dev_put(pkt_dev->odev);
 		pkt_dev->odev = NULL;
 	}
@@ -1726,6 +1774,14 @@ static struct net_device *pktgen_setup_d
 	}
 	pkt_dev->odev = odev;
 
+#ifdef USE_NQW_CALLBACK
+	/* Set the nqw callback hooks */
+	rtnl_lock();
+	set_nqw_hook(t, pkt_dev->odev, GFP_ATOMIC);
+	rtnl_unlock();
+#endif
+	
+	pkt_dev->odev->pkt_dev = pkt_dev;
 	return pkt_dev->odev;
 
 out_put:
@@ -1742,7 +1798,7 @@ static void pktgen_setup_inject(struct p
 {
 	/* Try once more, just in case it works now. */
 	if (!pkt_dev->odev)
-		pktgen_setup_dev(pkt_dev);
+		pktgen_setup_dev(pkt_dev, pkt_dev->pg_thread);
 
 	if (!pkt_dev->odev) {
 		printk("pktgen: ERROR: pkt_dev->odev == NULL in setup_inject.\n");
@@ -1845,30 +1901,188 @@ #endif
 	pkt_dev->nflows = 0;
 }
 
-static void spin(struct pktgen_dev *pkt_dev, __u64 spin_until_us)
-{
-	__u64 start;
-	__u64 now;
-
-	start = now = getCurUs();
-	printk(KERN_INFO "sleeping for %d\n", (int)(spin_until_us - now));
-	while (now < spin_until_us) {
-		/* TODO: optimize sleeping behavior */
-		if (spin_until_us - now > jiffies_to_usecs(1) + 1)
-			schedule_timeout_interruptible(1);
-		else if (spin_until_us - now > 100) {
-			do_softirq();
-			if (!pkt_dev->running)
-				return;
-			if (need_resched())
-				schedule();
+
+#ifdef USE_NQW_CALLBACK
+/* Runs from interrupt */
+int pg_notify_queue_woken(struct net_device* dev) {
+	/* Find the thread that needs waking. */
+	struct pktgen_thread* t = ((struct pg_nqw_data*)(dev->nqw_data))->pg_thread;
+	t->control |= T_WAKE_BLOCKED;
+	wake_up_interruptible(&(t->queue));
+	return 0;
+}
+
+/* Must hold RTNL lock while calling this. */
+static int set_nqw_hook(struct pktgen_thread* t, struct net_device* dev, int gfp) {
+	/* The notify-queue-woken magic only works for physical
+	 * devices at this time.  So, apply hook to underlying
+	 * device.
+	 */
+	struct pg_nqw_data* nqwd;
+	ASSERT_RTNL();
+	BUG_ON(!t);
+	
+	if (!dev) {
+		WARN_ON(!dev);
+		return -ENODEV;
+	}
+#if defined(CONFIG_MACVLAN) || defined(CONFIG_MACVLAN_MODULE)
+	if (dev->priv_flags & IFF_MAC_VLAN) {
+		struct macvlan_vlan *vlan = dev->priv;
+		printk("pktgen: setting nqw_hook on lower mac-vlan dev: %p\n", vlan->lowerdev);
+		return set_nqw_hook(t, vlan->lowerdev, gfp);
+	}
+#endif
+
+	if (dev->priv_flags & IFF_802_1Q_VLAN) {
+		printk("pktgen: setting nqw_hook on real-dev of .1q vlan: %s\n", dev->name);
+		return set_nqw_hook(t, VLAN_DEV_INFO(dev)->real_dev, gfp);
+	}
+
+	nqwd = (struct pg_nqw_data*)(dev->nqw_data);
+
+	if (nqwd) {
+		if (nqwd->magic == PG_NQW_MAGIC) {
+			if (nqwd->pg_thread == t) {
+				atomic_inc(&(nqwd->nqw_ref_count));
+			
+				printk("pktgen: Incremented nqw_ref_count: %d  device: %s\n",
+				       (int)(atomic_read(&(nqwd->nqw_ref_count))), dev->name);
+				return 0;
+			}
+			else {
+				printk("pktgen:  ERROR:  set_nqw_hook: nqwd thread does not match, dev: %s",
+				       dev->name);
+				return -EINVAL;
+			}
+		}
+		else {
+			printk("wanlink:  WARNING:  set_nqw_hook: nqwd magic is NOT WanLink, dev: %s  magic: 0x%x",
+			       dev->name, nqwd->magic);
+			return 0;
 		}
+	}
+	else {
+		nqwd = kmalloc(sizeof(*nqwd), gfp);
+		if (nqwd) {
+			memset(nqwd, 0, sizeof(*nqwd));
+			nqwd->magic = PG_NQW_MAGIC;
+			atomic_inc(&(nqwd->nqw_ref_count));
+			nqwd->pg_thread = t;
+			dev->nqw_data = nqwd;
+			dev->notify_queue_woken = pg_notify_queue_woken;
+			printk("pktgen: Added nqw callback to device: %s\n",
+			       dev->name);
+			return 0;
+		}
+		else {
+			printk("pktgen: ERROR:  could not allocate nqwd for dev: %s\n", dev->name);
+			return -ENOBUFS;
+		}
+	}
+}//set_nqw_hook
+
 
-		now = getCurUs();
+/* Must hold RTNL lock while calling this. */
+static void clear_nqw_hook(struct pktgen_thread* t, struct net_device* dev) {
+	/* The notify-queue-woken magic only works for physical
+	 * devices at this time.  So, apply hook to underlying
+	 * device.
+	 */
+	ASSERT_RTNL();
+	BUG_ON(!t);
+	
+#if defined(CONFIG_MACVLAN) || defined(CONFIG_MACVLAN_MODULE)
+	if (dev->priv_flags & IFF_MAC_VLAN) {
+		struct macvlan_vlan *vlan = dev->priv;
+		clear_nqw_hook(t, vlan->lowerdev);
+		return;
 	}
+#endif
+
+	if (dev->priv_flags & IFF_802_1Q_VLAN) {
+		clear_nqw_hook(t, VLAN_DEV_INFO(dev)->real_dev);
+		return;
+	}
+
+	if (dev->nqw_data) {
+		struct pg_nqw_data* nqwd = (struct pg_nqw_data*)(dev->nqw_data);
+		if (nqwd->magic == PG_NQW_MAGIC) {
+			if (t != nqwd->pg_thread) {
+				printk("pktgen ERROR: t != nqwd->pg_thread\n");
+			}
+			atomic_dec(&(nqwd->nqw_ref_count));
+
+			printk("pktgen: Decremented nqw_ref_count: %d  device: %s\n",
+			       (int)(atomic_read(&(nqwd->nqw_ref_count))),
+			       dev->name);
+			
+			BUG_ON(atomic_read(&(nqwd->nqw_ref_count)) < 0);
+			
+			if (atomic_read(&(nqwd->nqw_ref_count)) == 0) {
+				printk("pktgen: Removing nqw reference from device: %s\n",
+				       dev->name);
+				dev->notify_queue_woken = NULL;
+				dev->nqw_data = NULL;
+				kfree(nqwd);
+			}
+		}
+		else {
+			printk("pktgen:  WARNING:  clear_nqw_hook: nqwd magic is NOT PKT-GEN, dev: %s  magic: 0x%x",
+			       dev->name, nqwd->magic);
+		}
+	}
+	else {
+		printk("pktgen:  Warning: nqw_data is null in clear_nqw_hook, dev: %s\n",
+		       dev->name);
+	}
+}//clear_nqw_hook
+
+#endif
+
+
+/* delay_ns is in nano-seconds */
+static void pg_nanodelay(u64 delay_ns, struct pktgen_dev* info) {
+	u64 idle_start = getRelativeCurNs();
+	u64 last_time;
+	u64 _diff;
+	u64 itmp = idle_start;
+	struct pktgen_dev *p = NULL;
+	struct pktgen_thread* t = info->pg_thread;
+	
+	info->nanodelays++;
+	info->accum_delay_ns += delay_ns;
+	while (info->accum_delay_ns > PG_MAX_ACCUM_DELAY_NS) {
+		info->sleeps++;
+		interruptible_sleep_on_timeout(&(t->queue), 1);
+		/* will wake after one tick */
+		last_time = itmp;
+
+		/* Subtract delay from all interfaces for this thread, since all are blocked when
+		 * any are blocked.
+		 */
+		itmp = getRelativeCurNs();
+		_diff = (itmp - last_time);
+		list_for_each_entry(p, &t->if_list, list) {
+			p->accum_delay_ns -= _diff;
+			/* Limit saving up too much time... */
+			if (p->accum_delay_ns < -10000000) {
+				p->accum_delay_ns = -10000000;
+			}
+		}
+
+		/* For accounting, only charge this guy for the idle though...*/
+		info->idle_acc_ns += _diff;
+
+		/* break out if we are stopped or if we should transmit (maybe our ipg changed?) */
+		if (info->removal_mark || (itmp >= info->next_tx_ns) ||
+		    (t->control && T_WAKE_BLOCKED) ||
+		    (t->control && T_STOP)) {
+			break;
+		}
+	}/* while */
+}//pg_nanodelay
 
-	pkt_dev->idle_acc += now - start;
-}
 
 /* Increment/randomize headers according to flags and current values
  * for IP src/dest, UDP src/dst port, MAC-Addr src/dst
@@ -2545,13 +2759,203 @@ static inline struct sk_buff *fill_packe
 		return fill_packet_ipv4(odev, pkt_dev);
 }
 
-static void pktgen_clear_counters(struct pktgen_dev *pkt_dev)
-{
-	pkt_dev->seq_num = 1;
-	pkt_dev->idle_acc = 0;
+
+static void record_latency(struct pktgen_dev* pkt_dev, int latency) {
+        /* NOTE:  Latency can be negative */
+        int div = 100;
+        int diff;
+        int vl;
+        int i;
+
+        pkt_dev->pkts_rcvd_since_clear++;
+        
+        if (pkt_dev->pkts_rcvd_since_clear < 100) {
+                div = pkt_dev->pkts_rcvd;
+                if (pkt_dev->pkts_rcvd_since_clear == 1) {
+                        pkt_dev->avg_latency = latency;
+                }
+        }
+
+        if ((div + 1) == 0) {
+                pkt_dev->avg_latency = 0;
+        }
+        else {
+                pkt_dev->avg_latency = ((pkt_dev->avg_latency * div + latency) / (div + 1));
+        }
+
+        if (latency < pkt_dev->min_latency) {
+                pkt_dev->min_latency = latency;
+        }
+        if (latency > pkt_dev->max_latency) {
+                pkt_dev->max_latency = latency;
+        }
+
+        /* Place the latency in the right 'bucket' */
+        diff = (latency - pkt_dev->min_latency);        
+        for (i = 0; i<LAT_BUCKETS_MAX; i++) {
+                vl = (1<<i);
+                if (latency <= vl) {
+                        pkt_dev->latency_bkts[i]++;
+                        break;
+                }
+        }
+}/* record latency */
+
+
+/* Returns < 0 if the skb is not a pktgen buffer. */
+int pktgen_receive(struct sk_buff* skb) {
+        /* See if we have a pktgen packet */
+	/* TODO:  Add support for detecting IPv6, TCP packets too.  This will only
+	 * catch UDP at the moment. --Ben
+	 */
+	/*printk("pktgen-rcv, skb->len: %d\n", skb->len);*/
+	
+        if ((skb->len >= (20 + 8 + sizeof(struct pktgen_hdr))) &&
+            (skb->protocol == __constant_htons(ETH_P_IP))) {
+		struct pktgen_hdr* pgh;
+                
+                /* It's IP, and long enough, lets check the magic number.
+                 * TODO:  This is a hack not always guaranteed to catch the right
+                 * packets.
+                 */
+		
+		/*printk("Length & protocol passed, skb->data: %p, raw: %p\n",
+		  skb->data, skb->h.raw);*/
+       
+                pgh = (struct pktgen_hdr*)(skb->data + 20 + 8);
+		
+                /*
+                tmp = (char*)(skb->data);
+                for (i = 0; i<90; i++) {
+                        printk("%02hx ", tmp[i]);
+                        if (((i + 1) % 15) == 0) {
+                                printk("\n");
+                        }
+                }
+                printk("\n");
+                */
+                
+                if (pgh->pgh_magic == __constant_ntohl(PKTGEN_MAGIC)) {
+                        struct net_device* dev = skb->dev;
+                        struct pktgen_dev* pkt_dev;
+                        __u32 seq = ntohl(pgh->seq_num);
+
+			// TODO:  Need lock..maybe 
+			pkt_dev = dev->pkt_dev;
+                        
+                        if (!pkt_dev) {
+				return -1;
+                        }
+
+                        pkt_dev->pkts_rcvd++;
+                        pkt_dev->bytes_rcvd += skb->len;
+                        
+                        /* Check for out-of-sequence packets */
+                        if (pkt_dev->last_seq_rcvd == seq) {
+                                pkt_dev->dup_rcvd++;
+                                pkt_dev->dup_since_incr++;
+                        }
+                        else {
+                                __s64 rx;
+                                __s64 tx;
+                                struct timeval txtv;
+				if (skb->tstamp.off_sec || skb->tstamp.off_usec) {
+					skb_get_timestamp(skb, &txtv);
+				}
+				else {
+					do_gettimeofday(&txtv);
+					skb_set_timestamp(skb, &txtv);
+				}
+                                rx = tv_to_us(&txtv);
+
+                                txtv.tv_usec = ntohl(pgh->tv_usec);
+                                txtv.tv_sec = ntohl(pgh->tv_sec);
+                                tx = tv_to_us(&txtv);
+                                record_latency(pkt_dev, rx - tx);
+                                
+                                if ((pkt_dev->last_seq_rcvd + 1) == seq) {
+                                        if ((pkt_dev->peer_clone_skb > 1) &&
+                                            (pkt_dev->peer_clone_skb > (pkt_dev->dup_since_incr + 1))) {
+                                                
+                                                pkt_dev->seq_gap_rcvd += (pkt_dev->peer_clone_skb -
+                                                                       pkt_dev->dup_since_incr - 1);
+                                        }
+                                        /* Great, in order...all is well */
+                                }
+                                else if (pkt_dev->last_seq_rcvd < seq) {
+                                        /* sequence gap, means we dropped a pkt most likely */
+                                        if (pkt_dev->peer_clone_skb > 1) {
+                                                /* We dropped more than one sequence number's worth,
+                                                 * and if we're using clone_skb, then this is quite
+                                                 * a few.  This number still will not be exact, but
+                                                 * it will be closer.
+                                                 */
+                                                pkt_dev->seq_gap_rcvd += (((seq - pkt_dev->last_seq_rcvd) *
+                                                                        pkt_dev->peer_clone_skb) -
+                                                                       pkt_dev->dup_since_incr);
+                                        }
+                                        else {
+                                                pkt_dev->seq_gap_rcvd += (seq - pkt_dev->last_seq_rcvd - 1);
+                                        }
+                                }
+                                else {
+                                        pkt_dev->ooo_rcvd++; /* out-of-order */
+                                }
+                                
+                                pkt_dev->dup_since_incr = 0;
+                        }
+                        pkt_dev->last_seq_rcvd = seq;
+                        kfree_skb(skb);
+                        if (debug > 1) {
+                                printk("done with pktgen_receive, free'd pkt\n");
+                        }
+                        return 0;
+                }
+        }
+        return -1; /* Let another protocol handle it, it's not for us! */
+}/* pktgen_receive */
+
+static void pg_reset_latency_counters(struct pktgen_dev* pkt_dev) {
+        int i;
+        pkt_dev->avg_latency = 0;
+        pkt_dev->min_latency = 0x7fffffff; /* largest integer */
+        pkt_dev->max_latency = 0x80000000; /* smallest integer */
+        pkt_dev->pkts_rcvd_since_clear = 0;
+        for (i = 0; i<LAT_BUCKETS_MAX; i++) {
+                pkt_dev->latency_bkts[i] = 0;
+        }
+}
+
+
+static void pktgen_clear_counters(struct pktgen_dev *pkt_dev, int seq_too) {
+	pkt_dev->idle_acc_ns = 0;
 	pkt_dev->sofar = 0;
 	pkt_dev->tx_bytes = 0;
 	pkt_dev->errors = 0;
+	pkt_dev->pkts_rcvd_since_clear = 0;
+	
+        pkt_dev->ooo_rcvd = 0;
+        pkt_dev->dup_rcvd = 0;
+        pkt_dev->pkts_rcvd = 0;
+        pkt_dev->bytes_rcvd = 0;
+        pkt_dev->non_pg_pkts_rcvd = 0;
+        pkt_dev->seq_gap_rcvd = 0; /* dropped */
+
+	/* Clear some transient state */
+	pkt_dev->accum_delay_ns = 0;
+	pkt_dev->sleeps = 0;
+	pkt_dev->nanodelays = 0;
+	
+        /* This is a bit of a hack, but it gets the dup counters
+         * in line so we don't have false alarms on dropped pkts.
+         */
+        if (seq_too) {
+                pkt_dev->dup_since_incr = pkt_dev->peer_clone_skb - 1;
+                pkt_dev->seq_num = 1;
+                pkt_dev->last_seq_rcvd = 0;
+        }
+        
+        pg_reset_latency_counters(pkt_dev);
 }
 
 /* Set up structure for sending pkts, clear counters */
@@ -2563,30 +2967,28 @@ static void pktgen_run(struct pktgen_thr
 
 	PG_DEBUG(printk("pktgen: entering pktgen_run. %p\n", t));
 
-	if_lock(t);
 	list_for_each_entry(pkt_dev, &t->if_list, list) {
-
-		/*
-		 * setup odev and create initial packet.
-		 */
-		pktgen_setup_inject(pkt_dev);
-
-		if (pkt_dev->odev) {
-			pktgen_clear_counters(pkt_dev);
-			pkt_dev->running = 1;	/* Cranke yeself! */
-			pkt_dev->skb = NULL;
-			pkt_dev->started_at = getCurUs();
-			pkt_dev->next_tx_us = getCurUs();	/* Transmit immediately */
-			pkt_dev->next_tx_ns = 0;
-
-			strcpy(pkt_dev->result, "Starting");
-			started++;
-		} else
-			strcpy(pkt_dev->result, "Error starting");
+		/* If already running, then ignore. */
+		if (! pkt_dev->running) {
+			/*
+			 * setup odev and create initial packet.
+			 */
+			pktgen_setup_inject(pkt_dev);
+			
+			if (pkt_dev->odev) {
+				pktgen_clear_counters(pkt_dev, 1);
+				pkt_dev->running = 1;	/* Cranke yeself! */
+				pkt_dev->skb = NULL;
+				pkt_dev->started_at = getCurUs();
+				/* Transmit first pkt immediately */
+				pkt_dev->next_tx_ns = getRelativeCurNs();
+				
+				strcpy(pkt_dev->result, "Starting");
+				started++;
+			} else
+				strcpy(pkt_dev->result, "Error starting");
+		}
 	}
-	if_unlock(t);
-	if (started)
-		t->control &= ~(T_STOP);
 }
 
 static void pktgen_stop_all_threads_ifs(void)
@@ -2603,65 +3005,12 @@ static void pktgen_stop_all_threads_ifs(
 	mutex_unlock(&pktgen_thread_lock);
 }
 
-static int thread_is_running(struct pktgen_thread *t)
-{
-	struct pktgen_dev *pkt_dev;
-	int res = 0;
-
-	list_for_each_entry(pkt_dev, &t->if_list, list)
-		if (pkt_dev->running) {
-			res = 1;
-			break;
-		}
-	return res;
-}
-
-static int pktgen_wait_thread_run(struct pktgen_thread *t)
-{
-	if_lock(t);
-
-	while (thread_is_running(t)) {
-
-		if_unlock(t);
-
-		msleep_interruptible(100);
-
-		if (signal_pending(current))
-			goto signal;
-		if_lock(t);
-	}
-	if_unlock(t);
-	return 1;
-signal:
-	return 0;
-}
-
-static int pktgen_wait_all_threads_run(void)
-{
-	struct pktgen_thread *t;
-	int sig = 1;
-
-	mutex_lock(&pktgen_thread_lock);
-
-	list_for_each_entry(t, &pktgen_threads, th_list) {
-		sig = pktgen_wait_thread_run(t);
-		if (sig == 0)
-			break;
-	}
-
-	if (sig == 0)
-		list_for_each_entry(t, &pktgen_threads, th_list)
-			t->control |= (T_STOP);
 
-	mutex_unlock(&pktgen_thread_lock);
-	return sig;
-}
-
-static void pktgen_run_all_threads(void)
-{
+static void pktgen_run_all_threads(int background) {
 	struct pktgen_thread *t;
 
-	PG_DEBUG(printk("pktgen: entering pktgen_run_all_threads.\n"));
+	PG_DEBUG(printk("pktgen: entering pktgen_run_all_threads, background: %d\n",
+			 background));
 
 	mutex_lock(&pktgen_thread_lock);
 
@@ -2670,9 +3019,14 @@ static void pktgen_run_all_threads(void)
 
 	mutex_unlock(&pktgen_thread_lock);
 
-	schedule_timeout_interruptible(msecs_to_jiffies(125));	/* Propagate thread->control  */
+	/* This is a hack at best...disabling, we should not have to depend on this. */
+	/*schedule_timeout_interruptible(msecs_to_jiffies(125));*/	/* Propagate thread->control  */
 
-	pktgen_wait_all_threads_run();
+	// Much harder to get rid of the if_lock if we allow this to block...
+	if (!background) {
+		printk("ERROR:  non-background mode no longer supported.\n");
+		//pktgen_wait_all_threads_run();
+	}
 }
 
 static void show_results(struct pktgen_dev *pkt_dev, int nr_frags)
@@ -2682,7 +3036,7 @@ static void show_results(struct pktgen_d
 
 	total_us = pkt_dev->stopped_at - pkt_dev->started_at;
 
-	idle = pkt_dev->idle_acc;
+	idle = do_div(pkt_dev->idle_acc_ns, 1000);
 
 	p += sprintf(p, "OK: %llu(c%llu+d%llu) usec, %llu (%dbyte,%dfrags)\n",
 		     (unsigned long long)total_us,
@@ -2731,22 +3085,62 @@ static int pktgen_stop_device(struct pkt
 	return 0;
 }
 
-static struct pktgen_dev *next_to_run(struct pktgen_thread *t)
-{
-	struct pktgen_dev *pkt_dev, *best = NULL;
-
-	if_lock(t);
+/**  Find the adapter that needs to tx next.
+ *  We need to take the blocked adapters into account, but can't ignore
+ * them forever just in case we missed the tx-queue-wake event for some
+ * reason.
+ */
+static struct pktgen_dev *next_to_run(struct pktgen_thread *t, u64 now, u64* next_running_delay) {
+	struct pktgen_dev *pkt_dev = NULL;
+	struct pktgen_dev *best = NULL;
+	struct pktgen_dev *best_blocked = NULL;
 
 	list_for_each_entry(pkt_dev, &t->if_list, list) {
 		if (!pkt_dev->running)
 			continue;
-		if (best == NULL)
-			best = pkt_dev;
-		else if (pkt_dev->next_tx_us < best->next_tx_us)
-			best = pkt_dev;
+		if (pkt_dev->tx_blocked) {
+			if (best_blocked == NULL)
+				best_blocked = pkt_dev;
+			else {
+				if (pkt_dev->next_tx_ns < best_blocked->next_tx_ns) {
+					best_blocked = pkt_dev;
+				}
+			}
+		}
+		else {
+			if (best == NULL)
+				best = pkt_dev;
+			else {
+				if (pkt_dev->next_tx_ns < best->next_tx_ns) {
+					best = pkt_dev;
+				}
+			}
+		}
 	}
-	if_unlock(t);
-	return best;
+
+	/** If we have a blocked device that is more than 1ms late, then try it again first.
+	 * Otherwise, take best non-blocked device.
+	 */
+	if (best) {
+		if (best->next_tx_ns <= now) {
+			*next_running_delay = 0;
+		}
+		else {
+			*next_running_delay = best->next_tx_ns - now;
+		}
+	}
+	else {
+		*next_running_delay = 10000000; /* 10ms */
+	}
+	
+	if (best_blocked && (best_blocked->next_tx_ns < (now - PG_TRY_TX_ANYWAY_NS))) {
+		return best_blocked;
+	}
+
+	if (best) {
+		return best;
+	}
+	return best_blocked;
 }
 
 static void pktgen_stop(struct pktgen_thread *t)
@@ -2755,8 +3149,6 @@ static void pktgen_stop(struct pktgen_th
 
 	PG_DEBUG(printk("pktgen: entering pktgen_stop\n"));
 
-	if_lock(t);
-
 	list_for_each_entry(pkt_dev, &t->if_list, list) {
 		pktgen_stop_device(pkt_dev);
 		if (pkt_dev->skb)
@@ -2764,8 +3156,6 @@ static void pktgen_stop(struct pktgen_th
 
 		pkt_dev->skb = NULL;
 	}
-
-	if_unlock(t);
 }
 
 /*
@@ -2779,8 +3169,6 @@ static void pktgen_rem_one_if(struct pkt
 
 	PG_DEBUG(printk("pktgen: entering pktgen_rem_one_if\n"));
 
-	if_lock(t);
-
 	list_for_each_safe(q, n, &t->if_list) {
 		cur = list_entry(q, struct pktgen_dev, list);
 
@@ -2795,10 +3183,15 @@ static void pktgen_rem_one_if(struct pkt
 
 		break;
 	}
-
-	if_unlock(t);
 }
 
+static void pktgen_unblock_all_ifs(struct pktgen_thread *t) {
+	struct pktgen_dev *p = NULL;;
+	list_for_each_entry(p, &t->if_list, list)
+		p->tx_blocked = 0;
+}/* wake all writers */
+
+
 static void pktgen_rem_all_ifs(struct pktgen_thread *t)
 {
 	struct list_head *q, *n;
@@ -2807,8 +3200,6 @@ static void pktgen_rem_all_ifs(struct pk
 	/* Remove all devices, free mem */
 
 	PG_DEBUG(printk("pktgen: entering pktgen_rem_all_ifs\n"));
-	if_lock(t);
-
 	list_for_each_safe(q, n, &t->if_list) {
 		cur = list_entry(q, struct pktgen_dev, list);
 
@@ -2818,8 +3209,6 @@ static void pktgen_rem_all_ifs(struct pk
 
 		pktgen_remove_device(t, cur);
 	}
-
-	if_unlock(t);
 }
 
 static void pktgen_rem_thread(struct pktgen_thread *t)
@@ -2835,34 +3224,41 @@ static void pktgen_rem_thread(struct pkt
 	mutex_unlock(&pktgen_thread_lock);
 }
 
-static __inline__ void pktgen_xmit(struct pktgen_dev *pkt_dev)
-{
-	struct net_device *odev = NULL;
+static __inline__ void pktgen_xmit(struct pktgen_dev *pkt_dev, u64 now) {
+	struct net_device *odev;
 	__u64 idle_start = 0;
 	int ret;
-
+	
 	odev = pkt_dev->odev;
 
-	if (pkt_dev->delay_us || pkt_dev->delay_ns) {
-		u64 now;
-
-		now = getCurUs();
-		if (now < pkt_dev->next_tx_us)
-			spin(pkt_dev, pkt_dev->next_tx_us);
-
+	if (pkt_dev->delay_ns || (pkt_dev->accum_delay_ns > 0)) {
+		if (now < pkt_dev->next_tx_ns) {
+			/* Don't tx early..*/
+			pkt_dev->req_tx_early++;
+			goto out;
+		}
+		
 		/* This is max DELAY, this has special meaning of
 		 * "never transmit"
 		 */
-		if (pkt_dev->delay_us == 0x7FFFFFFF) {
-			pkt_dev->next_tx_us = getCurUs() + pkt_dev->delay_us;
-			pkt_dev->next_tx_ns = pkt_dev->delay_ns;
+		if (pkt_dev->delay_ns == 0x7FFFFFFF) {
+			pkt_dev->next_tx_ns = getRelativeCurNs() + pkt_dev->delay_ns;
 			goto out;
 		}
 	}
 
-	if (netif_queue_stopped(odev) || need_resched()) {
-		idle_start = getCurUs();
-
+	if (need_resched()) {
+		idle_start = getRelativeCurNs();
+		schedule();
+		pkt_dev->idle_acc_ns += getRelativeCurNs() - idle_start;
+	}
+	
+	if (netif_queue_stopped(odev)) {
+		pkt_dev->queue_stopped++;
+		pkt_dev->tx_blocked = 1;
+		/* change tx time to now to show work was at least attempted. */
+		pkt_dev->next_tx_ns = now;
+		
 		if (!netif_running(odev)) {
 			pktgen_stop_device(pkt_dev);
 			if (pkt_dev->skb)
@@ -2870,16 +3266,8 @@ static __inline__ void pktgen_xmit(struc
 			pkt_dev->skb = NULL;
 			goto out;
 		}
-		if (need_resched())
-			schedule();
 
-		pkt_dev->idle_acc += getCurUs() - idle_start;
-
-		if (netif_queue_stopped(odev)) {
-			pkt_dev->next_tx_us = getCurUs();	/* TODO */
-			pkt_dev->next_tx_ns = 0;
-			goto out;	/* Try the next interface */
-		}
+		goto out;	/* Try the next interface */
 	}
 
 	if (pkt_dev->last_ok || !pkt_dev->skb) {
@@ -2910,40 +3298,44 @@ static __inline__ void pktgen_xmit(struc
 		if (likely(ret == NETDEV_TX_OK)) {
 			pkt_dev->last_ok = 1;
 			pkt_dev->sofar++;
-			pkt_dev->seq_num++;
 			pkt_dev->tx_bytes += pkt_dev->cur_pkt_size;
+			pkt_dev->next_tx_ns = getRelativeCurNs() + pkt_dev->delay_ns;
+			pkt_dev->tx_blocked = 0;
 
 		} else if (ret == NETDEV_TX_LOCKED
 			   && (odev->features & NETIF_F_LLTX)) {
 			cpu_relax();
 			goto retry_now;
 		} else {	/* Retry it next time */
-
+			static int do_once_hsx_wrn = 1;
+			if (do_once_hsx_wrn) {
+				printk(KERN_INFO "pktgen: Hard xmit error, driver for %s doesn't do queue-stopped quite right.\n", odev->name);
+				printk(KERN_INFO "pktgen:  Transmit request will be retried, and this error msg will not be printed again..\n");
+				do_once_hsx_wrn = 0;
+			}
+			
 			atomic_dec(&(pkt_dev->skb->users));
 
-			if (debug && net_ratelimit())
-				printk(KERN_INFO "pktgen: Hard xmit error\n");
-
+			pkt_dev->queue_stopped++;
 			pkt_dev->errors++;
 			pkt_dev->last_ok = 0;
-		}
-
-		pkt_dev->next_tx_us = getCurUs();
-		pkt_dev->next_tx_ns = 0;
 
-		pkt_dev->next_tx_us += pkt_dev->delay_us;
-		pkt_dev->next_tx_ns += pkt_dev->delay_ns;
-
-		if (pkt_dev->next_tx_ns > 1000) {
-			pkt_dev->next_tx_us++;
-			pkt_dev->next_tx_ns -= 1000;
+			/* Try a little later..flag us as wanting to tx, but unable.  Will try again shortly.
+			 */
+			pkt_dev->tx_blocked = 1;
+			/* change tx time to now to show work was at least attempted. */
+			pkt_dev->next_tx_ns = now;
 		}
 	}
 
 	else {			/* Retry it next time */
+		pkt_dev->queue_stopped++;
 		pkt_dev->last_ok = 0;
-		pkt_dev->next_tx_us = getCurUs();	/* TODO */
-		pkt_dev->next_tx_ns = 0;
+		/* Try a little later..flag us as wanting to tx, but unable.  Will try again shortly.
+		 */
+		pkt_dev->tx_blocked = 1;
+		/* change tx time to now to show work was at least attempted. */
+		pkt_dev->next_tx_ns = now;
 	}
 
 	netif_tx_unlock_bh(odev);
@@ -2951,14 +3343,14 @@ static __inline__ void pktgen_xmit(struc
 	/* If pkt_dev->count is zero, then run forever */
 	if ((pkt_dev->count != 0) && (pkt_dev->sofar >= pkt_dev->count)) {
 		if (atomic_read(&(pkt_dev->skb->users)) != 1) {
-			idle_start = getCurUs();
+			idle_start = getRelativeCurNs();
 			while (atomic_read(&(pkt_dev->skb->users)) != 1) {
 				if (signal_pending(current)) {
 					break;
 				}
 				schedule();
 			}
-			pkt_dev->idle_acc += getCurUs() - idle_start;
+			pkt_dev->idle_acc_ns += getRelativeCurNs() - idle_start;
 		}
 
 		/* Done with this */
@@ -2982,7 +3374,9 @@ static void pktgen_thread_worker(struct 
 	sigset_t tmpsig;
 	u32 max_before_softirq;
 	u32 tx_since_softirq = 0;
-
+	u64 now;
+	u64 next_running_delay;
+	
 	daemonize("pktgen/%d", cpu);
 
 	/* Block all signals except SIGKILL, SIGSTOP and SIGTERM */
@@ -3007,6 +3401,8 @@ static void pktgen_thread_worker(struct 
 	t->control &= ~(T_STOP);
 	t->control &= ~(T_REMDEVALL);
 	t->control &= ~(T_REMDEV);
+	t->control &= ~(T_WAKE_BLOCKED);
+	t->control &= ~(T_ADD_DEV);
 
 	t->pid = current->pid;
 
@@ -3024,13 +3420,52 @@ static void pktgen_thread_worker(struct 
 		/*
 		 * Get next dev to xmit -- if any.
 		 */
+	find_best:
 
-		pkt_dev = next_to_run(t);
+		if (t->control & T_WAKE_BLOCKED) {
+			pktgen_unblock_all_ifs(t);
+			t->control &= ~(T_WAKE_BLOCKED);
+		}
+		
+		now = getRelativeCurNs();
+		pkt_dev = next_to_run(t, now, &next_running_delay);
 
 		if (pkt_dev) {
 
-			pktgen_xmit(pkt_dev);
+			if (pkt_dev->tx_blocked) {
+				/* If blocked for less than 1ms, then sleep for up to 1ms.  If the
+				 * device un-blocks, then we will be woken by the wait-queue callback.
+				 */
+				u64 tx_anyway_ns = (now - PG_TRY_TX_ANYWAY_NS);
+				if (pkt_dev->next_tx_ns > tx_anyway_ns) {
+					pg_nanodelay(min(next_running_delay, (u64)(PG_TRY_TX_ANYWAY_NS)),
+						     pkt_dev);
+					/* Maybe things have changed since we went to sleep. */
+					goto find_best;
+				}
+			}
+			
+			/* If the best to run should not run yet, then sleep (or accumulate sleep) */
+			if (now < pkt_dev->next_tx_ns) {
+				/* spin(pkt_dev, pkt_dev->next_tx_us); */
+				u64 next_ipg = pkt_dev->next_tx_ns - now;
+                                        
+				/* These will not actually busy-spin now.  Will run as
+				 * much as 1ms fast, and will sleep in 1ms units, assuming
+				 * our tick is 1ms.
+				 */
+				pg_nanodelay(next_ipg, pkt_dev);
+				now = getRelativeCurNs();
+				if (pkt_dev->removal_mark ||
+				    (pkt_dev->pg_thread->control && T_STOP)) {
+					goto skip_tx;
+				}
+			}
+
+			
+			pktgen_xmit(pkt_dev, now);
 
+		skip_tx:
 			/*
 			 * We like to stay RUNNING but must also give
 			 * others fair share.
@@ -3068,6 +3503,11 @@ static void pktgen_thread_worker(struct 
 			t->control &= ~(T_RUN);
 		}
 
+		if (t->control & T_ADD_DEV) {
+			pktgen_add_device(t, (char*)(t->control_arg));
+			t->control &= ~(T_ADD_DEV);
+		}
+
 		if (t->control & T_REMDEVALL) {
 			pktgen_rem_all_ifs(t);
 			t->control &= ~(T_REMDEVALL);
@@ -3098,16 +3538,12 @@ static struct pktgen_dev *pktgen_find_de
 					  const char *ifname)
 {
 	struct pktgen_dev *p, *pkt_dev = NULL;
-	if_lock(t);
-
 	list_for_each_entry(p, &t->if_list, list)
 		if (strncmp(p->ifname, ifname, IFNAMSIZ) == 0) {
 			pkt_dev = p;
 			break;
 		}
-
-	if_unlock(t);
-	PG_DEBUG(printk("pktgen: find_dev(%s) returning %p\n", ifname, pkt_dev));
+	/* PG_DEBUG(printk("pktgen: find_dev(%s) returning %p\n", ifname, pkt_dev)); */
 	return pkt_dev;
 }
 
@@ -3120,8 +3556,6 @@ static int add_dev_to_thread(struct pktg
 {
 	int rv = 0;
 
-	if_lock(t);
-
 	if (pkt_dev->pg_thread) {
 		printk("pktgen: ERROR:  already assigned to a thread.\n");
 		rv = -EBUSY;
@@ -3133,12 +3567,9 @@ static int add_dev_to_thread(struct pktg
 	pkt_dev->running = 0;
 
 out:
-	if_unlock(t);
 	return rv;
 }
 
-/* Called under thread lock */
-
 static int pktgen_add_device(struct pktgen_thread *t, const char *ifname)
 {
 	struct pktgen_dev *pkt_dev;
@@ -3151,7 +3582,10 @@ static int pktgen_add_device(struct pktg
 		printk("pktgen: ERROR: interface already used.\n");
 		return -EBUSY;
 	}
-
+	else {
+		printk("pktgen:  Attempting to add device: %s\n", ifname);
+	}
+	
 	pkt_dev = kzalloc(sizeof(struct pktgen_dev), GFP_KERNEL);
 	if (!pkt_dev)
 		return -ENOMEM;
@@ -3168,8 +3602,7 @@ static int pktgen_add_device(struct pktg
 	pkt_dev->max_pkt_size = ETH_ZLEN;
 	pkt_dev->nfrags = 0;
 	pkt_dev->clone_skb = pg_clone_skb_d;
-	pkt_dev->delay_us = pg_delay_d / 1000;
-	pkt_dev->delay_ns = pg_delay_d % 1000;
+	pkt_dev->delay_ns = pg_delay_d;
 	pkt_dev->count = pg_count_d;
 	pkt_dev->sofar = 0;
 	pkt_dev->udp_src_min = 9;	/* sink port */
@@ -3179,7 +3612,7 @@ static int pktgen_add_device(struct pktg
 
 	strncpy(pkt_dev->ifname, ifname, IFNAMSIZ);
 
-	if (!pktgen_setup_dev(pkt_dev)) {
+	if (!pktgen_setup_dev(pkt_dev, t)) {
 		printk("pktgen: ERROR: pktgen_setup_dev failed.\n");
 		if (pkt_dev->flows)
 			vfree(pkt_dev->flows);
@@ -3202,7 +3635,7 @@ static int pktgen_add_device(struct pktg
 	return add_dev_to_thread(t, pkt_dev);
 }
 
-static struct pktgen_thread *__init pktgen_find_thread(const char *name)
+static struct pktgen_thread* pktgen_find_thread(const char *name)
 {
 	struct pktgen_thread *t;
 
@@ -3241,7 +3674,6 @@ static int __init pktgen_create_thread(c
 	}
 
 	strcpy(t->name, name);
-	spin_lock_init(&t->if_lock);
 	t->cpu = cpu;
 
 	pe = create_proc_entry(t->name, 0600, pg_proc_dir);
@@ -3304,6 +3736,14 @@ static int pktgen_remove_device(struct p
 	/* Dis-associate from the interface */
 
 	if (pkt_dev->odev) {
+
+#ifdef USE_NQW_CALLBACK
+		/* Set the nqw callback hooks */
+		rtnl_lock();
+		clear_nqw_hook(t, pkt_dev->odev);
+		rtnl_unlock();
+#endif
+		pkt_dev->odev->pkt_dev = NULL;
 		dev_put(pkt_dev->odev);
 		pkt_dev->odev = NULL;
 	}
@@ -3314,6 +3754,7 @@ static int pktgen_remove_device(struct p
 
 	/* Clean up proc file system */
 
+	PG_DEBUG(printk("pktgen:  removing proc entry: %s (0x%p)\n", pkt_dev->ifname, pg_proc_dir));
 	remove_proc_entry(pkt_dev->ifname, pg_proc_dir);
 
 	if (pkt_dev->flows)
@@ -3328,7 +3769,17 @@ static int __init pg_init(void)
 	struct proc_dir_entry *pe;
 
 	printk(version);
-
+	printk("sizeof report: %d, sizeof in6_addr: %d\n",
+	       (int)(sizeof(struct pktgen_dev_report)),
+	       (int)(sizeof(struct in6_addr)));
+	
+        if (handle_pktgen_hook) {
+                printk("pktgen: ERROR: pktgen is already loaded it seems..\n");
+                /* Already loaded */
+                return -EEXIST;
+        }
+
+	
 	pg_proc_dir = proc_mkdir(PG_PROC_DIR, proc_net);
 	if (!pg_proc_dir)
 		return -ENODEV;
@@ -3367,6 +3818,9 @@ static int __init pg_init(void)
 		return -ENODEV;
 	}
 
+	handle_pktgen_hook = pktgen_receive;
+	PG_DEBUG(printk("pktgen initialization complete.\n"));
+ 
 	return 0;
 }
 
@@ -3377,10 +3831,15 @@ static void __exit pg_cleanup(void)
 	wait_queue_head_t queue;
 	init_waitqueue_head(&queue);
 
+        /* Un-Register receive handler */
+        handle_pktgen_hook = NULL;
+	
 	/* Stop all interfaces & threads */
-
+	pktgen_stop_all_threads_ifs();
+	
 	list_for_each_safe(q, n, &pktgen_threads) {
 		t = list_entry(q, struct pktgen_thread, th_list);
+		t->control |= (T_STOP);
 		t->control |= (T_TERMINATE);
 
 		wait_event_interruptible_timeout(queue, (t->removed == 1), HZ);
@@ -3390,6 +3849,7 @@ static void __exit pg_cleanup(void)
 	unregister_netdevice_notifier(&pktgen_notifier_block);
 
 	/* Clean up proc file system */
+	PG_DEBUG(printk("pktgen:  removing proc entry: %s (0x%p)\n", PGCTRL, pg_proc_dir));
 	remove_proc_entry(PGCTRL, pg_proc_dir);
 	proc_net_remove(PG_PROC_DIR);
 }
diff --git a/net/core/pktgen.h b/net/core/pktgen.h
new file mode 100644
index 0000000..921f3b0
--- /dev/null
+++ b/net/core/pktgen.h
@@ -0,0 +1,335 @@
+/* -*-linux-c-*-
+ * $Id: candela_2.6.13.patch,v 1.3 2005/09/30 04:45:31 greear Exp $
+ * pktgen.c: Packet Generator for performance evaluation.
+ *
+ * See pktgen.c for details of changes, etc.
+*/
+
+
+#ifndef PKTGEN_H_INCLUDE_KERNEL__
+#define PKTGEN_H_INCLUDE_KERNEL__
+
+#include <linux/version.h>
+#include <linux/in6.h>
+
+/* The buckets are exponential in 'width' */
+#define LAT_BUCKETS_MAX 32
+#define PG_MAX_ACCUM_DELAY_NS 1000000 /* one ms */
+#define PG_TRY_TX_ANYWAY_NS 1000000 /* try a blocked tx queue after 1 ms. */
+
+#define IP_NAME_SZ 32
+#define MAX_MPLS_LABELS 16 /* This is the max label stack depth */
+#define MPLS_STACK_BOTTOM __constant_htonl(0x00000100)
+
+/* Device flag bits */
+#define F_IPSRC_RND   (1<<0)	/* IP-Src Random  */
+#define F_IPDST_RND   (1<<1)	/* IP-Dst Random  */
+#define F_UDPSRC_RND  (1<<2)	/* UDP-Src Random */
+#define F_UDPDST_RND  (1<<3)	/* UDP-Dst Random */
+#define F_MACSRC_RND  (1<<4)	/* MAC-Src Random */
+#define F_MACDST_RND  (1<<5)	/* MAC-Dst Random */
+#define F_TXSIZE_RND  (1<<6)      /* Transmit packet size is random */
+#define F_IPV6        (1<<7)	/* Interface in IPV6 Mode */
+#define F_MPLS_RND    (1<<8)	/* Random MPLS labels */
+
+/* Thread control flag bits */
+#define T_TERMINATE   (1<<0)
+#define T_STOP        (1<<1)	/* Stop run */
+#define T_RUN         (1<<2)	/* Start run */
+#define T_REMDEVALL   (1<<3)	/* Remove all devs */
+#define T_REMDEV      (1<<4)	/* Remove one dev */
+#define T_WAKE_BLOCKED (1<<5)	/* Wake up all blocked net-devices. */
+#define T_ADD_DEV     (1<<6)	/* Add a device. */
+
+/* Used to help with determining the pkts on receive */
+#define PKTGEN_MAGIC 0xbe9be955
+#define PG_PROC_DIR "pktgen"
+#define PGCTRL	    "pgctrl"
+
+struct pktgen_dev {
+
+	/*
+	 * Try to keep frequent/infrequent used vars. separated.
+	 */
+
+	char ifname[IFNAMSIZ];
+	char result[512];
+
+	struct pktgen_thread *pg_thread;	/* the owner */
+	struct list_head list;		/* Used for chaining in the thread's run-queue */
+
+	int running;		/* if this changes to false, the test will stop */
+
+	/* If min != max, then we will either do a linear iteration, or
+	 * we will do a random selection from within the range.
+	 */
+	__u32 flags;
+	int removal_mark;	/* non-zero => the device is marked for
+				 * removal by worker thread */
+
+	__u32 min_pkt_size;	/* = ETH_ZLEN; */
+	__u32 max_pkt_size;	/* = ETH_ZLEN; */
+	__u32 nfrags;
+	__u64 delay_ns;          /* Delay this much between sending packets. */
+	__u64 count;		/* Default No packets to send */
+	__u64 sofar;		/* How many pkts we've sent so far */
+	__u64 tx_bytes;		/* How many bytes we've transmitted */
+	__u64 errors;		/* Errors when trying to transmit, pkts will be re-sent */
+	__u64 nanodelays;        /* how many times have we called nano-delay on this device? */
+	__s64 accum_delay_ns;    /* Accumulated delay..when >= 1ms, we'll sleep on a wait queue. */
+	__u64 sleeps;            /* How many times have we gone to sleep on the wait queue. */
+	__u64 queue_stopped;     /* How many times was queue stopped when we tried to xmit? */
+	/* runtime counters relating to clone_skb */
+	__u64 next_tx_ns;	/* timestamp of when to tx next */
+	__u64 req_tx_early; /* requested to tx, but is too early for us to tx. */
+
+	__u64 allocated_skbs;
+	__u32 clone_count;
+	
+	int tx_blocked; /* Need to tx as soon as able... */
+	int last_ok;		/* Was last skb sent?
+				 * Or a failed transmit of some sort?  This will keep
+				 * sequence numbers in order, for example.
+				 */
+	__u64 started_at;	/* micro-seconds */
+	__u64 stopped_at;	/* micro-seconds */
+	__u64 idle_acc_ns; /* nano-seconds */
+	__u32 seq_num;
+
+	__u32 clone_skb;		/* Use multiple SKBs during packet gen.  If this number
+				 * is greater than 1, then that many copies of the same
+				 * packet will be sent before a new packet is allocated.
+				 * For instance, if you want to send 1024 identical packets
+				 * before creating a new packet, set clone_skb to 1024.
+				 */
+	__u32 peer_clone_skb;      /* Peer (transmitter's) clone setting. */
+
+	char dst_min[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+	char dst_max[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+	char src_min[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+	char src_max[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+
+	struct in6_addr in6_saddr;
+	struct in6_addr in6_daddr;
+	struct in6_addr cur_in6_daddr;
+	struct in6_addr cur_in6_saddr;
+	/* For ranges */
+	struct in6_addr min_in6_daddr;
+	struct in6_addr max_in6_daddr;
+	struct in6_addr min_in6_saddr;
+	struct in6_addr max_in6_saddr;
+
+	/* If we're doing ranges, random or incremental, then this
+	 * defines the min/max for those ranges.
+	 */
+	__u32 saddr_min;	/* inclusive, source IP address */
+	__u32 saddr_max;	/* exclusive, source IP address */
+	__u32 daddr_min;	/* inclusive, dest IP address */
+	__u32 daddr_max;	/* exclusive, dest IP address */
+
+	__u16 udp_src_min;	/* inclusive, source UDP port */
+	__u16 udp_src_max;	/* exclusive, source UDP port */
+	__u16 udp_dst_min;	/* inclusive, dest UDP port */
+	__u16 udp_dst_max;	/* exclusive, dest UDP port */
+
+	/* MPLS */
+	unsigned nr_labels;	/* Depth of stack, 0 = no MPLS */
+	__be32 labels[MAX_MPLS_LABELS];
+
+	__u32 src_mac_count;	/* How many MACs to iterate through */
+	__u32 dst_mac_count;	/* How many MACs to iterate through */
+
+	unsigned char dst_mac[ETH_ALEN];
+	unsigned char src_mac[ETH_ALEN];
+
+	__u32 cur_dst_mac_offset;
+	__u32 cur_src_mac_offset;
+	__u32 cur_saddr;
+	__u32 cur_daddr;
+	__u16 cur_udp_dst;
+	__u16 cur_udp_src;
+	__u32 cur_pkt_size;
+
+	__u8 hh[14];
+	/* = {
+	   0x00, 0x80, 0xC8, 0x79, 0xB3, 0xCB,
+
+	   We fill in SRC address later
+	   0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+	   0x08, 0x00
+	   };
+	 */
+	__u16 pad;		/* pad out the hh struct to an even 16 bytes */
+
+	struct sk_buff *skb;	/* skb we are to transmit next, mainly used for when we
+				 * are transmitting the same one multiple times
+				 */
+	struct net_device *odev;	/* The out-going device.  Note that the device should
+					 * have it's pg_info pointer pointing back to this
+					 * device.  This will be set when the user specifies
+					 * the out-going device name (not when the inject is
+					 * started as it used to do.)
+					 */
+	struct flow_state *flows;
+	unsigned cflows;	/* Concurrent flows (config) */
+	unsigned lflow;		/* Flow length  (config) */
+	unsigned nflows;	/* accumulated flows (stats) */
+
+	int avg_latency; /* in micro-seconds */
+	int min_latency;
+	int max_latency;
+	__u64 latency_bkts[LAT_BUCKETS_MAX];
+	__u64 pkts_rcvd_since_clear; /* with regard to clearing/resetting the latency logic */
+	
+
+	/* Fields relating to receiving pkts */
+        __u32 last_seq_rcvd;
+        __u64 ooo_rcvd;  /* out-of-order packets received */
+        __u64 pkts_rcvd; /* packets received */
+        __u64 dup_rcvd;  /* duplicate packets received */
+        __u64 bytes_rcvd; /* total bytes received, as obtained from the skb */
+        __u64 seq_gap_rcvd; /* how many gaps we received.  This coorelates to
+                             * dropped pkts, except perhaps in cases where we also
+                             * have re-ordered pkts.  In that case, you have to tie-break
+                             * by looking at send v/s received pkt totals for the interfaces
+                             * involved.
+                             */
+        __u64 non_pg_pkts_rcvd; /* Count how many non-pktgen skb's we are sent to check. */
+        __u64 dup_since_incr; /* How many dumplicates since the last seq number increment,
+                               * used to detect gaps when multiskb > 1
+                               */
+};
+
+struct pktgen_hdr {
+	__u32 pgh_magic;
+	__u32 seq_num;
+	__u32 tv_sec;
+	__u32 tv_usec;
+};
+
+struct pktgen_thread {
+	struct list_head if_list;	/* All device here */
+	struct list_head th_list;
+	int removed;
+	char name[32];
+	char result[512];
+	u32 max_before_softirq;	/* We'll call do_softirq to prevent starvation. */
+
+	/* Field for thread to receive "posted" events terminate, stop ifs etc. */
+
+	u32 control;
+	char* control_arg;
+	int pid;
+	int cpu;
+
+	wait_queue_head_t queue;
+};
+
+struct pg_nqw_data {
+	#define PG_NQW_MAGIC 0x82743ab6
+	u32 magic;
+	atomic_t nqw_ref_count;
+	struct pktgen_thread* pg_thread;
+};
+
+struct pktgen_dev_report {
+	__u32 api_version;
+	__u32 flags;
+	__u32 min_pkt_size;
+	__u32 max_pkt_size;
+	__u32 nfrags;
+
+	__u32 clone_skb;		/* Use multiple SKBs during packet gen.  If this number
+				 * is greater than 1, then that many copies of the same
+				 * packet will be sent before a new packet is allocated.
+				 * For instance, if you want to send 1024 identical packets
+				 * before creating a new packet, set clone_skb to 1024.
+				 */
+	__u32 peer_clone_skb;      /* Peer (transmitter's) clone setting. */
+	__s32 avg_latency; /* in micro-seconds */
+	__s32 min_latency;
+	__s32 max_latency;
+
+	char thread_name[32];
+	char interface_name[32];
+	char dst_min[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+	char dst_max[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+	char src_min[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+	char src_max[IP_NAME_SZ];	/* IP, ie 1.2.3.4 */
+	unsigned char dst_mac[ETH_ALEN];
+	unsigned char src_mac[ETH_ALEN];
+	__u32 pad_32; /* pad to 8-byte boundary */
+	
+	/* If we're doing ranges, random or incremental, then this
+	 * defines the min/max for those ranges.
+	 */
+	__u32 saddr_min;	/* inclusive, source IP address */
+	__u32 saddr_max;	/* exclusive, source IP address */
+	__u32 daddr_min;	/* inclusive, dest IP address */
+	__u32 daddr_max;	/* exclusive, dest IP address */
+
+	__u16 udp_src_min;	/* inclusive, source UDP port */
+	__u16 udp_src_max;	/* exclusive, source UDP port */
+	__u16 udp_dst_min;	/* inclusive, dest UDP port */
+	__u16 udp_dst_max;	/* exclusive, dest UDP port */
+
+	/* MPLS */
+	__u32 nr_labels;	/* Depth of stack, 0 = no MPLS */
+	__be32 labels[MAX_MPLS_LABELS];
+
+	__u32 src_mac_count;	/* How many MACs to iterate through */
+	__u32 dst_mac_count;	/* How many MACs to iterate through */
+
+	__u64 nflows;	/* accumulated flows (stats) */
+	__u32 cflows;	/* Concurrent flows (config) */
+	__u32 lflow;	/* Flow length  (config) */
+
+	__u64 delay_ns; /* Delay this much between sending packets. */
+	__u64 count;  /* Default No packets to send */
+	__u64 sofar;  /* How many pkts we've sent so far */
+	__u64 tx_bytes; /* How many bytes we've transmitted */
+	__u64 errors;    /* Errors when trying to transmit, pkts will be re-sent */
+	__u64 latency_bkts[LAT_BUCKETS_MAX];
+	__u64 pkts_rcvd_since_clear; /* with regard to clearing/resetting the latency logic */
+
+		/* Fields relating to receiving pkts */
+        __u64 ooo_rcvd;  /* out-of-order packets received */
+        __u64 pkts_rcvd; /* packets received */
+        __u64 dup_rcvd;  /* duplicate packets received */
+        __u64 bytes_rcvd; /* total bytes received, as obtained from the skb */
+        __u64 seq_gap_rcvd; /* how many gaps we received.  This coorelates to
+                             * dropped pkts, except perhaps in cases where we also
+                             * have re-ordered pkts.  In that case, you have to tie-break
+                             * by looking at send v/s received pkt totals for the interfaces
+                             * involved.
+                             */
+        __u64 non_pg_pkts_rcvd; /* Count how many non-pktgen skb's we are sent to check. */
+
+	struct in6_addr in6_saddr;
+	struct in6_addr in6_daddr;
+	/* For ranges */
+	struct in6_addr min_in6_daddr;
+	struct in6_addr max_in6_daddr;
+	struct in6_addr min_in6_saddr;
+	struct in6_addr max_in6_saddr;
+
+	char future_use[256]; /* Give us some room for growth w/out changing structure size */
+} __attribute__((__packed__));
+
+/* Define some IOCTLs.  Just picking random numbers, basically. */
+#define GET_PKTGEN_INTERFACE_INFO 0x7450
+struct pktgen_ioctl_info {
+        char thread_name[32];
+        char interface_name[32];
+        struct pktgen_dev_report report;
+};
+
+
+/* Defined in dev.c */
+extern int (*handle_pktgen_hook)(struct sk_buff *skb);
+
+/* Returns < 0 if the skb is not a pktgen buffer. */
+int pktgen_receive(struct sk_buff* skb);
+
+
+#endif
diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index 7de9857..2b544a9 100644
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -470,6 +470,7 @@ #endif
 #if defined(CONFIG_IP_VS) || defined(CONFIG_IP_VS_MODULE)
 	C(ipvs_property);
 #endif
+	C(use_specified_ether_crc);
 	C(protocol);
 	n->destructor = NULL;
 #ifdef CONFIG_NETFILTER
@@ -545,6 +546,7 @@ #endif
 #if defined(CONFIG_IP_VS) || defined(CONFIG_IP_VS_MODULE)
 	new->ipvs_property = old->ipvs_property;
 #endif
+	new->use_specified_ether_crc = old->use_specified_ether_crc;
 #ifdef CONFIG_BRIDGE_NETFILTER
 	new->nf_bridge	= old->nf_bridge;
 	nf_bridge_get(old->nf_bridge);
diff --git a/net/core/sock.c b/net/core/sock.c
index 51fcfbc..3b304ad 100644
--- a/net/core/sock.c
+++ b/net/core/sock.c
@@ -515,6 +515,18 @@ #endif
 			sock_warn_obsolete_bsdism("setsockopt");
 			break;
 
+#ifdef CONFIG_SUPPORT_SEND_BAD_CRC
+                case SO_NOFCS:
+                   /* printk("SO_NOFCS, valbool: %d, sk: %p\n",
+                      (int)(valbool), sk); */
+                        if (valbool) {
+                                sk->sk_flags |= SOCK_DONT_DO_LL_FCS;
+                        }
+                        else {
+                                sk->sk_flags &= ~(SOCK_DONT_DO_LL_FCS);
+                        }
+                        break;
+#endif
 		case SO_PASSCRED:
 			if (valbool)
 				set_bit(SOCK_PASSCRED, &sock->flags);
diff --git a/net/decnet/dn_fib.c b/net/decnet/dn_fib.c
index fa20e2e..bdb6780 100644
--- a/net/decnet/dn_fib.c
+++ b/net/decnet/dn_fib.c
@@ -490,7 +490,8 @@ static int dn_fib_check_attr(struct rtms
 		if (attr) {
 			if (RTA_PAYLOAD(attr) < 4 && RTA_PAYLOAD(attr) != 2)
 				return -EINVAL;
-			if (i != RTA_MULTIPATH && i != RTA_METRICS)
+			if (i != RTA_MULTIPATH && i != RTA_METRICS &&
+			    i != RTA_TABLE)
 				rta[i-1] = (struct rtattr *)RTA_DATA(attr);
 		}
 	}
@@ -507,7 +508,7 @@ int dn_fib_rtm_delroute(struct sk_buff *
 	if (dn_fib_check_attr(r, rta))
 		return -EINVAL;
 
-	tb = dn_fib_get_table(r->rtm_table, 0);
+	tb = dn_fib_get_table(rtm_get_table(r, rta), 0);
 	if (tb)
 		return tb->delete(tb, r, (struct dn_kern_rta *)rta, nlh, &NETLINK_CB(skb));
 
@@ -523,7 +524,7 @@ int dn_fib_rtm_newroute(struct sk_buff *
 	if (dn_fib_check_attr(r, rta))
 		return -EINVAL;
 
-	tb = dn_fib_get_table(r->rtm_table, 1);
+	tb = dn_fib_get_table(rtm_get_table(r, rta), 1);
 	if (tb) 
 		return tb->insert(tb, r, (struct dn_kern_rta *)rta, nlh, &NETLINK_CB(skb));
 
@@ -533,8 +534,8 @@ int dn_fib_rtm_newroute(struct sk_buff *
 
 int dn_fib_dump(struct sk_buff *skb, struct netlink_callback *cb)
 {
-	int t;
-	int s_t;
+	u32 t;
+	u32 s_t;
 	struct dn_fib_table *tb;
 
 	if (NLMSG_PAYLOAD(cb->nlh, 0) >= sizeof(struct rtmsg) &&
@@ -764,7 +765,7 @@ void dn_fib_flush(void)
 {
         int flushed = 0;
         struct dn_fib_table *tb;
-        int id;
+        u32 id;
 
         for(id = RT_TABLE_MAX; id > 0; id--) {
                 if ((tb = dn_fib_get_table(id, 0)) == NULL)
diff --git a/net/decnet/dn_route.c b/net/decnet/dn_route.c
index 743e9fc..d74f712 100644
--- a/net/decnet/dn_route.c
+++ b/net/decnet/dn_route.c
@@ -1485,6 +1485,7 @@ static int dn_rt_fill_info(struct sk_buf
 	r->rtm_src_len = 0;
 	r->rtm_tos = 0;
 	r->rtm_table = RT_TABLE_MAIN;
+	RTA_PUT_U32(skb, RTA_TABLE, RT_TABLE_MAIN);
 	r->rtm_type = rt->rt_type;
 	r->rtm_flags = (rt->rt_flags & ~0xFFFF) | RTM_F_CLONED;
 	r->rtm_scope = RT_SCOPE_UNIVERSE;
diff --git a/net/decnet/dn_rules.c b/net/decnet/dn_rules.c
index 6986be7..bc3ee9e 100644
--- a/net/decnet/dn_rules.c
+++ b/net/decnet/dn_rules.c
@@ -43,7 +43,7 @@ struct dn_fib_rule
 	struct hlist_node	r_hlist;
 	atomic_t		r_clntref;
 	u32			r_preference;
-	unsigned char		r_table;
+	u32			r_table;
 	unsigned char		r_action;
 	unsigned char		r_dst_len;
 	unsigned char		r_src_len;
@@ -77,6 +77,7 @@ int dn_fib_rtm_delrule(struct sk_buff *s
 	struct rtmsg *rtm = NLMSG_DATA(nlh);
 	struct dn_fib_rule *r;
 	struct hlist_node *node;
+	u32 table = rtm_get_table(rtm, rta);
 	int err = -ESRCH;
 
 	hlist_for_each_entry(r, node, &dn_fib_rules, r_hlist) {
@@ -90,7 +91,7 @@ #endif
 			(!rtm->rtm_type || rtm->rtm_type == r->r_action) &&
 			(!rta[RTA_PRIORITY-1] || memcmp(RTA_DATA(rta[RTA_PRIORITY-1]), &r->r_preference, 4) == 0) &&
 			(!rta[RTA_IIF-1] || rtattr_strcmp(rta[RTA_IIF-1], r->r_ifname) == 0) &&
-			(!rtm->rtm_table || (r && rtm->rtm_table == r->r_table))) {
+			(!table || (r && table == r->r_table))) {
 
 			err = -EPERM;
 			if (r == &default_rule)
@@ -130,7 +131,7 @@ int dn_fib_rtm_newrule(struct sk_buff *s
 	struct rtmsg *rtm = NLMSG_DATA(nlh);
 	struct dn_fib_rule *r, *new_r, *last = NULL;
 	struct hlist_node *node = NULL;
-	unsigned char table_id;
+	u32 table_id;
 
 	if (rtm->rtm_src_len > 16 || rtm->rtm_dst_len > 16)
 		return -EINVAL;
@@ -141,7 +142,7 @@ int dn_fib_rtm_newrule(struct sk_buff *s
 	if (rtm->rtm_type == RTN_NAT)
 		return -EINVAL;
 
-	table_id = rtm->rtm_table;
+	table_id = rtm_get_table(rtm, rta);
 	if (table_id == RT_TABLE_UNSPEC) {
 		struct dn_fib_table *tb;
 		if (rtm->rtm_type == RTN_UNICAST) {
@@ -364,6 +365,7 @@ #ifdef CONFIG_DECNET_ROUTE_FWMARK
 		RTA_PUT(skb, RTA_PROTOINFO, 4, &r->r_fwmark);
 #endif
 	rtm->rtm_table = r->r_table;
+	RTA_PUT_U32(skb, RTA_TABLE, r->r_table);
 	rtm->rtm_protocol = 0;
 	rtm->rtm_scope = 0;
 	rtm->rtm_type = r->r_action;
diff --git a/net/decnet/dn_table.c b/net/decnet/dn_table.c
index e926c95..d39d75e 100644
--- a/net/decnet/dn_table.c
+++ b/net/decnet/dn_table.c
@@ -263,7 +263,7 @@ static int dn_fib_nh_match(struct rtmsg 
 }
 
 static int dn_fib_dump_info(struct sk_buff *skb, u32 pid, u32 seq, int event,
-                        u8 tb_id, u8 type, u8 scope, void *dst, int dst_len,
+                        u32 tb_id, u8 type, u8 scope, void *dst, int dst_len,
                         struct dn_fib_info *fi, unsigned int flags)
 {
         struct rtmsg *rtm;
@@ -277,6 +277,7 @@ static int dn_fib_dump_info(struct sk_bu
         rtm->rtm_src_len = 0;
         rtm->rtm_tos = 0;
         rtm->rtm_table = tb_id;
+	RTA_PUT_U32(skb, RTA_TABLE, tb_id);
         rtm->rtm_flags = fi->fib_flags;
         rtm->rtm_scope = scope;
 	rtm->rtm_type  = type;
@@ -326,7 +327,7 @@ rtattr_failure:
 }
 
 
-static void dn_rtmsg_fib(int event, struct dn_fib_node *f, int z, int tb_id,
+static void dn_rtmsg_fib(int event, struct dn_fib_node *f, int z, u32 tb_id,
                         struct nlmsghdr *nlh, struct netlink_skb_parms *req)
 {
         struct sk_buff *skb;
@@ -739,7 +740,7 @@ out:
 }
 
 
-struct dn_fib_table *dn_fib_get_table(int n, int create)
+struct dn_fib_table *dn_fib_get_table(u32 n, int create)
 {
         struct dn_fib_table *t;
 
@@ -776,7 +777,7 @@ struct dn_fib_table *dn_fib_get_table(in
         return t;
 }
 
-static void dn_fib_del_tree(int n)
+static void dn_fib_del_tree(u32 n)
 {
 	struct dn_fib_table *t;
 
@@ -790,7 +791,7 @@ static void dn_fib_del_tree(int n)
 
 struct dn_fib_table *dn_fib_empty_table(void)
 {
-        int id;
+        u32 id;
 
         for(id = RT_TABLE_MIN; id <= RT_TABLE_MAX; id++)
                 if (dn_fib_tables[id] == NULL)
diff --git a/net/ipv4/arp.c b/net/ipv4/arp.c
index c8a3723..5df5b1a 100644
--- a/net/ipv4/arp.c
+++ b/net/ipv4/arp.c
@@ -420,6 +420,28 @@ static int arp_ignore(struct in_device *
 	return !inet_confirm_addr(dev, sip, tip, scope);
 }
 
+
+static int is_ip_on_dev(struct net_device* dev, __u32 ip) {
+	int rv = 0;
+	struct in_device* in_dev = in_dev_get(dev);
+	if (in_dev) {
+		struct in_ifaddr *ifa;
+
+		rcu_read_lock();
+		for (ifa = in_dev->ifa_list; ifa; ifa = ifa->ifa_next) {
+			if (ifa->ifa_address == ip) {
+				/* match */
+				rv = 1;
+				break;
+			}
+		}
+		rcu_read_unlock();
+		in_dev_put(in_dev);
+	}
+	return rv;
+}
+
+
 static int arp_filter(__u32 sip, __u32 tip, struct net_device *dev)
 {
 	struct flowi fl = { .nl_u = { .ip4_u = { .daddr = sip,
@@ -431,9 +453,35 @@ static int arp_filter(__u32 sip, __u32 t
 	if (ip_route_output_key(&rt, &fl) < 0) 
 		return 1;
 	if (rt->u.dst.dev != dev) { 
-		NET_INC_STATS_BH(LINUX_MIB_ARPFILTER);
-		flag = 1;
-	} 
+		if ((dev->priv_flags & IFF_ACCEPT_LOCAL_ADDRS) &&
+		    (rt->u.dst.dev == &loopback_dev))  {
+ 			/* Accept these IFF target-ip == dev's IP */
+ 			/* TODO:  Need to force the ARP response back out the interface
+ 			 * instead of letting it route locally.
+ 			 */
+ 			
+ 			if (is_ip_on_dev(dev, tip)) {
+ 				/* OK, we'll let this special case slide, so that we can
+ 				 * arp from one local interface to another.  This seems
+ 				 * to work, but could use some review. --Ben
+ 				 */
+ 				/*printk("arp_filter, sip: %x tip: %x  dev: %s, STS override (ip on dev)\n",
+				  sip, tip, dev->name);*/
+ 			}
+ 			else {
+ 				/*printk("arp_filter, sip: %x tip: %x  dev: %s, IP is NOT on dev\n",
+				  sip, tip, dev->name);*/
+				NET_INC_STATS_BH(LINUX_MIB_ARPFILTER);
+ 				flag = 1;
+ 			}
+		}
+		else {
+			/*printk("arp_filter, not lpbk  sip: %x tip: %x  dev: %s  flgs: %hx  dst.dev: %p  lbk: %p\n",
+			  sip, tip, dev->name, dev->priv_flags, rt->u.dst.dev, &loopback_dev);*/
+			NET_INC_STATS_BH(LINUX_MIB_ARPFILTER);
+			flag = 1;
+		}
+	}
 	ip_rt_put(rt); 
 	return flag; 
 } 
diff --git a/net/ipv4/fib_frontend.c b/net/ipv4/fib_frontend.c
index ba2a707..a150071 100644
--- a/net/ipv4/fib_frontend.c
+++ b/net/ipv4/fib_frontend.c
@@ -36,6 +36,7 @@ #include <linux/if_arp.h>
 #include <linux/skbuff.h>
 #include <linux/netlink.h>
 #include <linux/init.h>
+#include <linux/list.h>
 
 #include <net/ip.h>
 #include <net/protocol.h>
@@ -50,48 +51,67 @@ #define FFprint(a...) printk(KERN_DEBUG 
 
 #ifndef CONFIG_IP_MULTIPLE_TABLES
 
-#define RT_TABLE_MIN RT_TABLE_MAIN
-
 struct fib_table *ip_fib_local_table;
 struct fib_table *ip_fib_main_table;
 
-#else
+#define FIB_TABLE_HASHSZ 1
+static struct hlist_head fib_table_hash[FIB_TABLE_HASHSZ];
 
-#define RT_TABLE_MIN 1
+#else
 
-struct fib_table *fib_tables[RT_TABLE_MAX+1];
+#define FIB_TABLE_HASHSZ 256
+static struct hlist_head fib_table_hash[FIB_TABLE_HASHSZ];
 
-struct fib_table *__fib_new_table(int id)
+struct fib_table *fib_new_table(u32 id)
 {
 	struct fib_table *tb;
+	unsigned int h;
 
+	if (id == 0)
+		id = RT_TABLE_MAIN;
+	tb = fib_get_table(id);
+	if (tb)
+		return tb;
 	tb = fib_hash_init(id);
 	if (!tb)
 		return NULL;
-	fib_tables[id] = tb;
+	h = id & (FIB_TABLE_HASHSZ - 1);
+	hlist_add_head_rcu(&tb->tb_hlist, &fib_table_hash[h]);
 	return tb;
 }
 
+struct fib_table *fib_get_table(u32 id)
+{
+	struct fib_table *tb;
+	struct hlist_node *node;
+	unsigned int h;
 
+	if (id == 0)
+		id = RT_TABLE_MAIN;
+	h = id & (FIB_TABLE_HASHSZ - 1);
+	rcu_read_lock();
+	hlist_for_each_entry_rcu(tb, node, &fib_table_hash[h], tb_hlist) {
+		if (tb->tb_id == id) {
+			rcu_read_unlock();
+			return tb;
+		}
+	}
+	rcu_read_unlock();
+	return NULL;
+}
 #endif /* CONFIG_IP_MULTIPLE_TABLES */
 
-
 static void fib_flush(void)
 {
 	int flushed = 0;
-#ifdef CONFIG_IP_MULTIPLE_TABLES
 	struct fib_table *tb;
-	int id;
+	struct hlist_node *node;
+	unsigned int h;
 
-	for (id = RT_TABLE_MAX; id>0; id--) {
-		if ((tb = fib_get_table(id))==NULL)
-			continue;
-		flushed += tb->tb_flush(tb);
+	for (h = 0; h < FIB_TABLE_HASHSZ; h++) {
+		hlist_for_each_entry(tb, node, &fib_table_hash[h], tb_hlist)
+			flushed += tb->tb_flush(tb);
 	}
-#else /* CONFIG_IP_MULTIPLE_TABLES */
-	flushed += ip_fib_main_table->tb_flush(ip_fib_main_table);
-	flushed += ip_fib_local_table->tb_flush(ip_fib_local_table);
-#endif /* CONFIG_IP_MULTIPLE_TABLES */
 
 	if (flushed)
 		rt_cache_flush(-1);
@@ -186,8 +206,17 @@ int fib_validate_source(u32 src, u32 dst
 
 	if (fib_lookup(&fl, &res))
 		goto last_resort;
-	if (res.type != RTN_UNICAST)
-		goto e_inval_res;
+        
+	if (res.type != RTN_UNICAST) {
+                if ((res.type == RTN_LOCAL) &&
+                    (dev->priv_flags & IFF_ACCEPT_LOCAL_ADDRS)) {
+                        /* All is OK */
+                }
+                else {
+                        goto e_inval_res;
+                }
+        }
+        
 	*spec_dst = FIB_RES_PREFSRC(res);
 	fib_combine_itag(itag, &res);
 #ifdef CONFIG_IP_ROUTE_MULTIPATH
@@ -293,7 +322,8 @@ static int inet_check_attr(struct rtmsg 
 		if (attr) {
 			if (RTA_PAYLOAD(attr) < 4)
 				return -EINVAL;
-			if (i != RTA_MULTIPATH && i != RTA_METRICS)
+			if (i != RTA_MULTIPATH && i != RTA_METRICS &&
+			    i != RTA_TABLE)
 				*rta = (struct rtattr*)RTA_DATA(attr);
 		}
 	}
@@ -309,7 +339,7 @@ int inet_rtm_delroute(struct sk_buff *sk
 	if (inet_check_attr(r, rta))
 		return -EINVAL;
 
-	tb = fib_get_table(r->rtm_table);
+	tb = fib_get_table(rtm_get_table(r, rta));
 	if (tb)
 		return tb->tb_delete(tb, r, (struct kern_rta*)rta, nlh, &NETLINK_CB(skb));
 	return -ESRCH;
@@ -324,7 +354,7 @@ int inet_rtm_newroute(struct sk_buff *sk
 	if (inet_check_attr(r, rta))
 		return -EINVAL;
 
-	tb = fib_new_table(r->rtm_table);
+	tb = fib_new_table(rtm_get_table(r, rta));
 	if (tb)
 		return tb->tb_insert(tb, r, (struct kern_rta*)rta, nlh, &NETLINK_CB(skb));
 	return -ENOBUFS;
@@ -332,29 +362,37 @@ int inet_rtm_newroute(struct sk_buff *sk
 
 int inet_dump_fib(struct sk_buff *skb, struct netlink_callback *cb)
 {
-	int t;
-	int s_t;
+	unsigned int h, s_h;
+	unsigned int e = 0, s_e;
 	struct fib_table *tb;
+	struct hlist_node *node;
+	int dumped = 0;
 
 	if (NLMSG_PAYLOAD(cb->nlh, 0) >= sizeof(struct rtmsg) &&
 	    ((struct rtmsg*)NLMSG_DATA(cb->nlh))->rtm_flags&RTM_F_CLONED)
 		return ip_rt_dump(skb, cb);
 
-	s_t = cb->args[0];
-	if (s_t == 0)
-		s_t = cb->args[0] = RT_TABLE_MIN;
-
-	for (t=s_t; t<=RT_TABLE_MAX; t++) {
-		if (t < s_t) continue;
-		if (t > s_t)
-			memset(&cb->args[1], 0, sizeof(cb->args)-sizeof(cb->args[0]));
-		if ((tb = fib_get_table(t))==NULL)
-			continue;
-		if (tb->tb_dump(tb, skb, cb) < 0) 
-			break;
+	s_h = cb->args[0];
+	s_e = cb->args[1];
+
+	for (h = s_h; h < FIB_TABLE_HASHSZ; h++, s_e = 0) {
+		e = 0;
+		hlist_for_each_entry(tb, node, &fib_table_hash[h], tb_hlist) {
+			if (e < s_e)
+				goto next;
+			if (dumped)
+				memset(&cb->args[2], 0, sizeof(cb->args) -
+				                 2 * sizeof(cb->args[0]));
+			if (tb->tb_dump(tb, skb, cb) < 0)
+				goto out;
+			dumped = 1;
+next:
+			e++;
+		}
 	}
-
-	cb->args[0] = t;
+out:
+	cb->args[1] = e;
+	cb->args[0] = h;
 
 	return skb->len;
 }
@@ -652,9 +690,15 @@ static struct notifier_block fib_netdev_
 
 void __init ip_fib_init(void)
 {
+	unsigned int i;
+
+	for (i = 0; i < FIB_TABLE_HASHSZ; i++)
+		INIT_HLIST_HEAD(&fib_table_hash[i]);
 #ifndef CONFIG_IP_MULTIPLE_TABLES
 	ip_fib_local_table = fib_hash_init(RT_TABLE_LOCAL);
+	hlist_add_head_rcu(&ip_fib_local_table->tb_hlist, &fib_table_hash[0]);
 	ip_fib_main_table  = fib_hash_init(RT_TABLE_MAIN);
+	hlist_add_head_rcu(&ip_fib_main_table->tb_hlist, &fib_table_hash[0]);
 #else
 	fib_rules_init();
 #endif
diff --git a/net/ipv4/fib_hash.c b/net/ipv4/fib_hash.c
index 72c633b..b5bee1a 100644
--- a/net/ipv4/fib_hash.c
+++ b/net/ipv4/fib_hash.c
@@ -684,7 +684,7 @@ fn_hash_dump_bucket(struct sk_buff *skb,
 	struct fib_node *f;
 	int i, s_i;
 
-	s_i = cb->args[3];
+	s_i = cb->args[4];
 	i = 0;
 	hlist_for_each_entry(f, node, head, fn_hash) {
 		struct fib_alias *fa;
@@ -704,14 +704,14 @@ fn_hash_dump_bucket(struct sk_buff *skb,
 					  fa->fa_tos,
 					  fa->fa_info,
 					  NLM_F_MULTI) < 0) {
-				cb->args[3] = i;
+				cb->args[4] = i;
 				return -1;
 			}
 		next:
 			i++;
 		}
 	}
-	cb->args[3] = i;
+	cb->args[4] = i;
 	return skb->len;
 }
 
@@ -722,21 +722,21 @@ fn_hash_dump_zone(struct sk_buff *skb, s
 {
 	int h, s_h;
 
-	s_h = cb->args[2];
+	s_h = cb->args[3];
 	for (h=0; h < fz->fz_divisor; h++) {
 		if (h < s_h) continue;
 		if (h > s_h)
-			memset(&cb->args[3], 0,
-			       sizeof(cb->args) - 3*sizeof(cb->args[0]));
+			memset(&cb->args[4], 0,
+			       sizeof(cb->args) - 4*sizeof(cb->args[0]));
 		if (fz->fz_hash == NULL ||
 		    hlist_empty(&fz->fz_hash[h]))
 			continue;
 		if (fn_hash_dump_bucket(skb, cb, tb, fz, &fz->fz_hash[h])<0) {
-			cb->args[2] = h;
+			cb->args[3] = h;
 			return -1;
 		}
 	}
-	cb->args[2] = h;
+	cb->args[3] = h;
 	return skb->len;
 }
 
@@ -746,28 +746,28 @@ static int fn_hash_dump(struct fib_table
 	struct fn_zone *fz;
 	struct fn_hash *table = (struct fn_hash*)tb->tb_data;
 
-	s_m = cb->args[1];
+	s_m = cb->args[2];
 	read_lock(&fib_hash_lock);
 	for (fz = table->fn_zone_list, m=0; fz; fz = fz->fz_next, m++) {
 		if (m < s_m) continue;
 		if (m > s_m)
-			memset(&cb->args[2], 0,
-			       sizeof(cb->args) - 2*sizeof(cb->args[0]));
+			memset(&cb->args[3], 0,
+			       sizeof(cb->args) - 3*sizeof(cb->args[0]));
 		if (fn_hash_dump_zone(skb, cb, tb, fz) < 0) {
-			cb->args[1] = m;
+			cb->args[2] = m;
 			read_unlock(&fib_hash_lock);
 			return -1;
 		}
 	}
 	read_unlock(&fib_hash_lock);
-	cb->args[1] = m;
+	cb->args[2] = m;
 	return skb->len;
 }
 
 #ifdef CONFIG_IP_MULTIPLE_TABLES
-struct fib_table * fib_hash_init(int id)
+struct fib_table * fib_hash_init(u32 id)
 #else
-struct fib_table * __init fib_hash_init(int id)
+struct fib_table * __init fib_hash_init(u32 id)
 #endif
 {
 	struct fib_table *tb;
diff --git a/net/ipv4/fib_lookup.h b/net/ipv4/fib_lookup.h
index ef6609e..ddd5249 100644
--- a/net/ipv4/fib_lookup.h
+++ b/net/ipv4/fib_lookup.h
@@ -30,11 +30,11 @@ extern struct fib_info *fib_create_info(
 extern int fib_nh_match(struct rtmsg *r, struct nlmsghdr *,
 			struct kern_rta *rta, struct fib_info *fi);
 extern int fib_dump_info(struct sk_buff *skb, u32 pid, u32 seq, int event,
-			 u8 tb_id, u8 type, u8 scope, void *dst,
+			 u32 tb_id, u8 type, u8 scope, void *dst,
 			 int dst_len, u8 tos, struct fib_info *fi,
 			 unsigned int);
 extern void rtmsg_fib(int event, u32 key, struct fib_alias *fa,
-		      int z, int tb_id,
+		      int z, u32 tb_id,
 		      struct nlmsghdr *n, struct netlink_skb_parms *req);
 extern struct fib_alias *fib_find_alias(struct list_head *fah,
 					u8 tos, u32 prio);
diff --git a/net/ipv4/fib_rules.c b/net/ipv4/fib_rules.c
index 79b0471..f5de697 100644
--- a/net/ipv4/fib_rules.c
+++ b/net/ipv4/fib_rules.c
@@ -56,7 +56,7 @@ struct fib_rule
 	struct hlist_node hlist;
 	atomic_t	r_clntref;
 	u32		r_preference;
-	unsigned char	r_table;
+	u32		r_table;
 	unsigned char	r_action;
 	unsigned char	r_dst_len;
 	unsigned char	r_src_len;
@@ -111,6 +111,7 @@ int inet_rtm_delrule(struct sk_buff *skb
 	struct rtmsg *rtm = NLMSG_DATA(nlh);
 	struct fib_rule *r;
 	struct hlist_node *node;
+	u32 table = rtm_get_table(rtm, rta);
 	int err = -ESRCH;
 
 	hlist_for_each_entry(r, node, &fib_rules, hlist) {
@@ -125,7 +126,7 @@ #endif
 		    (!rtm->rtm_type || rtm->rtm_type == r->r_action) &&
 		    (!rta[RTA_PRIORITY-1] || memcmp(RTA_DATA(rta[RTA_PRIORITY-1]), &r->r_preference, 4) == 0) &&
 		    (!rta[RTA_IIF-1] || rtattr_strcmp(rta[RTA_IIF-1], r->r_ifname) == 0) &&
-		    (!rtm->rtm_table || (r && rtm->rtm_table == r->r_table))) {
+		    (!table || (r && table == r->r_table))) {
 			err = -EPERM;
 			if (r == &local_rule)
 				break;
@@ -145,11 +146,11 @@ #endif
 
 static struct fib_table *fib_empty_table(void)
 {
-	int id;
+	u32 id;
 
 	for (id = 1; id <= RT_TABLE_MAX; id++)
-		if (fib_tables[id] == NULL)
-			return __fib_new_table(id);
+		if (fib_get_table(id) == NULL)
+			return fib_new_table(id);
 	return NULL;
 }
 
@@ -177,7 +178,7 @@ int inet_rtm_newrule(struct sk_buff *skb
 	struct rtmsg *rtm = NLMSG_DATA(nlh);
 	struct fib_rule *r, *new_r, *last = NULL;
 	struct hlist_node *node = NULL;
-	unsigned char table_id;
+	u32 table_id;
 
 	if (rtm->rtm_src_len > 32 || rtm->rtm_dst_len > 32 ||
 	    (rtm->rtm_tos & ~IPTOS_TOS_MASK))
@@ -186,7 +187,7 @@ int inet_rtm_newrule(struct sk_buff *skb
 	if (rta[RTA_IIF-1] && RTA_PAYLOAD(rta[RTA_IIF-1]) > IFNAMSIZ)
 		return -EINVAL;
 
-	table_id = rtm->rtm_table;
+	table_id = rtm_get_table(rtm, rta);
 	if (table_id == RT_TABLE_UNSPEC) {
 		struct fib_table *table;
 		if (rtm->rtm_type == RTN_UNICAST) {
@@ -319,7 +320,7 @@ #endif
 		    (r->r_ifindex && r->r_ifindex != flp->iif))
 			continue;
 
-FRprintk("tb %d r %d ", r->r_table, r->r_action);
+FRprintk("tb %u r %d ", r->r_table, r->r_action);
 		switch (r->r_action) {
 		case RTN_UNICAST:
 			policy = r;
@@ -402,6 +403,7 @@ #ifdef CONFIG_IP_ROUTE_FWMARK
 		RTA_PUT(skb, RTA_PROTOINFO, 4, &r->r_fwmark);
 #endif
 	rtm->rtm_table = r->r_table;
+	RTA_PUT_U32(skb, RTA_TABLE, r->r_table);
 	rtm->rtm_protocol = 0;
 	rtm->rtm_scope = 0;
 	rtm->rtm_type = r->r_action;
diff --git a/net/ipv4/fib_semantics.c b/net/ipv4/fib_semantics.c
index 5173800..765da80 100644
--- a/net/ipv4/fib_semantics.c
+++ b/net/ipv4/fib_semantics.c
@@ -273,7 +273,7 @@ int ip_fib_check_default(u32 gw, struct 
 }
 
 void rtmsg_fib(int event, u32 key, struct fib_alias *fa,
-	       int z, int tb_id,
+	       int z, u32 tb_id,
 	       struct nlmsghdr *n, struct netlink_skb_parms *req)
 {
 	struct sk_buff *skb;
@@ -939,7 +939,7 @@ u32 __fib_res_prefsrc(struct fib_result 
 
 int
 fib_dump_info(struct sk_buff *skb, u32 pid, u32 seq, int event,
-	      u8 tb_id, u8 type, u8 scope, void *dst, int dst_len, u8 tos,
+	      u32 tb_id, u8 type, u8 scope, void *dst, int dst_len, u8 tos,
 	      struct fib_info *fi, unsigned int flags)
 {
 	struct rtmsg *rtm;
@@ -953,6 +953,7 @@ fib_dump_info(struct sk_buff *skb, u32 p
 	rtm->rtm_src_len = 0;
 	rtm->rtm_tos = tos;
 	rtm->rtm_table = tb_id;
+	RTA_PUT_U32(skb, RTA_TABLE, tb_id);
 	rtm->rtm_type = type;
 	rtm->rtm_flags = fi->fib_flags;
 	rtm->rtm_scope = scope;
diff --git a/net/ipv4/fib_trie.c b/net/ipv4/fib_trie.c
index 01801c0..2a580eb 100644
--- a/net/ipv4/fib_trie.c
+++ b/net/ipv4/fib_trie.c
@@ -1148,7 +1148,7 @@ fn_trie_insert(struct fib_table *tb, str
 
 	key = ntohl(key);
 
-	pr_debug("Insert table=%d %08x/%d\n", tb->tb_id, key, plen);
+	pr_debug("Insert table=%u %08x/%d\n", tb->tb_id, key, plen);
 
 	mask = ntohl(inet_make_mask(plen));
 
@@ -1848,7 +1848,7 @@ static int fn_trie_dump_fa(t_key key, in
 
 	u32 xkey = htonl(key);
 
-	s_i = cb->args[3];
+	s_i = cb->args[4];
 	i = 0;
 
 	/* rcu_read_lock is hold by caller */
@@ -1870,12 +1870,12 @@ static int fn_trie_dump_fa(t_key key, in
 				  plen,
 				  fa->fa_tos,
 				  fa->fa_info, 0) < 0) {
-			cb->args[3] = i;
+			cb->args[4] = i;
 			return -1;
 		}
 		i++;
 	}
-	cb->args[3] = i;
+	cb->args[4] = i;
 	return skb->len;
 }
 
@@ -1886,14 +1886,14 @@ static int fn_trie_dump_plen(struct trie
 	struct list_head *fa_head;
 	struct leaf *l = NULL;
 
-	s_h = cb->args[2];
+	s_h = cb->args[3];
 
 	for (h = 0; (l = nextleaf(t, l)) != NULL; h++) {
 		if (h < s_h)
 			continue;
 		if (h > s_h)
-			memset(&cb->args[3], 0,
-			       sizeof(cb->args) - 3*sizeof(cb->args[0]));
+			memset(&cb->args[4], 0,
+			       sizeof(cb->args) - 4*sizeof(cb->args[0]));
 
 		fa_head = get_fa_head(l, plen);
 
@@ -1904,11 +1904,11 @@ static int fn_trie_dump_plen(struct trie
 			continue;
 
 		if (fn_trie_dump_fa(l->key, plen, fa_head, tb, skb, cb)<0) {
-			cb->args[2] = h;
+			cb->args[3] = h;
 			return -1;
 		}
 	}
-	cb->args[2] = h;
+	cb->args[3] = h;
 	return skb->len;
 }
 
@@ -1917,23 +1917,23 @@ static int fn_trie_dump(struct fib_table
 	int m, s_m;
 	struct trie *t = (struct trie *) tb->tb_data;
 
-	s_m = cb->args[1];
+	s_m = cb->args[2];
 
 	rcu_read_lock();
 	for (m = 0; m <= 32; m++) {
 		if (m < s_m)
 			continue;
 		if (m > s_m)
-			memset(&cb->args[2], 0,
-				sizeof(cb->args) - 2*sizeof(cb->args[0]));
+			memset(&cb->args[3], 0,
+				sizeof(cb->args) - 3*sizeof(cb->args[0]));
 
 		if (fn_trie_dump_plen(t, 32-m, tb, skb, cb)<0) {
-			cb->args[1] = m;
+			cb->args[2] = m;
 			goto out;
 		}
 	}
 	rcu_read_unlock();
-	cb->args[1] = m;
+	cb->args[2] = m;
 	return skb->len;
 out:
 	rcu_read_unlock();
@@ -1943,9 +1943,9 @@ out:
 /* Fix more generic FIB names for init later */
 
 #ifdef CONFIG_IP_MULTIPLE_TABLES
-struct fib_table * fib_hash_init(int id)
+struct fib_table * fib_hash_init(u32 id)
 #else
-struct fib_table * __init fib_hash_init(int id)
+struct fib_table * __init fib_hash_init(u32 id)
 #endif
 {
 	struct fib_table *tb;
diff --git a/net/ipv4/route.c b/net/ipv4/route.c
index b873cbc..12128b8 100644
--- a/net/ipv4/route.c
+++ b/net/ipv4/route.c
@@ -2652,6 +2652,7 @@ #endif
 	r->rtm_src_len	= 0;
 	r->rtm_tos	= rt->fl.fl4_tos;
 	r->rtm_table	= RT_TABLE_MAIN;
+	RTA_PUT_U32(skb, RTA_TABLE, RT_TABLE_MAIN);
 	r->rtm_type	= rt->rt_type;
 	r->rtm_scope	= RT_SCOPE_UNIVERSE;
 	r->rtm_protocol = RTPROT_UNSPEC;
diff --git a/net/ipv6/route.c b/net/ipv6/route.c
index d9baca0..fa1ecd4 100644
--- a/net/ipv6/route.c
+++ b/net/ipv6/route.c
@@ -1756,6 +1756,7 @@ static int rt6_fill_node(struct sk_buff 
 	rtm->rtm_src_len = rt->rt6i_src.plen;
 	rtm->rtm_tos = 0;
 	rtm->rtm_table = RT_TABLE_MAIN;
+	RTA_PUT_U32(skb, RTA_TABLE, RT_TABLE_MAIN);
 	if (rt->rt6i_flags&RTF_REJECT)
 		rtm->rtm_type = RTN_UNREACHABLE;
 	else if (rt->rt6i_dev && (rt->rt6i_dev->flags&IFF_LOOPBACK))
diff --git a/net/macvlan/Kconfig b/net/macvlan/Kconfig
new file mode 100644
index 0000000..9568916
--- /dev/null
+++ b/net/macvlan/Kconfig
@@ -0,0 +1,6 @@
+config MACVLAN
+	tristate "MAC-VLAN support"
+	depends on EXPERIMENTAL
+	---help---
+	  This allows one to create virtual interfaces that map packets to
+          or from specific MAC addresses to a particular interface.
diff --git a/net/macvlan/Makefile b/net/macvlan/Makefile
new file mode 100644
index 0000000..a7d6002
--- /dev/null
+++ b/net/macvlan/Makefile
@@ -0,0 +1,10 @@
+#
+# Note! Dependencies are done automagically by 'make dep', which also
+# removes any old dependencies. DON'T put your own dependencies here
+# unless it's something special (ie not a .c file).
+#
+# Note 2! The CFLAGS definition is now in the main makefile...
+
+obj-$(CONFIG_MACVLAN) := macvlan.o
+
+
diff --git a/net/macvlan/macvlan.c b/net/macvlan/macvlan.c
new file mode 100644
index 0000000..682e188
--- /dev/null
+++ b/net/macvlan/macvlan.c
@@ -0,0 +1,1805 @@
+/* -*- linux-c -*-
+#######################################################################
+#
+# (C) Copyright 2001-2006
+# Alex Zeffertt, Cambridge Broadband Ltd, ajz@cambridgebroadband.com
+# Re-worked by Ben Greear <greearb@candelatech.com>
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU General Public License as
+# published by the Free Software Foundation; either version 2 of
+# the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.	See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with this program; if not, write to the Free Software
+# Foundation, Inc., 59 Temple Place, Suite 330, Boston,
+# MA 02111-1307 USA
+#######################################################################
+# Notes:
+# 
+# This file implements the macvlan.o MAC address based VLAN support 
+# module.
+#
+# This provides an IOCTL interface which allows you to
+# It uses an IOCTL interface which allows you to
+#
+# 1. enable/disable MAC address based VLANS over an ether type net_device
+# 2. add/remove a MAC address based VLAN - which is an ether type net_device
+#    layered over the original MACVLAN enabled ether type net_device.
+# 3. bind/unbind MAC addresses to/from particular MAC address based VLANs
+# 4. discover the state of MAC address based VLANs on the system.
+# 5. set/get port flags, including whether to bind to destination MAC
+#    or source mac.
+# 6. Traffic to/from eth0 will not be affected.
+
+# Example: (Assuming you are using source binding)
+#
+# If you enable MAC address based VLANS over eth0
+#
+# You may then create further VLANs, e.g. eth0#1 eth0#2 ....
+# These will not receive any frames until you bind MAC addresses to them.
+# If you bind 11:22:33:44:55:66 to eth0#1, then any frames received by
+# eth0 with source MAC 11:22:33:44:55:66 will be routed up through eth0#1
+# instead of eth0.
+#
+# Example: (Assuming you are using destination (local) binding)
+#
+# If you enable MAC address based VLANS over eth0
+#
+# You may then create further VLANs, e.g. eth0#1 eth0#2 ....
+# These will not receive any frames until you bind MAC addresses to them.
+# If you bind 11:22:33:44:55:66 to eth0#1, then any broadcast/multicast
+# frames, or frames with a destination MAC 11:22:33:44:55:66
+# will be routed up through eth0#1 instead of eth0
+#
+# For broadcasts, the packet will be duplicated for every VLAN
+# with at least one MAC attached.  Attaching more than one MAC
+# when destination binding makes no sense...don't do it!
+#
+# 2-27-2006:  Removing source-mac binding logic.  I never use it,
+#             and the code is rotting.  If someone wants to re-add it,
+#             I'm open to patches..but it must be done more cleanly
+#             than the old code. --Ben Greear
+#
+# 
+#######################################################################
+*/
+#include <linux/config.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/kernel.h> 
+#include <linux/fs.h>	  
+#include <linux/errno.h>  
+#include <linux/delay.h>  
+#include <linux/slab.h>
+#include <linux/mm.h>
+#include <linux/ioport.h>
+#include <linux/interrupt.h>
+#include <linux/poll.h>
+#include <linux/types.h>
+#include <linux/string.h>
+#include <linux/if_macvlan.h>
+#include <linux/if_arp.h>
+#include <linux/etherdevice.h>
+#include <net/arp.h>
+#include <linux/notifier.h>
+
+#include <asm/io.h>
+#include <asm/irq.h>
+#include <asm/uaccess.h>
+#include <asm/semaphore.h>
+
+#ifdef CONFIG_PROC_FS
+#include <linux/proc_fs.h>
+#define MVL_PROC_DIR "macvlan"
+#define MVL_PROC_CFG "config"
+#define PORT_CFG_FILE_NAME "config"
+static struct proc_dir_entry *mvl_proc_dir;
+static struct proc_dir_entry *mvl_proc_cfg;
+#endif
+
+#include "macvlan.h"
+
+/* Defined in socket.c */
+void macvlan_ioctl_set(int (*hook)(unsigned long));
+
+static int macvlan_device_event(struct notifier_block *unused,
+				unsigned long event, void *ptr);
+static struct notifier_block macvlan_notifier_block = {
+	.notifier_call = macvlan_device_event,
+};
+
+
+/*********************************************************/
+/*			defines				 */
+/*********************************************************/
+
+#if 0
+#define DEBUG(format,args...) printk(KERN_ERR format, ##args);
+#else
+#define DEBUG(format,args...)
+#endif
+
+
+#undef MVL_USE_RW_LOCKS
+#ifdef MVL_USE_RW_LOCKS
+/*  Must hold this lock to make any changes to the macvlan structures.
+ */
+static rwlock_t mvl_cfg_lock = RW_LOCK_UNLOCKED;
+
+#define MVL_READ_LOCK /* printk("%i: read-lock port list\n", __LINE__); */ \
+                            BUG_ON(in_interrupt()); \
+			    read_lock(&mvl_cfg_lock);
+#define MVL_READ_UNLOCK /* printk("%i: read-unlock port list\n", __LINE__); */ \
+                            BUG_ON(in_interrupt()); \
+			    read_unlock(&mvl_cfg_lock);
+
+#define MVL_WRITE_LOCK /* printk("%i: write-lock port list\n", __LINE__); */ \
+                            BUG_ON(in_interrupt()); \
+			    write_lock(&mvl_cfg_lock);
+#define MVL_WRITE_UNLOCK /* printk("%i: write-unlock port list\n", __LINE__); */ \
+                            BUG_ON(in_interrupt()); \
+			    write_unlock(&mvl_cfg_lock);
+
+
+#define MVL_IRQ_RLOCK(a) /* printk("%i: read-unlock port list\n", __LINE__); */ { \
+                            __u64 now = getCurUs(); \
+	                    __u64 later; \
+			    read_lock_irqsave(&mvl_cfg_lock, a); \
+                            later = getCurUs(); \
+                            if ((later - now) > 100) { \
+	                       printk("took: %lluus to acquire read lock, line: %i\n", \
+				      later - now, __LINE__); \
+                            }}
+
+#define MVL_IRQ_RUNLOCK(a) /* printk("%i: read-unlock port list\n", __LINE__); */ \
+			    read_unlock_irqrestore(&mvl_cfg_lock, a);
+#else
+/*  Must hold this lock to make any changes to the macvlan structures.
+ */
+static spinlock_t mvl_cfg_lock = SPIN_LOCK_UNLOCKED;
+
+#define MVL_READ_LOCK(a) MVL_WRITE_LOCK(a)
+#define MVL_READ_UNLOCK(a) MVL_WRITE_UNLOCK(a)
+
+#define MVL_WRITE_LOCK(a) /* printk("%i: write-lock port list\n", __LINE__); */	\
+		           spin_lock_irqsave(&mvl_cfg_lock, a);
+#define MVL_WRITE_UNLOCK(a) /* printk("%i: write-unlock port list\n", __LINE__); */ \
+                           spin_unlock_irqrestore(&mvl_cfg_lock, a); \
+
+
+#define MVL_IRQ_RLOCK(a) /* printk("%i: read-unlock port list\n", __LINE__); */ \
+			    spin_lock_irqsave(&mvl_cfg_lock, a); \
+
+#define MVL_IRQ_RUNLOCK(a) /* printk("%i: read-unlock port list\n", __LINE__); */ \
+			    spin_unlock_irqrestore(&mvl_cfg_lock, a);
+#endif
+
+
+/*********************************************************/
+/*		       file scope variables		 */
+/*********************************************************/
+
+static struct macvlan_port *port_list = NULL;
+
+static atomic_t macvlan_nports;
+static atomic_t mvl_vlan_counter;
+static atomic_t mvl_netdev_counter;
+
+static int debug_lvl = 0;
+
+
+/*********************************************************/
+/*		   forward declarations			 */
+/*********************************************************/
+
+#ifdef MVL_CONFIG_PROC_FS
+static int read_mvl_glbl(char *page, char **start, off_t off,
+			 int count, int *eof, void *data);
+static int write_mvl_glbl(struct file *file, const char *buffer,
+			  unsigned long count, void *data);
+static int read_mvl(char *page, char **start, off_t off,
+		    int count, int *eof, void *data);
+static int write_mvl(struct file *file, const char *buffer,
+		     unsigned long count, void *data);
+static int read_mvl_port(char *page, char **start, off_t off,
+			 int count, int *eof, void *data);
+static int write_mvl_port(struct file *file, const char *buffer,
+			  unsigned long count, void *data);
+#endif
+
+static int macvlan_port_cleanup(const char* ifname, int grab_rtnl_lock);
+
+
+/*********************************************************/
+/*		   function definitions			 */
+/*********************************************************/
+
+/** Convert to micro-seconds */
+static inline __u64 tv_to_us(const struct timeval* tv) {
+        __u64 us = tv->tv_usec;
+        us += (__u64)tv->tv_sec * (__u64)1000000;
+        return us;
+}
+
+
+/* Since the epoc.  More precise over long periods of time than
+ * getRelativeCurMs
+ */
+static inline __u64 getCurUs(void) {
+        struct timeval tv;
+        do_gettimeofday(&tv);
+        return tv_to_us(&tv);
+}
+
+/*
+char toupper(char in) {
+	if ((in >= 'a') && (in <= 'z')) {
+		in -= ('a' - 'A');
+	}
+	return in;
+}
+*/
+
+#define iswhitespace(x)\
+	((x) == ' ' || (x) == '\n' || (x) == '\r' || (x) == '\r' )
+
+#define skip_whitespace(x) 	{ while (iswhitespace(*x)) (x)++; }
+
+static int copy_next_word(char *dst, char *src, int len) {
+	char *p;
+	for (p=src; p < src + len ; p++) {
+		if ( iswhitespace(*p))
+			break;
+		*dst++ = *p;
+	}
+	return p - src;
+}
+
+#if 0
+static int toMacString(unsigned char* rslt_mac, const char* raw_mac) {
+	// Turn HEX into bytes.  First, gather all the useful HEX
+	char tmp[12]; //More than 12 is useless, at least right now
+	char c;
+	int j = 0; //tmp's index.
+	int i;
+	char tmp_bt[3];
+	for (i = 0; i<strlen(raw_mac); i++) {
+		c = toupper(raw_mac[i]);
+		if (((c >= '0') && (c <= '9')) || ((c >= 'A') && (c <= 'F'))) {
+			tmp[j] = c;
+			//VLOG_ERR(VLOG << " c: " << c << endl);
+			if (j == 11) {
+				break; //done
+			}
+			j++;
+		}
+		else {
+			if ((c == ':') || (c == ' ') || (c == '.')) {
+				// Ok, valid divider
+			}
+			else {
+				// Invalid header
+				return -EINVAL;
+			}
+		}
+	}
+	
+	if (j != 11) {
+		//msg->append("ERROR:  Not enough HEX values in the input string.\n");
+		return -EINVAL;
+	}
+	
+	for (i = 0; i<6; i++) {
+		tmp_bt[0] = tmp[i*2];
+		tmp_bt[1] = tmp[i*2 +1];
+		tmp_bt[2] = 0;
+		//VLOG_ERR(VLOG << " tmp_bt -:" << tmp_bt << ":- i: " << i << endl);
+		rslt_mac[i] = (unsigned char)(simple_strtol(tmp_bt, NULL, 16) & 0xFF);
+		//VLOG_ERR(VLOG << " rslt_mac[" << i << "]  -:" << rslt_mac[i] << ":-\n");
+	}
+	return 0;
+}//toMacString
+#endif
+
+
+struct macvlan_vlan* macvlan_find_vlan_in_port(struct macvlan_port* port,
+					       const char* ifname) {
+	int i;
+	for (i = 0; i<MACVLAN_DEV_HASH_LEN; i++) {
+		struct macvlan_vlan* vlan;
+		for (vlan = port->vlan_hash[i]; vlan; vlan = vlan->next) {
+			if (!strcmp(vlan->dev->name, ifname)) {
+				return vlan;
+			}
+		}
+	}
+	return NULL;
+}	  
+
+
+/* Find port by mac-vlan interface name (eth1#777) */
+struct macvlan_port* macvlan_find_port_for_mvlan_ifname(const char* ifname) {
+	struct macvlan_port* port;
+	for (port = port_list; port; port = port->next) {
+		if (macvlan_find_vlan_in_port(port, ifname)) {
+			break;
+		}
+	}
+	return port;
+}
+
+struct macvlan_port* macvlan_find_port_for_underlying_ifname(const char* ifname) {
+	struct macvlan_port* port;
+	//printk("finding port for underlying ifname: %s\n", ifname);
+	for (port = port_list; port; port = port->next) {
+		//printk("Testing port: %p name: %s\n", port, port->dev->name);
+		if (strcmp(port->dev->name, ifname) == 0) {
+			break;
+		}
+	}
+	//printk("done finding port: %p\n", port);
+	return port;
+}	 
+
+/*
+ *	Rebuild the Ethernet MAC header. This is called after an ARP
+ *	(or in future other address resolution) has completed on this
+ *	sk_buff. We now let ARP fill in the other fields.
+ *
+ *	This routine CANNOT use cached dst->neigh!
+ *	Really, it is used only when dst->neigh is wrong.
+ *
+ */
+int macvlan_dev_rebuild_header(struct sk_buff *skb) {
+	struct net_device *dev = skb->dev;
+	struct ethhdr *veth = (struct ethhdr *)(skb->data);
+
+	switch (veth->h_proto) {
+#ifdef CONFIG_INET
+	case __constant_htons(ETH_P_IP):
+
+		return arp_find(veth->h_dest, skb);
+#endif	
+	default:
+		DEBUG("%s: unable to resolve type %X addresses.\n", 
+		      dev->name, (int)veth->h_proto);
+	 
+		memcpy(veth->h_source, dev->dev_addr, ETH_ALEN);
+		break;
+	};
+
+	return 0;
+}
+
+
+
+static struct net_device_stats *macvlan_get_stats(struct net_device *dev)
+{
+	struct macvlan_vlan *vlan = dev->priv;
+
+	return &vlan->statistics;
+}
+
+static int macvlan_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	struct macvlan_vlan *vlan = dev->priv;
+	int rv;
+	struct sk_buff* skb2;
+	
+	DEBUG("%s: \n", __PRETTY_FUNCTION__);
+
+	skb->dev = vlan->lowerdev;
+
+	/* Please note, dev_queue_xmit consumes the pkt regardless of the
+	 * error value.  So, will copy the skb first and free if successful.
+	 */
+	skb2 = skb_get(skb);
+	rv = dev_queue_xmit(skb2); /* Upon return, skb2 is considered freed */
+	if (rv != 0) {
+		/* The skb memory should still be valid since we made a copy,
+		 * so can return error code here.
+		 */
+		return rv;
+	}
+	else {
+		/* Was success, need to free the skb reference since we bumped up the
+		 * user count above.
+		 */
+		vlan->statistics.tx_packets++;
+		vlan->statistics.tx_bytes += skb->len;
+		kfree_skb(skb);
+		return 0;
+	}
+}/* macvlan xmit */
+
+static int macvlan_open(struct net_device *dev) {
+	netif_start_queue(dev);
+	return 0;
+}
+
+static void macvlan_set_multicast_list(struct net_device *dev) {
+	/* TODO ??? */
+}
+
+static int remove_vlan_from_hash(struct macvlan_vlan* vlan) {
+	struct macvlan_vlan* walker;
+	struct macvlan_vlan* prev;
+	int i;
+	
+	/*
+	 * remove the vlan in question from the list.
+	 */
+	for (i = 0; i<265; i++) {
+		prev = NULL;
+		walker = vlan->port->vlan_hash[i];
+		while (walker) {
+			if (walker == vlan) {
+				if (prev) {
+					prev->next = walker->next;
+				}
+				else {
+					vlan->port->vlan_hash[i] = walker->next;
+				}
+				return 1;
+			}
+			prev = walker;
+			walker = walker->next;
+		}/* while */
+	}//for
+	printk("ERROR:  Failed to find and remove MAC-VLAN from hash.\n");
+	return -ENODEV;
+}
+
+
+static int macvlan_set_mac_address(struct net_device *dev, void* p) {
+	struct sockaddr *addr = p;
+	struct macvlan_port* port;
+	struct macvlan_vlan* vlan;
+	unsigned long flags;
+	
+	if (netif_running(dev))
+		return -EBUSY;
+
+	MVL_WRITE_LOCK(flags);
+	port = macvlan_find_port_for_mvlan_ifname(dev->name);
+	if (!port) {
+		printk("ERROR:  macvlan:  can't find port for dev: %s\n", dev->name);
+		MVL_WRITE_UNLOCK(flags);
+		return -ENODEV;
+	}
+
+	vlan = macvlan_find_vlan_in_port(port, dev->name);
+	if (!vlan) {
+		printk("ERROR:  macvlan:  can't find vlan for dev: %s\n", dev->name);
+		MVL_WRITE_UNLOCK(flags);
+		return -ENODEV;
+	}
+	
+	/* Remove device from the port's hash, since changing mac will change
+	 * it's bucket.
+	 */
+	remove_vlan_from_hash(vlan);
+
+	BUG_ON(vlan->dev != dev);
+	
+	/* Set the new MAC */
+	memcpy(vlan->dev->dev_addr, addr->sa_data, dev->addr_len);
+
+	/* And now re-add to the hash */
+	vlan->next = port->vlan_hash[vlan->dev->dev_addr[5]];
+	port->vlan_hash[vlan->dev->dev_addr[5]] = vlan;
+	MVL_WRITE_UNLOCK(flags);
+	return 0;
+}
+
+static int macvlan_stop(struct net_device *dev) {
+	netif_stop_queue(dev);
+	return 0;
+}
+
+
+/*
+ *	Create the VLAN header for an arbitrary protocol layer 
+ *
+ *	saddr=NULL	means use device source address
+ *	daddr=NULL	means leave destination address (eg unresolved arp)
+ *
+ *  This is called when the SKB is moving down the stack towards the
+ *  physical devices.
+ */
+int macvlan_hard_header(struct sk_buff *skb, struct net_device *dev,
+			unsigned short type, void *daddr, void *saddr,
+			unsigned len)
+{
+	struct macvlan_vlan *vlan = dev->priv;
+	
+	DEBUG("%s: \n", __PRETTY_FUNCTION__);
+
+	/* Before delegating work to the lower layer, enter our MAC-address */
+	saddr = dev->dev_addr;
+
+	dev = vlan->lowerdev;
+
+	/* Now make the underlying real hard header */
+	return dev->hard_header(skb, dev, type, daddr, saddr, len);
+}
+
+
+void macvlan_dev_destructor(struct net_device *dev) {
+	atomic_dec(&mvl_netdev_counter);
+	if (dev->priv) {
+		//printk("dst: %s", dev->name);
+		kfree(dev->priv); /* priv holds the macvlan_vlan struct */
+		atomic_dec(&mvl_vlan_counter);
+		dev->priv = NULL;
+	}
+	else {
+		//printk("dst2: %s", dev->name);
+	}
+}
+
+int macvlan_dev_change_mtu(struct net_device *dev, int new_mtu) {
+	struct macvlan_vlan *vlan = dev->priv;
+	if (vlan->lowerdev->mtu < new_mtu) {
+		printk("ERROR:  Cannot set MTU for macvlan: %s (%i) less than MTU of lower-device: %s (%i)\n",
+		       dev->name, new_mtu, vlan->lowerdev->name,
+		       vlan->lowerdev->mtu);
+		return -EINVAL;
+	}
+	
+	dev->mtu = new_mtu;
+	return 0;
+}
+
+
+static int macvlan_vlan_create(const char* port_name, int newifidx) {
+	struct macvlan_vlan *vlan = NULL;
+	struct macvlan_port* port;
+	char newifname[IFNAMSIZ+1];
+	struct net_device* td = NULL;
+	struct net_device* nnd = NULL;
+	unsigned long flags;
+	int rv;
+	int i;
+	
+	//printk("in macvlan_vlan_create  ");
+
+	//printk("malloc ");
+	if ((vlan = kmalloc(sizeof(*vlan), GFP_KERNEL)) == NULL) {
+		DEBUG("macvlan: kmalloc failure\n");
+		rv = -ENOMEM;
+		goto outfree;
+	}
+	memset(vlan, 0, sizeof(*vlan));
+	atomic_inc(&mvl_vlan_counter);
+	
+	//printk("4 ");
+	if ((nnd = kmalloc(sizeof(struct net_device), GFP_KERNEL)) == NULL) {
+		DEBUG("macvlan: kmalloc net_device failure\n");
+		rv = -ENOMEM;
+		goto outfree;
+	}
+	memset(nnd, 0, sizeof(*nnd));
+	atomic_inc(&mvl_netdev_counter);
+	
+
+	MVL_WRITE_LOCK(flags);
+
+	//printk("--*-- ");
+	/* find the port to which ifname belongs */
+	port = macvlan_find_port_for_underlying_ifname(port_name);
+	//printk("port: %p  name: %s\n", port, port_name);
+	if (!port) {
+		rv = -ENODEV;
+		goto unlockout;
+	}
+
+	BUG_ON(!port->dev);
+
+	//printk("1 ");
+	if (newifidx < 0) {
+		/* Find the next free index */
+		int i;
+		for (i = 0; i<MAX_MACVLANS_PER_PORT; i++) {
+			snprintf(newifname, IFNAMSIZ, "%s#%d", port->dev->name, i);
+			newifname[IFNAMSIZ] = 0;
+			if ((td = dev_get_by_name(newifname)) == NULL) {
+				newifidx = i;
+				break;
+			}
+			dev_put(td);
+		}
+
+		if (newifidx < 0) {
+			printk("macvlan: Could not find a free index, reached max: %i\n", i);
+		}
+		td = NULL; // Make sure we don't accidentally use this later.
+	}
+	
+	//printk("2 ");
+	/* generate a name for the new vlan */
+	snprintf(newifname, IFNAMSIZ, "%s#%d", port->dev->name, newifidx);
+	newifname[IFNAMSIZ] = 0;
+
+	if ((td = dev_get_by_name(newifname)) != NULL) {
+		DEBUG("macvlan: vlan by that name already exists\n");
+		rv = -EEXIST;
+		dev_put(td);
+		goto unlockout;
+	}
+
+	//printk("4 ");
+	vlan->dev = nnd;
+	
+	//printk("5 ");
+	strcpy(vlan->dev->name, newifname);
+	ether_setup(vlan->dev);
+	
+	//printk("dev_hold(vlan->dev: %s 0x%x\n", vlan->dev->name, vlan->dev);
+	dev_hold(vlan->dev); /* MVL code holds reference */
+	
+	vlan->dev->priv = vlan;
+	vlan->port = port;
+	vlan->lowerdev = port->dev;
+	/* Set a flag so we know this is a vlan device */
+	vlan->dev->priv_flags |= (IFF_MAC_VLAN);
+	/* Start with random MAC address (first octet is 0) */
+	for (i = 1; i<6; i++) {
+		vlan->dev->dev_addr[i] = net_random();
+	}
+	/* Set MTU to that of lower-dev by default. */
+	vlan->dev->mtu = port->dev->mtu;
+	
+	//printk("6 ");
+	/* dev->do_ioctl = macvlan_do_ioctl; */
+	vlan->dev->get_stats = macvlan_get_stats;
+	vlan->dev->hard_start_xmit = macvlan_xmit;
+	vlan->dev->hard_header = macvlan_hard_header;
+	vlan->dev->rebuild_header = macvlan_dev_rebuild_header;
+	vlan->dev->open = macvlan_open;
+	vlan->dev->set_multicast_list = macvlan_set_multicast_list;
+	vlan->dev->stop = macvlan_stop;
+	vlan->dev->tx_queue_len = 0;
+	vlan->dev->set_mac_address = macvlan_set_mac_address;
+	vlan->dev->priv = vlan;
+	vlan->dev->destructor = macvlan_dev_destructor;
+	vlan->dev->change_mtu = macvlan_dev_change_mtu;
+	
+	DEBUG("macvlan: created vlan %p\n", vlan);
+
+	atomic_inc(&port->ndevs);
+	
+	/* link to list */
+	//printk("8 ");
+	vlan->next = port->vlan_hash[vlan->dev->dev_addr[5]];
+	port->vlan_hash[vlan->dev->dev_addr[5]] = vlan;
+
+	//printk("End of mac_vlan create1\n");
+	
+	MVL_WRITE_UNLOCK(flags);
+	register_netdev(vlan->dev);
+
+#ifdef MVL_CONFIG_PROC_FS
+	//printk("7 ");
+	if (vlan->port->proc_dir) {
+		vlan->proc_ent = create_proc_read_entry(vlan->dev->name, S_IRUGO,
+							vlan->port->proc_dir,
+							read_mvl, vlan);
+		if (!vlan->proc_ent) {
+			printk("ERROR:  Could not create proc entry for device: %s\n",
+			       vlan->dev->name);
+		}
+		else {
+			vlan->proc_ent->write_proc = write_mvl;
+			vlan->proc_ent->owner = THIS_MODULE;
+		}
+	}
+#endif
+
+	//printk("End of mac_vlan create2\n");
+
+	//printk("9\n");
+	rv = 0;
+	goto out;
+
+	/* Error case, clean up vlan memory */
+ unlockout:
+	MVL_WRITE_UNLOCK(flags);
+ outfree:
+	if (vlan) {
+		kfree(vlan);
+		atomic_dec(&mvl_vlan_counter);
+	}
+	if (nnd) {
+		kfree(nnd);
+		atomic_dec(&mvl_netdev_counter);
+	}
+ out:
+	return rv;
+} /* macvlan_vlan_create */
+
+static int macvlan_device_event(struct notifier_block *unused,
+				unsigned long event, void *ptr) {
+	struct net_device* dev = ptr;
+	struct macvlan_port* mvl;
+	unsigned long flags;
+	
+	MVL_WRITE_LOCK(flags);
+	mvl = macvlan_find_port_for_underlying_ifname(dev->name);
+	MVL_WRITE_UNLOCK(flags);
+	
+	if (!mvl) {
+		//printk("redirdev: Ignoring event: %lu for device: %s\n",
+		//       event, dev->name);
+		goto out;
+	}
+
+
+	/* It is OK that we do not hold the group lock right now,
+	 * as we run under the RTNL lock.
+	 */
+
+	switch (event) {
+	case NETDEV_CHANGE:
+	case NETDEV_DOWN:
+	case NETDEV_UP:
+		//printk("macvlan: Ignoring change/up/down for device: %s\n",
+		//       dev->name);
+		/* Ignore for now */
+		break;
+
+	case NETDEV_UNREGISTER:
+		/* Get rid of the mac-vlan too */
+		printk("MACVLAN: Device: %s is going away, removing associated mac-vlans too.\n",
+		       dev->name);
+		/* The zero means do not try to grab rtnl..since rtnl is already held. */
+		macvlan_port_cleanup(dev->name, 0);
+		break;
+
+	};
+
+out:
+	return NOTIFY_DONE;
+}//event handler
+
+
+/* Has locking internally */
+int macvlan_vlan_cleanup(const char* ifname, int grab_rtnl_lock) {
+	struct macvlan_port* port;
+	struct macvlan_vlan* vlan;
+	unsigned long flags;
+	int rv;
+	
+	DEBUG(__FUNCTION__"(%p)\n",vlan);
+	//printk("mvl_cln: %s", ifname);
+
+	MVL_WRITE_LOCK(flags);
+	
+	/* NOTE:  Cannot depend on device name, it can be changed. --Ben */
+	port = macvlan_find_port_for_mvlan_ifname(ifname);
+	if (!port) {
+		printk("ERROR:  macvlan_vlan_cleanup:  can't find port for mvlan: %s\n", ifname);
+		rv = -ENODEV;
+		goto unlockout;
+	}
+
+	//printk("1 ");
+	vlan = macvlan_find_vlan_in_port(port, ifname);
+	BUG_ON(!vlan);
+
+#ifdef MVL_CONFIG_PROC_FS
+	if (vlan->proc_ent) {
+		remove_proc_entry(vlan->dev->name, vlan->port->proc_dir);
+		vlan->proc_ent = NULL;
+	}
+#endif
+
+	remove_vlan_from_hash(vlan);
+	
+	atomic_dec(&port->ndevs);
+
+	//printk("4 ");
+	//printk("End of mac_vlan cleanup1, ref-cnt: %i\n", atomic_read(&vlan->dev->refcnt));
+	MVL_WRITE_UNLOCK(flags);
+	
+	//printk("End of mac_vlan cleanup2, ref-cnt: %i\n", atomic_read(&vlan->dev->refcnt));
+	dev_put(vlan->dev);
+
+	if (grab_rtnl_lock) {
+		unregister_netdev(vlan->dev);
+	}
+	else {
+		unregister_netdevice(vlan->dev);
+	}
+
+
+	/* VLAN struct is freed by the device destructor, so no need to free it
+	 * here.
+	 */
+	
+	//printk("5 ");
+	rv = 0;
+	goto out;
+
+ unlockout:
+	MVL_WRITE_UNLOCK(flags);
+
+ out:
+	return rv;
+	
+} /* mac_vlan cleanup */
+
+
+
+static int macvlan_port_set_flags(const char* ifname, int flags) {
+	struct macvlan_port *port;
+			
+	/* find the port to which ifname belongs */
+	port = macvlan_find_port_for_underlying_ifname(ifname);
+	if (!port) {
+		return -ENODEV;
+	}
+	else {
+		port->flags = flags;
+	}
+	return 0;
+}/* macvlan_port_set_flags */
+
+static int macvlan_port_create(const char* ifname) {
+	struct macvlan_port *port;
+	struct net_device* dev;
+	int rv = 0;
+	unsigned long flags;
+	
+	MVL_WRITE_LOCK(flags);
+	port = macvlan_find_port_for_underlying_ifname(ifname);
+	if (port != NULL) {
+		rv = -EEXIST;
+		goto outunlock;
+	}
+		
+	dev = dev_get_by_name(ifname);
+	if (dev == NULL) {
+		rv = -ENODEV;
+		goto outunlock;
+	}
+
+	if ((dev->macvlan_priv != NULL)
+	    || (dev->flags & IFF_LOOPBACK)
+	    || (dev->type != ARPHRD_ETHER)) {
+		printk("macvlan: lower layer failed port_create, dev: %s "
+		       "dev->macvlan_priv=%p dev->flags=%08x dev->type=%08x\n",
+		       dev->name, dev->macvlan_priv, dev->flags, dev->type);
+		dev_put(dev);
+		rv = -EINVAL;
+		goto outunlock;
+	}
+
+	if ((port = kmalloc(sizeof(*port), GFP_ATOMIC)) == NULL) {
+		dev_put(dev);
+		rv = -ENOBUFS;
+		goto outunlock;
+	}
+	memset(port, 0, sizeof(*port));
+	atomic_inc(&macvlan_nports);
+
+	port->dev = dev;
+	port->flags = 0; // none defined currently
+
+	dev->macvlan_priv = port;
+
+	/* Link into our list */
+	port->next = port_list;
+	port_list = port;
+
+	/* Unlock our write lock on the ports... */
+	MVL_WRITE_UNLOCK(flags);
+	
+	/* TODO:  Could use multicast filters in some NICs at least. */
+	dev_set_promiscuity(dev, 1);
+
+#ifdef MVL_CONFIG_PROC_FS
+	if (mvl_proc_dir) {
+		port->proc_dir = proc_mkdir(port->dev->name, mvl_proc_dir);
+
+		if (port->proc_dir) {
+			port->proc_ent = create_proc_read_entry(PORT_CFG_FILE_NAME, S_IRUGO,
+								port->proc_dir,
+								read_mvl_port, port);
+			if (port->proc_ent) {
+				port->proc_ent->write_proc = write_mvl_port;
+				port->proc_ent->owner = THIS_MODULE;
+			}
+			else {
+				printk("macvlan: ERROR: failed to create proc entry for port: %s\n",
+				       port->dev->name);
+			}
+		}
+	}
+#endif
+
+	DEBUG("macvlan: created port=%p\n", port);
+	return 0;
+
+outunlock:
+	MVL_WRITE_UNLOCK(flags);
+	return rv;
+	
+}/* macvlan_port_create */
+
+
+/* Clears all memory, kfree's it if possible.
+ */
+static int macvlan_port_cleanup(const char* ifname, int grab_rtnl_lock) {
+	struct macvlan_port *port;
+	struct macvlan_port *prev;
+	struct macvlan_port *walker;
+	int i;
+	unsigned long flags;
+	int rv = 0;
+	char nm[24];
+	
+	MVL_WRITE_LOCK(flags);
+	port = macvlan_find_port_for_underlying_ifname(ifname);
+	if (!port) {
+		printk("MACVLAN:  Could not find port: %s in macvlan_port_cleanup.\n",
+		       ifname);
+		rv = -ENODEV;
+		goto outunlock;
+	}
+
+vlan_clean_check:
+	for (i = 0; i<MACVLAN_DEV_HASH_LEN; i++) {
+		if (port->vlan_hash[i]) {
+			MVL_WRITE_UNLOCK(flags);
+			strncpy(nm, port->vlan_hash[i]->dev->name, sizeof(nm));
+			macvlan_vlan_cleanup(nm, grab_rtnl_lock);
+			MVL_WRITE_LOCK(flags);
+			goto vlan_clean_check;
+		}
+	}
+
+	/* Remove from our port list */
+	prev = NULL;
+	walker = port_list;
+	while (walker) {
+		if (walker == port) {
+			if (prev) {
+				prev->next = walker->next;
+			}
+			else {
+				port_list = walker->next;
+			}
+			break;
+		}
+		prev = walker;
+		walker = walker->next;
+	}
+	BUG_ON(walker != port);
+	
+	
+	port->dev->macvlan_priv = NULL;
+
+	MVL_WRITE_UNLOCK(flags);
+	
+#ifdef MVL_CONFIG_PROC_FS
+	if (port->proc_dir) {
+		if (port->proc_ent) {
+			remove_proc_entry(PORT_CFG_FILE_NAME, port->proc_dir);
+			port->proc_ent = NULL;
+		}
+		
+		remove_proc_entry(port->dev->name, mvl_proc_dir);
+		port->proc_dir = NULL;
+	}
+#endif
+	
+	dev_set_promiscuity(port->dev, -1);
+	dev_put(port->dev);
+	
+	kfree(port);
+	atomic_dec(&macvlan_nports);
+	
+	return 0;
+
+outunlock:
+	MVL_WRITE_UNLOCK(flags);
+	return rv;
+	
+}/* macvlan_port_cleanup */
+
+
+static inline struct macvlan_vlan *macvlan_hash_lookup(struct macvlan_port *port,
+						       const unsigned char *src) {
+	/* 
+	 * The hashing function is to simply
+	 * take the bottom source address byte
+	 */
+	struct macvlan_vlan *entry;
+	for (entry = port->vlan_hash[src[5]]; entry; entry = entry->next) {
+		if (memcmp(entry->dev->dev_addr, src, ETH_ALEN) == 0) {
+			/*DEBUG("macvlan: matched %02x:%02x:%02x:%02x:%02x:%02x to vlan %p\n", 
+			  src[0],src[1],src[2],src[3],src[4],src[5],entry->vlan); */
+			return entry;
+		}
+	}
+	return NULL;
+}
+
+
+static int macvlan_ioctl_deviceless_stub(unsigned long arg) {
+	int err = 0;
+	struct macvlan_ioctl req;
+	int32_t _cmd;
+	unsigned long flags;
+	
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	/* Special hack:  Allow old API probe 'MACVLAN_IS_MACVLAN' with
+	 * undersized 'req' to fail gracefully.  This lets user-space be
+	 * backwards compatible if it's paying attention.
+	 */
+	if (copy_from_user(&_cmd, (void *)arg, sizeof(_cmd))) {
+		return -EFAULT;
+	}
+	else {
+		if (_cmd == MACVLAN_IS_MACVLAN) {
+			return -EINVAL;
+		}
+	}
+
+	if (copy_from_user(&req, (void *)arg, sizeof(req)))
+		return -EFAULT;
+
+	/* ensure field termination */
+	req.reply.name[0] = 0;
+	req.ifname[IFNAMSIZ - 1] = 0;
+
+	//printk("req.cmd: %d  req.ifname: %s\n", req.cmd, req.ifname);
+
+	switch (req.cmd)
+	{
+	case MACVLAN_ENABLE:
+	{
+		/* 
+		 * enable creation of mac based vlans
+		 * layered over an ethernet device
+		 */
+		err = macvlan_port_create(req.ifname);
+		break;
+	}
+	case MACVLAN_DISABLE:
+	{
+		err = macvlan_port_cleanup(req.ifname, 1);
+		break;
+	}
+	case MACVLAN_ADD:
+	{
+		/* 
+		 * create a new mac based vlan
+		 */
+		/* Has internal locking. */
+		err = macvlan_vlan_create(req.ifname, req.ifidx);
+		
+		break;
+	}
+	case MACVLAN_SET_PORT_FLAGS:
+	{
+		/* 
+		 * Set a macvlan_port's flags
+		 */
+		MVL_WRITE_LOCK(flags);
+		err = macvlan_port_set_flags(req.ifname, req.ifidx);
+		MVL_WRITE_UNLOCK(flags);		
+		break;
+	}
+	case MACVLAN_GET_PORT_FLAGS:
+	{
+		/* 
+		 * Set a macvlan_port's flags
+		 */
+		struct macvlan_port *port;
+		
+		MVL_READ_LOCK(flags);
+		/* find the port to which ifname belongs */
+		port = macvlan_find_port_for_mvlan_ifname(req.ifname);
+		if (!port) {
+			err = -ENODEV;
+		}
+		else {
+			req.reply.num = port->flags;
+		}
+		MVL_READ_UNLOCK(flags);
+		
+		if (copy_to_user((void *)(arg), &req, sizeof(req))) {
+			err = -EFAULT;
+		}
+
+		break;
+	}
+	case MACVLAN_DEL:
+	{
+		/*
+		 * destroy a mac based vlan
+		 */
+
+		/* Has internal locking */
+		err = macvlan_vlan_cleanup(req.ifname, 1);
+		break;
+	}
+	
+	case MACVLAN_IS_MACVLAN2:
+	{
+		/* 
+		 * Give user-space a chance of determining if we are a MAC-VLAN nor not.
+		 *  (If the IOCTL fails, we are not, otherwise we are.)
+		 */
+		struct macvlan_port *port;
+		MVL_READ_LOCK(flags);
+		/* find the port in question */
+		port = macvlan_find_port_for_mvlan_ifname(req.ifname);
+		MVL_READ_UNLOCK(flags);
+
+		if (!port) {
+			/* printk("device: %s is NOT a MAC-VLAN\n", ifname); */
+			err = -ENODEV;
+		}
+		else {
+			/* printk("device: %s IS a MAC-VLAN\n", ifname); */
+			err = 0;
+		}
+		break;
+	}
+	case MACVLAN_GET_NUM_PORTS:
+	{
+		/* 
+		 * how many ethernet devices have mac based vlans enabled over them
+		 */
+		req.reply.num = atomic_read(&macvlan_nports);
+		if (copy_to_user((void *)(&arg), &req, sizeof(req))) {
+			err = -EFAULT;
+			break;
+		}
+		break;
+	}
+	case MACVLAN_GET_PORT_NAME:
+	{
+		/* 
+		 * name the nth device which has mac based vlans enabled over it
+		 */
+		struct macvlan_port *port;
+		int n = req.portidx;
+
+		MVL_READ_LOCK(flags);
+		/* find the port in question */
+		for (port = port_list; port && n; port = port->next, n--);
+		if (!port) {
+			err = -ENODEV;
+			MVL_READ_UNLOCK(flags);
+		}
+		else {
+			strncpy(req.reply.name, port->dev->name, IFNAMSIZ);
+			MVL_READ_UNLOCK(flags);
+					
+			if (copy_to_user((void *)(arg), &req, sizeof(req))) {
+				err = -EFAULT;
+			}
+		}
+		break;
+	}
+	case MACVLAN_GET_NUM_VLANS:
+	{
+		/*
+		 * how many vlans are layered over the nth mac-based
+		 * vlan enabled device
+		 */
+
+		struct macvlan_port *port;
+		int n = req.portidx;
+
+		MVL_READ_LOCK(flags);
+		/* find the port in question */
+		for (port = port_list; port && n; port = port->next, n--);
+
+		if (!port) {
+			err = -ENODEV;
+			MVL_READ_UNLOCK(flags);
+		}
+		else {
+			req.reply.num = atomic_read(&port->ndevs);
+			MVL_READ_UNLOCK(flags);
+			if (copy_to_user((void *)arg, &req, sizeof(req))) {
+				err = -EFAULT;
+			}
+		}
+	
+		break;
+	}
+	case MACVLAN_GET_VLAN_NAME:
+	{
+		/* 
+		 * what's the name of the mth vlan layered over the nth
+		 * mac-based-vlan enabled ethernet device
+		 */
+		struct macvlan_port *port;
+		struct macvlan_vlan *vlan;
+		int n = req.portidx;
+		int m = req.ifidx;
+		int i;
+		int sofar = 0;
+		
+		MVL_READ_LOCK(flags);
+		/* find the port in question */
+		for (port = port_list; port && n; port = port->next, n--);
+		if (!port) {
+			err = -EINVAL;
+			MVL_READ_UNLOCK(flags);
+		}
+		else {
+			/* find the vlan in question */
+			for (i = 0; i<MACVLAN_DEV_HASH_LEN; i++) {
+				for (vlan = port->vlan_hash[i]; vlan; vlan = vlan->next) {
+					sofar++;
+					if (sofar == m) {
+						goto after_for;
+					}
+				}
+			}
+		after_for:
+
+			if (!vlan) {
+				err = -ENODEV;
+			}
+			else {
+				strncpy(req.reply.name, vlan->dev->name, IFNAMSIZ);
+			}
+			MVL_READ_UNLOCK(flags);
+			if (copy_to_user((void *)arg, &req, sizeof(req))) {
+				err = -EFAULT;
+			}
+		}
+		break;
+	}
+	default:
+		err = -EOPNOTSUPP;
+		break;
+	}
+
+	/* printk("Returning err: %i\n", err); */
+	return err;
+}/* ioctl handler */
+
+
+/* Return >= 0 if packet is consumed, otherwise return < 0. */
+static inline int mvl_handle_frame_fod(struct macvlan_port* port, struct sk_buff* skb) {
+	struct macvlan_vlan *vlan; /* the higher layer i/f to which skbuff is mapped */
+	int rv;
+	unsigned long flags;
+	struct net_device* sdev;
+	int i;
+	
+	/* Filtering on destination.. */
+	/* If it's a broadcast pkt, send it to all of them.  Otherwise,
+	 * send it to just one of them.
+	 */
+	if ((skb->pkt_type == PACKET_BROADCAST) || (skb->pkt_type == PACKET_MULTICAST)) {
+		/* never consume if we take this code branch, because it's bcast */
+		DEBUG("%s:  got port: %p, filtering on DEST, type is bcast or multicast\n",
+		      __PRETTY_FUNCTION__, port);
+		//printk("fod: ");
+		MVL_IRQ_RLOCK(flags);
+		//printk("1 ");
+		for (i = 0; i<MACVLAN_DEV_HASH_LEN; i++) {
+			for (vlan = port->vlan_hash[i]; vlan; vlan = vlan->next) {
+				//printk(".");
+				DEBUG("%s:  got vlan: %s, nmacs: %i, up: %i\n",
+				      __PRETTY_FUNCTION__, vlan->dev->name,
+				      vlan->nmacs, (vlan->dev->flags & IFF_UP));
+				if (vlan->dev->flags & IFF_UP) {
+					struct sk_buff* nskb;
+					
+					atomic_inc(&skb->users);
+					nskb = skb_share_check(skb, GFP_ATOMIC);
+					if (!nskb) {
+						vlan->statistics.rx_fifo_errors++;
+						vlan->statistics.rx_errors++;
+					}
+					else {
+						vlan->statistics.rx_packets++;
+						/*  Count the lower-level's header to make our counters
+						 *  look more like an ethernet device. */
+						vlan->statistics.rx_bytes +=
+							(nskb->len + vlan->lowerdev->hard_header_len);
+						vlan->statistics.multicast++;
+						
+						nskb->dev = vlan->dev;
+						netif_rx(nskb);
+					}
+				}
+			}/* for each device in the bucket */
+		}/* for each bucket */
+		//printk("2 ");
+		rv = -1; /* did not consume this pkt, merely tasted it */
+		MVL_IRQ_RUNLOCK(flags);
+		goto out;
+	}
+	else {
+		struct ethhdr *eth = eth_hdr(skb);
+		char* d = eth->h_dest;
+		/* Not a broadcast, try to find our port based on DESTINATION */
+		//printk("fodNB ");
+		MVL_IRQ_RLOCK(flags);
+		if (!(vlan = macvlan_hash_lookup(port, d))) {
+			/* not for us */
+			DEBUG("%s:  not a broadcast, and could not find vlan for dest: %2hx:%2hx:%2hx:%2hx:%2hx:%2hx\n",
+			      __PRETTY_FUNCTION__, d[0], d[1], d[2], d[3], d[4], d[5]);
+			
+			rv = -ENODEV;
+			//printk("1 ");
+		}
+		else {
+			DEBUG("%s:  not a broadcast, found vlan for dest: "
+			      "%2hx:%2hx:%2hx:%2hx:%2hx:%2hx, up: %i\n",
+			      __PRETTY_FUNCTION__, d[0], d[1], d[2], d[3], d[4], d[5],
+			      (vlan->dev->flags & IFF_UP));
+		
+			if (!(vlan->dev->flags & IFF_UP)) {
+				kfree_skb(skb);
+				rv = 0; /* consume */
+			}
+			else {
+				vlan->statistics.rx_packets++;
+				/*  Count the lower-level's header to make our counters
+				 *  look more like an ethernet device. */
+				vlan->statistics.rx_bytes +=
+					(skb->len + vlan->lowerdev->hard_header_len);
+		
+				skb->dev = vlan->dev;
+				sdev = skb->dev;
+				if (!(eth->h_dest[0] & 1)) {
+					/* if it's not multicast, see if it's
+					 * for us, or not.
+					 */
+					if (memcmp(sdev->dev_addr, eth->h_dest, ETH_ALEN)) {
+						skb->pkt_type = PACKET_OTHERHOST;
+					}
+					else {
+						skb->pkt_type = PACKET_HOST;
+					}
+				}
+				dev_hold(sdev);
+				MVL_IRQ_RUNLOCK(flags);
+				//printk("2 ");
+				netif_rx(skb);
+				dev_put(sdev);
+				//printk("3 ");
+				rv = 0;
+				goto out;
+			}
+		}
+	}/* else, was not broadcast */
+
+	MVL_IRQ_RUNLOCK(flags);
+	//printk("4 ");
+
+ out:
+	//printk("5 ");
+	return rv;
+}/* filter on dest */
+
+
+/* global entry point when receiving a pkt from lower-level devices.  Return
+ * >= 0 if we consume, otherwise packet will be sent to the rest of the stack
+ * as normal.
+ *
+ */
+static int macvlan_handle_frame(struct sk_buff *skb)
+{
+	struct macvlan_port *port;  /* maps skbuffs arriving from a lower layer
+				     * i/f to a higher layer i/f */
+	port = skb->dev->macvlan_priv;
+	return mvl_handle_frame_fod(port, skb);
+}
+
+
+#ifdef MVL_CONFIG_PROC_FS
+
+static int read_mvl_glbl(char *page, char **start, off_t off,
+			 int count, int *eof, void *data) {
+	int	ret = -1;
+	char	*p = page;
+	int mx_len = (4096 - (p - page));
+	
+	if (! *eof ) {
+		struct macvlan_port* port;
+		int cnt;
+		unsigned long flags;
+		
+                /* Global counts here... */
+		p += sprintf(p, "MAC-VLAN module: (NEW_API)\n");
+
+		p += sprintf(p, " port count: %i  vlan_counter: %i\n",
+			     atomic_read(&macvlan_nports),
+			     atomic_read(&mvl_vlan_counter));
+
+		MVL_READ_LOCK(flags);
+		port = port_list;
+		while (port) {
+			p += sprintf(p, " %s  num_vlans: %i  flags: %x\n",
+				     port->dev->name, atomic_read(&port->ndevs), port->flags);
+
+			/* catch overflow */
+			cnt = p - page;
+			if (cnt > (mx_len - 60)) {
+				if (mx_len - cnt >= 20) {
+					p += sprintf(p, "OUT_OF_SPACE!\n");
+				}
+				break;
+			}
+
+			port = port->next;
+		}
+
+		ret = p - page;
+		MVL_READ_UNLOCK(flags);
+	}
+	return ret;
+} /* read_mvl_glbl */
+
+static int write_mvl_glbl(struct file *file, const char *buffer,
+			  unsigned long count, void *data) {
+	char		*p;
+	const char	*end;
+	int		ret=count;
+	int		len;
+	char		dev_name[2][IFNAMSIZ];
+        char* tmps = NULL;
+	unsigned long flags;
+        
+        MVL_WRITE_LOCK(flags);
+        
+	end = buffer+count;
+
+	for (p= (char *) buffer; p< end ; ) {
+		if (iswhitespace(*p)) {
+			p++;
+			continue;
+		}
+
+		memset(dev_name[0], 0 ,IFNAMSIZ);
+		memset(dev_name[1], 0 ,IFNAMSIZ);
+
+		len = strlen("add_port ");
+		if (strncmp(p, "add_port ", len)==0)
+		{
+			p += len;
+
+			if ( (p + IFNAMSIZ) <= end)
+				p += copy_next_word(dev_name[0], p, IFNAMSIZ);
+			else
+				p += copy_next_word(dev_name[0], p, end-p );
+
+			skip_whitespace(p);
+
+                        /* This can fail, but not sure how to return failure
+                         * to user-space here.
+			 * NOTE:  Does it's own internal locking, so release
+			 *  the lock here..then re-acquire after.
+                         */
+			MVL_WRITE_UNLOCK(flags);
+                        macvlan_port_create(dev_name[0]);
+			MVL_WRITE_LOCK(flags);
+                        goto forend;
+		}
+                
+		len = strlen("remove_port ");
+		if (strncmp(p,"remove_port ",len)==0) {
+			p += len;
+
+			if ( (p + IFNAMSIZ) <= end)
+				p += copy_next_word(dev_name[0], p, IFNAMSIZ);
+			else
+				p += copy_next_word(dev_name[0], p, end-p );
+
+                        skip_whitespace(p);
+
+			MVL_WRITE_UNLOCK(flags);
+                        macvlan_port_cleanup(dev_name[0], 1);
+			MVL_WRITE_LOCK(flags);
+                        goto forend;
+		}
+
+                len = strlen("debug_lvl ");
+		if (strncmp(p,"debug_lvl ",len)==0)
+		{
+			p += len;
+
+			if ( (p + IFNAMSIZ) <= end)
+				p += copy_next_word(dev_name[0], p, IFNAMSIZ);
+			else
+				p += copy_next_word(dev_name[0], p, end-p );
+
+                        skip_whitespace(p);
+
+                        debug_lvl = simple_strtoul(dev_name[0], &tmps, 10);
+                        goto forend;
+		}
+
+                printk("ERROR:  Unsupported command\n");
+
+        forend:
+		p++;
+	}
+
+        MVL_WRITE_UNLOCK(flags);
+
+	return ret;
+} /* write_mvl_glbl */
+
+/* Proc file read for mac-vlan. */
+static int read_mvl(char *page, char **start, off_t off,
+		    int count, int *eof, void *data) {
+	int	ret = -1;
+	if (! *eof ) {
+		char	*p = page;
+		struct macvlan_vlan* vlan = (struct macvlan_vlan*)(data);
+		unsigned long flags;
+		
+		
+		MVL_READ_LOCK(flags);
+		
+                /* Global counts here... */
+		p += sprintf(p, "MAC-VLAN %s:\n", vlan->dev->name);
+
+		p += sprintf(p, " lower_dev: %s  macvlan-port: %s\n",
+			     vlan->lowerdev->name, vlan->port->dev->name);
+
+		p += sprintf(p, "  MAC: %02hx:%02hx:%02hx:%02hx:%02hx:%02hx\n",
+			     vlan->dev->dev_addr[0], vlan->dev->dev_addr[1],
+			     vlan->dev->dev_addr[2], vlan->dev->dev_addr[3],
+			     vlan->dev->dev_addr[4], vlan->dev->dev_addr[5]);
+
+		ret = p - page;
+
+		MVL_READ_UNLOCK(flags);
+	}
+	return ret;
+} /* read_mvl_glbl */
+
+
+static int write_mvl(struct file *file, const char *buffer,
+		     unsigned long count, void *data) {
+	return printk("ERROR:  (No cmds supported at this time.)\n");
+} /* write_mvl */
+
+
+static int read_mvl_port(char *page, char **start, off_t off,
+			 int count, int *eof, void *data) {
+	int	ret = -1;
+	char	*p = page;
+	int mx_len = (4096 - (p - page));
+	int i;
+	
+	if (! *eof ) {
+		struct macvlan_port* port = (struct macvlan_port*)(data);
+		int cnt;
+		struct macvlan_vlan* vlan;
+		unsigned long flags;
+
+		MVL_READ_LOCK(flags);
+		
+                /* Global counts here... */
+		p += sprintf(p, "MAC-VLAN Port: %s\n", port->dev->name);
+
+		p += sprintf(p, " vlan count: %i\n", atomic_read(&port->ndevs));
+
+		for (i = 0; i<MACVLAN_DEV_HASH_LEN; i++) {
+			vlan = port->vlan_hash[i];
+			while (vlan) {
+				p += sprintf(p, " %s\n", vlan->dev->name);
+				
+				/* catch overflow */
+				cnt = p - page;
+				if (cnt > (mx_len - 40)) {
+					if (mx_len - cnt >= 20) {
+						p += sprintf(p, "OUT_OF_SPACE!\n");
+					}
+					goto outofspace;
+				}
+				
+				vlan = vlan->next;
+			}
+		}
+
+	outofspace:
+		ret = p - page;
+		MVL_READ_UNLOCK(flags);
+	}
+	return ret;
+} /* read_mvl_glbl */
+
+
+static int write_mvl_port(struct file *file, const char *buffer,
+			  unsigned long count, void *data) {
+	char		*p;
+	const char	*end;
+	int		ret=count;
+	int		len;
+	char		dev_name[2][IFNAMSIZ];
+        char* tmps = NULL;
+        struct macvlan_port* port = (struct macvlan_port*)(data);
+	unsigned long flags;
+	
+	end = buffer+count;
+
+	for (p= (char *) buffer; p< end ; ) {
+		if (iswhitespace(*p)) {
+			p++;
+			continue;
+		}
+
+		memset(dev_name[0], 0 ,IFNAMSIZ);
+		memset(dev_name[1], 0 ,IFNAMSIZ);
+
+		len = strlen("add_vlan ");
+		if (strncmp(p, "add_vlan ", len)==0) {
+			p += len;
+
+			if ( (p + IFNAMSIZ) <= end)
+				p += copy_next_word(dev_name[0], p, IFNAMSIZ);
+			else
+				p += copy_next_word(dev_name[0], p, end-p );
+
+			skip_whitespace(p);
+
+                        /* This can fail, but not sure how to return failure
+                         * to user-space here.
+                         */
+			/* has internal locking */
+                        macvlan_vlan_create(port->dev->name,
+					    simple_strtoul(dev_name[0], &tmps, 10));
+                        goto forend;
+		}
+
+		len = strlen("set_flags ");
+		if (strncmp(p, "set_flags ", len)==0) {
+			p += len;
+
+			if ( (p + IFNAMSIZ) <= end)
+				p += copy_next_word(dev_name[0], p, IFNAMSIZ);
+			else
+				p += copy_next_word(dev_name[0], p, end-p );
+
+			skip_whitespace(p);
+
+                        /* This can fail, but not sure how to return failure
+                         * to user-space here.
+                         */
+
+			MVL_WRITE_LOCK(flags);
+                        macvlan_port_set_flags(port->dev->name,
+					       simple_strtoul(dev_name[0], &tmps, 16));
+			MVL_WRITE_UNLOCK(flags);
+			goto forend;
+		}
+
+		len = strlen("remove_vlan ");
+		if (strncmp(p,"remove_vlan ",len)==0) {
+			p += len;
+
+			if ( (p + IFNAMSIZ) <= end)
+				p += copy_next_word(dev_name[0], p, IFNAMSIZ);
+			else
+				p += copy_next_word(dev_name[0], p, end-p );
+
+                        skip_whitespace(p);
+
+			/* Has internal locking */
+                        macvlan_vlan_cleanup(dev_name[0], 1);
+                        goto forend;
+		}
+
+                printk("ERROR:  Unsupported command\n");
+
+        forend:
+		p++;
+	}
+
+	return ret;
+} /* write_mvl_port */
+
+
+#endif
+
+
+static int __init macvlan_init(void) {
+	printk (KERN_INFO "MAC address based VLAN: 2.0 (2-28-2006)\n");
+
+	port_list = NULL;
+
+	macvlan_ioctl_set(macvlan_ioctl_deviceless_stub);
+	macvlan_handle_frame_hook = macvlan_handle_frame;
+
+#ifdef MVL_CONFIG_PROC_FS
+
+        mvl_proc_dir = proc_mkdir(MVL_PROC_DIR, proc_net);
+        if (mvl_proc_dir) {
+		mvl_proc_cfg = create_proc_read_entry(MVL_PROC_CFG, S_IRUGO, mvl_proc_dir,
+						      read_mvl_glbl, NULL);
+		if (mvl_proc_cfg) {
+			mvl_proc_cfg->write_proc = write_mvl_glbl;
+			mvl_proc_cfg->owner = THIS_MODULE;
+		}
+	}
+#endif
+
+	/* Register us to receive netdevice events */
+	register_netdevice_notifier(&macvlan_notifier_block);
+	
+	return 0;
+}
+
+static void macvlan_cleanup(void) {
+	struct macvlan_port *port;
+	char nm[IFNAMSIZ+1];
+	unsigned long flags;
+	int tst;
+	
+	macvlan_handle_frame_hook = NULL;
+	macvlan_ioctl_set(NULL);
+
+	MVL_WRITE_LOCK(flags);
+	/* destroy all existing ports */
+	while ((port = port_list)) {
+		strncpy(nm, port->dev->name, IFNAMSIZ);
+		MVL_WRITE_UNLOCK(flags);
+		if ((tst = macvlan_port_cleanup(nm, 1)) < 0) {
+			printk("macvlan: WARNING:  Failed port_cleanup in macvlan_cleanup, err: %d  name: %s\n", tst, nm);
+			BUG();
+			MVL_WRITE_LOCK(flags);
+			break;
+		}
+		MVL_WRITE_LOCK(flags);
+	}
+	MVL_WRITE_UNLOCK(flags);
+
+	/* Un-register us from receiving netdevice events */
+	unregister_netdevice_notifier(&macvlan_notifier_block);
+	
+#ifdef MVL_CONFIG_PROC_FS
+	if (mvl_proc_cfg) {
+		remove_proc_entry(MVL_PROC_CFG, mvl_proc_dir);
+		mvl_proc_cfg = NULL;
+	}
+	if (mvl_proc_dir) {
+		remove_proc_entry(MVL_PROC_DIR, proc_net);
+		mvl_proc_dir = NULL;
+	}
+#endif
+
+	/* Check for memory leaks */
+	if (atomic_read(&macvlan_nports) != 0) {
+		printk("ERROR: macvlan leaked ports: %d\n", atomic_read(&macvlan_nports));
+	}
+	if (atomic_read(&mvl_vlan_counter) != 0) {
+		printk("ERROR: macvlan leaked vlan structs: %d\n", atomic_read(&mvl_vlan_counter));
+	}
+	if (atomic_read(&mvl_netdev_counter) != 0) {
+		printk("ERROR: macvlan leaked netdevs: %d\n", atomic_read(&mvl_netdev_counter));
+	}
+	printk("macvlan module cleanup completed.\n");
+
+}/* macvlan_cleanup */
+
+
+module_init(macvlan_init);
+module_exit(macvlan_cleanup);
+MODULE_LICENSE("GPL");
diff --git a/net/macvlan/macvlan.h b/net/macvlan/macvlan.h
new file mode 100644
index 0000000..6316015
--- /dev/null
+++ b/net/macvlan/macvlan.h
@@ -0,0 +1,75 @@
+/* -*- linux-c -*-
+
+# (C) Copyright 2001-2003
+# Alex Zeffertt, Cambridge Broadband Ltd, ajz@cambridgebroadband.com
+# Re-worked by Ben Greear <greearb@candelatech.com>
+
+*/
+
+#ifndef MACVLAN_KERNEL_H_FILE__
+#define MACVLAN_KERNEL_H_FILE__
+
+
+/* NOTE:  If you change this below, you should probably change macvlan_hash_lookup as
+ * well.  Especially if you make this bigger.
+ */
+#define MACVLAN_DEV_HASH_LEN 256
+
+/* This can be made as large as desired, and mainly helps keep bad
+ * IOCTL arguments from taking down the box.
+ */
+#define MAX_MACVLANS_PER_PORT 10000
+
+/* Proc file related */
+#define MVL_MX_ARG_LEN 80
+
+#ifdef CONFIG_PROC_FS
+
+/* To use or not to use the PROC-FS */
+#define MVL_CONFIG_PROC_FS
+
+#endif
+
+
+/*********************************************************/
+/*		       types				 */
+/*********************************************************/
+/* a macvlan_vlan represents an upper layer interface */
+struct macvlan_vlan {
+	struct net_device* dev;
+	struct net_device_stats	statistics;
+	struct macvlan_vlan *next;
+	struct macvlan_port *port;
+	struct net_device *lowerdev;
+
+#ifdef MVL_CONFIG_PROC_FS
+        struct proc_dir_entry* proc_ent;
+#endif        
+
+};
+
+/*
+ * a macvlan_port represents a mux/demux between a mac-
+ * based-vlan enabled ethernet device and vlans
+ * layered on top of it
+ */
+struct macvlan_port {
+	/* MAC to vlan lookup */
+	struct net_device *dev;  /* the mac-based-vlan enabled ethernet device */
+	atomic_t ndevs;    /* number of vlans layered over dev */
+	/* hash of vlans layered over this port.  Hash on mac[5] */
+	struct macvlan_vlan *vlan_hash[MACVLAN_DEV_HASH_LEN];
+	struct macvlan_port *next;    /* next port */
+	
+	int flags; /* none defined currently */
+
+#ifdef MVL_CONFIG_PROC_FS
+        struct proc_dir_entry* proc_dir;
+        struct proc_dir_entry* proc_ent;
+#endif        
+
+};
+
+
+#endif
+
diff --git a/net/packet/af_packet.c b/net/packet/af_packet.c
index 4172a52..77f5698 100644
--- a/net/packet/af_packet.c
+++ b/net/packet/af_packet.c
@@ -77,6 +77,7 @@ #include <linux/seq_file.h>
 #include <linux/poll.h>
 #include <linux/module.h>
 #include <linux/init.h>
+#include <linux/if_macvlan.h>
 
 #ifdef CONFIG_INET
 #include <net/inet_common.h>
@@ -333,6 +334,13 @@ static int packet_sendmsg_spkt(struct ki
 	struct net_device *dev;
 	unsigned short proto=0;
 	int err;
+        int kludge = 0;
+
+#ifdef CONFIG_SUPPORT_SEND_BAD_CRC
+        if (sk->sk_flags & SOCK_DONT_DO_LL_FCS) {
+           kludge = 4; // We're doing our own CRC
+        }
+#endif
 	
 	/*
 	 *	Get and verify the address. 
@@ -353,7 +361,7 @@ static int packet_sendmsg_spkt(struct ki
 	 */
 
 	saddr->spkt_device[13] = 0;
-	dev = dev_get_by_name(saddr->spkt_device);
+	dev = dev_get_by_name(saddr->spkt_device); /* DAMN, we aught to hash this! */
 	err = -ENODEV;
 	if (dev == NULL)
 		goto out_unlock;
@@ -364,7 +372,7 @@ static int packet_sendmsg_spkt(struct ki
 	 */
 	 
 	err = -EMSGSIZE;
-	if (len > dev->mtu + dev->hard_header_len)
+ 	if (len > (dev->mtu + dev->hard_header_len + kludge))
 		goto out_unlock;
 
 	err = -ENOBUFS;
@@ -406,6 +414,15 @@ static int packet_sendmsg_spkt(struct ki
 	if (err)
 		goto out_free;
 
+#ifdef CONFIG_SUPPORT_SEND_BAD_CRC
+	if (sk->sk_flags & SOCK_DONT_DO_LL_FCS) {
+		skb->use_specified_ether_crc = 1;
+	}
+	else {
+		skb->use_specified_ether_crc = 0;
+	}
+#endif
+
 	err = -ENETDOWN;
 	if (!(dev->flags & IFF_UP))
 		goto out_free;
@@ -714,6 +731,13 @@ static int packet_sendmsg(struct kiocb *
 	unsigned short proto;
 	unsigned char *addr;
 	int ifindex, err, reserve = 0;
+        int kludge = 0;
+        
+#ifdef CONFIG_SUPPORT_SEND_BAD_CRC
+        if (sk->sk_flags & SOCK_DONT_DO_LL_FCS) {
+           kludge = 4; // We're doing our own CRC
+        }
+#endif
 
 	/*
 	 *	Get and verify the address. 
@@ -745,7 +769,7 @@ static int packet_sendmsg(struct kiocb *
 		reserve = dev->hard_header_len;
 
 	err = -EMSGSIZE;
-	if (len > dev->mtu+reserve)
+	if (len > (dev->mtu + reserve + kludge))
 		goto out_unlock;
 
 	skb = sock_alloc_send_skb(sk, len + LL_RESERVED_SPACE(dev),
@@ -776,6 +800,15 @@ static int packet_sendmsg(struct kiocb *
 	skb->dev = dev;
 	skb->priority = sk->sk_priority;
 
+#ifdef CONFIG_SUPPORT_SEND_BAD_CRC
+	if (sk->sk_flags & SOCK_DONT_DO_LL_FCS) {
+		skb->use_specified_ether_crc = 1;
+	}
+	else {
+		skb->use_specified_ether_crc = 0;
+	}
+#endif
+    
 	err = -ENETDOWN;
 	if (!(dev->flags & IFF_UP))
 		goto out_free;
diff --git a/net/redir/Kconfig b/net/redir/Kconfig
new file mode 100644
index 0000000..3abfbe1
--- /dev/null
+++ b/net/redir/Kconfig
@@ -0,0 +1,7 @@
+config REDIRDEV
+	tristate "Redirect-net-device support"
+	depends on EXPERIMENTAL
+	---help---
+	  This allows one to create virtual interfaces that effectively
+          swap tx for rx, allowing one to create bridges and similar
+          constructs all in the same machine.
diff --git a/net/redir/Makefile b/net/redir/Makefile
new file mode 100644
index 0000000..70d4dcb
--- /dev/null
+++ b/net/redir/Makefile
@@ -0,0 +1,10 @@
+#
+# Note! Dependencies are done automagically by 'make dep', which also
+# removes any old dependencies. DON'T put your own dependencies here
+# unless it's something special (ie not a .c file).
+#
+# Note 2! The CFLAGS definition is now in the main makefile...
+
+obj-$(CONFIG_REDIRDEV) := redirdev.o
+
+
diff --git a/net/redir/redirdev.c b/net/redir/redirdev.c
new file mode 100644
index 0000000..48003c3
--- /dev/null
+++ b/net/redir/redirdev.c
@@ -0,0 +1,919 @@
+/* -*- linux-c -*-
+#######################################################################
+#
+# (C) Copyright 2005
+# Ben Greear <greearb@candelatech.com>
+#
+# This program is free software; you can redistribute it and/or
+# modify it under the terms of the GNU General Public License as
+# published by the Free Software Foundation; either version 2 of
+# the License, or (at your option) any later version.
+#
+# This program is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.	See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with this program; if not, write to the Free Software
+# Foundation, Inc., 59 Temple Place, Suite 330, Boston,
+# MA 02111-1307 USA
+#######################################################################
+# Notes:
+# 
+# This file implements the Redirect-net-device module.  A pair of
+# redir devices linked to each other act like two ethernet interfaces
+# connected with a cross-over cable.
+#
+# This provides an IOCTL interface which allows you to
+# It uses an IOCTL interface which allows you to
+#
+# 1. create redirect device
+# 2. delete redirect device
+# 
+#######################################################################
+*/
+
+#include <linux/config.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/kernel.h> 
+#include <linux/fs.h>	  
+#include <linux/errno.h>  
+#include <linux/delay.h>  
+#include <linux/slab.h>
+#include <linux/mm.h>
+#include <linux/ioport.h>
+#include <linux/interrupt.h>
+#include <linux/poll.h>
+#include <linux/types.h>
+#include <linux/string.h>
+#include <linux/if_redirdev.h>
+#include <linux/if_arp.h>
+#include <linux/etherdevice.h>
+#include <net/arp.h>
+#include <linux/rtnetlink.h>
+#include <linux/notifier.h>
+
+#include <asm/io.h>
+#include <asm/irq.h>
+#include <asm/uaccess.h>
+#include <asm/semaphore.h>
+#include <net/dst.h>
+
+#ifdef CONFIG_PROC_FS
+#include <linux/proc_fs.h>
+#define RDD_PROC_DIR "redirdev"
+#define RDD_PROC_CFG "config"
+static struct proc_dir_entry *rdd_proc_dir;
+static struct proc_dir_entry *rdd_proc_cfg;
+#endif
+
+#include "redirdev.h"
+
+/* Defined in socket.c */
+void redirdev_ioctl_set(int (*hook)(void*));
+static int redirdev_device_event(struct notifier_block *unused,
+				 unsigned long event, void *ptr);
+
+static struct notifier_block redirdev_notifier_block = {
+	.notifier_call = redirdev_device_event,
+};
+
+/*********************************************************/
+/*			defines				 */
+/*********************************************************/
+
+/*  Must hold this lock to make any changes to the Redirect-Device structures.
+ */
+static spinlock_t rdd_cfg_lock = SPIN_LOCK_UNLOCKED;
+
+
+/*********************************************************/
+/*		       file scope variables		 */
+/*********************************************************/
+
+static struct redirdev* rdds = NULL;
+
+static atomic_t rdd_dev_counter;
+
+static int debug_lvl = 0;
+
+
+/*********************************************************/
+/*		   forward declarations			 */
+/*********************************************************/
+
+#ifdef RDD_CONFIG_PROC_FS
+static int read_rdd_glbl(char *page, char **start, off_t off,
+			 int count, int *eof, void *data);
+static int write_rdd_glbl(struct file *file, const char *buffer,
+			  unsigned long count, void *data);
+#endif
+
+
+
+/*********************************************************/
+/*		   function definitions			 */
+/*********************************************************/
+
+
+#define iswhitespace(x)\
+	((x) == ' ' || (x) == '\n' || (x) == '\r' || (x) == '\r' )
+
+#define skip_whitespace(x) 	{ while (iswhitespace(*x)) (x)++; }
+
+static int copy_next_word(char *dst, char *src, int len) {
+	char *p;
+	for (p=src; p < src + len ; p++) {
+		if ( iswhitespace(*p))
+			break;
+		*dst++ = *p;
+	}
+	return p - src;
+}
+
+/*  Grab the RDD lock before calling this method. */
+struct redirdev* rdd_find_dev_by_name(const char* ifname) {
+	struct redirdev* d;
+	//printk("finding port for underlying ifname: %s\n", ifname);
+	for (d = rdds; d; d = d->next) {
+		//printk("Testing port: %p name: %s\n", port, port->dev->name);
+		if (strcmp(d->dev->name, ifname) == 0) {
+			break;
+		}
+	}
+	//printk("done finding port: %p\n", port);
+	return d;
+}
+
+/*  Grab the RDD lock before calling this method. */
+struct redirdev* rdd_find_dev_by_txdev_name(const char* ifname) {
+	struct redirdev* d;
+	for (d = rdds; d; d = d->next) {
+		if (d->tx_dev) {
+			if (strcmp(d->tx_dev->name, ifname) == 0) {
+				break;
+			}
+		}
+	}
+	return d;
+}	 
+
+
+static struct net_device_stats *redirdev_get_stats(struct net_device *dev)
+{
+	struct redirdev* rdd = dev->priv;
+
+	return &rdd->statistics;
+}
+
+/** Bump our tx counters and then act as if this was received from
+ * the network on the tx_dev device.  Since we don't do any CSUM
+ * activity in this driver, make sure SKB as marked as not checksummed
+ * yet.
+ */
+static int redirdev_xmit(struct sk_buff *iskb, struct net_device *dev) {
+	struct redirdev* rdd = dev->priv;
+	struct net_device_stats* txs;
+	
+	if (unlikely((!rdd->tx_dev) && rdd->wants_to_run)) {
+		rdd->tx_dev = dev_get_by_name(rdd->tx_dev_name);
+		if (rdd->tx_dev) {
+			printk("redir:  Associated tx_dev_name: %s with device: %p in redirdev_xmit\n",
+			       rdd->tx_dev_name, rdd->tx_dev);
+		}
+	}
+
+	if (unlikely(!rdd->tx_dev)) {
+		printk("ERROR:  tx_dev null in redirdev_xmit.\n");
+		kfree_skb(iskb);
+		rdd->statistics.tx_errors++;
+		return 0;
+	}
+
+	//printk("%s: dev: %s tx_dev: %s\n",
+	//       __PRETTY_FUNCTION__, dev->name, rdd->tx_dev->name);
+
+	if (netif_running(rdd->tx_dev)) {
+
+		/* We need to free the old skb so that the socket
+		 * account works correctly.  We'll make a copy and
+		 * then forward that to the other device.
+		 */
+
+		struct sk_buff* skb = skb_clone(iskb, GFP_ATOMIC);
+
+		kfree_skb(iskb); //Let the sending socket reclaim it's memory
+		
+		if (!skb) {
+			rdd->statistics.tx_dropped++;
+		}
+		else {
+			int rv;
+			skb->dev = rdd->tx_dev;
+
+			/* We didn't calculate the csum, so mark as such. */
+			skb->ip_summed = CHECKSUM_UNNECESSARY;//NONE;
+	
+			rdd->statistics.tx_packets++;
+			rdd->statistics.tx_bytes += skb->len;
+
+			txs = rdd->tx_dev->get_stats(rdd->tx_dev);
+			txs->rx_packets++;
+			txs->rx_bytes += skb->len;
+
+			/* Zero out the time-stamp so that receiving code is forced
+			 * to recalculate it.
+			 */
+			skb->tstamp.off_sec = 0;
+			skb->tstamp.off_usec = 0;
+			
+			/* Call this on the receiving net device.  This assumes
+			 * that all devices are ethernet or ethernet-like.  Valid
+			 * for now.  TODO:  Generalize tx_dev ??
+			 */
+			skb->pkt_type = PACKET_HOST; //Reset this to default.
+
+			skb->protocol = eth_type_trans(skb, skb->dev);
+
+			if (skb->dst) {
+				dst_release(skb->dst);
+				skb->dst = NULL;
+			}
+			
+			//printk("skb->protocol: %x  pkt_type: %u\n",
+			//       (unsigned int)(skb->protocol),
+			//       (unsigned int)(skb->pkt_type));
+			rv = netif_rx(skb);
+			if (rv != NET_RX_SUCCESS) {
+				// TODO:  Remove
+				//printk("netif_rx rv: %i\n", (int)(rv));
+			}
+			rdd->tx_dev->last_rx = jiffies;
+			rdd->dev->trans_start = jiffies;
+		}
+	}
+	else {
+		/* Chunk the packet and log some errors */
+		rdd->statistics.tx_errors++;
+		kfree_skb(iskb);
+	}
+	return 0;
+}/* redir xmit */
+
+static int redirdev_open(struct net_device *dev) {
+	struct redirdev* rdd = dev->priv;
+	rdd->wants_to_run = 1;
+	if (!rdd->tx_dev) {
+		rdd->tx_dev = dev_get_by_name(rdd->tx_dev_name);
+	}
+	if (!rdd->tx_dev) {
+		printk("redir: %s  Warning:  Could not find tx_dev: %s, will try later in redirdev_xmit.\n",
+		       dev->name, rdd->tx_dev_name);
+	}
+
+	printk("redirdev:  Starting device: %s\n", dev->name);
+	netif_start_queue(dev);
+	return 0;
+}
+
+//static void redirdev_set_multicast_list(struct net_device *dev) {
+	/* TODO ??? */
+//}
+
+static int redirdev_stop(struct net_device *dev) {
+	struct redirdev* rdd = dev->priv;
+	printk("redirdev:  stopping device: %s\n", dev->name);
+	netif_stop_queue(dev);
+	rdd->wants_to_run = 0;
+	if (rdd->tx_dev) {
+		struct net_device* tmp = rdd->tx_dev;
+		rdd->tx_dev = NULL;
+		printk("  releasing reference to dev: %s\n", tmp->name);
+		dev_put(tmp);
+	}
+	printk("   done stopping %s\n", dev->name);
+	return 0;
+}
+
+
+void redirdev_dev_destructor(struct net_device *dev) {
+	atomic_dec(&rdd_dev_counter);
+	if (dev->priv) {
+		//printk("dst: %s", dev->name);
+		kfree(dev->priv);
+		dev->priv = NULL;
+	}
+	else {
+		//printk("dst2: %s", dev->name);
+	}
+}
+
+int redirdev_change_mtu(struct net_device *dev, int new_mtu) {
+	dev->mtu = new_mtu;
+	return 0;
+}
+
+static int redirdev_create(const char* newifname,
+			   const char* txdevname) {
+	struct redirdev *rdd = NULL;
+	struct net_device* td = NULL;
+	struct net_device* nnd = NULL;
+	struct net_device* txd = NULL;
+	unsigned long flags;
+	int rv;
+
+	if ((strlen(txdevname) == 0) ||
+	    (strlen(newifname) == 0)) {
+		printk("redirdev: ERROR:  Must specify ifname and txifname"
+		       " when creating redirect devices!\n");
+		rv = -ENODEV;
+		goto out;
+	}	   
+
+	printk("redirdev: creating interface: -:%s:- with tx_dev: -:%s:-\n",
+	       newifname, txdevname);
+	
+
+	//printk("malloc ");
+	if ((rdd = kmalloc(sizeof(*rdd), GFP_KERNEL)) == NULL) {
+		//printk("redirdev: kmalloc failure\n");
+		rv = -ENOMEM;
+		goto outfree;
+	}
+	memset(rdd, 0, sizeof(*rdd));
+
+	//printk("4 ");
+	if ((nnd = kmalloc(sizeof(struct net_device), GFP_KERNEL)) == NULL) {
+		//printk("redirdev: kmalloc net_device failure\n");
+		rv = -ENOMEM;
+		goto outfree;
+	}
+	memset(nnd, 0, sizeof(struct net_device));	
+
+	if ((td = dev_get_by_name(newifname)) != NULL) {
+		//printk("redirdev: device by that name already exists\n");
+		rv = -EEXIST;
+		goto outfree;
+	}
+
+	/* If it's not here yet, no problem, will associate later */
+	txd = dev_get_by_name(txdevname);
+	strncpy(rdd->tx_dev_name, txdevname, IFNAMSIZ);
+
+	//printk("4 ");
+	rdd->dev = nnd;
+	
+	//printk("5 ");
+	strncpy(rdd->dev->name, newifname, IFNAMSIZ-1);
+	rdd->dev->name[IFNAMSIZ-1] = 0; //Ensure null termination.
+	ether_setup(rdd->dev);
+	
+	dev_hold(rdd->dev); /* RDD code holds reference */
+	
+	rdd->dev->priv = rdd;
+	rdd->tx_dev = txd;
+	
+	//printk("6 ");
+	rdd->dev->get_stats = redirdev_get_stats;
+	rdd->dev->hard_start_xmit = redirdev_xmit;
+	rdd->dev->change_mtu = redirdev_change_mtu;
+	rdd->dev->open = redirdev_open;
+	rdd->dev->stop = redirdev_stop;
+	rdd->dev->destructor = redirdev_dev_destructor;
+
+	// Defaults are fine for these
+	//rdd->dev->rebuild_header = redirdev_dev_rebuild_header;
+	//rdd->dev->set_multicast_list = redirdev_set_multicast_list;
+	//rdd->dev->hard_header = redirdev_hard_header;	
+
+	rdd->dev->dev_addr[0] = 0;
+	rdd->dev->dev_addr[1] = net_random();
+	rdd->dev->dev_addr[2] = net_random();
+	rdd->dev->dev_addr[3] = net_random();
+	rdd->dev->dev_addr[4] = net_random();
+	rdd->dev->dev_addr[5] = net_random();
+
+	/* No qdisc for us */
+	rdd->dev->qdisc = NULL;
+	rdd->dev->tx_queue_len = 0;
+	
+	//printk("redirdev: created redirect-device %p\n", vlan);
+
+	/* link to list */
+	//printk("8 ");
+	spin_lock_irqsave(&rdd_cfg_lock, flags);
+	rdd->next = rdds;
+	rdds = rdd;
+	spin_unlock_irqrestore(&rdd_cfg_lock, flags);
+
+	//printk("End of redirdev_create, registering rdd->dev: %p (%s)\n",
+	//       rdd->dev, rdd->dev->name);
+	
+	register_netdev(rdd->dev);
+
+	//printk("End of mac_vlan create2\n");
+
+	atomic_inc(&rdd_dev_counter);
+	//printk("9\n");
+	rv = 0;
+	goto out;
+
+	/* Error case, clean up vlan memory */
+ outfree:
+	if (rdd) {
+		kfree(rdd);
+	}
+	if (nnd) {
+		kfree(nnd);
+	}
+	if (td) {
+		dev_put(td);
+	}
+	if (txd) {
+		dev_put(txd);
+	}
+ out:
+	return rv;
+} /* redirdev_create */
+
+static int redirdev_device_event(struct notifier_block *unused,
+				 unsigned long event, void *ptr) {
+	struct net_device* dev = ptr;
+	struct redirdev* rdd;
+	unsigned long flags;
+	
+	spin_lock_irqsave(&rdd_cfg_lock, flags);
+	rdd = rdd_find_dev_by_txdev_name(dev->name);
+	spin_unlock_irqrestore(&rdd_cfg_lock, flags);
+
+	if (!rdd) {
+		//printk("redirdev: Ignoring event: %lu for device: %s\n",
+		//       event, dev->name);
+		goto out;
+	}
+
+
+	/* It is OK that we do not hold the group lock right now,
+	 * as we run under the RTNL lock.
+	 */
+
+	switch (event) {
+	case NETDEV_CHANGE:
+	case NETDEV_DOWN:
+		//printk("redirdev: Ignoring change/up/down for device: %s\n",
+		//       dev->name);
+		/* Ignore for now */
+		break;
+
+	case NETDEV_UP:
+		/* Start the redir-dev too if it wants to run */
+		if ((!netif_running(rdd->dev)) && rdd->wants_to_run) {
+			printk("Device: %s is up, starting redir-device: %s too.\n",
+			       dev->name, rdd->dev->name);
+			dev_open(rdd->dev);
+		}
+		break;
+
+	case NETDEV_UNREGISTER:
+		/* Stop the redir-dev too */
+		printk("Device: %s is going away, closing redir-device: %s too.\n",
+		       dev->name, rdd->dev->name);
+		if (rdd->dev->flags & IFF_UP) {
+			/* Graceful shutdown, drop links to our peer. */
+			dev_close(rdd->dev);
+		}
+		else {
+			/* Still drop links to peer...but dev_close would not have done anything. */
+			redirdev_stop(rdd->dev);
+		}
+		rdd->wants_to_run = 1; /* was forced down. */
+		break;
+
+	};
+
+out:
+	return NOTIFY_DONE;
+}
+
+/* Has locking internally */
+int redirdev_cleanup(const char* ifname, int force) {
+	struct redirdev* d; //walker
+	struct redirdev* prev = NULL;
+	unsigned long flags;
+	int rv;
+	
+	//printk(__FUNCTION__"(%p)\n",vlan);
+	//printk("rdd_cln: %s", ifname);
+
+	spin_lock_irqsave(&rdd_cfg_lock, flags);
+	for (d = rdds; d; d = d->next) {
+		if (strcmp(d->dev->name, ifname) == 0) {			
+			if ((d->dev->flags & IFF_UP) && (!force)) {
+				rv = -EBUSY;
+				goto unlockout;
+			}
+
+			// Un-link from the list.
+			if (prev) {
+				prev->next = d->next;
+				d->next = NULL;
+			}
+			else {
+				// This means we're first in line
+				rdds = d->next;
+				d->next = NULL;
+			}
+
+			break;
+		}
+		prev = d;
+	}
+
+	spin_unlock_irqrestore(&rdd_cfg_lock, flags);
+
+	if (d) {
+		if (d->dev->flags & IFF_UP) {
+			BUG_ON(!force);
+
+			rtnl_lock();
+			dev_close(d->dev);
+			rtnl_unlock();
+		}
+
+		if (d->tx_dev) {
+			dev_put(d->tx_dev);
+		}
+		
+		dev_put(d->dev);
+		unregister_netdev(d->dev);
+		rv = 0;
+	}
+	else {
+		rv = -ENODEV;
+	}
+	goto out;
+
+ unlockout:
+	spin_unlock_irqrestore(&rdd_cfg_lock, flags);
+
+ out:
+	return rv;	
+} /* redirdev cleanup */
+
+
+static int redirdev_ioctl_deviceless_stub(void* arg) {
+	int err = 0;
+	struct redirdev_ioctl req;
+	unsigned long flags;
+	
+	if (!capable(CAP_NET_ADMIN))
+		return -EPERM;
+
+	if (copy_from_user(&req, arg, sizeof(req)))
+		return -EFAULT;
+
+	switch (req.cmd) {
+	case REDIRDEV_ADD: {
+		/* 
+		 * create a new redirect device
+		 */
+		req.txifname[IFNAMSIZ-1] = '\0';
+		req.ifname[IFNAMSIZ-1] = '\0';
+		printk("Creating redir via ioctl, ifname: %s  txifname: %s\n",
+		       req.ifname, req.txifname);
+
+		/* Has internal locking. */
+		err = redirdev_create(req.ifname, req.txifname);
+		break;
+	}
+	case REDIRDEV_DEL: {
+		/*
+		 * destroy a redirect device
+		 */
+		req.ifname[IFNAMSIZ-1] = '\0';
+
+		/* Has internal locking */
+		err = redirdev_cleanup(req.ifname, 0);
+		break;
+	}
+
+	case REDIRDEV_IS_REDIRDEV: {
+		/* 
+		 * Give user-space a chance of determining if we are a redirect-device
+		 * or not.
+		 *  (If the IOCTL fails, we are not, otherwise we are.)
+		 */
+		struct redirdev* rdd;
+		req.ifname[IFNAMSIZ-1] = '\0';
+	    
+		spin_lock_irqsave(&rdd_cfg_lock, flags);
+		/* find the port in question */
+		rdd = rdd_find_dev_by_name(req.ifname);
+		spin_unlock_irqrestore(&rdd_cfg_lock, flags);
+
+		if (!rdd) {
+			/* printk("device: %s is NOT a REDIR device\n", ifname); */
+			err = -ENODEV;
+		}
+		else {
+			/* printk("device: %s IS a MAC-VLAN\n", ifname); */
+			err = 0;
+		}
+		break;
+	}
+	case REDIRDEV_GET_BY_IDX: {
+		/* 
+		 * get the nth redirdev name
+		 */
+		struct redirdev *rdd;
+		int n = req.ifidx;
+
+		spin_lock_irqsave(&rdd_cfg_lock, flags);
+		/* find the port in question */
+		for (rdd = rdds; rdd && n; rdd = rdd->next, n--);
+		if (!rdd) {
+			err = -ENODEV;
+			spin_unlock_irqrestore(&rdd_cfg_lock, flags);
+		}
+		else {
+			memcpy(req.ifname, rdd->dev->name, IFNAMSIZ);
+			memcpy(req.txifname, rdd->tx_dev_name, IFNAMSIZ);
+			if (rdd->tx_dev) {
+				req.flags |= RDD_ASSOCIATED;
+			}
+			else {
+				req.flags &= ~RDD_ASSOCIATED;
+			}
+			spin_unlock_irqrestore(&rdd_cfg_lock, flags);
+					
+			if (copy_to_user(arg, &req, sizeof(req))) {
+				err = -EFAULT;
+			}
+		}
+		break;
+	}
+	case REDIRDEV_GET_BY_NAME: {
+		/* 
+		 * get info on the specified redirect device
+		 */
+		struct redirdev *rdd;
+		req.ifname[IFNAMSIZ-1] = '\0';
+	    
+		spin_lock_irqsave(&rdd_cfg_lock, flags);
+		/* find the port in question */
+		rdd = rdd_find_dev_by_name(req.ifname);
+
+		if (!rdd) {
+			err = -ENODEV;
+			spin_unlock_irqrestore(&rdd_cfg_lock, flags);
+		}
+		else {
+			memcpy(req.ifname, rdd->dev->name, IFNAMSIZ);
+			memcpy(req.txifname, rdd->tx_dev_name, IFNAMSIZ);
+			if (rdd->tx_dev) {
+				req.flags |= RDD_ASSOCIATED;
+			}
+			else {
+				req.flags &= ~RDD_ASSOCIATED;
+			}
+			spin_unlock_irqrestore(&rdd_cfg_lock, flags);
+					
+			if (copy_to_user(arg, &req, sizeof(req))) {
+				err = -EFAULT;
+			}
+		}
+		break;
+	}
+	default:
+		printk("ERROR:  Un-supported redirdev ioctl command: %u\n",
+		       (unsigned int)(req.cmd));
+		send_sig(SIGSEGV, current, 1); // TODO:  Remove
+		err = -EOPNOTSUPP;
+		break;
+	}//switch
+
+	/* printk("Returning err: %i\n", err); */
+	return err;
+}/* ioctl handler */
+
+
+#ifdef RDD_CONFIG_PROC_FS
+
+static int read_rdd_glbl(char *page, char **start, off_t off,
+			 int count, int *eof, void *data) {
+	int	ret = -1;
+	char	*p = page;
+	int mx_len = (4096 - (p - page));
+	
+	if (! *eof ) {
+		struct redirdev* rdd;
+		int cnt;
+		unsigned long flags;
+		
+                /* Global counts here... */
+		p += sprintf(p, "Redirect-Device module:\n");
+
+		p += sprintf(p, " redirect-devices: %i\n",
+			     atomic_read(&rdd_dev_counter));
+
+		spin_lock_irqsave(&rdd_cfg_lock, flags);
+		rdd = rdds;
+		while (rdd) {
+			if (rdd->tx_dev) {
+				p += sprintf(p, " %s  tx-dev: %s\n",
+					     rdd->dev->name, rdd->tx_dev->name);
+			}
+			else {
+				p += sprintf(p, " %s  tx-dev: [%s]\n",
+					     rdd->dev->name, rdd->tx_dev_name);
+			}
+
+			/* catch overflow */
+			cnt = p - page;
+			if (cnt > (mx_len - 60)) {
+				if (mx_len - cnt >= 20) {
+					p += sprintf(p, "OUT_OF_SPACE!\n");
+				}
+				break;
+			}
+
+			rdd = rdd->next;
+		}
+
+		ret = p - page;
+		spin_unlock_irqrestore(&rdd_cfg_lock, flags);
+	}
+	return ret;
+} /* read_rdd_glbl */
+
+static int write_rdd_glbl(struct file *file, const char *buffer,
+			  unsigned long count, void *data) {
+	char		*p;
+	const char	*end;
+	int		ret=count;
+	int		len;
+	char		dev_name[2][IFNAMSIZ];
+        char* tmps = NULL;
+	int tmp_rv;
+        char ss[50];
+	end = buffer + count;
+
+	snprintf(ss, 50, "redir proc cmd: %%.%lus", count);
+
+	printk(ss, buffer);
+	
+	for (p= (char *) buffer; p< end ; ) {
+		if (iswhitespace(*p)) {
+			p++;
+			continue;
+		}
+
+		memset(dev_name[0], 0 ,IFNAMSIZ);
+		memset(dev_name[1], 0 ,IFNAMSIZ);
+
+		len = strlen("add_rdd ");
+		if (strncmp(p, "add_rdd ", len)==0)
+		{
+			p += len;
+
+			if ( (p + IFNAMSIZ) <= end)
+				p += copy_next_word(dev_name[0], p, IFNAMSIZ);
+			else
+				p += copy_next_word(dev_name[0], p, end-p );
+
+			skip_whitespace(p);
+
+			if ( (p + IFNAMSIZ) <= end)
+				p += copy_next_word(dev_name[1], p, IFNAMSIZ);
+			else
+				p += copy_next_word(dev_name[1], p, end-p );
+
+			skip_whitespace(p);
+
+                        /* This can fail, but not sure how to return failure
+                         * to user-space here.
+			 * NOTE:  Does it's own internal locking.
+                         */
+                        redirdev_create(dev_name[0], dev_name[1]);
+                        goto forend;
+		}
+                
+		len = strlen("remove_rdd ");
+		if (strncmp(p,"remove_rdd ", len)==0) {
+			p += len;
+
+			if ( (p + IFNAMSIZ) <= end)
+				p += copy_next_word(dev_name[0], p, IFNAMSIZ);
+			else
+				p += copy_next_word(dev_name[0], p, end-p );
+
+                        skip_whitespace(p);
+
+                        tmp_rv = redirdev_cleanup(dev_name[0], 0);
+			if (tmp_rv < 0) {
+				printk("redirdev: ERROR:  Failed redirdev_cleanup, error: %d\n", tmp_rv);
+			}
+			
+                        goto forend;
+		}
+
+                len = strlen("debug_lvl ");
+		if (strncmp(p,"debug_lvl ",len)==0)
+		{
+			p += len;
+
+			if ( (p + IFNAMSIZ) <= end)
+				p += copy_next_word(dev_name[0], p, IFNAMSIZ);
+			else
+				p += copy_next_word(dev_name[0], p, end-p );
+
+                        skip_whitespace(p);
+
+                        debug_lvl = simple_strtoul(dev_name[0], &tmps, 10);
+                        goto forend;
+		}
+
+                printk("ERROR:  Unsupported command\n");
+
+        forend:
+		p++;
+	}
+
+	return ret;
+} /* write_rdd_glbl */
+
+#endif
+
+
+static int __init redirdev_init(void) {
+	int err;
+	printk(KERN_INFO "Redirect-Network-Device: 1.0 <greearb@candelatech.com>\n");
+
+	rdds = NULL;
+
+	redirdev_ioctl_set(redirdev_ioctl_deviceless_stub);
+
+#ifdef RDD_CONFIG_PROC_FS
+
+        rdd_proc_dir = proc_mkdir(RDD_PROC_DIR, proc_net);
+        if (rdd_proc_dir) {
+		rdd_proc_cfg = create_proc_read_entry(RDD_PROC_CFG, S_IRUGO, rdd_proc_dir,
+						      read_rdd_glbl, NULL);
+		if (rdd_proc_cfg) {
+			rdd_proc_cfg->write_proc = write_rdd_glbl;
+			rdd_proc_cfg->owner = THIS_MODULE;
+		}
+	}
+#endif
+
+	/* Register us to receive netdevice events */
+	err = register_netdevice_notifier(&redirdev_notifier_block);
+	if (err < 0) {
+		printk("ERROR:  redirdev: Failed to register netdevice notifier callback!\n");
+	}	
+
+	return 0;
+}
+
+static void redirdev_module_cleanup(void) {
+	char nm[IFNAMSIZ+1];
+	unsigned long flags;
+	
+	redirdev_ioctl_set(NULL);
+
+	spin_lock_irqsave(&rdd_cfg_lock, flags);
+	/* destroy all redirect devices */
+	while (rdds) {
+		strncpy(nm, rdds->dev->name, IFNAMSIZ);
+		spin_unlock_irqrestore(&rdd_cfg_lock, flags);
+		if (redirdev_cleanup(nm, 1) < 0) {
+			printk("redirdev: ERROR:  Failed redir_cleanup in redir_module_cleanup\n");
+			
+		}
+		spin_lock_irqsave(&rdd_cfg_lock, flags);
+	}
+	spin_unlock_irqrestore(&rdd_cfg_lock, flags);
+
+	/* Un-register us from receiving netdevice events */
+	unregister_netdevice_notifier(&redirdev_notifier_block);
+	
+#ifdef RDD_CONFIG_PROC_FS
+	if (rdd_proc_cfg) {
+		remove_proc_entry(RDD_PROC_CFG, rdd_proc_dir);
+		rdd_proc_cfg = NULL;
+	}
+	if (rdd_proc_dir) {
+		remove_proc_entry(RDD_PROC_DIR, proc_net);
+		rdd_proc_dir = NULL;
+	}
+#endif
+
+}/* redirdev_cleanup */
+
+
+module_init(redirdev_init);
+module_exit(redirdev_module_cleanup);
+MODULE_LICENSE("GPL");
diff --git a/net/redir/redirdev.h b/net/redir/redirdev.h
new file mode 100644
index 0000000..4c36896
--- /dev/null
+++ b/net/redir/redirdev.h
@@ -0,0 +1,40 @@
+/* -*- linux-c -*-
+
+# (C) Copyright 2005
+# Ben Greear <greearb@candelatech.com>
+# Released under the GPL version 2
+*/
+
+#ifndef REDIRDEV_KERNEL_H_FILE__
+#define REDIRDEV_KERNEL_H_FILE__
+
+
+/* Proc file related */
+#define RDD_MX_ARG_LEN 80
+
+#ifdef CONFIG_PROC_FS
+
+/* To use or not to use the PROC-FS */
+#define RDD_CONFIG_PROC_FS
+
+#endif
+
+
+/*********************************************************/
+/*		       types				 */
+/*********************************************************/
+struct redirdev {
+	/* Can be NULL if not yet associated */
+	struct net_device* tx_dev; /* Call rx on this device when a packet
+				    * is _transmitted_ on this redirect
+				    * device.
+				    */
+	struct net_device* dev; /* the device struct this belongs too */
+	struct redirdev *next;
+	char tx_dev_name[IFNAMSIZ];
+	struct net_device_stats	statistics;
+	int wants_to_run; /* Should we be running if we can? */
+};
+
+#endif
+
diff --git a/net/socket.c b/net/socket.c
index 6d261bf..a564105 100644
--- a/net/socket.c
+++ b/net/socket.c
@@ -860,6 +860,30 @@ void vlan_ioctl_set(int (*hook)(void __u
 }
 EXPORT_SYMBOL(vlan_ioctl_set);
 
+static DEFINE_MUTEX(macvlan_ioctl_mutex);
+static int (*macvlan_ioctl_hook)(void __user*);
+
+void macvlan_ioctl_set(int (*hook)(void __user*))
+{
+	mutex_lock(&macvlan_ioctl_mutex);
+	macvlan_ioctl_hook = hook;
+	mutex_unlock(&macvlan_ioctl_mutex);
+}
+EXPORT_SYMBOL(macvlan_ioctl_set);
+
+
+static DEFINE_MUTEX(redirdev_ioctl_mutex);
+static int (*redirdev_ioctl_hook)(void __user*);
+
+void redirdev_ioctl_set(int (*hook)(void __user*))
+{
+	mutex_lock(&redirdev_ioctl_mutex);
+	redirdev_ioctl_hook = hook;
+	mutex_unlock(&redirdev_ioctl_mutex);
+}
+EXPORT_SYMBOL(redirdev_ioctl_set);
+
+
 static DEFINE_MUTEX(dlci_ioctl_mutex);
 static int (*dlci_ioctl_hook)(unsigned int, void __user *);
 
@@ -927,6 +951,28 @@ #endif	/* CONFIG_WIRELESS_EXT */
 				err = vlan_ioctl_hook(argp);
 			mutex_unlock(&vlan_ioctl_mutex);
 			break;
+		case SIOCGIFMACVLAN:
+		case SIOCSIFMACVLAN:
+			err = -ENOPKG;
+			if (!macvlan_ioctl_hook)
+				request_module("macvlan");
+
+			mutex_lock(&macvlan_ioctl_mutex);
+			if (macvlan_ioctl_hook)
+				err = macvlan_ioctl_hook(argp);
+			mutex_unlock(&macvlan_ioctl_mutex);
+			break;
+                case SIOCGIFREDIRDEV:
+                case SIOCSIFREDIRDEV:
+			err = -ENOPKG;
+			if (!redirdev_ioctl_hook)
+				request_module("redirdev");
+
+			mutex_lock(&redirdev_ioctl_mutex);
+			if (redirdev_ioctl_hook)
+				err = redirdev_ioctl_hook(argp);
+			mutex_unlock(&redirdev_ioctl_mutex);
+			break;
 		case SIOCGIFDIVERT:
 		case SIOCSIFDIVERT:
 		/* Convert this to call through a hook */
diff --git a/net/sunrpc/xprtsock.c b/net/sunrpc/xprtsock.c
index 441bd53..009ff27 100644
--- a/net/sunrpc/xprtsock.c
+++ b/net/sunrpc/xprtsock.c
@@ -984,6 +984,8 @@ static int xs_bindresvport(struct rpc_xp
 	int err;
 	unsigned short port = xprt->port;
 
+	myaddr.sin_addr.s_addr = xprt->local_address;
+	
 	do {
 		myaddr.sin_port = htons(port);
 		err = sock->ops->bind(sock, (struct sockaddr *) &myaddr,
